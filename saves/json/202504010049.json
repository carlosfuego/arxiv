[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Güra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Güra"
                },
                "author": "Fabian Güra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v4",
                "updated": "2025-03-26T17:42:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    42,
                    17,
                    2,
                    85,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "José-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio González"
                    }
                ],
                "author_detail": {
                    "name": "Antonio González"
                },
                "author": "Antonio González",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v1",
                "updated": "2025-03-26T04:16:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: A Comprehensive Framework for Accelerating Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: A Comprehensive Framework for Accelerating Vision-Language\n  Models"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19950v1",
                "updated": "2025-03-25T16:24:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:24:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation"
                },
                "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."
                },
                "authors": [
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "arxiv_comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ramé"
                    },
                    {
                        "name": "Morgane Rivière"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "Gaël Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "André Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-Plucińska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim Põder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Léonard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "Léonard Hussenot"
                },
                "author": "Léonard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v2",
                "updated": "2025-03-24T18:16:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    16,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v1",
                "updated": "2025-03-24T04:32:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18265v1",
                "updated": "2025-03-24T01:15:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:15:43Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "title": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence"
                },
                "summary": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems."
                },
                "authors": [
                    {
                        "name": "Akaash Vishal Hazarika"
                    },
                    {
                        "name": "Mahak Shah"
                    },
                    {
                        "name": "Swapnil Patil"
                    },
                    {
                        "name": "Pradyumna Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Pradyumna Shukla"
                },
                "author": "Pradyumna Shukla",
                "arxiv_comment": "International Conference on AI and Financial Innovation AIFI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v1",
                "updated": "2025-03-23T20:18:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18030v1",
                "updated": "2025-03-23T11:07:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T11:07:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Formal Verification of Parameterized Systems based on Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Parameterized Systems based on Induction"
                },
                "summary": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiu"
                    },
                    {
                        "name": "Yongjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Li"
                },
                "author": "Yongjian Li",
                "arxiv_comment": "9 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10425v2",
                "updated": "2025-03-23T06:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    6,
                    14,
                    35,
                    6,
                    82,
                    0
                ],
                "published": "2023-12-16T11:40:49Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    40,
                    49,
                    5,
                    350,
                    0
                ],
                "title": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaorui Jiang"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Hengwei Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17913v1",
                "updated": "2025-03-23T03:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks"
                },
                "summary": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%."
                },
                "authors": [
                    {
                        "name": "Shuo Yuan"
                    },
                    {
                        "name": "Yaohua Sun"
                    },
                    {
                        "name": "Mugen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Mugen Peng"
                },
                "author": "Mugen Peng",
                "arxiv_doi": "10.1109/TVT.2024.3463548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3463548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Vehicular Technology",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v1",
                "updated": "2025-03-23T03:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "16 pages, the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17895v1",
                "updated": "2025-03-23T01:17:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T01:17:08Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO"
                },
                "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Shane M. W. Witsell"
                    },
                    {
                        "name": "John F. Conley"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17603v1",
                "updated": "2025-03-22T01:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:17:56Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "title": "A Generative Caching System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Caching System for Large Language Models"
                },
                "summary": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
                },
                "authors": [
                    {
                        "name": "Arun Iyengar"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Sai Nandan Mamidi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Nandan Mamidi"
                },
                "author": "Sai Nandan Mamidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17602v1",
                "updated": "2025-03-22T01:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:16:24Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "title": "Multiport Support for Vortex OpenGPU Memory Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiport Support for Vortex OpenGPU Memory Hierarchy"
                },
                "summary": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead."
                },
                "authors": [
                    {
                        "name": "Injae Shin"
                    },
                    {
                        "name": "Blaise Tine"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Tine"
                },
                "author": "Blaise Tine",
                "arxiv_comment": "OSSMPIC2025, 1st workshop on Open Source Solutions for Massively\n  Parallel Integrated Circuits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v2",
                "updated": "2025-03-21T21:10:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    10,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v2",
                "updated": "2025-03-21T19:26:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    26,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v3",
                "updated": "2025-03-20T21:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v1",
                "updated": "2025-03-20T17:37:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v1",
                "updated": "2025-03-20T13:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Zongming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zongming Guo"
                },
                "author": "Zongming Guo",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15927v1",
                "updated": "2025-03-20T08:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers"
                },
                "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."
                },
                "authors": [
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tingwei Gao"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18921v2",
                "updated": "2025-03-20T05:23:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    5,
                    23,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-09T13:47:05Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    47,
                    5,
                    1,
                    191,
                    0
                ],
                "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey"
                },
                "summary": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14881v1",
                "updated": "2025-03-19T04:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T04:18:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers"
                },
                "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."
                },
                "authors": [
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yekun Ke"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Song"
                },
                "author": "Zhao Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14805v1",
                "updated": "2025-03-19T00:30:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T00:30:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C"
                },
                "summary": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C."
                },
                "authors": [
                    {
                        "name": "Hunter Ellis"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Imteaz Rahaman"
                    },
                    {
                        "name": "Apostoli Hillas"
                    },
                    {
                        "name": "Botong Li"
                    },
                    {
                        "name": "Michael A. Scarpulla"
                    },
                    {
                        "name": "Berardi Sensale Rodriguez"
                    },
                    {
                        "name": "Kai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fu"
                },
                "author": "Kai Fu",
                "arxiv_comment": "7 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14708v1",
                "updated": "2025-03-18T20:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T20:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16"
                },
                "summary": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama."
                },
                "authors": [
                    {
                        "name": "Viansa Schmulbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Ethan Gao"
                    },
                    {
                        "name": "Lucy Revina"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Ethan Wu"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    }
                ],
                "author_detail": {
                    "name": "Borivoje Nikolic"
                },
                "author": "Borivoje Nikolic",
                "arxiv_doi": "10.1109/HCS61935.2024.10665203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HCS61935.2024.10665203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14647v1",
                "updated": "2025-03-18T18:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T18:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache"
                },
                "summary": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13679v1",
                "updated": "2025-03-17T19:32:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T19:32:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning"
                },
                "summary": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis."
                },
                "authors": [
                    {
                        "name": "Risheng Xu"
                    },
                    {
                        "name": "Philipp Sieweck"
                    },
                    {
                        "name": "Hermann von Hasseln"
                    },
                    {
                        "name": "Dirk Nowotka"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Nowotka"
                },
                "author": "Dirk Nowotka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16525v1",
                "updated": "2025-03-17T16:43:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference"
                },
                "summary": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Renji Zhang"
                    },
                    {
                        "name": "Deyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhang"
                },
                "author": "Deyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12991v1",
                "updated": "2025-03-17T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge"
                },
                "summary": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Sam Albin"
                    },
                    {
                        "name": "Garhan Attebury"
                    },
                    {
                        "name": "Kenneth Bloom"
                    },
                    {
                        "name": "Brian Paul Bockelman"
                    },
                    {
                        "name": "Benjamin Tovar Lopez"
                    },
                    {
                        "name": "Carl Lundstedt"
                    },
                    {
                        "name": "Oksana Shadura"
                    },
                    {
                        "name": "John Thiltges"
                    },
                    {
                        "name": "Derek Weitzel"
                    },
                    {
                        "name": "Andrew Wightman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Wightman"
                },
                "arxiv_affiliation": "University of Nebraska-Lincoln",
                "author": "Andrew Wightman",
                "arxiv_comment": "Draft submitted to EPJ journal (CHEP 2024 conference proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v2",
                "updated": "2025-03-17T03:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    3,
                    30,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12491v1",
                "updated": "2025-03-16T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12450v1",
                "updated": "2025-03-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T10:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching"
                },
                "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."
                },
                "authors": [
                    {
                        "name": "Feihong Yan"
                    },
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v1",
                "updated": "2025-03-15T02:48:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11946v1",
                "updated": "2025-03-15T01:35:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T01:35:53Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks"
                },
                "summary": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zhishu Shen"
                    },
                    {
                        "name": "Dawen Jiang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qiushi Zheng"
                    },
                    {
                        "name": "Jiong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Jiong Jin"
                },
                "author": "Jiong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06348v2",
                "updated": "2025-03-15T00:49:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    0,
                    49,
                    55,
                    5,
                    74,
                    0
                ],
                "published": "2024-03-11T00:30:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    30,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation"
                },
                "summary": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs."
                },
                "authors": [
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Ahmed E. Helal"
                    },
                    {
                        "name": "S. Isaac Geronimo Anderson"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Yongseok Soh"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Teresa Ranadive"
                    },
                    {
                        "name": "Brian J Gravelle"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Jee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jee Choi"
                },
                "author": "Jee Choi",
                "arxiv_comment": "Accepted to TPDS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v1",
                "updated": "2025-03-14T19:02:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11750v1",
                "updated": "2025-03-14T17:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization"
                },
                "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."
                },
                "authors": [
                    {
                        "name": "Shuyang Hao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.22678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22678v1",
                "updated": "2025-03-28T17:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    59,
                    53,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T17:59:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    59,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions"
                },
                "summary": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}."
                },
                "authors": [
                    {
                        "name": "Mohammad Almansoori"
                    },
                    {
                        "name": "Komal Kumar"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "arxiv_comment": "14 page, 4 figures, 61 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22675v1",
                "updated": "2025-03-28T17:59:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    59,
                    3,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T17:59:03Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    59,
                    3,
                    4,
                    87,
                    0
                ],
                "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation"
                },
                "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation."
                },
                "authors": [
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Wu Jian"
                    },
                    {
                        "name": "Yuning Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yuning Jiang"
                },
                "author": "Yuning Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22674v1",
                "updated": "2025-03-28T17:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    58,
                    40,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T17:58:40Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    58,
                    40,
                    4,
                    87,
                    0
                ],
                "title": "QuestBench: Can LLMs ask the right question to acquire information in\n  reasoning tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuestBench: Can LLMs ask the right question to acquire information in\n  reasoning tasks?"
                },
                "summary": "Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities."
                },
                "authors": [
                    {
                        "name": "Belinda Z. Li"
                    },
                    {
                        "name": "Been Kim"
                    },
                    {
                        "name": "Zi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zi Wang"
                },
                "author": "Zi Wang",
                "arxiv_comment": "Code and dataset are available at\n  \\url{https://github.com/google-deepmind/questbench}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22655v1",
                "updated": "2025-03-28T17:43:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    43,
                    0,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T17:43:00Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    43,
                    0,
                    4,
                    87,
                    0
                ],
                "title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training"
                },
                "summary": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git."
                },
                "authors": [
                    {
                        "name": "Xiaomin Yu"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Kejian Wu"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13360v3",
                "updated": "2025-03-28T17:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    28,
                    44,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-17T09:10:26Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    9,
                    10,
                    26,
                    3,
                    291,
                    0
                ],
                "title": "RAP: Retrieval-Augmented Personalization for Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: Retrieval-Augmented Personalization for Multimodal Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://hoar012.github.io/RAP-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://hoar012.github.io/RAP-Project/."
                },
                "authors": [
                    {
                        "name": "Haoran Hao"
                    },
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Changsheng Li"
                    },
                    {
                        "name": "Yu-Feng Li"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Accepted by CVPR 2025. Code: https://github.com/Hoar012/RAP-MLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13222v2",
                "updated": "2025-03-28T17:17:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    17,
                    40,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-17T14:31:37Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    31,
                    37,
                    0,
                    76,
                    0
                ],
                "title": "Can Language Models Follow Multiple Turns of Entangled Instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Follow Multiple Turns of Entangled Instructions?"
                },
                "summary": "Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions."
                },
                "authors": [
                    {
                        "name": "Chi Han"
                    }
                ],
                "author_detail": {
                    "name": "Chi Han"
                },
                "author": "Chi Han",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07877v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07877v3",
                "updated": "2025-03-28T17:14:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    14,
                    39,
                    4,
                    87,
                    0
                ],
                "published": "2024-02-12T18:41:55Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    18,
                    41,
                    55,
                    0,
                    43,
                    0
                ],
                "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and\n  Adaptation"
                },
                "summary": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work, we propose a Retrieval-Augmented Generation\n(RAG)-based multi-agent LLM system to support analysis and decision-making in\nthe context of natural hazards and extreme weather events. As a proof of\nconcept, we present WildfireGPT, a specialized system focused on wildfire\nscenarios. The architecture employs a user-centered, multi-agent design to\ndeliver tailored risk insights across diverse stakeholder groups. By\nintegrating domain-specific projection data, observational datasets, and\nscientific literature through a RAG framework, the system ensures both accuracy\nand contextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support in natural hazard and extreme\nweather contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work, we propose a Retrieval-Augmented Generation\n(RAG)-based multi-agent LLM system to support analysis and decision-making in\nthe context of natural hazards and extreme weather events. As a proof of\nconcept, we present WildfireGPT, a specialized system focused on wildfire\nscenarios. The architecture employs a user-centered, multi-agent design to\ndeliver tailored risk insights across diverse stakeholder groups. By\nintegrating domain-specific projection data, observational datasets, and\nscientific literature through a RAG framework, the system ensures both accuracy\nand contextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support in natural hazard and extreme\nweather contexts."
                },
                "authors": [
                    {
                        "name": "Yangxinyu Xie"
                    },
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Tanwi Mallick"
                    },
                    {
                        "name": "Joshua David Bergerson"
                    },
                    {
                        "name": "John K. Hutchison"
                    },
                    {
                        "name": "Duane R. Verner"
                    },
                    {
                        "name": "Jordan Branham"
                    },
                    {
                        "name": "M. Ross Alexander"
                    },
                    {
                        "name": "Robert B. Ross"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Leslie-Anne Levy"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Camillo J. Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Camillo J. Taylor"
                },
                "author": "Camillo J. Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07877v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07877v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22616v1",
                "updated": "2025-03-28T16:58:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    58,
                    27,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:58:27Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    58,
                    27,
                    4,
                    87,
                    0
                ],
                "title": "A Unified Approach for Estimating Various Treatment Effects in Causal\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Approach for Estimating Various Treatment Effects in Causal\n  Inference"
                },
                "summary": "In this paper, we introduce a unified estimator to analyze various treatment\neffects in causal inference, including but not limited to the average treatment\neffect (ATE) and the quantile treatment effect (QTE). The proposed estimator is\ndeveloped under the statistical functional and cumulative distribution function\nstructure, which leads to a flexible and robust estimator and covers some\nfrequent treatment effects. In addition, our approach also takes variable\nselection into account, so that informative and network structure in\nconfounders can be identified and be implemented in our estimation procedure.\nThe theoretical properties, including variable selection consistency and\nasymptotic normality of the statistical functional estimator, are established.\nVarious treatment effects estimations are also conducted in numerical studies,\nand the results reveal that the proposed estimator generally outperforms the\nexisting methods and is more efficient than its competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a unified estimator to analyze various treatment\neffects in causal inference, including but not limited to the average treatment\neffect (ATE) and the quantile treatment effect (QTE). The proposed estimator is\ndeveloped under the statistical functional and cumulative distribution function\nstructure, which leads to a flexible and robust estimator and covers some\nfrequent treatment effects. In addition, our approach also takes variable\nselection into account, so that informative and network structure in\nconfounders can be identified and be implemented in our estimation procedure.\nThe theoretical properties, including variable selection consistency and\nasymptotic normality of the statistical functional estimator, are established.\nVarious treatment effects estimations are also conducted in numerical studies,\nand the results reveal that the proposed estimator generally outperforms the\nexisting methods and is more efficient than its competitors."
                },
                "authors": [
                    {
                        "name": "Kuan-Hsun Wu"
                    },
                    {
                        "name": "Li-Pang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Li-Pang Chen"
                },
                "author": "Li-Pang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22600v1",
                "updated": "2025-03-28T16:44:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    44,
                    28,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:44:28Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    44,
                    28,
                    4,
                    87,
                    0
                ],
                "title": "Generative Latent Neural PDE Solver using Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Latent Neural PDE Solver using Flow Matching"
                },
                "summary": "Autoregressive next-step prediction models have become the de-facto standard\nfor building data-driven neural solvers to forecast time-dependent partial\ndifferential equations (PDEs). Denoise training that is closely related to\ndiffusion probabilistic model has been shown to enhance the temporal stability\nof neural solvers, while its stochastic inference mechanism enables ensemble\npredictions and uncertainty quantification. In principle, such training\ninvolves sampling a series of discretized diffusion timesteps during both\ntraining and inference, inevitably increasing computational overhead. In\naddition, most diffusion models apply isotropic Gaussian noise on structured,\nuniform grids, limiting their adaptability to irregular domains. We propose a\nlatent diffusion model for PDE simulation that embeds the PDE state in a\nlower-dimensional latent space, which significantly reduces computational\ncosts. Our framework uses an autoencoder to map different types of meshes onto\na unified structured latent grid, capturing complex geometries. By analyzing\ncommon diffusion paths, we propose to use a coarsely sampled noise schedule\nfrom flow matching for both training and testing. Numerical experiments show\nthat the proposed model outperforms several deterministic baselines in both\naccuracy and long-term stability, highlighting the potential of diffusion-based\napproaches for robust data-driven PDE learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next-step prediction models have become the de-facto standard\nfor building data-driven neural solvers to forecast time-dependent partial\ndifferential equations (PDEs). Denoise training that is closely related to\ndiffusion probabilistic model has been shown to enhance the temporal stability\nof neural solvers, while its stochastic inference mechanism enables ensemble\npredictions and uncertainty quantification. In principle, such training\ninvolves sampling a series of discretized diffusion timesteps during both\ntraining and inference, inevitably increasing computational overhead. In\naddition, most diffusion models apply isotropic Gaussian noise on structured,\nuniform grids, limiting their adaptability to irregular domains. We propose a\nlatent diffusion model for PDE simulation that embeds the PDE state in a\nlower-dimensional latent space, which significantly reduces computational\ncosts. Our framework uses an autoencoder to map different types of meshes onto\na unified structured latent grid, capturing complex geometries. By analyzing\ncommon diffusion paths, we propose to use a coarsely sampled noise schedule\nfrom flow matching for both training and testing. Numerical experiments show\nthat the proposed model outperforms several deterministic baselines in both\naccuracy and long-term stability, highlighting the potential of diffusion-based\napproaches for robust data-driven PDE learning."
                },
                "authors": [
                    {
                        "name": "Zijie Li"
                    },
                    {
                        "name": "Anthony Zhou"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20679v2",
                "updated": "2025-03-28T16:40:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    40,
                    45,
                    4,
                    87,
                    0
                ],
                "published": "2024-09-25T14:37:49Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    37,
                    49,
                    2,
                    269,
                    0
                ],
                "title": "MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and\n  Improved GRU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and\n  Improved GRU"
                },
                "summary": "As financial markets grow increasingly complex in the big data era, accurate\nstock prediction has become more critical. Traditional time series models, such\nas GRUs, have been widely used but often struggle to capture the intricate\nnonlinear dynamics of markets, particularly in the flexible selection and\neffective utilization of key historical information. Recently, methods like\nGraph Neural Networks and Reinforcement Learning have shown promise in stock\nprediction but require high data quality and quantity, and they tend to exhibit\ninstability when dealing with data sparsity and noise. Moreover, the training\nand inference processes for these models are typically complex and\ncomputationally expensive, limiting their broad deployment in practical\napplications. Existing approaches also generally struggle to capture\nunobservable latent market states effectively, such as market sentiment and\nexpectations, microstructural factors, and participant behavior patterns,\nleading to an inadequate understanding of market dynamics and subsequently\nimpact prediction accuracy. To address these challenges, this paper proposes a\nstock prediction model, MCI-GRU, based on a multi-head cross-attention\nmechanism and an improved GRU. First, we enhance the GRU model by replacing the\nreset gate with an attention mechanism, thereby increasing the model's\nflexibility in selecting and utilizing historical information. Second, we\ndesign a multi-head cross-attention mechanism for learning unobservable latent\nmarket state representations, which are further enriched through interactions\nwith both temporal features and cross-sectional features. Finally, extensive\nexperiments on four main stock markets show that the proposed method\noutperforms SOTA techniques across multiple metrics. Additionally, its\nsuccessful application in real-world fund management operations confirms its\neffectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As financial markets grow increasingly complex in the big data era, accurate\nstock prediction has become more critical. Traditional time series models, such\nas GRUs, have been widely used but often struggle to capture the intricate\nnonlinear dynamics of markets, particularly in the flexible selection and\neffective utilization of key historical information. Recently, methods like\nGraph Neural Networks and Reinforcement Learning have shown promise in stock\nprediction but require high data quality and quantity, and they tend to exhibit\ninstability when dealing with data sparsity and noise. Moreover, the training\nand inference processes for these models are typically complex and\ncomputationally expensive, limiting their broad deployment in practical\napplications. Existing approaches also generally struggle to capture\nunobservable latent market states effectively, such as market sentiment and\nexpectations, microstructural factors, and participant behavior patterns,\nleading to an inadequate understanding of market dynamics and subsequently\nimpact prediction accuracy. To address these challenges, this paper proposes a\nstock prediction model, MCI-GRU, based on a multi-head cross-attention\nmechanism and an improved GRU. First, we enhance the GRU model by replacing the\nreset gate with an attention mechanism, thereby increasing the model's\nflexibility in selecting and utilizing historical information. Second, we\ndesign a multi-head cross-attention mechanism for learning unobservable latent\nmarket state representations, which are further enriched through interactions\nwith both temporal features and cross-sectional features. Finally, extensive\nexperiments on four main stock markets show that the proposed method\noutperforms SOTA techniques across multiple metrics. Additionally, its\nsuccessful application in real-world fund management operations confirms its\neffectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Peng Zhu"
                    },
                    {
                        "name": "Yuante Li"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Sheng Xiang"
                    },
                    {
                        "name": "Qinyuan Liu"
                    },
                    {
                        "name": "Dawei Cheng"
                    },
                    {
                        "name": "Yuqi Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Liang"
                },
                "author": "Yuqi Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22589v1",
                "updated": "2025-03-28T16:36:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    36,
                    23,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:36:23Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    36,
                    23,
                    4,
                    87,
                    0
                ],
                "title": "Using AI to Summarize US Presidential Campaign TV Advertisement Videos,\n  1952-2012",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using AI to Summarize US Presidential Campaign TV Advertisement Videos,\n  1952-2012"
                },
                "summary": "This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets."
                },
                "authors": [
                    {
                        "name": "Adam Breuer"
                    },
                    {
                        "name": "Bryce J. Dietrich"
                    },
                    {
                        "name": "Michael H. Crespin"
                    },
                    {
                        "name": "Matthew Butler"
                    },
                    {
                        "name": "J. A. Pyrse"
                    },
                    {
                        "name": "Kosuke Imai"
                    }
                ],
                "author_detail": {
                    "name": "Kosuke Imai"
                },
                "author": "Kosuke Imai",
                "arxiv_comment": "17 pages, 7 tables, 4 figures, and linked datasets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22587v1",
                "updated": "2025-03-28T16:34:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    34,
                    29,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:34:29Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    34,
                    29,
                    4,
                    87,
                    0
                ],
                "title": "LLM-enabled Instance Model Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enabled Instance Model Generation"
                },
                "summary": "In the domain of model-based engineering, models are essential components\nthat enable system design and analysis. Traditionally, the creation of these\nmodels has been a manual process requiring not only deep modeling expertise but\nalso substantial domain knowledge of target systems. With the rapid advancement\nof generative artificial intelligence, large language models (LLMs) show\npotential for automating model generation. This work explores the generation of\ninstance models using LLMs, focusing specifically on producing XMI-based\ninstance models from Ecore metamodels and natural language specifications. We\nobserve that current LLMs struggle to directly generate valid XMI models. To\naddress this, we propose a two-step approach: first, using LLMs to produce a\nsimplified structured output containing all necessary instance model\ninformation, namely a conceptual instance model, and then compiling this\nintermediate representation into a valid XMI file. The conceptual instance\nmodel is format-independent, allowing it to be transformed into various\nmodeling formats via different compilers. The feasibility of the proposed\nmethod has been demonstrated using several LLMs, including GPT-4o, o1-preview,\nLlama 3.1 (8B and 70B). Results show that the proposed method significantly\nimproves the usability of LLMs for instance model generation tasks. Notably,\nthe smaller open-source model, Llama 3.1 70B, demonstrated performance\ncomparable to proprietary GPT models within the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of model-based engineering, models are essential components\nthat enable system design and analysis. Traditionally, the creation of these\nmodels has been a manual process requiring not only deep modeling expertise but\nalso substantial domain knowledge of target systems. With the rapid advancement\nof generative artificial intelligence, large language models (LLMs) show\npotential for automating model generation. This work explores the generation of\ninstance models using LLMs, focusing specifically on producing XMI-based\ninstance models from Ecore metamodels and natural language specifications. We\nobserve that current LLMs struggle to directly generate valid XMI models. To\naddress this, we propose a two-step approach: first, using LLMs to produce a\nsimplified structured output containing all necessary instance model\ninformation, namely a conceptual instance model, and then compiling this\nintermediate representation into a valid XMI file. The conceptual instance\nmodel is format-independent, allowing it to be transformed into various\nmodeling formats via different compilers. The feasibility of the proposed\nmethod has been demonstrated using several LLMs, including GPT-4o, o1-preview,\nLlama 3.1 (8B and 70B). Results show that the proposed method significantly\nimproves the usability of LLMs for instance model generation tasks. Notably,\nthe smaller open-source model, Llama 3.1 70B, demonstrated performance\ncomparable to proprietary GPT models within the proposed framework."
                },
                "authors": [
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Long Wen"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22585v1",
                "updated": "2025-03-28T16:33:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    33,
                    24,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:33:24Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    33,
                    24,
                    4,
                    87,
                    0
                ],
                "title": "Historical Ink: Exploring Large Language Models for Irony Detection in\n  19th-Century Spanish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historical Ink: Exploring Large Language Models for Irony Detection in\n  19th-Century Spanish"
                },
                "summary": "This study explores the use of large language models (LLMs) to enhance\ndatasets and improve irony detection in 19th-century Latin American newspapers.\nTwo strategies were employed to evaluate the efficacy of BERT and GPT-4o models\nin capturing the subtle nuances nature of irony, through both multi-class and\nbinary classification tasks. First, we implemented dataset enhancements focused\non enriching emotional and contextual cues; however, these showed limited\nimpact on historical language analysis. The second strategy, a semi-automated\nannotation process, effectively addressed class imbalance and augmented the\ndataset with high-quality annotations. Despite the challenges posed by the\ncomplexity of irony, this work contributes to the advancement of sentiment\nanalysis through two key contributions: introducing a new historical Spanish\ndataset tagged for sentiment analysis and irony detection, and proposing a\nsemi-automated annotation methodology where human expertise is crucial for\nrefining LLMs results, enriched by incorporating historical and cultural\ncontexts as core features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of large language models (LLMs) to enhance\ndatasets and improve irony detection in 19th-century Latin American newspapers.\nTwo strategies were employed to evaluate the efficacy of BERT and GPT-4o models\nin capturing the subtle nuances nature of irony, through both multi-class and\nbinary classification tasks. First, we implemented dataset enhancements focused\non enriching emotional and contextual cues; however, these showed limited\nimpact on historical language analysis. The second strategy, a semi-automated\nannotation process, effectively addressed class imbalance and augmented the\ndataset with high-quality annotations. Despite the challenges posed by the\ncomplexity of irony, this work contributes to the advancement of sentiment\nanalysis through two key contributions: introducing a new historical Spanish\ndataset tagged for sentiment analysis and irony detection, and proposing a\nsemi-automated annotation methodology where human expertise is crucial for\nrefining LLMs results, enriched by incorporating historical and cultural\ncontexts as core features."
                },
                "authors": [
                    {
                        "name": "Kevin Cohen"
                    },
                    {
                        "name": "Laura Manrique-Gómez"
                    },
                    {
                        "name": "Rubén Manrique"
                    }
                ],
                "author_detail": {
                    "name": "Rubén Manrique"
                },
                "author": "Rubén Manrique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22576v1",
                "updated": "2025-03-28T16:25:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    25,
                    24,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:25:24Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    25,
                    24,
                    4,
                    87,
                    0
                ],
                "title": "Drop the Golden Apples: Identifying Third-Party Reuse by DB-Less\n  Software Composition Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drop the Golden Apples: Identifying Third-Party Reuse by DB-Less\n  Software Composition Analysis"
                },
                "summary": "The prevalent use of third-party libraries (TPLs) in modern software\ndevelopment introduces significant security and compliance risks, necessitating\nthe implementation of Software Composition Analysis (SCA) to manage these\nthreats. However, the accuracy of SCA tools heavily relies on the quality of\nthe integrated feature database to cross-reference with user projects. While\nunder the circumstance of the exponentially growing of open-source ecosystems\nand the integration of large models into software development, it becomes even\nmore challenging to maintain a comprehensive feature database for potential\nTPLs. To this end, after referring to the evolution of LLM applications in\nterms of external data interactions, we propose the first framework of DB-Less\nSCA, to get rid of the traditional heavy database and embrace the flexibility\nof LLMs to mimic the manual analysis of security analysts to retrieve identical\nevidence and confirm the identity of TPLs by supportive information from the\nopen Internet. Our experiments on two typical scenarios, native library\nidentification for Android and copy-based TPL reuse for C/C++, especially on\nartifacts that are not that underappreciated, have demonstrated the favorable\nfuture for implementing database-less strategies in SCA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevalent use of third-party libraries (TPLs) in modern software\ndevelopment introduces significant security and compliance risks, necessitating\nthe implementation of Software Composition Analysis (SCA) to manage these\nthreats. However, the accuracy of SCA tools heavily relies on the quality of\nthe integrated feature database to cross-reference with user projects. While\nunder the circumstance of the exponentially growing of open-source ecosystems\nand the integration of large models into software development, it becomes even\nmore challenging to maintain a comprehensive feature database for potential\nTPLs. To this end, after referring to the evolution of LLM applications in\nterms of external data interactions, we propose the first framework of DB-Less\nSCA, to get rid of the traditional heavy database and embrace the flexibility\nof LLMs to mimic the manual analysis of security analysts to retrieve identical\nevidence and confirm the identity of TPLs by supportive information from the\nopen Internet. Our experiments on two typical scenarios, native library\nidentification for Android and copy-based TPL reuse for C/C++, especially on\nartifacts that are not that underappreciated, have demonstrated the favorable\nfuture for implementing database-less strategies in SCA."
                },
                "authors": [
                    {
                        "name": "Lyuye Zhang"
                    },
                    {
                        "name": "Chengwei Liu"
                    },
                    {
                        "name": "Jiahui Wu"
                    },
                    {
                        "name": "Shiyang Zhang"
                    },
                    {
                        "name": "Chengyue Liu"
                    },
                    {
                        "name": "Zhengzi Xu"
                    },
                    {
                        "name": "Sen Chen"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "6 pages, 1 figure, FSE25-IVR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22573v1",
                "updated": "2025-03-28T16:20:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    20,
                    57,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:20:57Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    20,
                    57,
                    4,
                    87,
                    0
                ],
                "title": "A Framework for Cryptographic Verifiability of End-to-End AI Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Cryptographic Verifiability of End-to-End AI Pipelines"
                },
                "summary": "The increasing integration of Artificial Intelligence across multiple\nindustry sectors necessitates robust mechanisms for ensuring transparency,\ntrust, and auditability of its development and deployment. This topic is\nparticularly important in light of recent calls in various jurisdictions to\nintroduce regulation and legislation on AI safety. In this paper, we propose a\nframework for complete verifiable AI pipelines, identifying key components and\nanalyzing existing cryptographic approaches that contribute to verifiability\nacross different stages of the AI lifecycle, from data sourcing to training,\ninference, and unlearning. This framework could be used to combat\nmisinformation by providing cryptographic proofs alongside AI-generated assets\nto allow downstream verification of their provenance and correctness. Our\nfindings underscore the importance of ongoing research to develop cryptographic\ntools that are not only efficient for isolated AI processes, but that are\nefficiently `linkable' across different processes within the AI pipeline, to\nsupport the development of end-to-end verifiable AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of Artificial Intelligence across multiple\nindustry sectors necessitates robust mechanisms for ensuring transparency,\ntrust, and auditability of its development and deployment. This topic is\nparticularly important in light of recent calls in various jurisdictions to\nintroduce regulation and legislation on AI safety. In this paper, we propose a\nframework for complete verifiable AI pipelines, identifying key components and\nanalyzing existing cryptographic approaches that contribute to verifiability\nacross different stages of the AI lifecycle, from data sourcing to training,\ninference, and unlearning. This framework could be used to combat\nmisinformation by providing cryptographic proofs alongside AI-generated assets\nto allow downstream verification of their provenance and correctness. Our\nfindings underscore the importance of ongoing research to develop cryptographic\ntools that are not only efficient for isolated AI processes, but that are\nefficiently `linkable' across different processes within the AI pipeline, to\nsupport the development of end-to-end verifiable AI technologies."
                },
                "authors": [
                    {
                        "name": "Kar Balan"
                    },
                    {
                        "name": "Robert Learney"
                    },
                    {
                        "name": "Tim Wood"
                    }
                ],
                "author_detail": {
                    "name": "Tim Wood"
                },
                "author": "Tim Wood",
                "arxiv_comment": "Accepted to 11th ACM International Workshop on Security and Privacy\n  Analytics (IWSPA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22567v1",
                "updated": "2025-03-28T16:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    14,
                    6,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    14,
                    6,
                    4,
                    87,
                    0
                ],
                "title": "Benchmarking Ultra-Low-Power $μ$NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Ultra-Low-Power $μ$NPUs"
                },
                "summary": "Efficient on-device neural network (NN) inference has various advantages over\ncloud-based processing, including predictable latency, enhanced privacy,\ngreater reliability, and reduced operating costs for vendors. This has sparked\nthe recent rapid development of microcontroller-scale NN accelerators, often\nreferred to as neural processing units ($\\mu$NPUs), designed specifically for\nultra-low-power applications.\n  In this paper we present the first comparative evaluation of a number of\ncommercially-available $\\mu$NPUs, as well as the first independent benchmarks\nfor several of these platforms. We develop and open-source a model compilation\nframework to enable consistent benchmarking of quantized models across diverse\n$\\mu$NPU hardware. Our benchmark targets end-to-end performance and includes\nmodel inference latency, power consumption, and memory overhead, alongside\nother factors. The resulting analysis uncovers both expected performance trends\nas well as surprising disparities between hardware specifications and actual\nperformance, including $\\mu$NPUs exhibiting unexpected scaling behaviors with\nincreasing model complexity. Our framework provides a foundation for further\nevaluation of $\\mu$NPU platforms alongside valuable insights for both hardware\ndesigners and software developers in this rapidly evolving space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient on-device neural network (NN) inference has various advantages over\ncloud-based processing, including predictable latency, enhanced privacy,\ngreater reliability, and reduced operating costs for vendors. This has sparked\nthe recent rapid development of microcontroller-scale NN accelerators, often\nreferred to as neural processing units ($\\mu$NPUs), designed specifically for\nultra-low-power applications.\n  In this paper we present the first comparative evaluation of a number of\ncommercially-available $\\mu$NPUs, as well as the first independent benchmarks\nfor several of these platforms. We develop and open-source a model compilation\nframework to enable consistent benchmarking of quantized models across diverse\n$\\mu$NPU hardware. Our benchmark targets end-to-end performance and includes\nmodel inference latency, power consumption, and memory overhead, alongside\nother factors. The resulting analysis uncovers both expected performance trends\nas well as surprising disparities between hardware specifications and actual\nperformance, including $\\mu$NPUs exhibiting unexpected scaling behaviors with\nincreasing model complexity. Our framework provides a foundation for further\nevaluation of $\\mu$NPU platforms alongside valuable insights for both hardware\ndesigners and software developers in this rapidly evolving space."
                },
                "authors": [
                    {
                        "name": "Josh Millar"
                    },
                    {
                        "name": "Yushan Huang"
                    },
                    {
                        "name": "Sarab Sethi"
                    },
                    {
                        "name": "Hamed Haddadi"
                    },
                    {
                        "name": "Anil Madhavapeddy"
                    }
                ],
                "author_detail": {
                    "name": "Anil Madhavapeddy"
                },
                "author": "Anil Madhavapeddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22562v1",
                "updated": "2025-03-28T16:04:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    4,
                    20,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:04:20Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    4,
                    20,
                    4,
                    87,
                    0
                ],
                "title": "Niyama : Breaking the Silos of LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Niyama : Breaking the Silos of LLM Inference Serving"
                },
                "summary": "The widespread adoption of Large Language Models (LLMs) has enabled diverse\napplications with very different latency requirements. Existing LLM serving\nframeworks rely on siloed infrastructure with coarse-grained workload\nsegregation -- interactive and batch -- leading to inefficient resource\nutilization and limited support for fine-grained Quality-of-Service (QoS)\ndifferentiation. This results in operational inefficiencies, over-provisioning\nand poor load management during traffic surges.\n  We present Niyama, a novel QoS-driven inference serving system that enables\nefficient co-scheduling of diverse workloads on shared infrastructure. Niyama\nintroduces fine-grained QoS classification allowing applications to specify\nprecise latency requirements, and dynamically adapts scheduling decisions based\non real-time system state. Leveraging the predictable execution characteristics\nof LLM inference, Niyama implements a dynamic chunking mechanism to improve\noverall throughput while maintaining strict QoS guarantees. Additionally,\nNiyama employs a hybrid prioritization policy that balances fairness and\nefficiency, and employs selective request relegation that enables graceful\nservice degradation during overload conditions. Our evaluation demonstrates\nthat Niyama increases serving capacity by 32% compared to current siloed\ndeployments, while maintaining QoS guarantees. Notably, under extreme load, our\nsystem reduces SLO violations by an order of magnitude compared to current\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs) has enabled diverse\napplications with very different latency requirements. Existing LLM serving\nframeworks rely on siloed infrastructure with coarse-grained workload\nsegregation -- interactive and batch -- leading to inefficient resource\nutilization and limited support for fine-grained Quality-of-Service (QoS)\ndifferentiation. This results in operational inefficiencies, over-provisioning\nand poor load management during traffic surges.\n  We present Niyama, a novel QoS-driven inference serving system that enables\nefficient co-scheduling of diverse workloads on shared infrastructure. Niyama\nintroduces fine-grained QoS classification allowing applications to specify\nprecise latency requirements, and dynamically adapts scheduling decisions based\non real-time system state. Leveraging the predictable execution characteristics\nof LLM inference, Niyama implements a dynamic chunking mechanism to improve\noverall throughput while maintaining strict QoS guarantees. Additionally,\nNiyama employs a hybrid prioritization policy that balances fairness and\nefficiency, and employs selective request relegation that enables graceful\nservice degradation during overload conditions. Our evaluation demonstrates\nthat Niyama increases serving capacity by 32% compared to current siloed\ndeployments, while maintaining QoS guarantees. Notably, under extreme load, our\nsystem reduces SLO violations by an order of magnitude compared to current\nstrategies."
                },
                "authors": [
                    {
                        "name": "Kanishk Goel"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Nipun Kwatra"
                    },
                    {
                        "name": "Ravi Shreyas Anupindi"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    }
                ],
                "author_detail": {
                    "name": "Ramachandran Ramjee"
                },
                "author": "Ramachandran Ramjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22551v1",
                "updated": "2025-03-28T15:51:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    51,
                    27,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:51:27Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    51,
                    27,
                    4,
                    87,
                    0
                ],
                "title": "Approximate stationarity in disjunctive optimization: concepts,\n  qualification conditions, and application to MPCCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate stationarity in disjunctive optimization: concepts,\n  qualification conditions, and application to MPCCs"
                },
                "summary": "In this paper, we are concerned with stationarity conditions and\nqualification conditions for optimization problems with disjunctive\nconstraints. This class covers, among others, optimization problems with\ncomplementarity, vanishing, or switching constraints which are notoriously\nchallenging due to their highly combinatorial structure. The focus of our study\nis twofold. First, we investigate approximate stationarity conditions and the\nassociated strict constraint qualifications which can be used to infer\nstationarity of local minimizers. While such concepts are already known in the\ncontext of so-called Mordukhovich-stationarity, we introduce suitable\nextensions associated with strong stationarity. Second, a qualification\ncondition is established which, based on an approximately Mordukhovich- or\nstrongly stationary point, can be used to infer its Mordukhovich- or strong\nstationarity, respectively. In contrast to the aforementioned strict constraint\nqualifications, this condition depends on the involved sequences justifying\napproximate stationarity and, thus, is not a constraint qualification in the\nnarrower sense. However, it is much easier to verify as it merely requires to\ncheck the (positive) linear independence of a certain family of gradients. In\nfact, it provides a characterization of stationarity and, thus, is weaker than\nthese strict constraint qualifications. In order to illustrate the obtained\nfindings, they are applied to optimization problems with complementarity\nconstraints where they can be naturally extended to the well-known concepts of\nweak and Clarke-stationarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we are concerned with stationarity conditions and\nqualification conditions for optimization problems with disjunctive\nconstraints. This class covers, among others, optimization problems with\ncomplementarity, vanishing, or switching constraints which are notoriously\nchallenging due to their highly combinatorial structure. The focus of our study\nis twofold. First, we investigate approximate stationarity conditions and the\nassociated strict constraint qualifications which can be used to infer\nstationarity of local minimizers. While such concepts are already known in the\ncontext of so-called Mordukhovich-stationarity, we introduce suitable\nextensions associated with strong stationarity. Second, a qualification\ncondition is established which, based on an approximately Mordukhovich- or\nstrongly stationary point, can be used to infer its Mordukhovich- or strong\nstationarity, respectively. In contrast to the aforementioned strict constraint\nqualifications, this condition depends on the involved sequences justifying\napproximate stationarity and, thus, is not a constraint qualification in the\nnarrower sense. However, it is much easier to verify as it merely requires to\ncheck the (positive) linear independence of a certain family of gradients. In\nfact, it provides a characterization of stationarity and, thus, is weaker than\nthese strict constraint qualifications. In order to illustrate the obtained\nfindings, they are applied to optimization problems with complementarity\nconstraints where they can be naturally extended to the well-known concepts of\nweak and Clarke-stationarity."
                },
                "authors": [
                    {
                        "name": "Isabella Käming"
                    },
                    {
                        "name": "Patrick Mehlitz"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Mehlitz"
                },
                "author": "Patrick Mehlitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49J53, 90C30, 90C33, 90C46",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14582v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14582v4",
                "updated": "2025-03-28T15:50:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    50,
                    55,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-18T16:32:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    32,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "Do LLMs estimate uncertainty well in instruction-following?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs estimate uncertainty well in instruction-following?"
                },
                "summary": "Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents."
                },
                "authors": [
                    {
                        "name": "Juyeon Heo"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Christina Heinze-Deml"
                    },
                    {
                        "name": "Jaya Narain"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Narain"
                },
                "author": "Jaya Narain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14582v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14582v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22547v1",
                "updated": "2025-03-28T15:47:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    47,
                    30,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:47:30Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    47,
                    30,
                    4,
                    87,
                    0
                ],
                "title": "Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction\n  in Transformers through Token Correlation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction\n  in Transformers through Token Correlation"
                },
                "summary": "The geometric evolution of token representations in large language models\n(LLMs) presents a fundamental paradox: while human language inherently\norganizes semantic information in low-dimensional spaces ($\\sim 10^1$\ndimensions), modern LLMs employ high-dimensional embeddings ($\\sim 10^3$\ndimensions) processed through Transformer architectures. To resolve this\nparadox, this work bridges this conceptual gap by developing a geometric\nframework that tracks token dynamics across Transformers layers. Through\nlayer-wise analysis of intrinsic dimensions across multiple architectures, we\nreveal an expansion-contraction pattern where tokens diffuse to a \"working\nspace\" and then progressively project onto lower-dimensional submanifolds. Our\nfinding implies a negative correlation between the working space dimension and\nparameter-sensitive performance of the LLMs, and indicates that effective\nmodels tend to compress tokens into approximately 10-dimensional submanifolds,\nclosely resembling human semantic spaces. This work not only advances LLM\ninterpretability by reframing Transformers layers as projectors that mediate\nbetween high-dimensional computation and low-dimensional semantics, but also\nprovides practical tools for model diagnostics that do not rely on\ntask-specific evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The geometric evolution of token representations in large language models\n(LLMs) presents a fundamental paradox: while human language inherently\norganizes semantic information in low-dimensional spaces ($\\sim 10^1$\ndimensions), modern LLMs employ high-dimensional embeddings ($\\sim 10^3$\ndimensions) processed through Transformer architectures. To resolve this\nparadox, this work bridges this conceptual gap by developing a geometric\nframework that tracks token dynamics across Transformers layers. Through\nlayer-wise analysis of intrinsic dimensions across multiple architectures, we\nreveal an expansion-contraction pattern where tokens diffuse to a \"working\nspace\" and then progressively project onto lower-dimensional submanifolds. Our\nfinding implies a negative correlation between the working space dimension and\nparameter-sensitive performance of the LLMs, and indicates that effective\nmodels tend to compress tokens into approximately 10-dimensional submanifolds,\nclosely resembling human semantic spaces. This work not only advances LLM\ninterpretability by reframing Transformers layers as projectors that mediate\nbetween high-dimensional computation and low-dimensional semantics, but also\nprovides practical tools for model diagnostics that do not rely on\ntask-specific evaluations."
                },
                "authors": [
                    {
                        "name": "Zhuo-Yang Song"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Qing-Hong Cao"
                    },
                    {
                        "name": "Ming-xing Luo"
                    },
                    {
                        "name": "Hua Xing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xing Zhu"
                },
                "author": "Hua Xing Zhu",
                "arxiv_comment": "17 pages, 9 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05305v2",
                "updated": "2025-03-28T15:45:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    45,
                    58,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-04T18:18:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    18,
                    18,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "Output Scouting: Auditing Large Language Models for Catastrophic\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output Scouting: Auditing Large Language Models for Catastrophic\n  Responses"
                },
                "summary": "Recent high profile incidents in which the use of Large Language Models\n(LLMs) resulted in significant harm to individuals have brought about a growing\ninterest in AI safety. One reason LLM safety issues occur is that models often\nhave at least some non-zero probability of producing harmful outputs. In this\nwork, we explore the following scenario: imagine an AI safety auditor is\nsearching for catastrophic responses from an LLM (e.g. a \"yes\" responses to\n\"can I fire an employee for being pregnant?\"), and is able to query the model a\nlimited number times (e.g. 1000 times). What is a strategy for querying the\nmodel that would efficiently find those failure responses? To this end, we\npropose output scouting: an approach that aims to generate semantically fluent\noutputs to a given prompt matching any target probability distribution. We then\nrun experiments using two LLMs and find numerous examples of catastrophic\nresponses. We conclude with a discussion that includes advice for practitioners\nwho are looking to implement LLM auditing for catastrophic responses. We also\nrelease an open-source toolkit (https://github.com/joaopfonseca/outputscouting)\nthat implements our auditing framework using the Hugging Face transformers\nlibrary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent high profile incidents in which the use of Large Language Models\n(LLMs) resulted in significant harm to individuals have brought about a growing\ninterest in AI safety. One reason LLM safety issues occur is that models often\nhave at least some non-zero probability of producing harmful outputs. In this\nwork, we explore the following scenario: imagine an AI safety auditor is\nsearching for catastrophic responses from an LLM (e.g. a \"yes\" responses to\n\"can I fire an employee for being pregnant?\"), and is able to query the model a\nlimited number times (e.g. 1000 times). What is a strategy for querying the\nmodel that would efficiently find those failure responses? To this end, we\npropose output scouting: an approach that aims to generate semantically fluent\noutputs to a given prompt matching any target probability distribution. We then\nrun experiments using two LLMs and find numerous examples of catastrophic\nresponses. We conclude with a discussion that includes advice for practitioners\nwho are looking to implement LLM auditing for catastrophic responses. We also\nrelease an open-source toolkit (https://github.com/joaopfonseca/outputscouting)\nthat implements our auditing framework using the Hugging Face transformers\nlibrary."
                },
                "authors": [
                    {
                        "name": "Andrew Bell"
                    },
                    {
                        "name": "Joao Fonseca"
                    }
                ],
                "author_detail": {
                    "name": "Joao Fonseca"
                },
                "author": "Joao Fonseca",
                "arxiv_comment": "Work not ready, further experiments needed to validate the method",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14516v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14516v5",
                "updated": "2025-03-28T15:40:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    40,
                    49,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-18T14:55:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    55,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "Do LLMs \"know\" internally when they follow instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs \"know\" internally when they follow instructions?"
                },
                "summary": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. In this work, we investigate whether LLMs encode\ninformation in their representations that correlate with instruction-following\nsuccess - a property we term knowing internally. Our analysis identifies a\ndirection in the input embedding space, termed the instruction-following\ndimension, that predicts whether a response will comply with a given\ninstruction. We find that this dimension generalizes well across unseen tasks\nbut not across unseen instruction types. We demonstrate that modifying\nrepresentations along this dimension improves instruction-following success\nrates compared to random changes, without compromising response quality.\nFurther investigation reveals that this dimension is more closely related to\nthe phrasing of prompts rather than the inherent difficulty of the task or\ninstructions. This work provides insight into the internal workings of LLMs'\ninstruction-following, paving the way for reliable LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. In this work, we investigate whether LLMs encode\ninformation in their representations that correlate with instruction-following\nsuccess - a property we term knowing internally. Our analysis identifies a\ndirection in the input embedding space, termed the instruction-following\ndimension, that predicts whether a response will comply with a given\ninstruction. We find that this dimension generalizes well across unseen tasks\nbut not across unseen instruction types. We demonstrate that modifying\nrepresentations along this dimension improves instruction-following success\nrates compared to random changes, without compromising response quality.\nFurther investigation reveals that this dimension is more closely related to\nthe phrasing of prompts rather than the inherent difficulty of the task or\ninstructions. This work provides insight into the internal workings of LLMs'\ninstruction-following, paving the way for reliable LLM agents."
                },
                "authors": [
                    {
                        "name": "Juyeon Heo"
                    },
                    {
                        "name": "Christina Heinze-Deml"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Shirley Ren"
                    },
                    {
                        "name": "Udhay Nallasamy"
                    },
                    {
                        "name": "Andy Miller"
                    },
                    {
                        "name": "Jaya Narain"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Narain"
                },
                "author": "Jaya Narain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14516v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14516v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22541v1",
                "updated": "2025-03-28T15:38:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    38,
                    21,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:38:21Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    38,
                    21,
                    4,
                    87,
                    0
                ],
                "title": "SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles"
                },
                "summary": "Accurate motion forecasting is essential for the safety and reliability of\nautonomous driving (AD) systems. While existing methods have made significant\nprogress, they often overlook explicit safety constraints and struggle to\ncapture the complex interactions among traffic agents, environmental factors,\nand motion dynamics. To address these challenges, we present SafeCast, a\nrisk-responsive motion forecasting model that integrates safety-aware\ndecision-making with uncertainty-aware adaptability. SafeCast is the first to\nincorporate the Responsibility-Sensitive Safety (RSS) framework into motion\nforecasting, encoding interpretable safety rules--such as safe distances and\ncollision avoidance--based on traffic norms and physical principles. To further\nenhance robustness, we introduce the Graph Uncertainty Feature (GUF), a\ngraph-based module that injects learnable noise into Graph Attention Networks,\ncapturing real-world uncertainties and enhancing generalization across diverse\nscenarios. We evaluate SafeCast on four real-world benchmark datasets--Next\nGeneration Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the\nMacao Connected Autonomous Driving (MoCAD)--covering highway, urban, and\nmixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA)\naccuracy while maintaining a lightweight architecture and low inference\nlatency, underscoring its potential for real-time deployment in safety-critical\nAD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate motion forecasting is essential for the safety and reliability of\nautonomous driving (AD) systems. While existing methods have made significant\nprogress, they often overlook explicit safety constraints and struggle to\ncapture the complex interactions among traffic agents, environmental factors,\nand motion dynamics. To address these challenges, we present SafeCast, a\nrisk-responsive motion forecasting model that integrates safety-aware\ndecision-making with uncertainty-aware adaptability. SafeCast is the first to\nincorporate the Responsibility-Sensitive Safety (RSS) framework into motion\nforecasting, encoding interpretable safety rules--such as safe distances and\ncollision avoidance--based on traffic norms and physical principles. To further\nenhance robustness, we introduce the Graph Uncertainty Feature (GUF), a\ngraph-based module that injects learnable noise into Graph Attention Networks,\ncapturing real-world uncertainties and enhancing generalization across diverse\nscenarios. We evaluate SafeCast on four real-world benchmark datasets--Next\nGeneration Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the\nMacao Connected Autonomous Driving (MoCAD)--covering highway, urban, and\nmixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA)\naccuracy while maintaining a lightweight architecture and low inference\nlatency, underscoring its potential for real-time deployment in safety-critical\nAD systems."
                },
                "authors": [
                    {
                        "name": "Haicheng Liao"
                    },
                    {
                        "name": "Hanlin Kong"
                    },
                    {
                        "name": "Bin Rao"
                    },
                    {
                        "name": "Bonan Wang"
                    },
                    {
                        "name": "Chengyue Wang"
                    },
                    {
                        "name": "Guyang Yu"
                    },
                    {
                        "name": "Yuming Huang"
                    },
                    {
                        "name": "Ruru Tang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Zhenning Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhenning Li"
                },
                "author": "Zhenning Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14739v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14739v4",
                "updated": "2025-03-28T15:21:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    21,
                    44,
                    4,
                    87,
                    0
                ],
                "published": "2025-02-20T17:05:58Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    5,
                    58,
                    3,
                    51,
                    0
                ],
                "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope."
                },
                "authors": [
                    {
                        "name": "M-A-P Team"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Bingli Wang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Zhenlin Wei"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Kaixin Deng"
                    },
                    {
                        "name": "Shawn Gavin"
                    },
                    {
                        "name": "Shian Jia"
                    },
                    {
                        "name": "Sichao Jiang"
                    },
                    {
                        "name": "Yiyan Liao"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Qinrui Li"
                    },
                    {
                        "name": "Sirun Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Yunwen Li"
                    },
                    {
                        "name": "David Ma"
                    },
                    {
                        "name": "Yuansheng Ni"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Qiyao Wang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Tyshawn Hsing"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Yuelin Bai"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Chengtuo Cheng"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Keyi Ding"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Yun Huang"
                    },
                    {
                        "name": "Yaoru Li"
                    },
                    {
                        "name": "Yizhe Li"
                    },
                    {
                        "name": "Zhaoqun Li"
                    },
                    {
                        "name": "Tianhao Liang"
                    },
                    {
                        "name": "Chengdong Lin"
                    },
                    {
                        "name": "Hongquan Lin"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Tianyang Pang"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Qige Qi"
                    },
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Yizhou Tan"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Chenqing Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yiya Wang"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Jiajun Xu"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Tianyang Zhan"
                    },
                    {
                        "name": "Chun Zhang"
                    },
                    {
                        "name": "Jinyang Zhang"
                    },
                    {
                        "name": "Xiyue Zhang"
                    },
                    {
                        "name": "Xingjian Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Xiangyu Zheng"
                    },
                    {
                        "name": "Chenghua Zhong"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Shi Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14739v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14739v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22517v1",
                "updated": "2025-03-28T15:21:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    21,
                    24,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:21:24Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    21,
                    24,
                    4,
                    87,
                    0
                ],
                "title": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative\n  Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative\n  Abilities"
                },
                "summary": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures."
                },
                "authors": [
                    {
                        "name": "Raman Dutt"
                    },
                    {
                        "name": "Harleen Hanspal"
                    },
                    {
                        "name": "Guoxuan Xia"
                    },
                    {
                        "name": "Petru-Daniel Tudosiu"
                    },
                    {
                        "name": "Alexander Black"
                    },
                    {
                        "name": "Yongxin Yang"
                    },
                    {
                        "name": "Steven McDonagh"
                    },
                    {
                        "name": "Sarah Parisot"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Parisot"
                },
                "author": "Sarah Parisot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22512v1",
                "updated": "2025-03-28T15:15:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    15,
                    56,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:15:56Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    15,
                    56,
                    4,
                    87,
                    0
                ],
                "title": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages\n  Through Cross-Language Translation and Multi-Agent Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages\n  Through Cross-Language Translation and Multi-Agent Refinement"
                },
                "summary": "Recent advances in leveraging LLMs for APR have demonstrated impressive\ncapabilities in fixing software defects. However, current LLM-based approaches\npredominantly focus on mainstream programming languages like Java and Python,\nneglecting less prevalent but emerging languages such as Rust due to expensive\ntraining resources, limited datasets, and insufficient community support. This\nnarrow focus creates a significant gap in repair capabilities across the\nprogramming language spectrum, where the full potential of LLMs for\ncomprehensive multilingual program repair remains largely unexplored. To\naddress this limitation, we introduce a novel cross-language program repair\napproach LANTERN that leverages LLMs' differential proficiency across languages\nthrough a multi-agent iterative repair paradigm. Our technique strategically\ntranslates defective code from languages where LLMs exhibit weaker repair\ncapabilities to languages where they demonstrate stronger performance, without\nrequiring additional training. A key innovation of our approach is an LLM-based\ndecision-making system that dynamically selects optimal target languages based\non bug characteristics and continuously incorporates feedback from previous\nrepair attempts. We evaluate our method on xCodeEval, a comprehensive\nmultilingual benchmark comprising 5,068 bugs across 11 programming languages.\nResults demonstrate significant enhancement in repair effectiveness,\nparticularly for underrepresented languages, with Rust showing a 22.09%\nimprovement in Pass@10 metrics. Our research provides the first empirical\nevidence that cross-language translation significantly expands the repair\ncapabilities of LLMs and effectively bridges the performance gap between\nprogramming languages with different levels of popularity, opening new avenues\nfor truly language-agnostic automated program repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in leveraging LLMs for APR have demonstrated impressive\ncapabilities in fixing software defects. However, current LLM-based approaches\npredominantly focus on mainstream programming languages like Java and Python,\nneglecting less prevalent but emerging languages such as Rust due to expensive\ntraining resources, limited datasets, and insufficient community support. This\nnarrow focus creates a significant gap in repair capabilities across the\nprogramming language spectrum, where the full potential of LLMs for\ncomprehensive multilingual program repair remains largely unexplored. To\naddress this limitation, we introduce a novel cross-language program repair\napproach LANTERN that leverages LLMs' differential proficiency across languages\nthrough a multi-agent iterative repair paradigm. Our technique strategically\ntranslates defective code from languages where LLMs exhibit weaker repair\ncapabilities to languages where they demonstrate stronger performance, without\nrequiring additional training. A key innovation of our approach is an LLM-based\ndecision-making system that dynamically selects optimal target languages based\non bug characteristics and continuously incorporates feedback from previous\nrepair attempts. We evaluate our method on xCodeEval, a comprehensive\nmultilingual benchmark comprising 5,068 bugs across 11 programming languages.\nResults demonstrate significant enhancement in repair effectiveness,\nparticularly for underrepresented languages, with Rust showing a 22.09%\nimprovement in Pass@10 metrics. Our research provides the first empirical\nevidence that cross-language translation significantly expands the repair\ncapabilities of LLMs and effectively bridges the performance gap between\nprogramming languages with different levels of popularity, opening new avenues\nfor truly language-agnostic automated program repair."
                },
                "authors": [
                    {
                        "name": "Wenqiang Luo"
                    },
                    {
                        "name": "Jacky Wai Keung"
                    },
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Tegawende F. Bissyande"
                    },
                    {
                        "name": "Haoye Tian"
                    },
                    {
                        "name": "Bach Le"
                    }
                ],
                "author_detail": {
                    "name": "Bach Le"
                },
                "author": "Bach Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19619v2",
                "updated": "2025-03-28T14:52:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    52,
                    31,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-25T13:08:26Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    8,
                    26,
                    1,
                    84,
                    0
                ],
                "title": "Exploring Next Token Prediction For Optimizing Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Next Token Prediction For Optimizing Databases"
                },
                "summary": "The Next Token Prediction paradigm (NTP, for short) lies at the forefront of\nmodern large foundational models that are pre-trained on diverse and large\ndatasets. These models generalize effectively and have proven to be very\nsuccessful in Natural Language Processing (NLP). Inspired by the generalization\ncapabilities of Large Language Models (LLMs), we investigate whether the same\nNTP paradigm can also be applied to DBMS design and optimization tasks.\nAdopting NTP directly for database optimization is non-trivial due to the\nfundamental differences between the domains. In this paper, we present a\nframework termed Probe and Learn (PoLe) for applying NTP to optimize database\nsystems. PoLe leverages Decision Transformers and hardware-generated tokens to\neffectively incorporate NTP into database systems. Preliminary results from the\nmain-memory index scheduling task demonstrate that adopting NTP can improve\nboth performance and generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Next Token Prediction paradigm (NTP, for short) lies at the forefront of\nmodern large foundational models that are pre-trained on diverse and large\ndatasets. These models generalize effectively and have proven to be very\nsuccessful in Natural Language Processing (NLP). Inspired by the generalization\ncapabilities of Large Language Models (LLMs), we investigate whether the same\nNTP paradigm can also be applied to DBMS design and optimization tasks.\nAdopting NTP directly for database optimization is non-trivial due to the\nfundamental differences between the domains. In this paper, we present a\nframework termed Probe and Learn (PoLe) for applying NTP to optimize database\nsystems. PoLe leverages Decision Transformers and hardware-generated tokens to\neffectively incorporate NTP into database systems. Preliminary results from the\nmain-memory index scheduling task demonstrate that adopting NTP can improve\nboth performance and generalizability."
                },
                "authors": [
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22473v1",
                "updated": "2025-03-28T14:33:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    33,
                    29,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T14:33:29Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    33,
                    29,
                    4,
                    87,
                    0
                ],
                "title": "WorkTeam: Constructing Workflows from Natural Language with Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorkTeam: Constructing Workflows from Natural Language with Multi-Agents"
                },
                "summary": "Workflows play a crucial role in enhancing enterprise efficiency by\norchestrating complex processes with multiple tools or components. However,\nhand-crafted workflow construction requires expert knowledge, presenting\nsignificant technical barriers. Recent advancements in Large Language Models\n(LLMs) have improved the generation of workflows from natural language\ninstructions (aka NL2Workflow), yet existing single LLM agent-based methods\nface performance degradation on complex tasks due to the need for specialized\nknowledge and the strain of task-switching. To tackle these challenges, we\npropose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor,\norchestrator, and filler agent, each with distinct roles that collaboratively\nenhance the conversion process. As there are currently no publicly available\nNL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which\nincludes 3,695 real-world business samples for training and evaluation.\nExperimental results show that our approach significantly increases the success\nrate of workflow construction, providing a novel and effective solution for\nenterprise NL2Workflow services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Workflows play a crucial role in enhancing enterprise efficiency by\norchestrating complex processes with multiple tools or components. However,\nhand-crafted workflow construction requires expert knowledge, presenting\nsignificant technical barriers. Recent advancements in Large Language Models\n(LLMs) have improved the generation of workflows from natural language\ninstructions (aka NL2Workflow), yet existing single LLM agent-based methods\nface performance degradation on complex tasks due to the need for specialized\nknowledge and the strain of task-switching. To tackle these challenges, we\npropose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor,\norchestrator, and filler agent, each with distinct roles that collaboratively\nenhance the conversion process. As there are currently no publicly available\nNL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which\nincludes 3,695 real-world business samples for training and evaluation.\nExperimental results show that our approach significantly increases the success\nrate of workflow construction, providing a novel and effective solution for\nenterprise NL2Workflow services."
                },
                "authors": [
                    {
                        "name": "Hanchao Liu"
                    },
                    {
                        "name": "Rongjun Li"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Ziyu Zhou"
                    },
                    {
                        "name": "Wei Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Peng"
                },
                "author": "Wei Peng",
                "arxiv_comment": "Accepted in NAACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04728v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04728v3",
                "updated": "2025-03-28T14:31:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    31,
                    42,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-07T14:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    8,
                    35,
                    3,
                    312,
                    0
                ],
                "title": "Neuromorphic Wireless Split Computing with Multi-Level Spikes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Wireless Split Computing with Multi-Level Spikes"
                },
                "summary": "Inspired by biological processes, neuromorphic computing leverages spiking\nneural networks (SNNs) to perform inference tasks, offering significant\nefficiency gains for workloads involving sequential data. Recent advances in\nhardware and software have shown that embedding a small payload within each\nspike exchanged between spiking neurons can enhance inference accuracy without\nincreasing energy consumption. To scale neuromorphic computing to larger\nworkloads, split computing - where an SNN is partitioned across two devices -\nis a promising solution. In such architectures, the device hosting the initial\nlayers must transmit information about the spikes generated by its output\nneurons to the second device. This establishes a trade-off between the benefits\nof multi-level spikes, which carry additional payload information, and the\ncommunication resources required for transmitting extra bits between devices.\nThis paper presents the first comprehensive study of a neuromorphic wireless\nsplit computing architecture that employs multi-level SNNs. We propose digital\nand analog modulation schemes for an orthogonal frequency division multiplexing\n(OFDM) radio interface to enable efficient communication. Simulation and\nexperimental results using software-defined radios reveal performance\nimprovements achieved by multi-level SNN models and provide insights into the\noptimal payload size as a function of the connection quality between the\ntransmitter and receiver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by biological processes, neuromorphic computing leverages spiking\nneural networks (SNNs) to perform inference tasks, offering significant\nefficiency gains for workloads involving sequential data. Recent advances in\nhardware and software have shown that embedding a small payload within each\nspike exchanged between spiking neurons can enhance inference accuracy without\nincreasing energy consumption. To scale neuromorphic computing to larger\nworkloads, split computing - where an SNN is partitioned across two devices -\nis a promising solution. In such architectures, the device hosting the initial\nlayers must transmit information about the spikes generated by its output\nneurons to the second device. This establishes a trade-off between the benefits\nof multi-level spikes, which carry additional payload information, and the\ncommunication resources required for transmitting extra bits between devices.\nThis paper presents the first comprehensive study of a neuromorphic wireless\nsplit computing architecture that employs multi-level SNNs. We propose digital\nand analog modulation schemes for an orthogonal frequency division multiplexing\n(OFDM) radio interface to enable efficient communication. Simulation and\nexperimental results using software-defined radios reveal performance\nimprovements achieved by multi-level SNN models and provide insights into the\noptimal payload size as a function of the connection quality between the\ntransmitter and receiver."
                },
                "authors": [
                    {
                        "name": "Dengyu Wu"
                    },
                    {
                        "name": "Jiechen Chen"
                    },
                    {
                        "name": "Bipin Rajendran"
                    },
                    {
                        "name": "H. Vincent Poor"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04728v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04728v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22467v1",
                "updated": "2025-03-28T14:27:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    27,
                    54,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T14:27:54Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    27,
                    54,
                    4,
                    87,
                    0
                ],
                "title": "An integrated method for clustering and association network inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An integrated method for clustering and association network inference"
                },
                "summary": "We consider high dimensional Gaussian graphical models inference. These\nmodels provide a rigorous framework to describe a network of statistical\ndependencies between entities, such as genes in genomic regulation studies or\nspecies in ecology. Penalized methods, including the standard Graphical-Lasso,\nare well-known approaches to infer the parameters of these models. As the\nnumber of variables in the model (of entities in the network) grow, the network\ninference and interpretation become more complex. We propose Normal-Block, a\nnew model that clusters variables and consider a network at the cluster level.\nNormal-Block both adds structure to the network and reduces its size. We build\non Graphical-Lasso to add a penalty on the network's edges and limit the\ndetection of spurious dependencies, we also propose a zero-inflated version of\nthe model to account for real-world data properties. For the inference\nprocedure, we propose a direct heuristic method and another more rigorous one\nthat simultaneously infers the clustering of variables and the association\nnetwork between clusters, using a penalized variational\nExpectation-Maximization approach. An implementation of the model in R, in a\npackage called normalblockr, is available on github\n(https://github.com/jeannetous/normalblockr). We present the results in terms\nof clustering and network inference using both simulated data and various types\nof real-world data (proteomics, words occurrences on webpages, and microbiota\ndistribution).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider high dimensional Gaussian graphical models inference. These\nmodels provide a rigorous framework to describe a network of statistical\ndependencies between entities, such as genes in genomic regulation studies or\nspecies in ecology. Penalized methods, including the standard Graphical-Lasso,\nare well-known approaches to infer the parameters of these models. As the\nnumber of variables in the model (of entities in the network) grow, the network\ninference and interpretation become more complex. We propose Normal-Block, a\nnew model that clusters variables and consider a network at the cluster level.\nNormal-Block both adds structure to the network and reduces its size. We build\non Graphical-Lasso to add a penalty on the network's edges and limit the\ndetection of spurious dependencies, we also propose a zero-inflated version of\nthe model to account for real-world data properties. For the inference\nprocedure, we propose a direct heuristic method and another more rigorous one\nthat simultaneously infers the clustering of variables and the association\nnetwork between clusters, using a penalized variational\nExpectation-Maximization approach. An implementation of the model in R, in a\npackage called normalblockr, is available on github\n(https://github.com/jeannetous/normalblockr). We present the results in terms\nof clustering and network inference using both simulated data and various types\nof real-world data (proteomics, words occurrences on webpages, and microbiota\ndistribution)."
                },
                "authors": [
                    {
                        "name": "Jeanne Tous"
                    },
                    {
                        "name": "Julien Chiquet"
                    }
                ],
                "author_detail": {
                    "name": "Julien Chiquet"
                },
                "author": "Julien Chiquet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07047v2",
                "updated": "2025-03-28T14:24:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    24,
                    35,
                    4,
                    87,
                    0
                ],
                "published": "2025-01-13T04:08:14Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    8,
                    14,
                    0,
                    13,
                    0
                ],
                "title": "Leveraging ASIC AI Chips for Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging ASIC AI Chips for Homomorphic Encryption"
                },
                "summary": "Cloud-based services are making the outsourcing of sensitive client data\nincreasingly common. Although homomorphic encryption (HE) offers strong privacy\nguarantee, it requires substantially more resources than computing on\nplaintext, often leading to unacceptably large latencies in getting the\nresults. HE accelerators have emerged to mitigate this latency issue, but with\nthe high cost of ASICs. In this paper we show that HE primitives can be\nconverted to AI operators and accelerated on existing ASIC AI accelerators,\nlike TPUs, which are already widely deployed in the cloud. Adapting such\naccelerators for HE requires (1) supporting modular multiplication, (2)\nhigh-precision arithmetic in software, and (3) efficient mapping on matrix\nengines. We introduce the CROSS compiler (1) to adopt Barrett reduction to\nprovide modular reduction support using multiplier and adder, (2) Basis Aligned\nTransformation (BAT) to convert high-precision multiplication as low-precision\nmatrix-vector multiplication, (3) Matrix Aligned Transformation (MAT) to covert\nvectorized modular operation with reduction into matrix multiplication that can\nbe efficiently processed on 2D spatial matrix engine. Our evaluation of CROSS\non a Google TPUv4 demonstrates significant performance improvements, with up to\n161x and 5x speedup compared to the previous work on many-core CPUs and V100.\nThe kernel-level codes are open-sourced at\nhttps://github.com/google/jaxite/tree/main/jaxite_word.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-based services are making the outsourcing of sensitive client data\nincreasingly common. Although homomorphic encryption (HE) offers strong privacy\nguarantee, it requires substantially more resources than computing on\nplaintext, often leading to unacceptably large latencies in getting the\nresults. HE accelerators have emerged to mitigate this latency issue, but with\nthe high cost of ASICs. In this paper we show that HE primitives can be\nconverted to AI operators and accelerated on existing ASIC AI accelerators,\nlike TPUs, which are already widely deployed in the cloud. Adapting such\naccelerators for HE requires (1) supporting modular multiplication, (2)\nhigh-precision arithmetic in software, and (3) efficient mapping on matrix\nengines. We introduce the CROSS compiler (1) to adopt Barrett reduction to\nprovide modular reduction support using multiplier and adder, (2) Basis Aligned\nTransformation (BAT) to convert high-precision multiplication as low-precision\nmatrix-vector multiplication, (3) Matrix Aligned Transformation (MAT) to covert\nvectorized modular operation with reduction into matrix multiplication that can\nbe efficiently processed on 2D spatial matrix engine. Our evaluation of CROSS\non a Google TPUv4 demonstrates significant performance improvements, with up to\n161x and 5x speedup compared to the previous work on many-core CPUs and V100.\nThe kernel-level codes are open-sourced at\nhttps://github.com/google/jaxite/tree/main/jaxite_word."
                },
                "authors": [
                    {
                        "name": "Jianming Tong"
                    },
                    {
                        "name": "Tianhao Huang"
                    },
                    {
                        "name": "Leo de Castro"
                    },
                    {
                        "name": "Anirudh Itagi"
                    },
                    {
                        "name": "Jingtian Dang"
                    },
                    {
                        "name": "Anupam Golder"
                    },
                    {
                        "name": "Asra Ali"
                    },
                    {
                        "name": "Jevin Jiang"
                    },
                    {
                        "name": "Arvind"
                    },
                    {
                        "name": "G. Edward Suh"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "16 pages, 11 figures, 4 algorithms, 9 tables. Enabling Google TPUs\n  for privacy-preserving AI inference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06931v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06931v3",
                "updated": "2025-03-28T14:19:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    19,
                    33,
                    4,
                    87,
                    0
                ],
                "published": "2024-12-09T19:21:05Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    19,
                    21,
                    5,
                    0,
                    344,
                    0
                ],
                "title": "Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based\n  Planning and Manoeuvrability-Driven Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based\n  Planning and Manoeuvrability-Driven Controls"
                },
                "summary": "The ability to wield tools was once considered exclusive to human\nintelligence, but it's now known that many other animals, like crows, possess\nthis capability. Yet, robotic systems still fall short of matching biological\ndexterity. In this paper, we investigate the use of Large Language Models\n(LLMs), tool affordances, and object manoeuvrability for non-prehensile\ntool-based manipulation tasks. Our novel method leverages LLMs based on scene\ninformation and natural language instructions to enable symbolic task planning\nfor tool-object manipulation. This approach allows the system to convert the\nhuman language sentence into a sequence of feasible motion functions. We have\ndeveloped a novel manoeuvrability-driven controller using a new tool affordance\nmodel derived from visual feedback. This controller helps guide the robot's\ntool utilization and manipulation actions, even within confined areas, using a\nstepping incremental approach. The proposed methodology is evaluated with\nexperiments to prove its effectiveness under various manipulation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to wield tools was once considered exclusive to human\nintelligence, but it's now known that many other animals, like crows, possess\nthis capability. Yet, robotic systems still fall short of matching biological\ndexterity. In this paper, we investigate the use of Large Language Models\n(LLMs), tool affordances, and object manoeuvrability for non-prehensile\ntool-based manipulation tasks. Our novel method leverages LLMs based on scene\ninformation and natural language instructions to enable symbolic task planning\nfor tool-object manipulation. This approach allows the system to convert the\nhuman language sentence into a sequence of feasible motion functions. We have\ndeveloped a novel manoeuvrability-driven controller using a new tool affordance\nmodel derived from visual feedback. This controller helps guide the robot's\ntool utilization and manipulation actions, even within confined areas, using a\nstepping incremental approach. The proposed methodology is evaluated with\nexperiments to prove its effectiveness under various manipulation scenarios."
                },
                "authors": [
                    {
                        "name": "Hoi-Yin Lee"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Anqing Duan"
                    },
                    {
                        "name": "Wanyu Ma"
                    },
                    {
                        "name": "Chenguang Yang"
                    },
                    {
                        "name": "David Navarro-Alarcon"
                    }
                ],
                "author_detail": {
                    "name": "David Navarro-Alarcon"
                },
                "author": "David Navarro-Alarcon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06931v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06931v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22458v1",
                "updated": "2025-03-28T14:08:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    8,
                    40,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T14:08:40Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    8,
                    40,
                    4,
                    87,
                    0
                ],
                "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey"
                },
                "summary": "This survey examines evaluation methods for large language model (LLM)-based\nagents in multi-turn conversational settings. Using a PRISMA-inspired\nframework, we systematically reviewed nearly 250 scholarly sources, capturing\nthe state of the art from various venues of publication, and establishing a\nsolid foundation for our analysis. Our study offers a structured approach by\ndeveloping two interrelated taxonomy systems: one that defines \\emph{what to\nevaluate} and another that explains \\emph{how to evaluate}. The first taxonomy\nidentifies key components of LLM-based agents for multi-turn conversations and\ntheir evaluation dimensions, including task completion, response quality, user\nexperience, memory and context retention, as well as planning and tool\nintegration. These components ensure that the performance of conversational\nagents is assessed in a holistic and meaningful manner. The second taxonomy\nsystem focuses on the evaluation methodologies. It categorizes approaches into\nannotation-based evaluations, automated metrics, hybrid strategies that combine\nhuman assessments with quantitative measures, and self-judging methods\nutilizing LLMs. This framework not only captures traditional metrics derived\nfrom language understanding, such as BLEU and ROUGE scores, but also\nincorporates advanced techniques that reflect the dynamic, interactive nature\nof multi-turn dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey examines evaluation methods for large language model (LLM)-based\nagents in multi-turn conversational settings. Using a PRISMA-inspired\nframework, we systematically reviewed nearly 250 scholarly sources, capturing\nthe state of the art from various venues of publication, and establishing a\nsolid foundation for our analysis. Our study offers a structured approach by\ndeveloping two interrelated taxonomy systems: one that defines \\emph{what to\nevaluate} and another that explains \\emph{how to evaluate}. The first taxonomy\nidentifies key components of LLM-based agents for multi-turn conversations and\ntheir evaluation dimensions, including task completion, response quality, user\nexperience, memory and context retention, as well as planning and tool\nintegration. These components ensure that the performance of conversational\nagents is assessed in a holistic and meaningful manner. The second taxonomy\nsystem focuses on the evaluation methodologies. It categorizes approaches into\nannotation-based evaluations, automated metrics, hybrid strategies that combine\nhuman assessments with quantitative measures, and self-judging methods\nutilizing LLMs. This framework not only captures traditional metrics derived\nfrom language understanding, such as BLEU and ROUGE scores, but also\nincorporates advanced techniques that reflect the dynamic, interactive nature\nof multi-turn dialogues."
                },
                "authors": [
                    {
                        "name": "Shengyue Guan"
                    },
                    {
                        "name": "Haoyi Xiong"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Bin Zhu"
                    },
                    {
                        "name": "Jian-guang Lou"
                    }
                ],
                "author_detail": {
                    "name": "Jian-guang Lou"
                },
                "author": "Jian-guang Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22456v1",
                "updated": "2025-03-28T14:07:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    7,
                    51,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T14:07:51Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    7,
                    51,
                    4,
                    87,
                    0
                ],
                "title": "Entropy-guided sequence weighting for efficient exploration in RL-based\n  LLM fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-guided sequence weighting for efficient exploration in RL-based\n  LLM fine-tuning"
                },
                "summary": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Abdullah Vanlioglu"
                    }
                ],
                "author_detail": {
                    "name": "Abdullah Vanlioglu"
                },
                "author": "Abdullah Vanlioglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22451v1",
                "updated": "2025-03-28T14:03:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    3,
                    14,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T14:03:14Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    3,
                    14,
                    4,
                    87,
                    0
                ],
                "title": "STADE: Standard Deviation as a Pruning Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STADE: Standard Deviation as a Pruning Metric"
                },
                "summary": "Recently, Large Language Models (LLMs) have become very widespread and are\nused to solve a wide variety of tasks. To successfully handle these tasks, LLMs\nrequire longer training times and larger model sizes. This makes LLMs ideal\ncandidates for pruning methods that reduce computational demands while\nmaintaining performance. Previous methods require a retraining phase after\npruning to maintain the original model's performance. However, state-of-the-art\npruning methods, such as Wanda, prune the model without retraining, making the\npruning process faster and more efficient. Building upon Wanda's work, this\nstudy provides a theoretical explanation of why the method is effective and\nleverages these insights to enhance the pruning process. Specifically, a\ntheoretical analysis of the pruning problem reveals a common scenario in\nMachine Learning where Wanda is the optimal pruning method. Furthermore, this\nanalysis is extended to cases where Wanda is no longer optimal, leading to the\ndevelopment of a new method, STADE, based on the standard deviation of the\ninput. From a theoretical standpoint, STADE demonstrates better generality\nacross different scenarios. Finally, extensive experiments on Llama and Open\nPre-trained Transformers (OPT) models validate these theoretical findings,\nshowing that depending on the training conditions, Wanda's optimal performance\nvaries as predicted by the theoretical framework. These insights contribute to\na more robust understanding of pruning strategies and their practical\nimplications. Code is available at: https://github.com/Coello-dev/STADE/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have become very widespread and are\nused to solve a wide variety of tasks. To successfully handle these tasks, LLMs\nrequire longer training times and larger model sizes. This makes LLMs ideal\ncandidates for pruning methods that reduce computational demands while\nmaintaining performance. Previous methods require a retraining phase after\npruning to maintain the original model's performance. However, state-of-the-art\npruning methods, such as Wanda, prune the model without retraining, making the\npruning process faster and more efficient. Building upon Wanda's work, this\nstudy provides a theoretical explanation of why the method is effective and\nleverages these insights to enhance the pruning process. Specifically, a\ntheoretical analysis of the pruning problem reveals a common scenario in\nMachine Learning where Wanda is the optimal pruning method. Furthermore, this\nanalysis is extended to cases where Wanda is no longer optimal, leading to the\ndevelopment of a new method, STADE, based on the standard deviation of the\ninput. From a theoretical standpoint, STADE demonstrates better generality\nacross different scenarios. Finally, extensive experiments on Llama and Open\nPre-trained Transformers (OPT) models validate these theoretical findings,\nshowing that depending on the training conditions, Wanda's optimal performance\nvaries as predicted by the theoretical framework. These insights contribute to\na more robust understanding of pruning strategies and their practical\nimplications. Code is available at: https://github.com/Coello-dev/STADE/"
                },
                "authors": [
                    {
                        "name": "Diego Coello de Portugal Mecke"
                    },
                    {
                        "name": "Haya Alyoussef"
                    },
                    {
                        "name": "Ilia Koloiarov"
                    },
                    {
                        "name": "Maximilian Stubbemann"
                    },
                    {
                        "name": "Lars Schmidt-Thieme"
                    }
                ],
                "author_detail": {
                    "name": "Lars Schmidt-Thieme"
                },
                "author": "Lars Schmidt-Thieme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00165v2",
                "updated": "2025-03-28T13:59:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    59,
                    44,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-31T19:16:00Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    16,
                    0,
                    3,
                    305,
                    0
                ],
                "title": "Accurate and Efficient Cardiac Digital Twin from surface ECGs: Insights\n  into Identifiability of Ventricular Conduction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and Efficient Cardiac Digital Twin from surface ECGs: Insights\n  into Identifiability of Ventricular Conduction System"
                },
                "summary": "Digital twins for cardiac electrophysiology are an enabling technology for\nprecision cardiology. Current forward models are advanced enough to simulate\nthe cardiac electric activity under different pathophysiological conditions and\naccurately replicate clinical signals like torso electrocardiograms (ECGs). In\nthis work, we address the challenge of matching subject-specific QRS complexes\nusing anatomically accurate, physiologically grounded cardiac digital twins. By\nfitting the initial conditions of a cardiac propagation model, our non-invasive\nmethod predicts activation patterns during sinus rhythm. For the first time, we\ndemonstrate that distinct activation maps can generate identical surface ECGs.\nTo address this non-uniqueness, we introduce a physiological prior based on the\ndistribution of Purkinje-muscle junctions. Additionally, we develop a digital\ntwin ensemble for probabilistic inference of cardiac activation. Our approach\nmarks a significant advancement in the calibration of cardiac digital twins and\nenhances their credibility for clinical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twins for cardiac electrophysiology are an enabling technology for\nprecision cardiology. Current forward models are advanced enough to simulate\nthe cardiac electric activity under different pathophysiological conditions and\naccurately replicate clinical signals like torso electrocardiograms (ECGs). In\nthis work, we address the challenge of matching subject-specific QRS complexes\nusing anatomically accurate, physiologically grounded cardiac digital twins. By\nfitting the initial conditions of a cardiac propagation model, our non-invasive\nmethod predicts activation patterns during sinus rhythm. For the first time, we\ndemonstrate that distinct activation maps can generate identical surface ECGs.\nTo address this non-uniqueness, we introduce a physiological prior based on the\ndistribution of Purkinje-muscle junctions. Additionally, we develop a digital\ntwin ensemble for probabilistic inference of cardiac activation. Our approach\nmarks a significant advancement in the calibration of cardiac digital twins and\nenhances their credibility for clinical application."
                },
                "authors": [
                    {
                        "name": "Thomas Grandits"
                    },
                    {
                        "name": "Karli Gillette"
                    },
                    {
                        "name": "Gernot Plank"
                    },
                    {
                        "name": "Simone Pezzuto"
                    }
                ],
                "author_detail": {
                    "name": "Simone Pezzuto"
                },
                "author": "Simone Pezzuto",
                "arxiv_comment": "27 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22436v1",
                "updated": "2025-03-28T13:55:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    55,
                    16,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:55:16Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    55,
                    16,
                    4,
                    87,
                    0
                ],
                "title": "NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous\n  Driving"
                },
                "summary": "Multi-view 3D visual grounding is critical for autonomous driving vehicles to\ninterpret natural languages and localize target objects in complex\nenvironments. However, existing datasets and methods suffer from coarse-grained\nlanguage instructions, and inadequate integration of 3D geometric reasoning\nwith linguistic comprehension. To this end, we introduce NuGrounding, the first\nlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.\nWe present a Hierarchy of Grounding (HoG) method to construct NuGrounding to\ngenerate hierarchical multi-level instructions, ensuring comprehensive coverage\nof human instruction patterns. To tackle this challenging dataset, we propose a\nnovel paradigm that seamlessly combines instruction comprehension abilities of\nmulti-modal LLMs (MLLMs) with precise localization abilities of specialist\ndetection models. Our approach introduces two decoupled task tokens and a\ncontext query to aggregate 3D geometric information and semantic instructions,\nfollowed by a fusion decoder to refine spatial-semantic feature fusion for\nprecise localization. Extensive experiments demonstrate that our method\nsignificantly outperforms the baselines adapted from representative 3D scene\nunderstanding methods by a significant margin and achieves 0.59 in precision\nand 0.64 in recall, with improvements of 50.8% and 54.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view 3D visual grounding is critical for autonomous driving vehicles to\ninterpret natural languages and localize target objects in complex\nenvironments. However, existing datasets and methods suffer from coarse-grained\nlanguage instructions, and inadequate integration of 3D geometric reasoning\nwith linguistic comprehension. To this end, we introduce NuGrounding, the first\nlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.\nWe present a Hierarchy of Grounding (HoG) method to construct NuGrounding to\ngenerate hierarchical multi-level instructions, ensuring comprehensive coverage\nof human instruction patterns. To tackle this challenging dataset, we propose a\nnovel paradigm that seamlessly combines instruction comprehension abilities of\nmulti-modal LLMs (MLLMs) with precise localization abilities of specialist\ndetection models. Our approach introduces two decoupled task tokens and a\ncontext query to aggregate 3D geometric information and semantic instructions,\nfollowed by a fusion decoder to refine spatial-semantic feature fusion for\nprecise localization. Extensive experiments demonstrate that our method\nsignificantly outperforms the baselines adapted from representative 3D scene\nunderstanding methods by a significant margin and achieves 0.59 in precision\nand 0.64 in recall, with improvements of 50.8% and 54.7%."
                },
                "authors": [
                    {
                        "name": "Fuhao Li"
                    },
                    {
                        "name": "Huan Jin"
                    },
                    {
                        "name": "Bin Gao"
                    },
                    {
                        "name": "Liaoyuan Fan"
                    },
                    {
                        "name": "Lihui Jiang"
                    },
                    {
                        "name": "Long Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Long Zeng"
                },
                "author": "Long Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22426v1",
                "updated": "2025-03-28T13:41:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    41,
                    7,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:41:07Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    41,
                    7,
                    4,
                    87,
                    0
                ],
                "title": "Long-Tail Crisis in Nearest Neighbor Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Tail Crisis in Nearest Neighbor Language Models"
                },
                "summary": "The $k$-nearest-neighbor language model ($k$NN-LM), one of the\nretrieval-augmented language models, improves the perplexity for given text by\ndirectly accessing a large datastore built from any text data during inference.\nA widely held hypothesis for the success of $k$NN-LM is that its explicit\nmemory, i.e., the datastore, enhances predictions for long-tail phenomena.\nHowever, prior works have primarily shown its ability to retrieve long-tail\ncontexts, leaving the model's performance remain underexplored in estimating\nthe probabilities of long-tail target tokens during inference. In this paper,\nwe investigate the behavior of $k$NN-LM on low-frequency tokens, examining\nprediction probability, retrieval accuracy, token distribution in the\ndatastore, and approximation error of the product quantization. Our\nexperimental results reveal that $k$NN-LM does not improve prediction\nperformance for low-frequency tokens but mainly benefits high-frequency tokens\nregardless of long-tail contexts in the datastore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$-nearest-neighbor language model ($k$NN-LM), one of the\nretrieval-augmented language models, improves the perplexity for given text by\ndirectly accessing a large datastore built from any text data during inference.\nA widely held hypothesis for the success of $k$NN-LM is that its explicit\nmemory, i.e., the datastore, enhances predictions for long-tail phenomena.\nHowever, prior works have primarily shown its ability to retrieve long-tail\ncontexts, leaving the model's performance remain underexplored in estimating\nthe probabilities of long-tail target tokens during inference. In this paper,\nwe investigate the behavior of $k$NN-LM on low-frequency tokens, examining\nprediction probability, retrieval accuracy, token distribution in the\ndatastore, and approximation error of the product quantization. Our\nexperimental results reveal that $k$NN-LM does not improve prediction\nperformance for low-frequency tokens but mainly benefits high-frequency tokens\nregardless of long-tail contexts in the datastore."
                },
                "authors": [
                    {
                        "name": "Yuto Nishida"
                    },
                    {
                        "name": "Makoto Morishita"
                    },
                    {
                        "name": "Hiroyuki Deguchi"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted to NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22424v1",
                "updated": "2025-03-28T13:36:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    36,
                    26,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:36:26Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    36,
                    26,
                    4,
                    87,
                    0
                ],
                "title": "CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph\n  Searching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph\n  Searching"
                },
                "summary": "Large language models (LLMs) have significantly advanced autonomous software\nengineering, leading to a growing number of software engineering agents that\nassist developers in automatic program repair. Issue localization forms the\nbasis for accurate patch generation. However, because of limitations caused by\nthe context window length of LLMs, existing issue localization methods face\nchallenges in balancing concise yet effective contexts and adequately\ncomprehensive search spaces. In this paper, we introduce CoSIL, an LLM driven,\nsimple yet powerful function level issue localization method without training\nor indexing. CoSIL reduces the search space through module call graphs,\niteratively searches the function call graph to obtain relevant contexts, and\nuses context pruning to control the search direction and manage contexts\neffectively. Importantly, the call graph is dynamically constructed by the LLM\nduring search, eliminating the need for pre-parsing. Experiment results\ndemonstrate that CoSIL achieves a Top-1 localization success rate of 43 percent\nand 44.6 percent on SWE bench Lite and SWE bench Verified, respectively, using\nQwen2.5 Coder 32B, outperforming existing methods by 8.6 to 98.2 percent. When\nCoSIL is applied to guide the patch generation stage, the resolved rate further\nimproves by 9.3 to 31.5 percent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced autonomous software\nengineering, leading to a growing number of software engineering agents that\nassist developers in automatic program repair. Issue localization forms the\nbasis for accurate patch generation. However, because of limitations caused by\nthe context window length of LLMs, existing issue localization methods face\nchallenges in balancing concise yet effective contexts and adequately\ncomprehensive search spaces. In this paper, we introduce CoSIL, an LLM driven,\nsimple yet powerful function level issue localization method without training\nor indexing. CoSIL reduces the search space through module call graphs,\niteratively searches the function call graph to obtain relevant contexts, and\nuses context pruning to control the search direction and manage contexts\neffectively. Importantly, the call graph is dynamically constructed by the LLM\nduring search, eliminating the need for pre-parsing. Experiment results\ndemonstrate that CoSIL achieves a Top-1 localization success rate of 43 percent\nand 44.6 percent on SWE bench Lite and SWE bench Verified, respectively, using\nQwen2.5 Coder 32B, outperforming existing methods by 8.6 to 98.2 percent. When\nCoSIL is applied to guide the patch generation stage, the resolved rate further\nimproves by 9.3 to 31.5 percent."
                },
                "authors": [
                    {
                        "name": "Zhonghao Jiang"
                    },
                    {
                        "name": "Xiaoxue Ren"
                    },
                    {
                        "name": "Meng Yan"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Zhongxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxin Liu"
                },
                "author": "Zhongxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.01795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.01795v2",
                "updated": "2025-03-28T13:33:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    33,
                    40,
                    4,
                    87,
                    0
                ],
                "published": "2022-06-03T19:45:43Z",
                "published_parsed": [
                    2022,
                    6,
                    3,
                    19,
                    45,
                    43,
                    4,
                    154,
                    0
                ],
                "title": "Adversarially Robust Topological Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarially Robust Topological Inference"
                },
                "summary": "The distance function to a compact set plays a crucial role in the paradigm\nof topological data analysis. In particular, the sublevel sets of the distance\nfunction are used in the computation of persistent homology -- a backbone of\nthe topological data analysis pipeline. Despite its stability to perturbations\nin the Hausdorff distance, persistent homology is highly sensitive to outliers.\nIn this work, we develop a framework of statistical inference for persistent\nhomology in the presence of outliers. Drawing inspiration from recent\ndevelopments in robust statistics, we propose a \\textit{median-of-means}\nvariant of the distance function (\\textsf{MoM Dist}) and establish its\nstatistical properties. In particular, we show that, even in the presence of\noutliers, the sublevel filtrations and weighted filtrations induced by\n\\textsf{MoM Dist} are both consistent estimators of the true underlying\npopulation counterpart and exhibit near minimax-optimal performance in\nadversarial settings. Finally, we demonstrate the advantages of the proposed\nmethodology through simulations and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The distance function to a compact set plays a crucial role in the paradigm\nof topological data analysis. In particular, the sublevel sets of the distance\nfunction are used in the computation of persistent homology -- a backbone of\nthe topological data analysis pipeline. Despite its stability to perturbations\nin the Hausdorff distance, persistent homology is highly sensitive to outliers.\nIn this work, we develop a framework of statistical inference for persistent\nhomology in the presence of outliers. Drawing inspiration from recent\ndevelopments in robust statistics, we propose a \\textit{median-of-means}\nvariant of the distance function (\\textsf{MoM Dist}) and establish its\nstatistical properties. In particular, we show that, even in the presence of\noutliers, the sublevel filtrations and weighted filtrations induced by\n\\textsf{MoM Dist} are both consistent estimators of the true underlying\npopulation counterpart and exhibit near minimax-optimal performance in\nadversarial settings. Finally, we demonstrate the advantages of the proposed\nmethodology through simulations and applications."
                },
                "authors": [
                    {
                        "name": "Siddharth Vishwanath"
                    },
                    {
                        "name": "Bharath K. Sriperumbudur"
                    },
                    {
                        "name": "Kenji Fukumizu"
                    },
                    {
                        "name": "Satoshi Kuriki"
                    }
                ],
                "author_detail": {
                    "name": "Satoshi Kuriki"
                },
                "author": "Satoshi Kuriki",
                "arxiv_comment": "54 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2206.01795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.01795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62R40, 55N31, 68T09",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22420v1",
                "updated": "2025-03-28T13:32:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    32,
                    29,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:32:29Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    32,
                    29,
                    4,
                    87,
                    0
                ],
                "title": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric\n  Evaluation with Chain-of-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric\n  Evaluation with Chain-of-Analysis"
                },
                "summary": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments."
                },
                "authors": [
                    {
                        "name": "Jiangyong Huang"
                    },
                    {
                        "name": "Baoxiong Jia"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Ziyu Zhu"
                    },
                    {
                        "name": "Xiongkun Linghu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Siyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Huang"
                },
                "author": "Siyuan Huang",
                "arxiv_comment": "CVPR 2025. Project page: https://beacon-3d.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22419v1",
                "updated": "2025-03-28T13:32:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    32,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:32:19Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    32,
                    19,
                    4,
                    87,
                    0
                ],
                "title": "Arboreal networks and their underlying trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arboreal networks and their underlying trees"
                },
                "summary": "Horizontal gene transfer (HGT) is an important process in bacterial\nevolution. Current phylogeny-based approaches to capture it cannot however\nappropriately account for the fact that HGT can occur between bacteria living\nin different ecological niches. Due to the fact that arboreal networks are a\ntype of multiple-rooted phylogenetic network that can be thought of as a forest\nof rooted phylogenetic trees along with a set of additional arcs each joining\ntwo different trees in the forest, understanding the combinatorial structure of\nsuch networks might therefore pave the way to extending current phylogeny-based\nHGT-inference methods in this direction. A central question in this context is,\nhow can we construct an arboreal network? Answering this question is strongly\ninformed by finding ways to \\textit{encode} an arboreal network, that is,\nbreaking up the network into simpler combinatorial structures that, in a well\ndefined sense uniquely determine the network. In the form of triplets, trinets\nand quarnets such encodings are known for certain types of single-rooted\nphylogenetic networks. By studying the underlying tree of an arboreal network,\nwe compliment them here with an answer for arboreal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Horizontal gene transfer (HGT) is an important process in bacterial\nevolution. Current phylogeny-based approaches to capture it cannot however\nappropriately account for the fact that HGT can occur between bacteria living\nin different ecological niches. Due to the fact that arboreal networks are a\ntype of multiple-rooted phylogenetic network that can be thought of as a forest\nof rooted phylogenetic trees along with a set of additional arcs each joining\ntwo different trees in the forest, understanding the combinatorial structure of\nsuch networks might therefore pave the way to extending current phylogeny-based\nHGT-inference methods in this direction. A central question in this context is,\nhow can we construct an arboreal network? Answering this question is strongly\ninformed by finding ways to \\textit{encode} an arboreal network, that is,\nbreaking up the network into simpler combinatorial structures that, in a well\ndefined sense uniquely determine the network. In the form of triplets, trinets\nand quarnets such encodings are known for certain types of single-rooted\nphylogenetic networks. By studying the underlying tree of an arboreal network,\nwe compliment them here with an answer for arboreal networks."
                },
                "authors": [
                    {
                        "name": "Katharina T. Huber"
                    },
                    {
                        "name": "Darren Overman"
                    }
                ],
                "author_detail": {
                    "name": "Darren Overman"
                },
                "author": "Darren Overman",
                "arxiv_comment": "26 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "05C05, 92D15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22413v1",
                "updated": "2025-03-28T13:28:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    28,
                    57,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:28:57Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    28,
                    57,
                    4,
                    87,
                    0
                ],
                "title": "Instance-Level Data-Use Auditing of Visual ML Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance-Level Data-Use Auditing of Visual ML Models"
                },
                "summary": "The growing trend of legal disputes over the unauthorized use of data in\nmachine learning (ML) systems highlights the urgent need for reliable data-use\nauditing mechanisms to ensure accountability and transparency in ML. In this\npaper, we present the first proactive instance-level data-use auditing method\ndesigned to enable data owners to audit the use of their individual data\ninstances in ML models, providing more fine-grained auditing results. Our\napproach integrates any black-box membership inference technique with a\nsequential hypothesis test, providing a quantifiable and tunable\nfalse-detection rate. We evaluate our method on three types of visual ML\nmodels: image classifiers, visual encoders, and Contrastive Image-Language\nPretraining (CLIP) models. In additional, we apply our method to evaluate the\nperformance of two state-of-the-art approximate unlearning methods. Our\nfindings reveal that neither method successfully removes the influence of the\nunlearned data instances from image classifiers and CLIP models even if\nsacrificing model utility by $10.33\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing trend of legal disputes over the unauthorized use of data in\nmachine learning (ML) systems highlights the urgent need for reliable data-use\nauditing mechanisms to ensure accountability and transparency in ML. In this\npaper, we present the first proactive instance-level data-use auditing method\ndesigned to enable data owners to audit the use of their individual data\ninstances in ML models, providing more fine-grained auditing results. Our\napproach integrates any black-box membership inference technique with a\nsequential hypothesis test, providing a quantifiable and tunable\nfalse-detection rate. We evaluate our method on three types of visual ML\nmodels: image classifiers, visual encoders, and Contrastive Image-Language\nPretraining (CLIP) models. In additional, we apply our method to evaluate the\nperformance of two state-of-the-art approximate unlearning methods. Our\nfindings reveal that neither method successfully removes the influence of the\nunlearned data instances from image classifiers and CLIP models even if\nsacrificing model utility by $10.33\\%$."
                },
                "authors": [
                    {
                        "name": "Zonghao Huang"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Michael K. Reiter"
                    }
                ],
                "author_detail": {
                    "name": "Michael K. Reiter"
                },
                "author": "Michael K. Reiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16021v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16021v3",
                "updated": "2025-03-28T13:23:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    23,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-20T10:37:29Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    37,
                    29,
                    3,
                    79,
                    0
                ],
                "title": "Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems"
                },
                "summary": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's impact on the\ndiversity and democratic value of information ecosystems. We introduce a\nlarge-scale simulation framework to examine AI-based imitation within news, a\ncontext crucial for public discourse. By systematically testing two distinct\nimitation strategies across a range of information environments varying in\ninitial diversity, we demonstrate that AI-generated articles do not uniformly\nhomogenize content. Instead, AI's influence is strongly context-dependent:\nAI-generated content can introduce valuable diversity in originally homogeneous\nnews environments but diminish diversity in initially heterogeneous contexts.\nThese results illustrate that the initial diversity of an information\nenvironment critically shapes AI's impact, challenging assumptions that\nAI-driven imitation threatens diversity. Instead, when information is initially\nhomogeneous, AI-driven imitation can expand perspectives, styles, and topics.\nThis is especially important in news contexts, where information diversity\nfosters richer public debate by exposing citizens to alternative viewpoints,\nchallenging biases, and preventing narrative monopolies, which is essential for\na resilient democracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's impact on the\ndiversity and democratic value of information ecosystems. We introduce a\nlarge-scale simulation framework to examine AI-based imitation within news, a\ncontext crucial for public discourse. By systematically testing two distinct\nimitation strategies across a range of information environments varying in\ninitial diversity, we demonstrate that AI-generated articles do not uniformly\nhomogenize content. Instead, AI's influence is strongly context-dependent:\nAI-generated content can introduce valuable diversity in originally homogeneous\nnews environments but diminish diversity in initially heterogeneous contexts.\nThese results illustrate that the initial diversity of an information\nenvironment critically shapes AI's impact, challenging assumptions that\nAI-driven imitation threatens diversity. Instead, when information is initially\nhomogeneous, AI-driven imitation can expand perspectives, styles, and topics.\nThis is especially important in news contexts, where information diversity\nfosters richer public debate by exposing citizens to alternative viewpoints,\nchallenging biases, and preventing narrative monopolies, which is essential for\na resilient democracy."
                },
                "authors": [
                    {
                        "name": "Emil Bakkensen Johansen"
                    },
                    {
                        "name": "Oliver Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Baumann"
                },
                "author": "Oliver Baumann",
                "arxiv_comment": "42 pages, 11 figures, 4 tables; v2: corrected typographical errors,\n  streamlined language, updated abstract, added supplementary information; v3:\n  restructured appendix, added temperature and embeddings sensitivity checks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16021v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16021v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22408v1",
                "updated": "2025-03-28T13:17:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    17,
                    58,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:17:58Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    17,
                    58,
                    4,
                    87,
                    0
                ],
                "title": "Smart Sensing Breaks the Accuracy Barrier in Battery State Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Sensing Breaks the Accuracy Barrier in Battery State Monitoring"
                },
                "summary": "Accurate state-of-charge (SOC) estimation is essential for optimizing battery\nperformance, ensuring safety, and maximizing economic value. Conventional\ncurrent and voltage measurements, however, have inherent limitations in fully\ninferring the multiphysics-resolved dynamics inside battery cells. This creates\nan accuracy barrier that constrains battery usage and reduces\ncost-competitiveness and sustainability across industries dependent on battery\ntechnology. In this work, we introduce an integrated sensor framework that\ncombines novel mechanical, thermal, gas, optical, and electrical sensors with\ntraditional measurements to break through this barrier. We generate three\nunique datasets with eleven measurement types and propose an explainable\nmachine-learning approach for SOC estimation. This approach renders the\nmeasured signals and the predictive result of machine learning physically\ninterpretable with respect to battery SOC, offering fundamental insights into\nthe time-varying importance of different signals. Our experimental results\nreveal a marked increase in SOC estimation accuracy--enhanced from 46.1% to\n74.5%--compared to conventional methods. This approach not only advances SOC\nmonitoring precision but also establishes a foundation for monitoring\nadditional battery states to further improve safety, extend lifespan, and\nfacilitate fast charging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate state-of-charge (SOC) estimation is essential for optimizing battery\nperformance, ensuring safety, and maximizing economic value. Conventional\ncurrent and voltage measurements, however, have inherent limitations in fully\ninferring the multiphysics-resolved dynamics inside battery cells. This creates\nan accuracy barrier that constrains battery usage and reduces\ncost-competitiveness and sustainability across industries dependent on battery\ntechnology. In this work, we introduce an integrated sensor framework that\ncombines novel mechanical, thermal, gas, optical, and electrical sensors with\ntraditional measurements to break through this barrier. We generate three\nunique datasets with eleven measurement types and propose an explainable\nmachine-learning approach for SOC estimation. This approach renders the\nmeasured signals and the predictive result of machine learning physically\ninterpretable with respect to battery SOC, offering fundamental insights into\nthe time-varying importance of different signals. Our experimental results\nreveal a marked increase in SOC estimation accuracy--enhanced from 46.1% to\n74.5%--compared to conventional methods. This approach not only advances SOC\nmonitoring precision but also establishes a foundation for monitoring\nadditional battery states to further improve safety, extend lifespan, and\nfacilitate fast charging."
                },
                "authors": [
                    {
                        "name": "Xiaolei Bian"
                    },
                    {
                        "name": "Changfu Zou"
                    },
                    {
                        "name": "Björn Fridholm"
                    },
                    {
                        "name": "Christian Sundvall"
                    },
                    {
                        "name": "Torsten Wik"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Wik"
                },
                "author": "Torsten Wik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22406v1",
                "updated": "2025-03-28T13:16:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    16,
                    27,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:16:27Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    16,
                    27,
                    4,
                    87,
                    0
                ],
                "title": "Training Large Language Models for Advanced Typosquatting Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models for Advanced Typosquatting Detection"
                },
                "summary": "Typosquatting is a long-standing cyber threat that exploits human error in\ntyping URLs to deceive users, distribute malware, and conduct phishing attacks.\nWith the proliferation of domain names and new Top-Level Domains (TLDs),\ntyposquatting techniques have grown more sophisticated, posing significant\nrisks to individuals, businesses, and national cybersecurity infrastructure.\nTraditional detection methods primarily focus on well-known impersonation\npatterns, leaving gaps in identifying more complex attacks. This study\nintroduces a novel approach leveraging large language models (LLMs) to enhance\ntyposquatting detection. By training an LLM on character-level transformations\nand pattern-based heuristics rather than domain-specific data, a more adaptable\nand resilient detection mechanism develops. Experimental results indicate that\nthe Phi-4 14B model outperformed other tested models when properly fine tuned\nachieving a 98% accuracy rate with only a few thousand training samples. This\nresearch highlights the potential of LLMs in cybersecurity applications,\nspecifically in mitigating domain-based deception tactics, and provides\ninsights into optimizing machine learning strategies for threat detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typosquatting is a long-standing cyber threat that exploits human error in\ntyping URLs to deceive users, distribute malware, and conduct phishing attacks.\nWith the proliferation of domain names and new Top-Level Domains (TLDs),\ntyposquatting techniques have grown more sophisticated, posing significant\nrisks to individuals, businesses, and national cybersecurity infrastructure.\nTraditional detection methods primarily focus on well-known impersonation\npatterns, leaving gaps in identifying more complex attacks. This study\nintroduces a novel approach leveraging large language models (LLMs) to enhance\ntyposquatting detection. By training an LLM on character-level transformations\nand pattern-based heuristics rather than domain-specific data, a more adaptable\nand resilient detection mechanism develops. Experimental results indicate that\nthe Phi-4 14B model outperformed other tested models when properly fine tuned\nachieving a 98% accuracy rate with only a few thousand training samples. This\nresearch highlights the potential of LLMs in cybersecurity applications,\nspecifically in mitigating domain-based deception tactics, and provides\ninsights into optimizing machine learning strategies for threat detection."
                },
                "authors": [
                    {
                        "name": "Jackson Welch"
                    }
                ],
                "author_detail": {
                    "name": "Jackson Welch"
                },
                "author": "Jackson Welch",
                "arxiv_comment": "6 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10297v3",
                "updated": "2025-03-28T13:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    16,
                    24,
                    4,
                    87,
                    0
                ],
                "published": "2025-02-14T16:59:05Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    59,
                    5,
                    4,
                    45,
                    0
                ],
                "title": "DeltaProduct: Improving State-Tracking in Linear RNNs via Householder\n  Products",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaProduct: Improving State-Tracking in Linear RNNs via Householder\n  Products"
                },
                "summary": "Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKV-7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet by proving that it can solve dihedral group\nword problems in just two layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKV-7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet by proving that it can solve dihedral group\nword problems in just two layers."
                },
                "authors": [
                    {
                        "name": "Julien Siems"
                    },
                    {
                        "name": "Timur Carstensen"
                    },
                    {
                        "name": "Arber Zela"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Massimiliano Pontil"
                    },
                    {
                        "name": "Riccardo Grazzi"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Grazzi"
                },
                "author": "Riccardo Grazzi",
                "arxiv_comment": "Accepted at ICLR 2025 Workshop on Foundation Models in the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22405v1",
                "updated": "2025-03-28T13:16:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    16,
                    2,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:16:02Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    16,
                    2,
                    4,
                    87,
                    0
                ],
                "title": "Modeling Multiple Normal Action Representations for Error Detection in\n  Procedural Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Multiple Normal Action Representations for Error Detection in\n  Procedural Tasks"
                },
                "summary": "Error detection in procedural activities is essential for consistent and\ncorrect outcomes in AR-assisted and robotic systems. Existing methods often\nfocus on temporal ordering errors or rely on static prototypes to represent\nnormal actions. However, these approaches typically overlook the common\nscenario where multiple, distinct actions are valid following a given sequence\nof executed actions. This leads to two issues: (1) the model cannot effectively\ndetect errors using static prototypes when the inference environment or action\nexecution distribution differs from training; and (2) the model may also use\nthe wrong prototypes to detect errors if the ongoing action label is not the\nsame as the predicted one. To address this problem, we propose an Adaptive\nMultiple Normal Action Representation (AMNAR) framework. AMNAR predicts all\nvalid next actions and reconstructs their corresponding normal action\nrepresentations, which are compared against the ongoing action to detect\nerrors. Extensive experiments demonstrate that AMNAR achieves state-of-the-art\nperformance, highlighting the effectiveness of AMNAR and the importance of\nmodeling multiple valid next actions in error detection. The code is available\nat https://github.com/iSEE-Laboratory/AMNAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error detection in procedural activities is essential for consistent and\ncorrect outcomes in AR-assisted and robotic systems. Existing methods often\nfocus on temporal ordering errors or rely on static prototypes to represent\nnormal actions. However, these approaches typically overlook the common\nscenario where multiple, distinct actions are valid following a given sequence\nof executed actions. This leads to two issues: (1) the model cannot effectively\ndetect errors using static prototypes when the inference environment or action\nexecution distribution differs from training; and (2) the model may also use\nthe wrong prototypes to detect errors if the ongoing action label is not the\nsame as the predicted one. To address this problem, we propose an Adaptive\nMultiple Normal Action Representation (AMNAR) framework. AMNAR predicts all\nvalid next actions and reconstructs their corresponding normal action\nrepresentations, which are compared against the ongoing action to detect\nerrors. Extensive experiments demonstrate that AMNAR achieves state-of-the-art\nperformance, highlighting the effectiveness of AMNAR and the importance of\nmodeling multiple valid next actions in error detection. The code is available\nat https://github.com/iSEE-Laboratory/AMNAR."
                },
                "authors": [
                    {
                        "name": "Wei-Jin Huang"
                    },
                    {
                        "name": "Yuan-Ming Li"
                    },
                    {
                        "name": "Zhi-Wei Xia"
                    },
                    {
                        "name": "Yu-Ming Tang"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Jian-Fang Hu"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Shi Zheng"
                },
                "author": "Wei-Shi Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22402v1",
                "updated": "2025-03-28T13:11:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    11,
                    27,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:11:27Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    11,
                    27,
                    4,
                    87,
                    0
                ],
                "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing"
                },
                "summary": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL."
                },
                "authors": [
                    {
                        "name": "Yizhang Zhu"
                    },
                    {
                        "name": "Runzhi Jiang"
                    },
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "arxiv_comment": "19 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22401v1",
                "updated": "2025-03-28T13:10:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    10,
                    4,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:10:04Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    10,
                    4,
                    4,
                    87,
                    0
                ],
                "title": "Generative Reliability-Based Design Optimization Using In-Context\n  Learning Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Reliability-Based Design Optimization Using In-Context\n  Learning Capabilities of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable in-context learning\ncapabilities, enabling flexible utilization of limited historical information\nto play pivotal roles in reasoning, problem-solving, and complex pattern\nrecognition tasks. Inspired by the successful applications of LLMs in multiple\ndomains, this paper proposes a generative design method by leveraging the\nin-context learning capabilities of LLMs with the iterative search mechanisms\nof metaheuristic algorithms for solving reliability-based design optimization\nproblems. In detail, reliability analysis is performed by engaging the LLMs and\nKriging surrogate modeling to overcome the computational burden. By dynamically\nproviding critical information of design points to the LLMs with prompt\nengineering, the method enables rapid generation of high-quality design\nalternatives that satisfy reliability constraints while achieving performance\noptimization. With the Deepseek-V3 model, three case studies are used to\ndemonstrated the performance of the proposed approach. Experimental results\nindicate that the proposed LLM-RBDO method successfully identifies feasible\nsolutions that meet reliability constraints while achieving a comparable\nconvergence rate compared to traditional genetic algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable in-context learning\ncapabilities, enabling flexible utilization of limited historical information\nto play pivotal roles in reasoning, problem-solving, and complex pattern\nrecognition tasks. Inspired by the successful applications of LLMs in multiple\ndomains, this paper proposes a generative design method by leveraging the\nin-context learning capabilities of LLMs with the iterative search mechanisms\nof metaheuristic algorithms for solving reliability-based design optimization\nproblems. In detail, reliability analysis is performed by engaging the LLMs and\nKriging surrogate modeling to overcome the computational burden. By dynamically\nproviding critical information of design points to the LLMs with prompt\nengineering, the method enables rapid generation of high-quality design\nalternatives that satisfy reliability constraints while achieving performance\noptimization. With the Deepseek-V3 model, three case studies are used to\ndemonstrated the performance of the proposed approach. Experimental results\nindicate that the proposed LLM-RBDO method successfully identifies feasible\nsolutions that meet reliability constraints while achieving a comparable\nconvergence rate compared to traditional genetic algorithms."
                },
                "authors": [
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Qian Tang"
                    },
                    {
                        "name": "Zequn Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zequn Wang"
                },
                "author": "Zequn Wang",
                "arxiv_comment": "17 pages, 11 figures, 4tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19458v2",
                "updated": "2025-03-28T13:04:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    4,
                    54,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-25T08:46:55Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    46,
                    55,
                    1,
                    84,
                    0
                ],
                "title": "GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian\n  Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian\n  Splatting"
                },
                "summary": "Reconstructing open surfaces from multi-view images is vital in digitalizing\ncomplex objects in daily life. A widely used strategy is to learn unsigned\ndistance functions (UDFs) by checking if their appearance conforms to the image\nobservations through neural rendering. However, it is still hard to learn\ncontinuous and implicit UDF representations through 3D Gaussians splatting\n(3DGS) due to the discrete and explicit scene representation, i.e., 3D\nGaussians. To resolve this issue, we propose a novel approach to bridge the gap\nbetween 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2D\nGaussian planes on surfaces, and then, leverage the self-supervision and\ngradient-based inference to supervise unsigned distances in both near and far\narea to surfaces. To this end, we introduce novel constraints and strategies to\nconstrain the learning of 2D Gaussians to pursue more stable optimization and\nmore reliable self-supervision, addressing the challenges brought by\ncomplicated gradient field on or near the zero level set of UDFs. We report\nnumerical and visual comparisons with the state-of-the-art on widely used\nbenchmarks and real data to show our advantages in terms of accuracy,\nefficiency, completeness, and sharpness of reconstructed open surfaces with\nboundaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing open surfaces from multi-view images is vital in digitalizing\ncomplex objects in daily life. A widely used strategy is to learn unsigned\ndistance functions (UDFs) by checking if their appearance conforms to the image\nobservations through neural rendering. However, it is still hard to learn\ncontinuous and implicit UDF representations through 3D Gaussians splatting\n(3DGS) due to the discrete and explicit scene representation, i.e., 3D\nGaussians. To resolve this issue, we propose a novel approach to bridge the gap\nbetween 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2D\nGaussian planes on surfaces, and then, leverage the self-supervision and\ngradient-based inference to supervise unsigned distances in both near and far\narea to surfaces. To this end, we introduce novel constraints and strategies to\nconstrain the learning of 2D Gaussians to pursue more stable optimization and\nmore reliable self-supervision, addressing the challenges brought by\ncomplicated gradient field on or near the zero level set of UDFs. We report\nnumerical and visual comparisons with the state-of-the-art on widely used\nbenchmarks and real data to show our advantages in terms of accuracy,\nefficiency, completeness, and sharpness of reconstructed open surfaces with\nboundaries."
                },
                "authors": [
                    {
                        "name": "Shujuan Li"
                    },
                    {
                        "name": "Yu-Shen Liu"
                    },
                    {
                        "name": "Zhizhong Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhizhong Han"
                },
                "author": "Zhizhong Han",
                "arxiv_comment": "Accepted by CVPR 2025. Project page:\n  https://lisj575.github.io/GaussianUDF/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22395v1",
                "updated": "2025-03-28T13:04:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    4,
                    41,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:04:41Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    4,
                    41,
                    4,
                    87,
                    0
                ],
                "title": "Negation: A Pink Elephant in the Large Language Models' Room?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negation: A Pink Elephant in the Large Language Models' Room?"
                },
                "summary": "Negations are key to determining sentence meaning, making them essential for\nlogical reasoning. Despite their importance, negations pose a substantial\nchallenge for large language models (LLMs) and remain underexplored.\n  We construct two multilingual natural language inference (NLI) datasets with\n\\textit{paired} examples differing in negation. We investigate how model size\nand language impact its ability to handle negation correctly by evaluating\npopular LLMs.\n  Contrary to previous work, we show that increasing the model size\nconsistently improves the models' ability to handle negations. Furthermore, we\nfind that both the models' reasoning accuracy and robustness to negation are\nlanguage-dependent and that the length and explicitness of the premise have a\ngreater impact on robustness than language.\n  Our datasets can facilitate further research and improvements of language\nmodel reasoning in multilingual settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negations are key to determining sentence meaning, making them essential for\nlogical reasoning. Despite their importance, negations pose a substantial\nchallenge for large language models (LLMs) and remain underexplored.\n  We construct two multilingual natural language inference (NLI) datasets with\n\\textit{paired} examples differing in negation. We investigate how model size\nand language impact its ability to handle negation correctly by evaluating\npopular LLMs.\n  Contrary to previous work, we show that increasing the model size\nconsistently improves the models' ability to handle negations. Furthermore, we\nfind that both the models' reasoning accuracy and robustness to negation are\nlanguage-dependent and that the length and explicitness of the premise have a\ngreater impact on robustness than language.\n  Our datasets can facilitate further research and improvements of language\nmodel reasoning in multilingual settings."
                },
                "authors": [
                    {
                        "name": "Tereza Vrabcová"
                    },
                    {
                        "name": "Marek Kadlčík"
                    },
                    {
                        "name": "Petr Sojka"
                    },
                    {
                        "name": "Michal Štefánik"
                    },
                    {
                        "name": "Michal Spiegel"
                    }
                ],
                "author_detail": {
                    "name": "Michal Spiegel"
                },
                "author": "Michal Spiegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22388v1",
                "updated": "2025-03-28T12:46:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    46,
                    54,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T12:46:54Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    46,
                    54,
                    4,
                    87,
                    0
                ],
                "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers\n  for Multi-Hop and Multi-Bug Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers\n  for Multi-Hop and Multi-Bug Errors"
                },
                "summary": "LLMs are transforming software development, yet current code generation and\ncode repair benchmarks mainly assess syntactic and functional correctness in\nsimple, single-error cases. LLMs' capabilities to autonomously find and fix\nruntime logical errors in complex data science code remain largely unexplored.\nTo address this gap, we introduce DSDBench: the Data Science Debugging\nBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hop\nerror tracing and multi-bug detection in data science code debugging. DSDBench\nadapts datasets from existing data science task benchmarks, such as DABench and\nMatPlotBench, featuring realistic data science debugging tasks with\nautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes\n1,117 annotated samples with 741 cause-effect error pairs and runtime error\nmessages. Evaluations of state-of-the-art LLMs on DSDBench show significant\nperformance gaps, highlighting challenges in debugging logical runtime errors\nin data science code. DSDBench offers a crucial resource to evaluate and\nimprove LLMs' debugging and reasoning capabilities, enabling more reliable\nAI-assisted data science in the future.DSDBench is publicly available at\nhttps://github.com/KevinCL16/DSDBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are transforming software development, yet current code generation and\ncode repair benchmarks mainly assess syntactic and functional correctness in\nsimple, single-error cases. LLMs' capabilities to autonomously find and fix\nruntime logical errors in complex data science code remain largely unexplored.\nTo address this gap, we introduce DSDBench: the Data Science Debugging\nBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hop\nerror tracing and multi-bug detection in data science code debugging. DSDBench\nadapts datasets from existing data science task benchmarks, such as DABench and\nMatPlotBench, featuring realistic data science debugging tasks with\nautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes\n1,117 annotated samples with 741 cause-effect error pairs and runtime error\nmessages. Evaluations of state-of-the-art LLMs on DSDBench show significant\nperformance gaps, highlighting challenges in debugging logical runtime errors\nin data science code. DSDBench offers a crucial resource to evaluate and\nimprove LLMs' debugging and reasoning capabilities, enabling more reliable\nAI-assisted data science in the future.DSDBench is publicly available at\nhttps://github.com/KevinCL16/DSDBench."
                },
                "authors": [
                    {
                        "name": "Zhiyu Yang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Yang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Deng"
                },
                "author": "Yang Deng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21480v2",
                "updated": "2025-03-28T12:34:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    34,
                    25,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-27T13:12:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    12,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs"
                },
                "summary": "The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs."
                },
                "authors": [
                    {
                        "name": "John Murzaku"
                    },
                    {
                        "name": "Owen Rambow"
                    }
                ],
                "author_detail": {
                    "name": "Owen Rambow"
                },
                "author": "Owen Rambow",
                "arxiv_comment": "Submitted to COLM 2025. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22369v1",
                "updated": "2025-03-28T12:22:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    22,
                    50,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T12:22:50Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    22,
                    50,
                    4,
                    87,
                    0
                ],
                "title": "Inference on effect size after multiple hypothesis testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on effect size after multiple hypothesis testing"
                },
                "summary": "Significant treatment effects are often emphasized when interpreting and\nsummarizing empirical findings in studies that estimate multiple, possibly\nmany, treatment effects. Under this kind of selective reporting, conventional\ntreatment effect estimates may be biased and their corresponding confidence\nintervals may undercover the true effect sizes. We propose new estimators and\nconfidence intervals that provide valid inferences on the effect sizes of the\nsignificant effects after multiple hypothesis testing. Our methods are based on\nthe principle of selective conditional inference and complement a wide range of\ntests, including step-up tests and bootstrap-based step-down tests. Our\napproach is scalable, allowing us to study an application with over 370\nestimated effects. We justify our procedure for asymptotically normal treatment\neffect estimators. We provide two empirical examples that demonstrate bias\ncorrection and confidence interval adjustments for significant effects. The\nmagnitude and direction of the bias correction depend on the correlation\nstructure of the estimated effects and whether the interpretation of the\nsignificant effects depends on the (in)significance of other effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant treatment effects are often emphasized when interpreting and\nsummarizing empirical findings in studies that estimate multiple, possibly\nmany, treatment effects. Under this kind of selective reporting, conventional\ntreatment effect estimates may be biased and their corresponding confidence\nintervals may undercover the true effect sizes. We propose new estimators and\nconfidence intervals that provide valid inferences on the effect sizes of the\nsignificant effects after multiple hypothesis testing. Our methods are based on\nthe principle of selective conditional inference and complement a wide range of\ntests, including step-up tests and bootstrap-based step-down tests. Our\napproach is scalable, allowing us to study an application with over 370\nestimated effects. We justify our procedure for asymptotically normal treatment\neffect estimators. We provide two empirical examples that demonstrate bias\ncorrection and confidence interval adjustments for significant effects. The\nmagnitude and direction of the bias correction depend on the correlation\nstructure of the estimated effects and whether the interpretation of the\nsignificant effects depends on the (in)significance of other effects."
                },
                "authors": [
                    {
                        "name": "Andreas Dzemski"
                    },
                    {
                        "name": "Ryo Okui"
                    },
                    {
                        "name": "Wenjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Wang"
                },
                "author": "Wenjie Wang",
                "arxiv_comment": "36 pages manuscript, 40 pages online appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21043v2",
                "updated": "2025-03-28T12:22:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    22,
                    45,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-26T23:23:33Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    23,
                    23,
                    33,
                    2,
                    85,
                    0
                ],
                "title": "Simulations of Global Solar Convection with a Fully Compressible\n  CHORUS++ Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulations of Global Solar Convection with a Fully Compressible\n  CHORUS++ Code"
                },
                "summary": "We present initial simulations of the solar convection zone using a fully\ncompressible hydrodynamic CHORUS++ code and discuss preliminary analysis. Fluid\ndynamics simulation of the global solar convection is a critically important\ntool to access the dynamics of solar cycle variations. The CHORUS++ code\nrobustly and efficiently solves the fully compressible hydrodynamic equations\nusing a compact local spectral method and semi-unstructured grid system. Using\nCHORUS++, we simulate, for the first time, the solar convection shell from 0.7\nto 0.99 of the solar radius, using the actual values of the total luminosity\nand the sidereal rotation rate. The simulation results include the\nlongitudinally averaged rotation rate, reasonably agreeing with the observed\nsolar-type differential rotation. The divergence of simulated mass flux infers\nthat the anelastic-type models are appropriate for modeling the global solar\nconvection, except for the outermost part of the Sun, for which the temporal\nscale of density variation is estimated at an order of days. The spherical\nharmonics analysis yields that the horizontal flows are dominant in the\nlarge-scale structure, and the degree of the anisotropy of the plasma flow is\nrather small and constant for the small-scale structures and for a wide range\nof the radius.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present initial simulations of the solar convection zone using a fully\ncompressible hydrodynamic CHORUS++ code and discuss preliminary analysis. Fluid\ndynamics simulation of the global solar convection is a critically important\ntool to access the dynamics of solar cycle variations. The CHORUS++ code\nrobustly and efficiently solves the fully compressible hydrodynamic equations\nusing a compact local spectral method and semi-unstructured grid system. Using\nCHORUS++, we simulate, for the first time, the solar convection shell from 0.7\nto 0.99 of the solar radius, using the actual values of the total luminosity\nand the sidereal rotation rate. The simulation results include the\nlongitudinally averaged rotation rate, reasonably agreeing with the observed\nsolar-type differential rotation. The divergence of simulated mass flux infers\nthat the anelastic-type models are appropriate for modeling the global solar\nconvection, except for the outermost part of the Sun, for which the temporal\nscale of density variation is estimated at an order of days. The spherical\nharmonics analysis yields that the horizontal flows are dominant in the\nlarge-scale structure, and the degree of the anisotropy of the plasma flow is\nrather small and constant for the small-scale structures and for a wide range\nof the radius."
                },
                "authors": [
                    {
                        "name": "Keiji Hayashi"
                    },
                    {
                        "name": "Alexander G. Kosovichev"
                    },
                    {
                        "name": "Chunlei Liang"
                    }
                ],
                "author_detail": {
                    "name": "Chunlei Liang"
                },
                "author": "Chunlei Liang",
                "arxiv_comment": "Figure 7 was corrected. Submitted to ApJS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22362v1",
                "updated": "2025-03-28T12:12:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    12,
                    38,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T12:12:38Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    12,
                    38,
                    4,
                    87,
                    0
                ],
                "title": "Supposedly Equivalent Facts That Aren't? Entity Frequency in\n  Pre-training Induces Asymmetry in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supposedly Equivalent Facts That Aren't? Entity Frequency in\n  Pre-training Induces Asymmetry in LLMs"
                },
                "summary": "Understanding and mitigating hallucinations in Large Language Models (LLMs)\nis crucial for ensuring reliable content generation. While previous research\nhas primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and\ndirectly links model behaviour to the pre-training data that forms their prior\nknowledge. Specifically, we demonstrate that an asymmetry exists in the\nrecognition of logically equivalent facts, which can be attributed to frequency\ndiscrepancies of entities appearing as subjects versus objects. Given that most\npre-training datasets are inaccessible, we leverage the fully open-source OLMo\nseries by indexing its Dolma dataset to estimate entity frequencies. Using\nrelational facts (represented as triples) from Wikidata5M, we construct probing\ndatasets to isolate this effect. Our experiments reveal that facts with a\nhigh-frequency subject and a low-frequency object are better recognised than\ntheir inverse, despite their logical equivalence. The pattern reverses in\nlow-to-high frequency settings, and no statistically significant asymmetry\nemerges when both entities are high-frequency. These findings highlight the\ninfluential role of pre-training data in shaping model predictions and provide\ninsights for inferring the characteristics of pre-training data in closed or\npartially closed LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and mitigating hallucinations in Large Language Models (LLMs)\nis crucial for ensuring reliable content generation. While previous research\nhas primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and\ndirectly links model behaviour to the pre-training data that forms their prior\nknowledge. Specifically, we demonstrate that an asymmetry exists in the\nrecognition of logically equivalent facts, which can be attributed to frequency\ndiscrepancies of entities appearing as subjects versus objects. Given that most\npre-training datasets are inaccessible, we leverage the fully open-source OLMo\nseries by indexing its Dolma dataset to estimate entity frequencies. Using\nrelational facts (represented as triples) from Wikidata5M, we construct probing\ndatasets to isolate this effect. Our experiments reveal that facts with a\nhigh-frequency subject and a low-frequency object are better recognised than\ntheir inverse, despite their logical equivalence. The pattern reverses in\nlow-to-high frequency settings, and no statistically significant asymmetry\nemerges when both entities are high-frequency. These findings highlight the\ninfluential role of pre-training data in shaping model predictions and provide\ninsights for inferring the characteristics of pre-training data in closed or\npartially closed LLMs."
                },
                "authors": [
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Bailan He"
                    },
                    {
                        "name": "Zifeng Ding"
                    },
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Yuqicheng Zhu"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Ian Horrocks"
                    }
                ],
                "author_detail": {
                    "name": "Ian Horrocks"
                },
                "author": "Ian Horrocks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05556v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05556v3",
                "updated": "2025-03-28T11:55:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    55,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-08T13:24:42Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    42,
                    4,
                    313,
                    0
                ],
                "title": "Gaussian process modelling of infectious diseases using the Greta\n  software package and GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian process modelling of infectious diseases using the Greta\n  software package and GPUs"
                },
                "summary": "Gaussian process are a widely-used statistical tool for conducting\nnon-parametric inference in applied sciences, with many computational packages\navailable to fit to data and predict future observations. We study the use of\nthe Greta software for Bayesian inference to apply Gaussian process regression\nto spatio-temporal data of infectious disease outbreaks and predict future\ndisease spread. Greta builds on Tensorflow, making it comparatively easy to\ntake advantage of the significant gain in speed offered by GPUs. In these\ncomplex spatio-temporal models, we show a reduction of up to 70\\% in\ncomputational time relative to fitting the same models on CPUs. We show how the\nchoice of covariance kernel impacts the ability to infer spread and extrapolate\nto unobserved spatial and temporal units. The inference pipeline is applied to\nweekly incidence data on tuberculosis in the East and West Midlands regions of\nEngland over a period of two years.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian process are a widely-used statistical tool for conducting\nnon-parametric inference in applied sciences, with many computational packages\navailable to fit to data and predict future observations. We study the use of\nthe Greta software for Bayesian inference to apply Gaussian process regression\nto spatio-temporal data of infectious disease outbreaks and predict future\ndisease spread. Greta builds on Tensorflow, making it comparatively easy to\ntake advantage of the significant gain in speed offered by GPUs. In these\ncomplex spatio-temporal models, we show a reduction of up to 70\\% in\ncomputational time relative to fitting the same models on CPUs. We show how the\nchoice of covariance kernel impacts the ability to infer spread and extrapolate\nto unobserved spatial and temporal units. The inference pipeline is applied to\nweekly incidence data on tuberculosis in the East and West Midlands regions of\nEngland over a period of two years."
                },
                "authors": [
                    {
                        "name": "Eva Gunn"
                    },
                    {
                        "name": "Nikhil Sengupta"
                    },
                    {
                        "name": "Ben Swallow"
                    }
                ],
                "author_detail": {
                    "name": "Ben Swallow"
                },
                "author": "Ben Swallow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05556v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05556v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22353v1",
                "updated": "2025-03-28T11:49:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    49,
                    56,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    49,
                    56,
                    4,
                    87,
                    0
                ],
                "title": "Firm or Fickle? Evaluating Large Language Models Consistency in\n  Sequential Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firm or Fickle? Evaluating Large Language Models Consistency in\n  Sequential Interactions"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications."
                },
                "authors": [
                    {
                        "name": "Yubo Li"
                    },
                    {
                        "name": "Yidi Miao"
                    },
                    {
                        "name": "Xueying Ding"
                    },
                    {
                        "name": "Ramayya Krishnan"
                    },
                    {
                        "name": "Rema Padman"
                    }
                ],
                "author_detail": {
                    "name": "Rema Padman"
                },
                "author": "Rema Padman",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22351v1",
                "updated": "2025-03-28T11:46:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    46,
                    50,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:46:50Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    46,
                    50,
                    4,
                    87,
                    0
                ],
                "title": "One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot\n  Monocular Depth Estimation Models on High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot\n  Monocular Depth Estimation Models on High-Resolution Images"
                },
                "summary": "Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches and results in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluation on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrates into which our PRO can be well harmonized, making their DE\ncapabilities still effective for the grid input of high-resolution images with\nlittle depth discontinuities at the grid boundaries. Our PRO runs fast at\ninference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches and results in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluation on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrates into which our PRO can be well harmonized, making their DE\ncapabilities still effective for the grid input of high-resolution images with\nlittle depth discontinuities at the grid boundaries. Our PRO runs fast at\ninference time."
                },
                "authors": [
                    {
                        "name": "Byeongjun Kwon"
                    },
                    {
                        "name": "Munchurl Kim"
                    }
                ],
                "author_detail": {
                    "name": "Munchurl Kim"
                },
                "author": "Munchurl Kim",
                "arxiv_comment": "Please visit our project page this\n  https://kaist-viclab.github.io/One-Look-is-Enough_site",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22345v1",
                "updated": "2025-03-28T11:35:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    35,
                    43,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:35:43Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    35,
                    43,
                    4,
                    87,
                    0
                ],
                "title": "Using a Large Language Model as Design Material for an Interactive\n  Museum Installation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using a Large Language Model as Design Material for an Interactive\n  Museum Installation"
                },
                "summary": "We present a work in progress that explores using a Large Language Model\n(LLM) as a design material for an interactive museum installation. LLMs offer\nthe possibility of creating chatbots that can facilitate dynamic and human-like\nconversation, engaging in a form of role play to bring historical persons to\nlife for visitors. However, LLMs are prone to producing misinformation, which\nruns counter to museums' core mission to educate the public. We use\nResearch-through-Design to explore some approaches to navigating this dilemma\nthrough rapid prototyping and evaluation and propose some directions for\nfurther research. We suggest that designers may shape interactions with the\nchatbot to emphasize personal narratives and role play rather than historical\nfacts or to intentionally highlight the unreliability of the chatbot outputs to\nprovoke critical reflection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a work in progress that explores using a Large Language Model\n(LLM) as a design material for an interactive museum installation. LLMs offer\nthe possibility of creating chatbots that can facilitate dynamic and human-like\nconversation, engaging in a form of role play to bring historical persons to\nlife for visitors. However, LLMs are prone to producing misinformation, which\nruns counter to museums' core mission to educate the public. We use\nResearch-through-Design to explore some approaches to navigating this dilemma\nthrough rapid prototyping and evaluation and propose some directions for\nfurther research. We suggest that designers may shape interactions with the\nchatbot to emphasize personal narratives and role play rather than historical\nfacts or to intentionally highlight the unreliability of the chatbot outputs to\nprovoke critical reflection."
                },
                "authors": [
                    {
                        "name": "Maria Padilla Engstrøm"
                    },
                    {
                        "name": "Anders Sundnes Løvlie"
                    }
                ],
                "author_detail": {
                    "name": "Anders Sundnes Løvlie"
                },
                "author": "Anders Sundnes Løvlie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22343v1",
                "updated": "2025-03-28T11:32:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    32,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:32:05Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    32,
                    5,
                    4,
                    87,
                    0
                ],
                "title": "Auto- and cross-correlations for multiple images of corotating hotspots\n  in accretion disks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto- and cross-correlations for multiple images of corotating hotspots\n  in accretion disks"
                },
                "summary": "Recent horizon-scale observations of the Sgr A* might open up a new window to\nstudy spacetime geometry and accretion matter in the strong-field regime of\ngravity. Due to the short gravitational timescale for Sgr A*, variable\nemissions near the galactic center are expected in the observations, including\nvariability in the Sgr A* images and flare motions within ten times\ngravitational radius. In this paper, we investigate the spatiotemporal auto-\nand cross-correlations for multiple images of a long-lived corotating hotspot\nnear the black hole. Using the recently developed efficient ray-tracing scheme\nfor point-like sources, we extensively consider multiple images from primary to\neighth-order in the correlations. We show that these correlations exhibit a\nrepeated inclined band-like structure in the $\\Delta \\Phi$-$\\Delta t$ plane.\nAnd there is a periodic modulation of width of the correlated bands, with the\nmaximum bandwidth increasing with the inclination angles. The point-slope of\ncorrelated bands is determined by the lapse in azimuthal angle and time of\nmultiple images, as well as the apparent rotation speed of hotspots. By\ncomparing correlations for lower-order with those for higher-order images, it\nis found that the fixed points for cross-correlations between lower-order\nimages depend on the orbital configurations of hotspots. It challenges the use\nof correlations for lower-order images to infer black hole geometries.\nAdditionally, we also examine the influence from hotspot shapes based on\nsemi-analytical formulas. Although the emission shape can significantly\ninfluence light curves, it does not change the correlated bands of the\ncorrelations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent horizon-scale observations of the Sgr A* might open up a new window to\nstudy spacetime geometry and accretion matter in the strong-field regime of\ngravity. Due to the short gravitational timescale for Sgr A*, variable\nemissions near the galactic center are expected in the observations, including\nvariability in the Sgr A* images and flare motions within ten times\ngravitational radius. In this paper, we investigate the spatiotemporal auto-\nand cross-correlations for multiple images of a long-lived corotating hotspot\nnear the black hole. Using the recently developed efficient ray-tracing scheme\nfor point-like sources, we extensively consider multiple images from primary to\neighth-order in the correlations. We show that these correlations exhibit a\nrepeated inclined band-like structure in the $\\Delta \\Phi$-$\\Delta t$ plane.\nAnd there is a periodic modulation of width of the correlated bands, with the\nmaximum bandwidth increasing with the inclination angles. The point-slope of\ncorrelated bands is determined by the lapse in azimuthal angle and time of\nmultiple images, as well as the apparent rotation speed of hotspots. By\ncomparing correlations for lower-order with those for higher-order images, it\nis found that the fixed points for cross-correlations between lower-order\nimages depend on the orbital configurations of hotspots. It challenges the use\nof correlations for lower-order images to infer black hole geometries.\nAdditionally, we also examine the influence from hotspot shapes based on\nsemi-analytical formulas. Although the emission shape can significantly\ninfluence light curves, it does not change the correlated bands of the\ncorrelations."
                },
                "authors": [
                    {
                        "name": "Qing-Hua Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qing-Hua Zhu"
                },
                "author": "Qing-Hua Zhu",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22338v1",
                "updated": "2025-03-28T11:25:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    25,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:25:05Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    25,
                    5,
                    4,
                    87,
                    0
                ],
                "title": "SKDU at De-Factify 4.0: Natural Language Features for AI-Generated\n  Text-Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKDU at De-Factify 4.0: Natural Language Features for AI-Generated\n  Text-Detection"
                },
                "summary": "The rapid advancement of large language models (LLMs) has introduced new\nchallenges in distinguishing human-written text from AI-generated content. In\nthis work, we explored a pipelined approach for AI-generated text detection\nthat includes a feature extraction step (i.e. prompt-based rewriting features\ninspired by RAIDAR and content-based features derived from the NELA toolkit)\nfollowed by a classification module. Comprehensive experiments were conducted\non the Defactify4.0 dataset, evaluating two tasks: binary classification to\ndifferentiate human-written and AI-generated text, and multi-class\nclassification to identify the specific generative model used to generate the\ninput text. Our findings reveal that NELA features significantly outperform\nRAIDAR features in both tasks, demonstrating their ability to capture nuanced\nlinguistic, stylistic, and content-based differences. Combining RAIDAR and NELA\nfeatures provided minimal improvement, highlighting the redundancy introduced\nby less discriminative features. Among the classifiers tested, XGBoost emerged\nas the most effective, leveraging the rich feature sets to achieve high\naccuracy and generalisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has introduced new\nchallenges in distinguishing human-written text from AI-generated content. In\nthis work, we explored a pipelined approach for AI-generated text detection\nthat includes a feature extraction step (i.e. prompt-based rewriting features\ninspired by RAIDAR and content-based features derived from the NELA toolkit)\nfollowed by a classification module. Comprehensive experiments were conducted\non the Defactify4.0 dataset, evaluating two tasks: binary classification to\ndifferentiate human-written and AI-generated text, and multi-class\nclassification to identify the specific generative model used to generate the\ninput text. Our findings reveal that NELA features significantly outperform\nRAIDAR features in both tasks, demonstrating their ability to capture nuanced\nlinguistic, stylistic, and content-based differences. Combining RAIDAR and NELA\nfeatures provided minimal improvement, highlighting the redundancy introduced\nby less discriminative features. Among the classifiers tested, XGBoost emerged\nas the most effective, leveraging the rich feature sets to achieve high\naccuracy and generalisation."
                },
                "authors": [
                    {
                        "name": "Shrikant Malviya"
                    },
                    {
                        "name": "Pablo Arnau-González"
                    },
                    {
                        "name": "Miguel Arevalillo-Herráez"
                    },
                    {
                        "name": "Stamos Katsigiannis"
                    }
                ],
                "author_detail": {
                    "name": "Stamos Katsigiannis"
                },
                "author": "Stamos Katsigiannis",
                "arxiv_comment": "De-Factify 4.0 Workshop at the 39th AAAI Conference on Artificial\n  Intelligence (AAAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Güra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Güra"
                },
                "author": "Fabian Güra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21080v2",
                "updated": "2025-03-28T10:57:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    57,
                    38,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-27T01:41:34Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    1,
                    41,
                    34,
                    3,
                    86,
                    0
                ],
                "title": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues"
                },
                "summary": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yunbo Long"
                    }
                ],
                "author_detail": {
                    "name": "Yunbo Long"
                },
                "author": "Yunbo Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22321v1",
                "updated": "2025-03-28T10:55:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    55,
                    20,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T10:55:20Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    55,
                    20,
                    4,
                    87,
                    0
                ],
                "title": "Estimation of Building Energy Demand Characteristics using Bayesian\n  Statistics and Energy Signature Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation of Building Energy Demand Characteristics using Bayesian\n  Statistics and Energy Signature Models"
                },
                "summary": "This work presents a scalable Bayesian modeling framework for evaluating\nbuilding energy performance using smart-meter data from 2,788 Danish\nsingle-family homes. The framework leverages Bayesian statistical inference\nintegrated with Energy Signature (ES) models to characterize thermal\nperformance in buildings. This approach quantifies key parameters such as the\nHeat Loss Coefficient (HLC), solar gain, and wind infiltration, while providing\nfull posterior distributions to reflect parameter uncertainty.\n  Three model variants are developed: a baseline ES model, an auto-regressive\nmodel (ARX-ES) to account for thermal inertia, and an auto-regressive moving\naverage model (ARMAX-ES) that approximates stochastic gray-box dynamics.\nResults show that model complexity improves one-step-ahead predictive\nperformance, with the ARMAX-ES model achieving a median Bayesian R^2 of 0.94\nacross the building stock. At the single-building level, the Bayesian approach\nyields credible intervals for yearly energy demand within $\\pm1\\%$, enabling\nmore robust diagnostics than deterministic methods.\n  Beyond improved accuracy, the Bayesian framework enhances decision-making by\nexplicitly representing uncertainty in building performance parameters. This\nprovides a more realistic foundation for investment prioritization, demand\nforecasting, and long-term energy planning. The method is readily applicable to\nother building typologies or geographies, offering a scalable tool for\ndata-driven energy management under uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a scalable Bayesian modeling framework for evaluating\nbuilding energy performance using smart-meter data from 2,788 Danish\nsingle-family homes. The framework leverages Bayesian statistical inference\nintegrated with Energy Signature (ES) models to characterize thermal\nperformance in buildings. This approach quantifies key parameters such as the\nHeat Loss Coefficient (HLC), solar gain, and wind infiltration, while providing\nfull posterior distributions to reflect parameter uncertainty.\n  Three model variants are developed: a baseline ES model, an auto-regressive\nmodel (ARX-ES) to account for thermal inertia, and an auto-regressive moving\naverage model (ARMAX-ES) that approximates stochastic gray-box dynamics.\nResults show that model complexity improves one-step-ahead predictive\nperformance, with the ARMAX-ES model achieving a median Bayesian R^2 of 0.94\nacross the building stock. At the single-building level, the Bayesian approach\nyields credible intervals for yearly energy demand within $\\pm1\\%$, enabling\nmore robust diagnostics than deterministic methods.\n  Beyond improved accuracy, the Bayesian framework enhances decision-making by\nexplicitly representing uncertainty in building performance parameters. This\nprovides a more realistic foundation for investment prioritization, demand\nforecasting, and long-term energy planning. The method is readily applicable to\nother building typologies or geographies, offering a scalable tool for\ndata-driven energy management under uncertainty."
                },
                "authors": [
                    {
                        "name": "Justinas Smertinas"
                    },
                    {
                        "name": "Nicolaj Hans Nielsen"
                    },
                    {
                        "name": "Matthias Y. C. Van Hove"
                    },
                    {
                        "name": "Peder Bacher"
                    },
                    {
                        "name": "Henrik Madsen"
                    }
                ],
                "author_detail": {
                    "name": "Henrik Madsen"
                },
                "author": "Henrik Madsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22315v1",
                "updated": "2025-03-28T10:43:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    43,
                    41,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T10:43:41Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    43,
                    41,
                    4,
                    87,
                    0
                ],
                "title": "Large Language Models Are Democracy Coders with Attitudes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Democracy Coders with Attitudes"
                },
                "summary": "Current political developments worldwide illustrate that research on\ndemocratic backsliding is as important as ever. A recent exchange in Political\nScience & Politics (2/2024) has highlighted again a fundamental challenge in\nthis literature: the measurement of democracy. With many democracy indicators\nconsisting of subjective assessments rather than factual observations, trends\nin democracy over time could be due to human biases in the coding of these\nindicators rather than empirical facts. In this paper, we leverage two\ncutting-edge Large Language Models (LLMs) for the coding of democracy\nindicators from the V-Dem project. With access to a huge amount of information,\nthese models may be able to rate the many \"soft\" characteristics of regimes\nwithout the cognitive biases that humans potentially possess. While\nLLM-generated codings largely align with expert coders for many countries, we\nshow that when these models deviate from human assessments, they do so in\ndifferent but consistent ways: Some LLMs are too pessimistic, while others\nconsistently overestimate the democratic quality of these countries. While the\ncombination of the two LLM codings can alleviate this concern, we conclude that\nit is difficult to replace human coders with LLMs, since the extent and\ndirection of these attitudes is not known a priori.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current political developments worldwide illustrate that research on\ndemocratic backsliding is as important as ever. A recent exchange in Political\nScience & Politics (2/2024) has highlighted again a fundamental challenge in\nthis literature: the measurement of democracy. With many democracy indicators\nconsisting of subjective assessments rather than factual observations, trends\nin democracy over time could be due to human biases in the coding of these\nindicators rather than empirical facts. In this paper, we leverage two\ncutting-edge Large Language Models (LLMs) for the coding of democracy\nindicators from the V-Dem project. With access to a huge amount of information,\nthese models may be able to rate the many \"soft\" characteristics of regimes\nwithout the cognitive biases that humans potentially possess. While\nLLM-generated codings largely align with expert coders for many countries, we\nshow that when these models deviate from human assessments, they do so in\ndifferent but consistent ways: Some LLMs are too pessimistic, while others\nconsistently overestimate the democratic quality of these countries. While the\ncombination of the two LLM codings can alleviate this concern, we conclude that\nit is difficult to replace human coders with LLMs, since the extent and\ndirection of these attitudes is not known a priori."
                },
                "authors": [
                    {
                        "name": "Nils B. Weidmann"
                    },
                    {
                        "name": "Mats Faulborn"
                    },
                    {
                        "name": "David García"
                    }
                ],
                "author_detail": {
                    "name": "David García"
                },
                "author": "David García",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22303v1",
                "updated": "2025-03-28T10:26:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    26,
                    49,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T10:26:49Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    26,
                    49,
                    4,
                    87,
                    0
                ],
                "title": "Preference-based Learning with Retrieval Augmented Generation for\n  Conversational Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based Learning with Retrieval Augmented Generation for\n  Conversational Question Answering"
                },
                "summary": "Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines."
                },
                "authors": [
                    {
                        "name": "Magdalena Kaiser"
                    },
                    {
                        "name": "Gerhard Weikum"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Weikum"
                },
                "author": "Gerhard Weikum",
                "arxiv_doi": "10.1145/3701716.3715544",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715544",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.22303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "WWW 2025 Short Paper, 5 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22289v1",
                "updated": "2025-03-28T10:04:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    4,
                    46,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T10:04:46Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    4,
                    46,
                    4,
                    87,
                    0
                ],
                "title": "Detection of Methane in the Closest Extreme Metal-poor T Dwarf WISEA\n  J181006.18-101000.5",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of Methane in the Closest Extreme Metal-poor T Dwarf WISEA\n  J181006.18-101000.5"
                },
                "summary": "WISEA J181006.18-101000.5 (WISE1810) is the nearest metal-poor ultracool\ndwarf to the Sun. It has a low effective temperature and has been classified as\nextreme early-T subdwarf. However, methane, the characteristic molecule of the\nspectral class T, was not seen in the previous low-resolution spectrum. Using\nthe 10.4-m Gran Telescopio Canarias, we collected a high-quality JHK-band\nintermediate-resolution R~5000 spectrum of WISE1810, in which a 17+/-6 ppm of\nmethane is clearly detected, while carbon monoxide is absent. Based on customly\ncomputed ATMO2020++ model, we estimated an effective temperature of 1000+/-100\nK, a high surface gravity of log g = 5.5+/-0.5 dex, a carbon abundance\n[C/H]=-1.5+/-0.2 dex, inferring [Fe/H]=-1.7+/-0.2 dex. Potassium is not seen in\nour data, and the upper limits of pseudo-equivalent width of J-band atomic\nlines are at least 25 to 60 times weaker than those measured from\nsolar-metallicity early-T counterparts. We measured a heliocentric radial\nvelocity of -83+/-13 km/s, inferring that WISE1810 is more likely a thick disk\nmember.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WISEA J181006.18-101000.5 (WISE1810) is the nearest metal-poor ultracool\ndwarf to the Sun. It has a low effective temperature and has been classified as\nextreme early-T subdwarf. However, methane, the characteristic molecule of the\nspectral class T, was not seen in the previous low-resolution spectrum. Using\nthe 10.4-m Gran Telescopio Canarias, we collected a high-quality JHK-band\nintermediate-resolution R~5000 spectrum of WISE1810, in which a 17+/-6 ppm of\nmethane is clearly detected, while carbon monoxide is absent. Based on customly\ncomputed ATMO2020++ model, we estimated an effective temperature of 1000+/-100\nK, a high surface gravity of log g = 5.5+/-0.5 dex, a carbon abundance\n[C/H]=-1.5+/-0.2 dex, inferring [Fe/H]=-1.7+/-0.2 dex. Potassium is not seen in\nour data, and the upper limits of pseudo-equivalent width of J-band atomic\nlines are at least 25 to 60 times weaker than those measured from\nsolar-metallicity early-T counterparts. We measured a heliocentric radial\nvelocity of -83+/-13 km/s, inferring that WISE1810 is more likely a thick disk\nmember."
                },
                "authors": [
                    {
                        "name": "Jerry Jun-Yan Zhang"
                    },
                    {
                        "name": "Nicolas Lodieu"
                    },
                    {
                        "name": "Eduardo L. Martín"
                    },
                    {
                        "name": "Pascal Tremblin"
                    },
                    {
                        "name": "María Rosa Zapatero Osorio"
                    },
                    {
                        "name": "Víctor J. S. Béjar"
                    },
                    {
                        "name": "Nikola Vitas"
                    },
                    {
                        "name": "Bartosz Gauza"
                    },
                    {
                        "name": "Yakiv V. Pavlenko"
                    },
                    {
                        "name": "Rafael Rebolo"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Rebolo"
                },
                "author": "Rafael Rebolo",
                "arxiv_comment": "7 pages, 2 figures in text; 5 figures in appendices. Accepted in ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22283v1",
                "updated": "2025-03-28T09:56:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    56,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T09:56:05Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    56,
                    5,
                    4,
                    87,
                    0
                ],
                "title": "BanglAssist: A Bengali-English Generative AI Chatbot for Code-Switching\n  and Dialect-Handling in Customer Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanglAssist: A Bengali-English Generative AI Chatbot for Code-Switching\n  and Dialect-Handling in Customer Service"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated exponential\nimprovements that promise transformative opportunities across various\nindustries. Their ability to generate human-like text and ensure continuous\navailability facilitates the creation of interactive service chatbots aimed at\nenhancing customer experience and streamlining enterprise operations. Despite\ntheir potential, LLMs face critical challenges, such as a susceptibility to\nhallucinations and difficulties handling complex linguistic scenarios, notably\ncode switching and dialectal variations. To address these challenges, this\npaper describes the design of a multilingual chatbot for Bengali-English\ncustomer service interactions utilizing retrieval-augmented generation (RAG)\nand targeted prompt engineering. This research provides valuable insights for\nthe human-computer interaction (HCI) community, emphasizing the importance of\ndesigning systems that accommodate linguistic diversity to benefit both\ncustomers and businesses. By addressing the intersection of generative AI and\ncultural heterogeneity, this late-breaking work inspires future innovations in\nmultilingual and multicultural HCI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated exponential\nimprovements that promise transformative opportunities across various\nindustries. Their ability to generate human-like text and ensure continuous\navailability facilitates the creation of interactive service chatbots aimed at\nenhancing customer experience and streamlining enterprise operations. Despite\ntheir potential, LLMs face critical challenges, such as a susceptibility to\nhallucinations and difficulties handling complex linguistic scenarios, notably\ncode switching and dialectal variations. To address these challenges, this\npaper describes the design of a multilingual chatbot for Bengali-English\ncustomer service interactions utilizing retrieval-augmented generation (RAG)\nand targeted prompt engineering. This research provides valuable insights for\nthe human-computer interaction (HCI) community, emphasizing the importance of\ndesigning systems that accommodate linguistic diversity to benefit both\ncustomers and businesses. By addressing the intersection of generative AI and\ncultural heterogeneity, this late-breaking work inspires future innovations in\nmultilingual and multicultural HCI."
                },
                "authors": [
                    {
                        "name": "Francesco Kruk"
                    },
                    {
                        "name": "Savindu Herath"
                    },
                    {
                        "name": "Prithwiraj Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Prithwiraj Choudhury"
                },
                "author": "Prithwiraj Choudhury",
                "arxiv_comment": "Accepted at the 2025 Conference on Human Factors in Computing Systems\n  (CHI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21419v2",
                "updated": "2025-03-28T09:44:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    44,
                    42,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-27T12:09:04Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    9,
                    4,
                    3,
                    86,
                    0
                ],
                "title": "Neuroplasticity in Artificial Intelligence -- An Overview and\n  Inspirations on Drop In & Out Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuroplasticity in Artificial Intelligence -- An Overview and\n  Inspirations on Drop In & Out Learning"
                },
                "summary": "Artificial Intelligence (AI) has achieved new levels of performance and\nspread in public usage with the rise of deep neural networks (DNNs). Initially\ninspired by human neurons and their connections, NNs have become the foundation\nof AI models for many advanced architectures. However, some of the most\nintegral processes in the human brain, particularly neurogenesis and\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\nignored in DNN architecture design. Instead, contemporary AI development\npredominantly focuses on constructing advanced frameworks, such as large\nlanguage models, which retain a static structure of neural connections during\ntraining and inference. In this light, we explore how neurogenesis,\nneuroapoptosis, and neuroplasticity can inspire future AI advances.\nSpecifically, we examine analogous activities in artificial NNs, introducing\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\nstructural pruning for neuroapoptosis. We additionally suggest neuroplasticity\ncombining the two for future large NNs in ``life-long learning'' settings\nfollowing the biological inspiration. We conclude by advocating for greater\nresearch efforts in this interdisciplinary domain and identifying promising\ndirections for future exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has achieved new levels of performance and\nspread in public usage with the rise of deep neural networks (DNNs). Initially\ninspired by human neurons and their connections, NNs have become the foundation\nof AI models for many advanced architectures. However, some of the most\nintegral processes in the human brain, particularly neurogenesis and\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\nignored in DNN architecture design. Instead, contemporary AI development\npredominantly focuses on constructing advanced frameworks, such as large\nlanguage models, which retain a static structure of neural connections during\ntraining and inference. In this light, we explore how neurogenesis,\nneuroapoptosis, and neuroplasticity can inspire future AI advances.\nSpecifically, we examine analogous activities in artificial NNs, introducing\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\nstructural pruning for neuroapoptosis. We additionally suggest neuroplasticity\ncombining the two for future large NNs in ``life-long learning'' settings\nfollowing the biological inspiration. We conclude by advocating for greater\nresearch efforts in this interdisciplinary domain and identifying promising\ndirections for future exploration."
                },
                "authors": [
                    {
                        "name": "Yupei Li"
                    },
                    {
                        "name": "Manuel Milling"
                    },
                    {
                        "name": "Björn W. Schuller"
                    }
                ],
                "author_detail": {
                    "name": "Björn W. Schuller"
                },
                "author": "Björn W. Schuller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22275v1",
                "updated": "2025-03-28T09:43:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    43,
                    47,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T09:43:47Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    43,
                    47,
                    4,
                    87,
                    0
                ],
                "title": "Make Some Noise: Towards LLM audio reasoning and generation using sound\n  tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Some Noise: Towards LLM audio reasoning and generation using sound\n  tokens"
                },
                "summary": "Integrating audio comprehension and generation into large language models\n(LLMs) remains challenging due to the continuous nature of audio and the\nresulting high sampling rates. Here, we introduce a novel approach that\ncombines Variational Quantization with Conditional Flow Matching to convert\naudio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless\nintegration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM\nusing Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true\nmultimodal capabilities, i.e., audio comprehension and generation. Our\ntokenizer outperforms a traditional VQ-VAE across various datasets with diverse\nacoustic events. Despite the substantial loss of fine-grained details through\naudio tokenization, our multimodal LLM trained with discrete tokens achieves\ncompetitive results in audio comprehension with state-of-the-art methods,\nthough audio generation is poor. Our results highlight the need for larger,\nmore diverse datasets and improved evaluation metrics to advance multimodal LLM\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating audio comprehension and generation into large language models\n(LLMs) remains challenging due to the continuous nature of audio and the\nresulting high sampling rates. Here, we introduce a novel approach that\ncombines Variational Quantization with Conditional Flow Matching to convert\naudio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless\nintegration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM\nusing Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true\nmultimodal capabilities, i.e., audio comprehension and generation. Our\ntokenizer outperforms a traditional VQ-VAE across various datasets with diverse\nacoustic events. Despite the substantial loss of fine-grained details through\naudio tokenization, our multimodal LLM trained with discrete tokens achieves\ncompetitive results in audio comprehension with state-of-the-art methods,\nthough audio generation is poor. Our results highlight the need for larger,\nmore diverse datasets and improved evaluation metrics to advance multimodal LLM\nperformance."
                },
                "authors": [
                    {
                        "name": "Shivam Mehta"
                    },
                    {
                        "name": "Nebojsa Jojic"
                    },
                    {
                        "name": "Hannes Gamper"
                    }
                ],
                "author_detail": {
                    "name": "Hannes Gamper"
                },
                "author": "Hannes Gamper",
                "arxiv_doi": "10.1109/ICASSP49660.2025.10888809",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICASSP49660.2025.10888809",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.22275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 2 figures, Accepted at ICASSP 2025",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; H.5.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22250v1",
                "updated": "2025-03-28T09:04:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    4,
                    10,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T09:04:10Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    4,
                    10,
                    4,
                    87,
                    0
                ],
                "title": "Beyond the Script: Testing LLMs for Authentic Patient Communication\n  Styles in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Script: Testing LLMs for Authentic Patient Communication\n  Styles in Healthcare"
                },
                "summary": "Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training."
                },
                "authors": [
                    {
                        "name": "Anna Bodonhelyi"
                    },
                    {
                        "name": "Christian Stegemann-Philipps"
                    },
                    {
                        "name": "Alessandra Sonanini"
                    },
                    {
                        "name": "Lea Herschbach"
                    },
                    {
                        "name": "Márton Szép"
                    },
                    {
                        "name": "Anne Herrmann-Werner"
                    },
                    {
                        "name": "Teresa Festl-Wietek"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Friederike Holderried"
                    }
                ],
                "author_detail": {
                    "name": "Friederike Holderried"
                },
                "author": "Friederike Holderried",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22241v1",
                "updated": "2025-03-28T08:45:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    45,
                    15,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T08:45:15Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    45,
                    15,
                    4,
                    87,
                    0
                ],
                "title": "Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs"
                },
                "summary": "Personalized multiple clustering aims to generate diverse partitions of a\ndataset based on different user-specific aspects, rather than a single\nclustering. It has recently drawn research interest for accommodating varying\nuser preferences. Recent approaches primarily use CLIP embeddings with proxy\nlearning to extract representations biased toward user clustering preferences.\nHowever, CLIP primarily focuses on coarse image-text alignment, lacking a deep\ncontextual understanding of user interests. To overcome these limitations, we\npropose an agent-centric personalized clustering framework that leverages\nmulti-modal large language models (MLLMs) as agents to comprehensively traverse\na relational graph to search for clusters based on user interests. Due to the\nadvanced reasoning mechanism of MLLMs, the obtained clusters align more closely\nwith user-defined criteria than those obtained from CLIP-based representations.\nTo reduce computational overhead, we shorten the agents' traversal path by\nconstructing a relational graph using user-interest-biased embeddings extracted\nby MLLMs. A large number of weakly connected edges can be filtered out based on\nembedding similarity, facilitating an efficient traversal search for agents.\nExperimental results show that the proposed method achieves NMI scores of\n0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively,\nlargely improving the SOTA model by over 140%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized multiple clustering aims to generate diverse partitions of a\ndataset based on different user-specific aspects, rather than a single\nclustering. It has recently drawn research interest for accommodating varying\nuser preferences. Recent approaches primarily use CLIP embeddings with proxy\nlearning to extract representations biased toward user clustering preferences.\nHowever, CLIP primarily focuses on coarse image-text alignment, lacking a deep\ncontextual understanding of user interests. To overcome these limitations, we\npropose an agent-centric personalized clustering framework that leverages\nmulti-modal large language models (MLLMs) as agents to comprehensively traverse\na relational graph to search for clusters based on user interests. Due to the\nadvanced reasoning mechanism of MLLMs, the obtained clusters align more closely\nwith user-defined criteria than those obtained from CLIP-based representations.\nTo reduce computational overhead, we shorten the agents' traversal path by\nconstructing a relational graph using user-interest-biased embeddings extracted\nby MLLMs. A large number of weakly connected edges can be filtered out based on\nembedding similarity, facilitating an efficient traversal search for agents.\nExperimental results show that the proposed method achieves NMI scores of\n0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively,\nlargely improving the SOTA model by over 140%."
                },
                "authors": [
                    {
                        "name": "Ziye Chen"
                    },
                    {
                        "name": "Yiqun Duan"
                    },
                    {
                        "name": "Riheng Zhu"
                    },
                    {
                        "name": "Zhenbang Sun"
                    },
                    {
                        "name": "Mingming Gong"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Gong"
                },
                "author": "Mingming Gong",
                "arxiv_comment": "10 pages, 7 figures, in submission to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T05, 05C82",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20466v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20466v2",
                "updated": "2025-03-28T08:41:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    41,
                    47,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-26T11:51:23Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    51,
                    23,
                    2,
                    85,
                    0
                ],
                "title": "Data-driven Seasonal Climate Predictions via Variational Inference and\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Seasonal Climate Predictions via Variational Inference and\n  Transformers"
                },
                "summary": "Most operational climate services providers base their seasonal predictions\non initialised general circulation models (GCMs) or statistical techniques that\nfit past observations. GCMs require substantial computational resources, which\nlimits their capacity. In contrast, statistical methods often lack robustness\ndue to short historical records. Recent works propose machine learning methods\ntrained on climate model output, leveraging larger sample sizes and simulated\nscenarios. Yet, many of these studies focus on prediction tasks that might be\nrestricted in spatial extent or temporal coverage, opening a gap with existing\noperational predictions. Thus, the present study evaluates the effectiveness of\na methodology that combines variational inference with transformer models to\npredict fields of seasonal anomalies. The predictions cover all four seasons\nand are initialised one month before the start of each season. The model was\ntrained on climate model output from CMIP6 and tested using ERA5 reanalysis\ndata. We analyse the method's performance in predicting interannual anomalies\nbeyond the climate change-induced trend. We also test the proposed methodology\nin a regional context with a use case focused on Europe. While climate change\ntrends dominate the skill of temperature predictions, the method presents\nadditional skill over the climatological forecast in regions influenced by\nknown teleconnections. We reach similar conclusions based on the validation of\nprecipitation predictions. Despite underperforming SEAS5 in most tropics, our\nmodel offers added value in numerous extratropical inland regions. This work\ndemonstrates the effectiveness of training generative models on climate model\noutput for seasonal predictions, providing skilful predictions beyond the\ninduced climate change trend at time scales and lead times relevant for user\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most operational climate services providers base their seasonal predictions\non initialised general circulation models (GCMs) or statistical techniques that\nfit past observations. GCMs require substantial computational resources, which\nlimits their capacity. In contrast, statistical methods often lack robustness\ndue to short historical records. Recent works propose machine learning methods\ntrained on climate model output, leveraging larger sample sizes and simulated\nscenarios. Yet, many of these studies focus on prediction tasks that might be\nrestricted in spatial extent or temporal coverage, opening a gap with existing\noperational predictions. Thus, the present study evaluates the effectiveness of\na methodology that combines variational inference with transformer models to\npredict fields of seasonal anomalies. The predictions cover all four seasons\nand are initialised one month before the start of each season. The model was\ntrained on climate model output from CMIP6 and tested using ERA5 reanalysis\ndata. We analyse the method's performance in predicting interannual anomalies\nbeyond the climate change-induced trend. We also test the proposed methodology\nin a regional context with a use case focused on Europe. While climate change\ntrends dominate the skill of temperature predictions, the method presents\nadditional skill over the climatological forecast in regions influenced by\nknown teleconnections. We reach similar conclusions based on the validation of\nprecipitation predictions. Despite underperforming SEAS5 in most tropics, our\nmodel offers added value in numerous extratropical inland regions. This work\ndemonstrates the effectiveness of training generative models on climate model\noutput for seasonal predictions, providing skilful predictions beyond the\ninduced climate change trend at time scales and lead times relevant for user\napplications."
                },
                "authors": [
                    {
                        "name": "Lluís Palma"
                    },
                    {
                        "name": "Alejandro Peraza"
                    },
                    {
                        "name": "David Civantos"
                    },
                    {
                        "name": "Amanda Duarte"
                    },
                    {
                        "name": "Stefano Materia"
                    },
                    {
                        "name": "Ángel G. Muñoz"
                    },
                    {
                        "name": "Jesús Peña-Izquierdo"
                    },
                    {
                        "name": "Laia Romero"
                    },
                    {
                        "name": "Albert Soret"
                    },
                    {
                        "name": "Markus G. Donat"
                    }
                ],
                "author_detail": {
                    "name": "Markus G. Donat"
                },
                "author": "Markus G. Donat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20466v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20466v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22238v1",
                "updated": "2025-03-28T08:41:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    41,
                    43,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T08:41:43Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    41,
                    43,
                    4,
                    87,
                    0
                ],
                "title": "Integrating LLMs in Software Engineering Education: Motivators,\n  Demotivators, and a Roadmap Towards a Framework for Finnish Higher Education\n  Institutes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLMs in Software Engineering Education: Motivators,\n  Demotivators, and a Roadmap Towards a Framework for Finnish Higher Education\n  Institutes"
                },
                "summary": "The increasing adoption of Large Language Models (LLMs) in software\nengineering education presents both opportunities and challenges. While LLMs\noffer benefits such as enhanced learning experiences, automated assessments,\nand personalized tutoring, their integration also raises concerns about\nacademic integrity, student over-reliance, and ethical considerations. In this\nstudy, we conducted a preliminary literature review to identify motivators and\ndemotivators for using LLMs in software engineering education. We applied a\nthematic mapping process to categorize and structure these factors (motivators\nand demotivators), offering a comprehensive view of their impact. In total, we\nidentified 25 motivators and 30 demotivators, which are further organized into\nfour high-level themes. This mapping provides a structured framework for\nunderstanding the factors that influence the integration of LLMs in software\nengineering education, both positively and negatively. As part of a larger\nresearch project, this study serves as a feasibility assessment, laying the\ngroundwork for future systematic literature review and empirical studies.\nUltimately, this project aims to develop a framework to assist Finnish higher\neducation institutions in effectively integrating LLMs into software\nengineering education while addressing potential risks and challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of Large Language Models (LLMs) in software\nengineering education presents both opportunities and challenges. While LLMs\noffer benefits such as enhanced learning experiences, automated assessments,\nand personalized tutoring, their integration also raises concerns about\nacademic integrity, student over-reliance, and ethical considerations. In this\nstudy, we conducted a preliminary literature review to identify motivators and\ndemotivators for using LLMs in software engineering education. We applied a\nthematic mapping process to categorize and structure these factors (motivators\nand demotivators), offering a comprehensive view of their impact. In total, we\nidentified 25 motivators and 30 demotivators, which are further organized into\nfour high-level themes. This mapping provides a structured framework for\nunderstanding the factors that influence the integration of LLMs in software\nengineering education, both positively and negatively. As part of a larger\nresearch project, this study serves as a feasibility assessment, laying the\ngroundwork for future systematic literature review and empirical studies.\nUltimately, this project aims to develop a framework to assist Finnish higher\neducation institutions in effectively integrating LLMs into software\nengineering education while addressing potential risks and challenges."
                },
                "authors": [
                    {
                        "name": "Maryam Khan"
                    },
                    {
                        "name": "Muhammad Azeem Akbar"
                    },
                    {
                        "name": "Jussi Kasurinen"
                    }
                ],
                "author_detail": {
                    "name": "Jussi Kasurinen"
                },
                "author": "Jussi Kasurinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08127v2",
                "updated": "2025-03-28T08:33:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    33,
                    36,
                    4,
                    87,
                    0
                ],
                "published": "2025-02-12T05:13:04Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    13,
                    4,
                    2,
                    43,
                    0
                ],
                "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance"
                },
                "summary": "While large language models (LLMs) have shown strong general reasoning\ncapabilities, their effectiveness in financial reasoning, which is crucial for\nreal-world financial applications remains underexplored. In this study, we\nconduct a comprehensive evaluation of 24 state-of-the-art general and\nreasoning-focused LLMs across four complex financial reasoning tasks involving\nfinancial text, tabular data, and equations. We assess key capabilities such as\nnumerical reasoning, tabular interpretation, financial terminology\ncomprehension, long-context understanding, and equation-based problem solving.\nOur analysis reveals that while data quality and pretraining contribute to\nperformance, general techniques like chain-of-thought (CoT) fine-tuning offer\nlimited gains in financial tasks. To address this, we propose two\ndomain-adapted models, Fino1-8B and Fino1-14B, trained with CoT fine-tuning and\nreinforcement learning using domain-specific reasoning paths. Our models are\ntrained on a carefully curated dataset integrating high-quality examples from\ndiverse sources, covering financial reports, tables, equations, and structured\nXBRL texts. Despite limited training data, they achieve an 7-9% performance\nimprovement, outperforming several advanced LLMs, including GPT-o1,\nGPT-o3-mini, GPT-4.5, and comparable with DeepSeek models (V3 and R1),\ndemonstrating strong practical value in resource, constrained scenarios. Our\nfindings highlight the need for domain-specific adaptations in financial\nreasoning, and we release all datasets, models, and code for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown strong general reasoning\ncapabilities, their effectiveness in financial reasoning, which is crucial for\nreal-world financial applications remains underexplored. In this study, we\nconduct a comprehensive evaluation of 24 state-of-the-art general and\nreasoning-focused LLMs across four complex financial reasoning tasks involving\nfinancial text, tabular data, and equations. We assess key capabilities such as\nnumerical reasoning, tabular interpretation, financial terminology\ncomprehension, long-context understanding, and equation-based problem solving.\nOur analysis reveals that while data quality and pretraining contribute to\nperformance, general techniques like chain-of-thought (CoT) fine-tuning offer\nlimited gains in financial tasks. To address this, we propose two\ndomain-adapted models, Fino1-8B and Fino1-14B, trained with CoT fine-tuning and\nreinforcement learning using domain-specific reasoning paths. Our models are\ntrained on a carefully curated dataset integrating high-quality examples from\ndiverse sources, covering financial reports, tables, equations, and structured\nXBRL texts. Despite limited training data, they achieve an 7-9% performance\nimprovement, outperforming several advanced LLMs, including GPT-o1,\nGPT-o3-mini, GPT-4.5, and comparable with DeepSeek models (V3 and R1),\ndemonstrating strong practical value in resource, constrained scenarios. Our\nfindings highlight the need for domain-specific adaptations in financial\nreasoning, and we release all datasets, models, and code for future research."
                },
                "authors": [
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Weipeng Zhou"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Han Yi"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Jianyun Nie"
                    }
                ],
                "author_detail": {
                    "name": "Jianyun Nie"
                },
                "author": "Jianyun Nie",
                "arxiv_comment": "13 pages, 2 figures, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22215v1",
                "updated": "2025-03-28T08:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    4,
                    51,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T08:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    4,
                    51,
                    4,
                    87,
                    0
                ],
                "title": "Learning to Instruct for Visual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Instruct for Visual Instruction Tuning"
                },
                "summary": "We propose LIT, an advancement of visual instruction tuning (VIT). While VIT\nequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, the\ncurrent design choices for VIT often result in overfitting and shortcut\nlearning, potentially degrading performance. This gap arises from an\noveremphasis on instruction-following abilities, while neglecting the proactive\nunderstanding of visual information. Inspired by this, LIT adopts a simple yet\neffective approach by incorporating the loss function into both the instruction\nand response sequences. It seamlessly expands the training data, and\nregularizes the MLLMs from overly relying on language priors. Based on this\nmerit, LIT achieves a significant relative improvement of up to 9% on\ncomprehensive multimodal benchmarks, requiring no additional training data and\nincurring negligible computational overhead. Surprisingly, LIT attains\nexceptional fundamental visual capabilities, yielding up to an 18% improvement\nin captioning performance, while simultaneously alleviating hallucination in\nMLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose LIT, an advancement of visual instruction tuning (VIT). While VIT\nequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, the\ncurrent design choices for VIT often result in overfitting and shortcut\nlearning, potentially degrading performance. This gap arises from an\noveremphasis on instruction-following abilities, while neglecting the proactive\nunderstanding of visual information. Inspired by this, LIT adopts a simple yet\neffective approach by incorporating the loss function into both the instruction\nand response sequences. It seamlessly expands the training data, and\nregularizes the MLLMs from overly relying on language priors. Based on this\nmerit, LIT achieves a significant relative improvement of up to 9% on\ncomprehensive multimodal benchmarks, requiring no additional training data and\nincurring negligible computational overhead. Surprisingly, LIT attains\nexceptional fundamental visual capabilities, yielding up to an 18% improvement\nin captioning performance, while simultaneously alleviating hallucination in\nMLLMs."
                },
                "authors": [
                    {
                        "name": "Zhihan Zhou"
                    },
                    {
                        "name": "Feng Hong"
                    },
                    {
                        "name": "Jiaan Luo"
                    },
                    {
                        "name": "Jiangchao Yao"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Wang"
                },
                "author": "Yanfeng Wang",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14883v2",
                "updated": "2025-03-28T07:56:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    56,
                    4,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-19T04:21:38Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    21,
                    38,
                    2,
                    78,
                    0
                ],
                "title": "Envisioning an AI-Enhanced Mental Health Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Envisioning an AI-Enhanced Mental Health Ecosystem"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs), reasoning models, and\nagentic AI approaches coincides with a growing global mental health crisis,\nwhere increasing demand has not translated into adequate access to professional\nsupport, particularly for underserved populations. This presents a unique\nopportunity for AI to complement human-led interventions, offering scalable and\ncontext-aware support while preserving human connection in this sensitive\ndomain. We explore various AI applications in peer support, self-help\ninterventions, proactive monitoring, and data-driven insights, using a\nhuman-centred approach that ensures AI supports rather than replaces human\ninteraction. However, AI deployment in mental health fields presents challenges\nsuch as ethical concerns, transparency, privacy risks, and risks of\nover-reliance. We propose a hybrid ecosystem where where AI assists but does\nnot replace human providers, emphasising responsible deployment and evaluation.\nWe also present some of our early work and findings in several of these AI\napplications. Finally, we outline future research directions for refining\nAI-enhanced interventions while adhering to ethical and culturally sensitive\nguidelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs), reasoning models, and\nagentic AI approaches coincides with a growing global mental health crisis,\nwhere increasing demand has not translated into adequate access to professional\nsupport, particularly for underserved populations. This presents a unique\nopportunity for AI to complement human-led interventions, offering scalable and\ncontext-aware support while preserving human connection in this sensitive\ndomain. We explore various AI applications in peer support, self-help\ninterventions, proactive monitoring, and data-driven insights, using a\nhuman-centred approach that ensures AI supports rather than replaces human\ninteraction. However, AI deployment in mental health fields presents challenges\nsuch as ethical concerns, transparency, privacy risks, and risks of\nover-reliance. We propose a hybrid ecosystem where where AI assists but does\nnot replace human providers, emphasising responsible deployment and evaluation.\nWe also present some of our early work and findings in several of these AI\napplications. Finally, we outline future research directions for refining\nAI-enhanced interventions while adhering to ethical and culturally sensitive\nguidelines."
                },
                "authors": [
                    {
                        "name": "Kellie Yu Hui Sim"
                    },
                    {
                        "name": "Kenny Tsu Wei Choo"
                    }
                ],
                "author_detail": {
                    "name": "Kenny Tsu Wei Choo"
                },
                "author": "Kenny Tsu Wei Choo",
                "arxiv_comment": "5 pages, 0 figures, accepted to the CHI'25 Envisioning the Future of\n  Interactive Health Workshop, to be published in HAL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22181v1",
                "updated": "2025-03-28T06:54:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    54,
                    44,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T06:54:44Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    54,
                    44,
                    4,
                    87,
                    0
                ],
                "title": "e-person Architecture and Framework for Human-AI Co-adventure\n  Relationship",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "e-person Architecture and Framework for Human-AI Co-adventure\n  Relationship"
                },
                "summary": "This paper proposes the e-person architecture for constructing a unified and\nincremental development of AI ethics. The e-person architecture takes the\nreduction of uncertainty through collaborative cognition and action with others\nas a unified basis for ethics. By classifying and defining uncertainty along\ntwo axes - (1) first, second, and third person perspectives, and (2) the\ndifficulty of inference based on the depth of information - we support the\ndevelopment of unified and incremental development of AI ethics. In addition,\nwe propose the e-person framework based on the free energy principle, which\nconsiders the reduction of uncertainty as a unifying principle of brain\nfunction, with the aim of implementing the e-person architecture, and we show\nour previous works and future challenges based on the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes the e-person architecture for constructing a unified and\nincremental development of AI ethics. The e-person architecture takes the\nreduction of uncertainty through collaborative cognition and action with others\nas a unified basis for ethics. By classifying and defining uncertainty along\ntwo axes - (1) first, second, and third person perspectives, and (2) the\ndifficulty of inference based on the depth of information - we support the\ndevelopment of unified and incremental development of AI ethics. In addition,\nwe propose the e-person framework based on the free energy principle, which\nconsiders the reduction of uncertainty as a unifying principle of brain\nfunction, with the aim of implementing the e-person architecture, and we show\nour previous works and future challenges based on the proposed framework."
                },
                "authors": [
                    {
                        "name": "Kanako Esaki"
                    },
                    {
                        "name": "Tadayuki Matsumura"
                    },
                    {
                        "name": "Yang Shao"
                    },
                    {
                        "name": "Hiroyuki Mizuno"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Mizuno"
                },
                "author": "Hiroyuki Mizuno",
                "arxiv_comment": "24 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22174v1",
                "updated": "2025-03-28T06:27:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    27,
                    55,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T06:27:55Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    27,
                    55,
                    4,
                    87,
                    0
                ],
                "title": "Synergistic Bleeding Region and Point Detection in Surgical Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Bleeding Region and Point Detection in Surgical Videos"
                },
                "summary": "Intraoperative bleeding in laparoscopic surgery causes rapid obscuration of\nthe operative field to hinder the surgical process. Intelligent detection of\nbleeding regions can quantify the blood loss to assist decision-making, while\nlocating the bleeding point helps surgeons quickly identify the source of\nbleeding and achieve hemostasis in time. In this study, we first construct a\nreal-world surgical bleeding detection dataset, named SurgBlood, comprising\n5,330 frames from 95 surgical video clips with bleeding region and point\nannotations. Accordingly, we develop a dual-task synergistic online detector\ncalled BlooDet, designed to perform simultaneous detection of bleeding regions\nand points in surgical videos. Our framework embraces a dual-branch\nbidirectional guidance design based on Segment Anything Model 2 (SAM 2). The\nmask branch detects bleeding regions through adaptive edge and point prompt\nembeddings, while the point branch leverages mask memory to induce bleeding\npoint memory modeling and captures the direction of bleed point movement\nthrough inter-frame optical flow. By interactive guidance and prompts, the two\nbranches explore potential spatial-temporal relationships while leveraging\nmemory modeling from previous frames to infer the current bleeding condition.\nExtensive experiments demonstrate that our approach outperforms other\ncounterparts on SurgBlood in both bleeding region and point detection tasks,\ne.g., achieving 64.88% IoU for bleeding region detection and 83.69% PCK-10% for\nbleeding point detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intraoperative bleeding in laparoscopic surgery causes rapid obscuration of\nthe operative field to hinder the surgical process. Intelligent detection of\nbleeding regions can quantify the blood loss to assist decision-making, while\nlocating the bleeding point helps surgeons quickly identify the source of\nbleeding and achieve hemostasis in time. In this study, we first construct a\nreal-world surgical bleeding detection dataset, named SurgBlood, comprising\n5,330 frames from 95 surgical video clips with bleeding region and point\nannotations. Accordingly, we develop a dual-task synergistic online detector\ncalled BlooDet, designed to perform simultaneous detection of bleeding regions\nand points in surgical videos. Our framework embraces a dual-branch\nbidirectional guidance design based on Segment Anything Model 2 (SAM 2). The\nmask branch detects bleeding regions through adaptive edge and point prompt\nembeddings, while the point branch leverages mask memory to induce bleeding\npoint memory modeling and captures the direction of bleed point movement\nthrough inter-frame optical flow. By interactive guidance and prompts, the two\nbranches explore potential spatial-temporal relationships while leveraging\nmemory modeling from previous frames to infer the current bleeding condition.\nExtensive experiments demonstrate that our approach outperforms other\ncounterparts on SurgBlood in both bleeding region and point detection tasks,\ne.g., achieving 64.88% IoU for bleeding region detection and 83.69% PCK-10% for\nbleeding point detection."
                },
                "authors": [
                    {
                        "name": "Jialun Pei"
                    },
                    {
                        "name": "Zhangjun Zhou"
                    },
                    {
                        "name": "Diandian Guo"
                    },
                    {
                        "name": "Zhixi Li"
                    },
                    {
                        "name": "Jing Qin"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22166v1",
                "updated": "2025-03-28T06:11:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    11,
                    4,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T06:11:04Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    11,
                    4,
                    4,
                    87,
                    0
                ],
                "title": "Reasoning of Large Language Models over Knowledge Graphs with\n  Super-Relations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning of Large Language Models over Knowledge Graphs with\n  Super-Relations"
                },
                "summary": "While large language models (LLMs) have made significant progress in\nprocessing and reasoning over knowledge graphs, current methods suffer from a\nhigh non-retrieval rate. This limitation reduces the accuracy of answering\nquestions based on these graphs. Our analysis reveals that the combination of\ngreedy search and forward reasoning is a major contributor to this issue. To\novercome these challenges, we introduce the concept of super-relations, which\nenables both forward and backward reasoning by summarizing and connecting\nvarious relational paths within the graph. This holistic approach not only\nexpands the search space, but also significantly improves retrieval efficiency.\nIn this paper, we propose the ReKnoS framework, which aims to Reason over\nKnowledge Graphs with Super-Relations. Our framework's key advantages include\nthe inclusion of multiple relation paths through super-relations, enhanced\nforward and backward reasoning capabilities, and increased efficiency in\nquerying LLMs. These enhancements collectively lead to a substantial\nimprovement in the successful retrieval rate and overall reasoning performance.\nWe conduct extensive experiments on nine real-world datasets to evaluate\nReKnoS, and the results demonstrate the superior performance of ReKnoS over\nexisting state-of-the-art baselines, with an average accuracy gain of 2.92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have made significant progress in\nprocessing and reasoning over knowledge graphs, current methods suffer from a\nhigh non-retrieval rate. This limitation reduces the accuracy of answering\nquestions based on these graphs. Our analysis reveals that the combination of\ngreedy search and forward reasoning is a major contributor to this issue. To\novercome these challenges, we introduce the concept of super-relations, which\nenables both forward and backward reasoning by summarizing and connecting\nvarious relational paths within the graph. This holistic approach not only\nexpands the search space, but also significantly improves retrieval efficiency.\nIn this paper, we propose the ReKnoS framework, which aims to Reason over\nKnowledge Graphs with Super-Relations. Our framework's key advantages include\nthe inclusion of multiple relation paths through super-relations, enhanced\nforward and backward reasoning capabilities, and increased efficiency in\nquerying LLMs. These enhancements collectively lead to a substantial\nimprovement in the successful retrieval rate and overall reasoning performance.\nWe conduct extensive experiments on nine real-world datasets to evaluate\nReKnoS, and the results demonstrate the superior performance of ReKnoS over\nexisting state-of-the-art baselines, with an average accuracy gain of 2.92%."
                },
                "authors": [
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Junhong Lin"
                    },
                    {
                        "name": "Xiaojie Guo"
                    },
                    {
                        "name": "Julian Shun"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Yada Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yada Zhu"
                },
                "author": "Yada Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22165v1",
                "updated": "2025-03-28T06:09:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    9,
                    51,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T06:09:51Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    9,
                    51,
                    4,
                    87,
                    0
                ],
                "title": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models"
                },
                "summary": "Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts-the first\nvisualization tool for users to inspect the reasoning paths of chain-of-thought\nand its derivatives on any multi-choice dataset. Specifically, we represent the\nstates in a reasoning path as feature vectors that quantify their distances to\nall answer choices. These features are then visualized in two-dimensional plots\nusing t-SNE. Qualitative and quantitative analysis with the landscape of\nthoughts effectively distinguishes between strong and weak models, correct and\nincorrect answers, as well as different reasoning tasks. It also uncovers\nundesirable reasoning patterns, such as low consistency and high uncertainty.\nAdditionally, users can adapt our tool to a model that predicts the property\nthey observe. We showcase this advantage by adapting our tool to a lightweight\nverifier that evaluates the correctness of reasoning paths. The code is\npublicly available at: https://github.com/tmlr-group/landscape-of-thoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts-the first\nvisualization tool for users to inspect the reasoning paths of chain-of-thought\nand its derivatives on any multi-choice dataset. Specifically, we represent the\nstates in a reasoning path as feature vectors that quantify their distances to\nall answer choices. These features are then visualized in two-dimensional plots\nusing t-SNE. Qualitative and quantitative analysis with the landscape of\nthoughts effectively distinguishes between strong and weak models, correct and\nincorrect answers, as well as different reasoning tasks. It also uncovers\nundesirable reasoning patterns, such as low consistency and high uncertainty.\nAdditionally, users can adapt our tool to a model that predicts the property\nthey observe. We showcase this advantage by adapting our tool to a lightweight\nverifier that evaluates the correctness of reasoning paths. The code is\npublicly available at: https://github.com/tmlr-group/landscape-of-thoughts."
                },
                "authors": [
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Zhaocheng Zhu"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Mikhail Galkin"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06220v2",
                "updated": "2025-03-28T06:08:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    8,
                    3,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-08T13:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    13,
                    44,
                    38,
                    5,
                    67,
                    0
                ],
                "title": "StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through\n  Event-Gated Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through\n  Event-Gated Cognition"
                },
                "summary": "With the rise of real-world human-AI interaction applications, such as AI\nassistants, the need for Streaming Video Dialogue is critical. To address this\nneed, we introduce StreamMind, a video LLM framework that achieves ultra-FPS\nstreaming video processing (100 fps on a single A100) and enables proactive,\nalways-on responses in real time, without explicit user intervention.\n  To solve the key challenge of the contradiction between linear video\nstreaming speed and quadratic transformer computation cost, we propose a novel\nperception-cognition interleaving paradigm named ''event-gated LLM\ninvocation'', in contrast to the existing per-time-step LLM invocation. By\nintroducing a Cognition Gate network between the video encoder and the LLM, LLM\nis only invoked when relevant events occur. To realize the event feature\nextraction with constant cost, we propose Event-Preserving Feature Extractor\n(EPFE) based on state-space method, generating a single perception token for\nspatiotemporal features. These techniques enable the video LLM with full-FPS\nperception and real-time cognition response.\n  Experiments on Ego4D and SoccerNet streaming tasks, as well as standard\noffline benchmarks, demonstrate state-of-the-art performance in both model\ncapability and real-time efficiency, paving the way for ultra-high-FPS\napplications, such as Game AI and interactive media. The code and data is\navailable at https://aka.ms/StreamMind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of real-world human-AI interaction applications, such as AI\nassistants, the need for Streaming Video Dialogue is critical. To address this\nneed, we introduce StreamMind, a video LLM framework that achieves ultra-FPS\nstreaming video processing (100 fps on a single A100) and enables proactive,\nalways-on responses in real time, without explicit user intervention.\n  To solve the key challenge of the contradiction between linear video\nstreaming speed and quadratic transformer computation cost, we propose a novel\nperception-cognition interleaving paradigm named ''event-gated LLM\ninvocation'', in contrast to the existing per-time-step LLM invocation. By\nintroducing a Cognition Gate network between the video encoder and the LLM, LLM\nis only invoked when relevant events occur. To realize the event feature\nextraction with constant cost, we propose Event-Preserving Feature Extractor\n(EPFE) based on state-space method, generating a single perception token for\nspatiotemporal features. These techniques enable the video LLM with full-FPS\nperception and real-time cognition response.\n  Experiments on Ego4D and SoccerNet streaming tasks, as well as standard\noffline benchmarks, demonstrate state-of-the-art performance in both model\ncapability and real-time efficiency, paving the way for ultra-high-FPS\napplications, such as Game AI and interactive media. The code and data is\navailable at https://aka.ms/StreamMind."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Donglin Bai"
                    },
                    {
                        "name": "Zhibo Chen"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22164v1",
                "updated": "2025-03-28T06:02:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    2,
                    53,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T06:02:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    2,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents"
                },
                "summary": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management."
                },
                "authors": [
                    {
                        "name": "Bowen Gao"
                    },
                    {
                        "name": "Yanwen Huang"
                    },
                    {
                        "name": "Yiqiao Liu"
                    },
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yanyan Lan"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Lan"
                },
                "author": "Yanyan Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22161v1",
                "updated": "2025-03-28T05:54:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    5,
                    54,
                    17,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T05:54:17Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    5,
                    54,
                    17,
                    4,
                    87,
                    0
                ],
                "title": "Traffic Modeling for Network Security and Privacy: Challenges Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic Modeling for Network Security and Privacy: Challenges Ahead"
                },
                "summary": "Traffic analysis using machine learning and deep learning models has made\nsignificant progress over the past decades. These models address various tasks\nin network security and privacy, including detection of anomalies and attacks,\ncountering censorship, etc. They also reveal privacy risks to users as\ndemonstrated by the research on LLM token inference as well as fingerprinting\n(and counter-fingerprinting) of user-visiting websites, IoT devices, and\ndifferent applications. However, challenges remain in securing our networks\nfrom threats and attacks. After briefly reviewing the tasks and recent ML\nmodels in network security and privacy, we discuss the challenges that lie\nahead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic analysis using machine learning and deep learning models has made\nsignificant progress over the past decades. These models address various tasks\nin network security and privacy, including detection of anomalies and attacks,\ncountering censorship, etc. They also reveal privacy risks to users as\ndemonstrated by the research on LLM token inference as well as fingerprinting\n(and counter-fingerprinting) of user-visiting websites, IoT devices, and\ndifferent applications. However, challenges remain in securing our networks\nfrom threats and attacks. After briefly reviewing the tasks and recent ML\nmodels in network security and privacy, we discuss the challenges that lie\nahead."
                },
                "authors": [
                    {
                        "name": "Dinil Mon Divakaran"
                    }
                ],
                "author_detail": {
                    "name": "Dinil Mon Divakaran"
                },
                "author": "Dinil Mon Divakaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22152v1",
                "updated": "2025-03-28T05:10:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    5,
                    10,
                    59,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T05:10:59Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    5,
                    10,
                    59,
                    4,
                    87,
                    0
                ],
                "title": "EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos"
                },
                "summary": "We introduce EgoToM, a new video question-answering benchmark that extends\nTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToM\nmodel, we generate multi-choice video QA instances for the Ego4D dataset to\nbenchmark the ability to predict a camera wearer's goals, beliefs, and next\nactions. We study the performance of both humans and state of the art\nmultimodal large language models (MLLMs) on these three interconnected\ninference problems. Our evaluation shows that MLLMs achieve close to\nhuman-level accuracy on inferring goals from egocentric videos. However, MLLMs\n(including the largest ones we tested with over 100B parameters) fall short of\nhuman performance when inferring the camera wearers' in-the-moment belief\nstates and future actions that are most consistent with the unseen video\nfuture. We believe that our results will shape the future design of an\nimportant class of egocentric digital assistants which are equipped with a\nreasonable model of the user's internal mental states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EgoToM, a new video question-answering benchmark that extends\nTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToM\nmodel, we generate multi-choice video QA instances for the Ego4D dataset to\nbenchmark the ability to predict a camera wearer's goals, beliefs, and next\nactions. We study the performance of both humans and state of the art\nmultimodal large language models (MLLMs) on these three interconnected\ninference problems. Our evaluation shows that MLLMs achieve close to\nhuman-level accuracy on inferring goals from egocentric videos. However, MLLMs\n(including the largest ones we tested with over 100B parameters) fall short of\nhuman performance when inferring the camera wearers' in-the-moment belief\nstates and future actions that are most consistent with the unseen video\nfuture. We believe that our results will shape the future design of an\nimportant class of egocentric digital assistants which are equipped with a\nreasonable model of the user's internal mental states."
                },
                "authors": [
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Vijay Veerabadran"
                    },
                    {
                        "name": "Michael L. Iuzzolino"
                    },
                    {
                        "name": "Brett D. Roads"
                    },
                    {
                        "name": "Asli Celikyilmaz"
                    },
                    {
                        "name": "Karl Ridgeway"
                    }
                ],
                "author_detail": {
                    "name": "Karl Ridgeway"
                },
                "author": "Karl Ridgeway",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21364v2",
                "updated": "2025-03-28T04:58:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    58,
                    11,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-27T10:55:36Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    55,
                    36,
                    3,
                    86,
                    0
                ],
                "title": "LandMarkSystem Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LandMarkSystem Technical Report"
                },
                "summary": "3D reconstruction is vital for applications in autonomous driving, virtual\nreality, augmented reality, and the metaverse. Recent advancements such as\nNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed\nthe field, yet traditional deep learning frameworks struggle to meet the\nincreasing demands for scene quality and scale. This paper introduces\nLandMarkSystem, a novel computing framework designed to enhance multi-scale\nscene reconstruction and rendering. By leveraging a componentized model\nadaptation layer, LandMarkSystem supports various NeRF and 3DGS structures\nwhile optimizing computational efficiency through distributed parallel\ncomputing and model parameter offloading. Our system addresses the limitations\nof existing frameworks, providing dedicated operators for complex 3D sparse\ncomputations, thus facilitating efficient training and rapid inference over\nextensive scenes. Key contributions include a modular architecture, a dynamic\nloading strategy for limited resources, and proven capabilities across multiple\nrepresentative algorithms.This comprehensive solution aims to advance the\nefficiency and effectiveness of 3D reconstruction tasks.To facilitate further\nresearch and collaboration, the source code and documentation for the\nLandMarkSystem project are publicly available in an open-source repository,\naccessing the repository at: https://github.com/InternLandMark/LandMarkSystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D reconstruction is vital for applications in autonomous driving, virtual\nreality, augmented reality, and the metaverse. Recent advancements such as\nNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed\nthe field, yet traditional deep learning frameworks struggle to meet the\nincreasing demands for scene quality and scale. This paper introduces\nLandMarkSystem, a novel computing framework designed to enhance multi-scale\nscene reconstruction and rendering. By leveraging a componentized model\nadaptation layer, LandMarkSystem supports various NeRF and 3DGS structures\nwhile optimizing computational efficiency through distributed parallel\ncomputing and model parameter offloading. Our system addresses the limitations\nof existing frameworks, providing dedicated operators for complex 3D sparse\ncomputations, thus facilitating efficient training and rapid inference over\nextensive scenes. Key contributions include a modular architecture, a dynamic\nloading strategy for limited resources, and proven capabilities across multiple\nrepresentative algorithms.This comprehensive solution aims to advance the\nefficiency and effectiveness of 3D reconstruction tasks.To facilitate further\nresearch and collaboration, the source code and documentation for the\nLandMarkSystem project are publicly available in an open-source repository,\naccessing the repository at: https://github.com/InternLandMark/LandMarkSystem."
                },
                "authors": [
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Hengjie Li"
                },
                "author": "Hengjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20776v2",
                "updated": "2025-03-28T04:48:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    48,
                    48,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-26T17:56:16Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    56,
                    16,
                    2,
                    85,
                    0
                ],
                "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields"
                },
                "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g., SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g., SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction."
                },
                "authors": [
                    {
                        "name": "Shijie Zhou"
                    },
                    {
                        "name": "Hui Ren"
                    },
                    {
                        "name": "Yijia Weng"
                    },
                    {
                        "name": "Shuwang Zhang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Dejia Xu"
                    },
                    {
                        "name": "Zhiwen Fan"
                    },
                    {
                        "name": "Suya You"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Achuta Kadambi"
                    }
                ],
                "author_detail": {
                    "name": "Achuta Kadambi"
                },
                "author": "Achuta Kadambi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22145v1",
                "updated": "2025-03-28T04:41:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    41,
                    9,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T04:41:09Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    41,
                    9,
                    4,
                    87,
                    0
                ],
                "title": "Tokenization of Gaze Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization of Gaze Data"
                },
                "summary": "A considerable part of the performance of today's large language models\n(LLM's) and multimodal large language models (MLLM's) depends on their\ntokenization strategies. While tokenizers are extensively researched for\ntextual and visual input, there is no research on tokenization strategies for\ngaze data due to its nature. However, a corresponding tokenization strategy\nwould allow using the vision capabilities of pre-trained MLLM's for gaze data,\nfor example, through fine-tuning.\n  In this paper, we aim to close this research gap by analyzing five different\ntokenizers for gaze data on three different datasets for the forecasting and\ngeneration of gaze data through LLMs (cf.~\\cref{fig:teaser}). We evaluate the\ntokenizers regarding their reconstruction and compression abilities. Further,\nwe train an LLM for each tokenization strategy, measuring its generative and\npredictive performance. Overall, we found that a quantile tokenizer outperforms\nall others in predicting the gaze positions and k-means is best when predicting\ngaze velocities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A considerable part of the performance of today's large language models\n(LLM's) and multimodal large language models (MLLM's) depends on their\ntokenization strategies. While tokenizers are extensively researched for\ntextual and visual input, there is no research on tokenization strategies for\ngaze data due to its nature. However, a corresponding tokenization strategy\nwould allow using the vision capabilities of pre-trained MLLM's for gaze data,\nfor example, through fine-tuning.\n  In this paper, we aim to close this research gap by analyzing five different\ntokenizers for gaze data on three different datasets for the forecasting and\ngeneration of gaze data through LLMs (cf.~\\cref{fig:teaser}). We evaluate the\ntokenizers regarding their reconstruction and compression abilities. Further,\nwe train an LLM for each tokenization strategy, measuring its generative and\npredictive performance. Overall, we found that a quantile tokenizer outperforms\nall others in predicting the gaze positions and k-means is best when predicting\ngaze velocities."
                },
                "authors": [
                    {
                        "name": "Tim Rolff"
                    },
                    {
                        "name": "Jurik Karimian"
                    },
                    {
                        "name": "Niklas Hypki"
                    },
                    {
                        "name": "Susanne Schmidt"
                    },
                    {
                        "name": "Markus Lappe"
                    },
                    {
                        "name": "Frank Steinicke"
                    }
                ],
                "author_detail": {
                    "name": "Frank Steinicke"
                },
                "author": "Frank Steinicke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01129v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01129v3",
                "updated": "2025-03-28T04:40:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    40,
                    20,
                    4,
                    87,
                    0
                ],
                "published": "2024-12-02T05:09:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    5,
                    9,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for\n  Boosting 2-bit Large Language Model Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for\n  Boosting 2-bit Large Language Model Accuracy"
                },
                "summary": "Low-rank adaptation (LoRA) has become the dominant method for\nparameter-efficient LLM fine-tuning, with LoRA-based quantization error\ncompensation (LQEC) emerging as a powerful tool for recovering accuracy in\ncompressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, with\nno prior investigation into understanding this limitation. We propose RILQ\n(Rank-Insensitive LoRA-based Quantization Error Compensation) to understand\nfundamental limitation and boost 2-bit LLM accuracy. Based on rank analysis\nrevealing model-wise activation discrepancy loss's rank-insensitive nature,\nRILQ employs this loss to adjust adapters cooperatively across layers, enabling\nrobust error compensation with low-rank adapters. Evaluations on LLaMA-2 and\nLLaMA-3 demonstrate RILQ's consistent improvements in 2-bit quantized inference\nacross various state-of-the-art quantizers and enhanced accuracy in\ntask-specific fine-tuning. RILQ maintains computational efficiency comparable\nto existing LoRA methods, enabling adapter-merged weight-quantized LLM\ninference with significantly enhanced accuracy, making it a promising approach\nfor boosting 2-bit LLM performance. Our code is available at\nhttps://github.com/aiha-lab/RILQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptation (LoRA) has become the dominant method for\nparameter-efficient LLM fine-tuning, with LoRA-based quantization error\ncompensation (LQEC) emerging as a powerful tool for recovering accuracy in\ncompressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, with\nno prior investigation into understanding this limitation. We propose RILQ\n(Rank-Insensitive LoRA-based Quantization Error Compensation) to understand\nfundamental limitation and boost 2-bit LLM accuracy. Based on rank analysis\nrevealing model-wise activation discrepancy loss's rank-insensitive nature,\nRILQ employs this loss to adjust adapters cooperatively across layers, enabling\nrobust error compensation with low-rank adapters. Evaluations on LLaMA-2 and\nLLaMA-3 demonstrate RILQ's consistent improvements in 2-bit quantized inference\nacross various state-of-the-art quantizers and enhanced accuracy in\ntask-specific fine-tuning. RILQ maintains computational efficiency comparable\nto existing LoRA methods, enabling adapter-merged weight-quantized LLM\ninference with significantly enhanced accuracy, making it a promising approach\nfor boosting 2-bit LLM performance. Our code is available at\nhttps://github.com/aiha-lab/RILQ."
                },
                "authors": [
                    {
                        "name": "Geonho Lee"
                    },
                    {
                        "name": "Janghwan Lee"
                    },
                    {
                        "name": "Sukjin Hong"
                    },
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Euijai Ahn"
                    },
                    {
                        "name": "Du-Seong Chang"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01129v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01129v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22144v1",
                "updated": "2025-03-28T04:39:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    39,
                    52,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T04:39:52Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    39,
                    52,
                    4,
                    87,
                    0
                ],
                "title": "FRASE: Structured Representations for Generalizable SPARQL Query\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRASE: Structured Representations for Generalizable SPARQL Query\n  Generation"
                },
                "summary": "Translating natural language questions into SPARQL queries enables Knowledge\nBase querying for factual and up-to-date responses. However, existing datasets\nfor this task are predominantly template-based, leading models to learn\nsuperficial mappings between question and query templates rather than\ndeveloping true generalization capabilities. As a result, models struggle when\nencountering naturally phrased, template-free questions. This paper introduces\nFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages Frame\nSemantic Role Labeling (FSRL) to address this limitation. We also present\nLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question is\nenriched using FRASE through frame detection and the mapping of frame-elements\nto their argument. We evaluate the impact of this approach through extensive\nexperiments on recent large language models (LLMs) under different fine-tuning\nconfigurations. Our results demonstrate that integrating frame-based structured\nrepresentations consistently improves SPARQL generation performance,\nparticularly in challenging generalization scenarios when test questions\nfeature unseen templates (unknown template splits) and when they are all\nnaturally phrased (reformulated questions).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating natural language questions into SPARQL queries enables Knowledge\nBase querying for factual and up-to-date responses. However, existing datasets\nfor this task are predominantly template-based, leading models to learn\nsuperficial mappings between question and query templates rather than\ndeveloping true generalization capabilities. As a result, models struggle when\nencountering naturally phrased, template-free questions. This paper introduces\nFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages Frame\nSemantic Role Labeling (FSRL) to address this limitation. We also present\nLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question is\nenriched using FRASE through frame detection and the mapping of frame-elements\nto their argument. We evaluate the impact of this approach through extensive\nexperiments on recent large language models (LLMs) under different fine-tuning\nconfigurations. Our results demonstrate that integrating frame-based structured\nrepresentations consistently improves SPARQL generation performance,\nparticularly in challenging generalization scenarios when test questions\nfeature unseen templates (unknown template splits) and when they are all\nnaturally phrased (reformulated questions)."
                },
                "authors": [
                    {
                        "name": "Papa Abdou Karim Karou Diallo"
                    },
                    {
                        "name": "Amal Zouaq"
                    }
                ],
                "author_detail": {
                    "name": "Amal Zouaq"
                },
                "author": "Amal Zouaq",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08723v2",
                "updated": "2025-03-28T04:38:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    38,
                    44,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-11T11:23:26Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    23,
                    26,
                    4,
                    285,
                    0
                ],
                "title": "Human-Computer Interaction and Visualization in Natural Language\n  Generation Models: Applications, Challenges, and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Computer Interaction and Visualization in Natural Language\n  Generation Models: Applications, Challenges, and Opportunities"
                },
                "summary": "Natural language generation (NLG) models have emerged as a focal point of\nresearch within natural language processing (NLP), exhibiting remarkable\nperformance in tasks such as text composition and dialogue generation. However,\ntheir intricate architectures and extensive model parameters pose significant\nchallenges to interpretability, limiting their applicability in high-stakes\ndecision-making scenarios. To address this issue, human-computer interaction\n(HCI) and visualization techniques offer promising avenues to enhance the\ntransparency and usability of NLG models by making their decision-making\nprocesses more interpretable. In this paper, we provide a comprehensive\ninvestigation into the roles, limitations, and impact of HCI and visualization\nin facilitating human understanding and control over NLG systems. We introduce\na taxonomy of interaction methods and visualization techniques, categorizing\nthree major research domains and their corresponding six key tasks in the\napplication of NLG models. Finally, we summarize the shortcomings in the\nexisting work and investigate the key challenges and emerging opportunities in\nthe era of large language models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language generation (NLG) models have emerged as a focal point of\nresearch within natural language processing (NLP), exhibiting remarkable\nperformance in tasks such as text composition and dialogue generation. However,\ntheir intricate architectures and extensive model parameters pose significant\nchallenges to interpretability, limiting their applicability in high-stakes\ndecision-making scenarios. To address this issue, human-computer interaction\n(HCI) and visualization techniques offer promising avenues to enhance the\ntransparency and usability of NLG models by making their decision-making\nprocesses more interpretable. In this paper, we provide a comprehensive\ninvestigation into the roles, limitations, and impact of HCI and visualization\nin facilitating human understanding and control over NLG systems. We introduce\na taxonomy of interaction methods and visualization techniques, categorizing\nthree major research domains and their corresponding six key tasks in the\napplication of NLG models. Finally, we summarize the shortcomings in the\nexisting work and investigate the key challenges and emerging opportunities in\nthe era of large language models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Yunchao Wang"
                    },
                    {
                        "name": "Guodao Sun"
                    },
                    {
                        "name": "Zihang Fu"
                    },
                    {
                        "name": "Ronghua Liang"
                    }
                ],
                "author_detail": {
                    "name": "Ronghua Liang"
                },
                "author": "Ronghua Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22137v1",
                "updated": "2025-03-28T04:22:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    22,
                    53,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T04:22:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    22,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has become a cornerstone of\nthe training and alignment pipeline for large language models (LLMs). Recent\nadvances, such as direct preference optimization (DPO), have simplified the\npreference learning step. However, collecting preference data remains a\nchallenging and costly process, often requiring expert annotation. This cost\ncan be mitigated by carefully selecting the data points presented for\nannotation. In this work, we propose an active learning approach to efficiently\nselect prompt and preference pairs using a risk assessment strategy based on\nthe Sharpe Ratio. To address the challenge of unknown preferences prior to\nannotation, our method evaluates the gradients of all potential preference\nannotations to assess their impact on model updates. These gradient-based\nevaluations enable risk assessment of data points regardless of the annotation\noutcome. By leveraging the DPO loss derivations, we derive a closed-form\nexpression for computing these Sharpe ratios on a per-tuple basis, ensuring our\napproach remains both tractable and computationally efficient. We also\nintroduce two variants of our method, each making different assumptions about\nprior information. Experimental results demonstrate that our method outperforms\nthe baseline by up to 5% in win rates against the chosen completion with\nlimited human preference data across several language models and real-world\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has become a cornerstone of\nthe training and alignment pipeline for large language models (LLMs). Recent\nadvances, such as direct preference optimization (DPO), have simplified the\npreference learning step. However, collecting preference data remains a\nchallenging and costly process, often requiring expert annotation. This cost\ncan be mitigated by carefully selecting the data points presented for\nannotation. In this work, we propose an active learning approach to efficiently\nselect prompt and preference pairs using a risk assessment strategy based on\nthe Sharpe Ratio. To address the challenge of unknown preferences prior to\nannotation, our method evaluates the gradients of all potential preference\nannotations to assess their impact on model updates. These gradient-based\nevaluations enable risk assessment of data points regardless of the annotation\noutcome. By leveraging the DPO loss derivations, we derive a closed-form\nexpression for computing these Sharpe ratios on a per-tuple basis, ensuring our\napproach remains both tractable and computationally efficient. We also\nintroduce two variants of our method, each making different assumptions about\nprior information. Experimental results demonstrate that our method outperforms\nthe baseline by up to 5% in win rates against the chosen completion with\nlimited human preference data across several language models and real-world\ndatasets."
                },
                "authors": [
                    {
                        "name": "Syrine Belakaria"
                    },
                    {
                        "name": "Joshua Kazdan"
                    },
                    {
                        "name": "Charles Marx"
                    },
                    {
                        "name": "Chris Cundy"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Barbara E. Engelhardt"
                    },
                    {
                        "name": "Stefano Ermon"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ermon"
                },
                "author": "Stefano Ermon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12523v3",
                "updated": "2025-03-28T04:13:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    13,
                    29,
                    4,
                    87,
                    0
                ],
                "published": "2024-05-21T06:27:12Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    6,
                    27,
                    12,
                    1,
                    142,
                    0
                ],
                "title": "Single Image Unlearning: Efficient Machine Unlearning in Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single Image Unlearning: Efficient Machine Unlearning in Multimodal\n  Large Language Models"
                },
                "summary": "Machine unlearning empowers individuals with the `right to be forgotten' by\nremoving their private or sensitive information encoded in machine learning\nmodels. However, it remains uncertain whether MU can be effectively applied to\nMultimodal Large Language Models (MLLMs), particularly in scenarios of\nforgetting the leaked visual data of concepts. To overcome the challenge, we\npropose an efficient method, Single Image Unlearning (SIU), to unlearn the\nvisual recognition of a concept by fine-tuning a single associated image for\nfew steps. SIU consists of two key aspects: (i) Constructing Multifaceted\nfine-tuning data. We introduce four targets, based on which we construct\nfine-tuning data for the concepts to be forgotten; (ii) Jointly training loss.\nTo synchronously forget the visual recognition of concepts and preserve the\nutility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence\nLoss combined with Cross Entropy loss. Alongside our method, we establish\nMMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics\nfor its evaluation. Experimental results on MMUBench show that SIU completely\nsurpasses the performance of existing methods. Furthermore, we surprisingly\nfind that SIU can avoid invasive membership inference attacks and jailbreak\nattacks. To the best of our knowledge, we are the first to explore MU in MLLMs.\nWe will release the code and benchmark in the near future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning empowers individuals with the `right to be forgotten' by\nremoving their private or sensitive information encoded in machine learning\nmodels. However, it remains uncertain whether MU can be effectively applied to\nMultimodal Large Language Models (MLLMs), particularly in scenarios of\nforgetting the leaked visual data of concepts. To overcome the challenge, we\npropose an efficient method, Single Image Unlearning (SIU), to unlearn the\nvisual recognition of a concept by fine-tuning a single associated image for\nfew steps. SIU consists of two key aspects: (i) Constructing Multifaceted\nfine-tuning data. We introduce four targets, based on which we construct\nfine-tuning data for the concepts to be forgotten; (ii) Jointly training loss.\nTo synchronously forget the visual recognition of concepts and preserve the\nutility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence\nLoss combined with Cross Entropy loss. Alongside our method, we establish\nMMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics\nfor its evaluation. Experimental results on MMUBench show that SIU completely\nsurpasses the performance of existing methods. Furthermore, we surprisingly\nfind that SIU can avoid invasive membership inference attacks and jailbreak\nattacks. To the best of our knowledge, we are the first to explore MU in MLLMs.\nWe will release the code and benchmark in the near future."
                },
                "authors": [
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Qianshan Wei"
                    },
                    {
                        "name": "Chuanyi Zhang"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Miaozeng Du"
                    },
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Sheng Bi"
                    },
                    {
                        "name": "Fan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Liu"
                },
                "author": "Fan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12496v2",
                "updated": "2025-03-28T03:51:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    3,
                    51,
                    10,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-16T13:12:45Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    13,
                    12,
                    45,
                    6,
                    75,
                    0
                ],
                "title": "Does Your Vision-Language Model Get Lost in the Long Video Sampling\n  Dilemma?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Your Vision-Language Model Get Lost in the Long Video Sampling\n  Dilemma?"
                },
                "summary": "The rise of Large Vision-Language Models (LVLMs) has significantly advanced\nvideo understanding. However, efficiently processing long videos remains a\nchallenge due to the ``Sampling Dilemma'': low-density sampling risks missing\ncritical information, while high-density sampling introduces redundancy. To\naddress this issue, we introduce LSDBench, the first benchmark designed to\nevaluate LVLMs on long-video tasks by constructing high Necessary Sampling\nDensity (NSD) questions, where NSD represents the minimum sampling density\nrequired to accurately answer a given question. LSDBench focuses on dense,\nshort-duration actions to rigorously assess the sampling strategies employed by\nLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel\nReasoning-Driven Hierarchical Sampling (RHS) framework, which combines global\nlocalization of question-relevant cues with local dense sampling for precise\ninference. Additionally, we develop a lightweight Semantic-Guided Frame\nSelector to prioritize informative frames, enabling RHS to achieve comparable\nor superior performance with significantly fewer sampled frames. Together, our\nLSDBench and RHS framework address the unique challenges of high-NSD long-video\ntasks, setting a new standard for evaluating and improving LVLMs in this\ndomain. Our benchmark and evaluation codes has been released at:\nhttps://github.com/dvlab-research/LSDBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Vision-Language Models (LVLMs) has significantly advanced\nvideo understanding. However, efficiently processing long videos remains a\nchallenge due to the ``Sampling Dilemma'': low-density sampling risks missing\ncritical information, while high-density sampling introduces redundancy. To\naddress this issue, we introduce LSDBench, the first benchmark designed to\nevaluate LVLMs on long-video tasks by constructing high Necessary Sampling\nDensity (NSD) questions, where NSD represents the minimum sampling density\nrequired to accurately answer a given question. LSDBench focuses on dense,\nshort-duration actions to rigorously assess the sampling strategies employed by\nLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel\nReasoning-Driven Hierarchical Sampling (RHS) framework, which combines global\nlocalization of question-relevant cues with local dense sampling for precise\ninference. Additionally, we develop a lightweight Semantic-Guided Frame\nSelector to prioritize informative frames, enabling RHS to achieve comparable\nor superior performance with significantly fewer sampled frames. Together, our\nLSDBench and RHS framework address the unique challenges of high-NSD long-video\ntasks, setting a new standard for evaluating and improving LVLMs in this\ndomain. Our benchmark and evaluation codes has been released at:\nhttps://github.com/dvlab-research/LSDBench"
                },
                "authors": [
                    {
                        "name": "Tianyuan Qu"
                    },
                    {
                        "name": "Longxiang Tang"
                    },
                    {
                        "name": "Bohao Peng"
                    },
                    {
                        "name": "Senqiao Yang"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18288v2",
                "updated": "2025-03-28T03:35:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    3,
                    35,
                    17,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-24T02:17:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "Sun-Shine: A Large Language Model for Tibetan Culture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sun-Shine: A Large Language Model for Tibetan Culture"
                },
                "summary": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Nyima Tashi"
                    },
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Xiangxiang Wang"
                    },
                    {
                        "name": "Thupten Tsering"
                    },
                    {
                        "name": "Ban Ma-bao"
                    },
                    {
                        "name": "Renzeg Duojie"
                    },
                    {
                        "name": "Gadeng Luosang"
                    },
                    {
                        "name": "Rinchen Dongrub"
                    },
                    {
                        "name": "Dorje Tashi"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yongbin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Yu"
                },
                "author": "Yongbin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.22678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22678v1",
                "updated": "2025-03-28T17:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    59,
                    53,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T17:59:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    59,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions"
                },
                "summary": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}."
                },
                "authors": [
                    {
                        "name": "Mohammad Almansoori"
                    },
                    {
                        "name": "Komal Kumar"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "arxiv_comment": "14 page, 4 figures, 61 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22674v1",
                "updated": "2025-03-28T17:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    58,
                    40,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T17:58:40Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    58,
                    40,
                    4,
                    87,
                    0
                ],
                "title": "QuestBench: Can LLMs ask the right question to acquire information in\n  reasoning tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuestBench: Can LLMs ask the right question to acquire information in\n  reasoning tasks?"
                },
                "summary": "Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities."
                },
                "authors": [
                    {
                        "name": "Belinda Z. Li"
                    },
                    {
                        "name": "Been Kim"
                    },
                    {
                        "name": "Zi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zi Wang"
                },
                "author": "Zi Wang",
                "arxiv_comment": "Code and dataset are available at\n  \\url{https://github.com/google-deepmind/questbench}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22655v1",
                "updated": "2025-03-28T17:43:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    43,
                    0,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T17:43:00Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    43,
                    0,
                    4,
                    87,
                    0
                ],
                "title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training"
                },
                "summary": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git."
                },
                "authors": [
                    {
                        "name": "Xiaomin Yu"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Kejian Wu"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13360v3",
                "updated": "2025-03-28T17:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    28,
                    44,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-17T09:10:26Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    9,
                    10,
                    26,
                    3,
                    291,
                    0
                ],
                "title": "RAP: Retrieval-Augmented Personalization for Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: Retrieval-Augmented Personalization for Multimodal Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://hoar012.github.io/RAP-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://hoar012.github.io/RAP-Project/."
                },
                "authors": [
                    {
                        "name": "Haoran Hao"
                    },
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Changsheng Li"
                    },
                    {
                        "name": "Yu-Feng Li"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Accepted by CVPR 2025. Code: https://github.com/Hoar012/RAP-MLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13222v2",
                "updated": "2025-03-28T17:17:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    17,
                    40,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-17T14:31:37Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    31,
                    37,
                    0,
                    76,
                    0
                ],
                "title": "Can Language Models Follow Multiple Turns of Entangled Instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Follow Multiple Turns of Entangled Instructions?"
                },
                "summary": "Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions."
                },
                "authors": [
                    {
                        "name": "Chi Han"
                    }
                ],
                "author_detail": {
                    "name": "Chi Han"
                },
                "author": "Chi Han",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07877v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07877v3",
                "updated": "2025-03-28T17:14:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    14,
                    39,
                    4,
                    87,
                    0
                ],
                "published": "2024-02-12T18:41:55Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    18,
                    41,
                    55,
                    0,
                    43,
                    0
                ],
                "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and\n  Adaptation"
                },
                "summary": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work, we propose a Retrieval-Augmented Generation\n(RAG)-based multi-agent LLM system to support analysis and decision-making in\nthe context of natural hazards and extreme weather events. As a proof of\nconcept, we present WildfireGPT, a specialized system focused on wildfire\nscenarios. The architecture employs a user-centered, multi-agent design to\ndeliver tailored risk insights across diverse stakeholder groups. By\nintegrating domain-specific projection data, observational datasets, and\nscientific literature through a RAG framework, the system ensures both accuracy\nand contextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support in natural hazard and extreme\nweather contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work, we propose a Retrieval-Augmented Generation\n(RAG)-based multi-agent LLM system to support analysis and decision-making in\nthe context of natural hazards and extreme weather events. As a proof of\nconcept, we present WildfireGPT, a specialized system focused on wildfire\nscenarios. The architecture employs a user-centered, multi-agent design to\ndeliver tailored risk insights across diverse stakeholder groups. By\nintegrating domain-specific projection data, observational datasets, and\nscientific literature through a RAG framework, the system ensures both accuracy\nand contextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support in natural hazard and extreme\nweather contexts."
                },
                "authors": [
                    {
                        "name": "Yangxinyu Xie"
                    },
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Tanwi Mallick"
                    },
                    {
                        "name": "Joshua David Bergerson"
                    },
                    {
                        "name": "John K. Hutchison"
                    },
                    {
                        "name": "Duane R. Verner"
                    },
                    {
                        "name": "Jordan Branham"
                    },
                    {
                        "name": "M. Ross Alexander"
                    },
                    {
                        "name": "Robert B. Ross"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Leslie-Anne Levy"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Camillo J. Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Camillo J. Taylor"
                },
                "author": "Camillo J. Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07877v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07877v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22604v1",
                "updated": "2025-03-28T16:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    47,
                    42,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:47:42Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    47,
                    42,
                    4,
                    87,
                    0
                ],
                "title": "Enhanced Variational Quantum Kolmogorov-Arnold Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Variational Quantum Kolmogorov-Arnold Network"
                },
                "summary": "The Kolmogorov-Arnold Network (KAN) is a novel multi-layer network model\nrecognized for its efficiency in neuromorphic computing, where synapses between\nneurons are trained linearly. Computations in KAN are performed by generating a\npolynomial vector from the state vector and layer-wise trained synapses,\nenabling efficient processing. While KAN can be implemented on quantum\ncomputers using block encoding and Quantum Signal Processing, these methods\nrequire fault-tolerant quantum devices, making them impractical for current\nNoisy Intermediate-Scale Quantum (NISQ) hardware. We propose the Enhanced\nVariational Quantum Kolmogorov-Arnold Network (EVQKAN) to overcome this\nlimitation, which emulates KAN through variational quantum algorithms. The\nEVQKAN ansatz employs a tiling technique to emulate layer matrices, leading to\nsignificantly higher accuracy compared to conventional Variational Quantum\nKolmogorov-Arnold Network (VQKAN) and Quantum Neural Networks (QNN), even with\na smaller number of layers. EVQKAN achieves superior performance with a\nsingle-layer architecture, whereas QNN and VQKAN typically struggle.\nAdditionally, EVQKAN eliminates the need for Quantum Signal Processing,\nenhancing its robustness to noise and making it well-suited for practical\ndeployment on NISQ-era quantum devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Kolmogorov-Arnold Network (KAN) is a novel multi-layer network model\nrecognized for its efficiency in neuromorphic computing, where synapses between\nneurons are trained linearly. Computations in KAN are performed by generating a\npolynomial vector from the state vector and layer-wise trained synapses,\nenabling efficient processing. While KAN can be implemented on quantum\ncomputers using block encoding and Quantum Signal Processing, these methods\nrequire fault-tolerant quantum devices, making them impractical for current\nNoisy Intermediate-Scale Quantum (NISQ) hardware. We propose the Enhanced\nVariational Quantum Kolmogorov-Arnold Network (EVQKAN) to overcome this\nlimitation, which emulates KAN through variational quantum algorithms. The\nEVQKAN ansatz employs a tiling technique to emulate layer matrices, leading to\nsignificantly higher accuracy compared to conventional Variational Quantum\nKolmogorov-Arnold Network (VQKAN) and Quantum Neural Networks (QNN), even with\na smaller number of layers. EVQKAN achieves superior performance with a\nsingle-layer architecture, whereas QNN and VQKAN typically struggle.\nAdditionally, EVQKAN eliminates the need for Quantum Signal Processing,\nenhancing its robustness to noise and making it well-suited for practical\ndeployment on NISQ-era quantum devices."
                },
                "authors": [
                    {
                        "name": "Hikaru Wakaura"
                    },
                    {
                        "name": "Rahmat Mulyawan"
                    },
                    {
                        "name": "Andriyan B. Suksmono"
                    }
                ],
                "author_detail": {
                    "name": "Andriyan B. Suksmono"
                },
                "author": "Andriyan B. Suksmono",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2503.21336",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22595v1",
                "updated": "2025-03-28T16:42:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    42,
                    21,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:42:21Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    42,
                    21,
                    4,
                    87,
                    0
                ],
                "title": "Reinforcement Learning for Machine Learning Model Deployment: Evaluating\n  Multi-Armed Bandits in ML Ops Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Machine Learning Model Deployment: Evaluating\n  Multi-Armed Bandits in ML Ops Environments"
                },
                "summary": "In modern ML Ops environments, model deployment is a critical process that\ntraditionally relies on static heuristics such as validation error comparisons\nand A/B testing. However, these methods require human intervention to adapt to\nreal-world deployment challenges, such as model drift or unexpected performance\ndegradation. We investigate whether reinforcement learning, specifically\nmulti-armed bandit (MAB) algorithms, can dynamically manage model deployment\ndecisions more effectively. Our approach enables more adaptive production\nenvironments by continuously evaluating deployed models and rolling back\nunderperforming ones in real-time. We test six model selection strategies\nacross two real-world datasets and find that RL based approaches match or\nexceed traditional methods in performance. Our findings suggest that\nreinforcement learning (RL)-based model management can improve automation,\nreduce reliance on manual interventions, and mitigate risks associated with\npost-deployment model failures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern ML Ops environments, model deployment is a critical process that\ntraditionally relies on static heuristics such as validation error comparisons\nand A/B testing. However, these methods require human intervention to adapt to\nreal-world deployment challenges, such as model drift or unexpected performance\ndegradation. We investigate whether reinforcement learning, specifically\nmulti-armed bandit (MAB) algorithms, can dynamically manage model deployment\ndecisions more effectively. Our approach enables more adaptive production\nenvironments by continuously evaluating deployed models and rolling back\nunderperforming ones in real-time. We test six model selection strategies\nacross two real-world datasets and find that RL based approaches match or\nexceed traditional methods in performance. Our findings suggest that\nreinforcement learning (RL)-based model management can improve automation,\nreduce reliance on manual interventions, and mitigate risks associated with\npost-deployment model failures."
                },
                "authors": [
                    {
                        "name": "S. Aaron McClendon"
                    },
                    {
                        "name": "Vishaal Venkatesh"
                    },
                    {
                        "name": "Juan Morinelli"
                    }
                ],
                "author_detail": {
                    "name": "Juan Morinelli"
                },
                "author": "Juan Morinelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.10368v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.10368v5",
                "updated": "2025-03-28T16:42:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    42,
                    3,
                    4,
                    87,
                    0
                ],
                "published": "2022-09-21T14:03:08Z",
                "published_parsed": [
                    2022,
                    9,
                    21,
                    14,
                    3,
                    8,
                    2,
                    264,
                    0
                ],
                "title": "USC: Uncompromising Spatial Constraints for Safety-Oriented 3D Object\n  Detectors in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USC: Uncompromising Spatial Constraints for Safety-Oriented 3D Object\n  Detectors in Autonomous Driving"
                },
                "summary": "In this work, we consider the safety-oriented performance of 3D object\ndetectors in autonomous driving contexts. Specifically, despite impressive\nresults shown by the mass literature, developers often find it hard to ensure\nthe safe deployment of these learning-based perception models. Attributing the\nchallenge to the lack of safety-oriented metrics, we hereby present\nuncompromising spatial constraints (USC), which characterize a simple yet\nimportant localization requirement demanding the predictions to fully cover the\nobjects when seen from the autonomous vehicle. The constraints, as we formulate\nusing the perspective and bird's-eye views, can be naturally reflected by\nquantitative measures, such that having an object detector with a higher score\nimplies a lower risk of collision. Finally, beyond model evaluation, we\nincorporate the quantitative measures into common loss functions to enable\nsafety-oriented fine-tuning for existing models. With experiments using the\nnuScenes dataset and a closed-loop simulation, our work demonstrates such\nconsiderations of safety notions at the perception level not only improve model\nperformances beyond accuracy but also allow for a more direct linkage to actual\nsystem safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the safety-oriented performance of 3D object\ndetectors in autonomous driving contexts. Specifically, despite impressive\nresults shown by the mass literature, developers often find it hard to ensure\nthe safe deployment of these learning-based perception models. Attributing the\nchallenge to the lack of safety-oriented metrics, we hereby present\nuncompromising spatial constraints (USC), which characterize a simple yet\nimportant localization requirement demanding the predictions to fully cover the\nobjects when seen from the autonomous vehicle. The constraints, as we formulate\nusing the perspective and bird's-eye views, can be naturally reflected by\nquantitative measures, such that having an object detector with a higher score\nimplies a lower risk of collision. Finally, beyond model evaluation, we\nincorporate the quantitative measures into common loss functions to enable\nsafety-oriented fine-tuning for existing models. With experiments using the\nnuScenes dataset and a closed-loop simulation, our work demonstrates such\nconsiderations of safety notions at the perception level not only improve model\nperformances beyond accuracy but also allow for a more direct linkage to actual\nsystem safety."
                },
                "authors": [
                    {
                        "name": "Brian Hsuan-Cheng Liao"
                    },
                    {
                        "name": "Chih-Hong Cheng"
                    },
                    {
                        "name": "Hasan Esen"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_doi": "10.1109/ITSC58415.2024.10919937",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ITSC58415.2024.10919937",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2209.10368v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.10368v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ITSC 2024, 8 pages (IEEE double column format), 7\n  figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20679v2",
                "updated": "2025-03-28T16:40:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    40,
                    45,
                    4,
                    87,
                    0
                ],
                "published": "2024-09-25T14:37:49Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    37,
                    49,
                    2,
                    269,
                    0
                ],
                "title": "MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and\n  Improved GRU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and\n  Improved GRU"
                },
                "summary": "As financial markets grow increasingly complex in the big data era, accurate\nstock prediction has become more critical. Traditional time series models, such\nas GRUs, have been widely used but often struggle to capture the intricate\nnonlinear dynamics of markets, particularly in the flexible selection and\neffective utilization of key historical information. Recently, methods like\nGraph Neural Networks and Reinforcement Learning have shown promise in stock\nprediction but require high data quality and quantity, and they tend to exhibit\ninstability when dealing with data sparsity and noise. Moreover, the training\nand inference processes for these models are typically complex and\ncomputationally expensive, limiting their broad deployment in practical\napplications. Existing approaches also generally struggle to capture\nunobservable latent market states effectively, such as market sentiment and\nexpectations, microstructural factors, and participant behavior patterns,\nleading to an inadequate understanding of market dynamics and subsequently\nimpact prediction accuracy. To address these challenges, this paper proposes a\nstock prediction model, MCI-GRU, based on a multi-head cross-attention\nmechanism and an improved GRU. First, we enhance the GRU model by replacing the\nreset gate with an attention mechanism, thereby increasing the model's\nflexibility in selecting and utilizing historical information. Second, we\ndesign a multi-head cross-attention mechanism for learning unobservable latent\nmarket state representations, which are further enriched through interactions\nwith both temporal features and cross-sectional features. Finally, extensive\nexperiments on four main stock markets show that the proposed method\noutperforms SOTA techniques across multiple metrics. Additionally, its\nsuccessful application in real-world fund management operations confirms its\neffectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As financial markets grow increasingly complex in the big data era, accurate\nstock prediction has become more critical. Traditional time series models, such\nas GRUs, have been widely used but often struggle to capture the intricate\nnonlinear dynamics of markets, particularly in the flexible selection and\neffective utilization of key historical information. Recently, methods like\nGraph Neural Networks and Reinforcement Learning have shown promise in stock\nprediction but require high data quality and quantity, and they tend to exhibit\ninstability when dealing with data sparsity and noise. Moreover, the training\nand inference processes for these models are typically complex and\ncomputationally expensive, limiting their broad deployment in practical\napplications. Existing approaches also generally struggle to capture\nunobservable latent market states effectively, such as market sentiment and\nexpectations, microstructural factors, and participant behavior patterns,\nleading to an inadequate understanding of market dynamics and subsequently\nimpact prediction accuracy. To address these challenges, this paper proposes a\nstock prediction model, MCI-GRU, based on a multi-head cross-attention\nmechanism and an improved GRU. First, we enhance the GRU model by replacing the\nreset gate with an attention mechanism, thereby increasing the model's\nflexibility in selecting and utilizing historical information. Second, we\ndesign a multi-head cross-attention mechanism for learning unobservable latent\nmarket state representations, which are further enriched through interactions\nwith both temporal features and cross-sectional features. Finally, extensive\nexperiments on four main stock markets show that the proposed method\noutperforms SOTA techniques across multiple metrics. Additionally, its\nsuccessful application in real-world fund management operations confirms its\neffectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Peng Zhu"
                    },
                    {
                        "name": "Yuante Li"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Sheng Xiang"
                    },
                    {
                        "name": "Qinyuan Liu"
                    },
                    {
                        "name": "Dawei Cheng"
                    },
                    {
                        "name": "Yuqi Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Liang"
                },
                "author": "Yuqi Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22589v1",
                "updated": "2025-03-28T16:36:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    36,
                    23,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:36:23Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    36,
                    23,
                    4,
                    87,
                    0
                ],
                "title": "Using AI to Summarize US Presidential Campaign TV Advertisement Videos,\n  1952-2012",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using AI to Summarize US Presidential Campaign TV Advertisement Videos,\n  1952-2012"
                },
                "summary": "This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets."
                },
                "authors": [
                    {
                        "name": "Adam Breuer"
                    },
                    {
                        "name": "Bryce J. Dietrich"
                    },
                    {
                        "name": "Michael H. Crespin"
                    },
                    {
                        "name": "Matthew Butler"
                    },
                    {
                        "name": "J. A. Pyrse"
                    },
                    {
                        "name": "Kosuke Imai"
                    }
                ],
                "author_detail": {
                    "name": "Kosuke Imai"
                },
                "author": "Kosuke Imai",
                "arxiv_comment": "17 pages, 7 tables, 4 figures, and linked datasets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22587v1",
                "updated": "2025-03-28T16:34:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    34,
                    29,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:34:29Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    34,
                    29,
                    4,
                    87,
                    0
                ],
                "title": "LLM-enabled Instance Model Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enabled Instance Model Generation"
                },
                "summary": "In the domain of model-based engineering, models are essential components\nthat enable system design and analysis. Traditionally, the creation of these\nmodels has been a manual process requiring not only deep modeling expertise but\nalso substantial domain knowledge of target systems. With the rapid advancement\nof generative artificial intelligence, large language models (LLMs) show\npotential for automating model generation. This work explores the generation of\ninstance models using LLMs, focusing specifically on producing XMI-based\ninstance models from Ecore metamodels and natural language specifications. We\nobserve that current LLMs struggle to directly generate valid XMI models. To\naddress this, we propose a two-step approach: first, using LLMs to produce a\nsimplified structured output containing all necessary instance model\ninformation, namely a conceptual instance model, and then compiling this\nintermediate representation into a valid XMI file. The conceptual instance\nmodel is format-independent, allowing it to be transformed into various\nmodeling formats via different compilers. The feasibility of the proposed\nmethod has been demonstrated using several LLMs, including GPT-4o, o1-preview,\nLlama 3.1 (8B and 70B). Results show that the proposed method significantly\nimproves the usability of LLMs for instance model generation tasks. Notably,\nthe smaller open-source model, Llama 3.1 70B, demonstrated performance\ncomparable to proprietary GPT models within the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of model-based engineering, models are essential components\nthat enable system design and analysis. Traditionally, the creation of these\nmodels has been a manual process requiring not only deep modeling expertise but\nalso substantial domain knowledge of target systems. With the rapid advancement\nof generative artificial intelligence, large language models (LLMs) show\npotential for automating model generation. This work explores the generation of\ninstance models using LLMs, focusing specifically on producing XMI-based\ninstance models from Ecore metamodels and natural language specifications. We\nobserve that current LLMs struggle to directly generate valid XMI models. To\naddress this, we propose a two-step approach: first, using LLMs to produce a\nsimplified structured output containing all necessary instance model\ninformation, namely a conceptual instance model, and then compiling this\nintermediate representation into a valid XMI file. The conceptual instance\nmodel is format-independent, allowing it to be transformed into various\nmodeling formats via different compilers. The feasibility of the proposed\nmethod has been demonstrated using several LLMs, including GPT-4o, o1-preview,\nLlama 3.1 (8B and 70B). Results show that the proposed method significantly\nimproves the usability of LLMs for instance model generation tasks. Notably,\nthe smaller open-source model, Llama 3.1 70B, demonstrated performance\ncomparable to proprietary GPT models within the proposed framework."
                },
                "authors": [
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Long Wen"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22585v1",
                "updated": "2025-03-28T16:33:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    33,
                    24,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:33:24Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    33,
                    24,
                    4,
                    87,
                    0
                ],
                "title": "Historical Ink: Exploring Large Language Models for Irony Detection in\n  19th-Century Spanish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historical Ink: Exploring Large Language Models for Irony Detection in\n  19th-Century Spanish"
                },
                "summary": "This study explores the use of large language models (LLMs) to enhance\ndatasets and improve irony detection in 19th-century Latin American newspapers.\nTwo strategies were employed to evaluate the efficacy of BERT and GPT-4o models\nin capturing the subtle nuances nature of irony, through both multi-class and\nbinary classification tasks. First, we implemented dataset enhancements focused\non enriching emotional and contextual cues; however, these showed limited\nimpact on historical language analysis. The second strategy, a semi-automated\nannotation process, effectively addressed class imbalance and augmented the\ndataset with high-quality annotations. Despite the challenges posed by the\ncomplexity of irony, this work contributes to the advancement of sentiment\nanalysis through two key contributions: introducing a new historical Spanish\ndataset tagged for sentiment analysis and irony detection, and proposing a\nsemi-automated annotation methodology where human expertise is crucial for\nrefining LLMs results, enriched by incorporating historical and cultural\ncontexts as core features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of large language models (LLMs) to enhance\ndatasets and improve irony detection in 19th-century Latin American newspapers.\nTwo strategies were employed to evaluate the efficacy of BERT and GPT-4o models\nin capturing the subtle nuances nature of irony, through both multi-class and\nbinary classification tasks. First, we implemented dataset enhancements focused\non enriching emotional and contextual cues; however, these showed limited\nimpact on historical language analysis. The second strategy, a semi-automated\nannotation process, effectively addressed class imbalance and augmented the\ndataset with high-quality annotations. Despite the challenges posed by the\ncomplexity of irony, this work contributes to the advancement of sentiment\nanalysis through two key contributions: introducing a new historical Spanish\ndataset tagged for sentiment analysis and irony detection, and proposing a\nsemi-automated annotation methodology where human expertise is crucial for\nrefining LLMs results, enriched by incorporating historical and cultural\ncontexts as core features."
                },
                "authors": [
                    {
                        "name": "Kevin Cohen"
                    },
                    {
                        "name": "Laura Manrique-Gómez"
                    },
                    {
                        "name": "Rubén Manrique"
                    }
                ],
                "author_detail": {
                    "name": "Rubén Manrique"
                },
                "author": "Rubén Manrique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22576v1",
                "updated": "2025-03-28T16:25:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    25,
                    24,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:25:24Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    25,
                    24,
                    4,
                    87,
                    0
                ],
                "title": "Drop the Golden Apples: Identifying Third-Party Reuse by DB-Less\n  Software Composition Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drop the Golden Apples: Identifying Third-Party Reuse by DB-Less\n  Software Composition Analysis"
                },
                "summary": "The prevalent use of third-party libraries (TPLs) in modern software\ndevelopment introduces significant security and compliance risks, necessitating\nthe implementation of Software Composition Analysis (SCA) to manage these\nthreats. However, the accuracy of SCA tools heavily relies on the quality of\nthe integrated feature database to cross-reference with user projects. While\nunder the circumstance of the exponentially growing of open-source ecosystems\nand the integration of large models into software development, it becomes even\nmore challenging to maintain a comprehensive feature database for potential\nTPLs. To this end, after referring to the evolution of LLM applications in\nterms of external data interactions, we propose the first framework of DB-Less\nSCA, to get rid of the traditional heavy database and embrace the flexibility\nof LLMs to mimic the manual analysis of security analysts to retrieve identical\nevidence and confirm the identity of TPLs by supportive information from the\nopen Internet. Our experiments on two typical scenarios, native library\nidentification for Android and copy-based TPL reuse for C/C++, especially on\nartifacts that are not that underappreciated, have demonstrated the favorable\nfuture for implementing database-less strategies in SCA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevalent use of third-party libraries (TPLs) in modern software\ndevelopment introduces significant security and compliance risks, necessitating\nthe implementation of Software Composition Analysis (SCA) to manage these\nthreats. However, the accuracy of SCA tools heavily relies on the quality of\nthe integrated feature database to cross-reference with user projects. While\nunder the circumstance of the exponentially growing of open-source ecosystems\nand the integration of large models into software development, it becomes even\nmore challenging to maintain a comprehensive feature database for potential\nTPLs. To this end, after referring to the evolution of LLM applications in\nterms of external data interactions, we propose the first framework of DB-Less\nSCA, to get rid of the traditional heavy database and embrace the flexibility\nof LLMs to mimic the manual analysis of security analysts to retrieve identical\nevidence and confirm the identity of TPLs by supportive information from the\nopen Internet. Our experiments on two typical scenarios, native library\nidentification for Android and copy-based TPL reuse for C/C++, especially on\nartifacts that are not that underappreciated, have demonstrated the favorable\nfuture for implementing database-less strategies in SCA."
                },
                "authors": [
                    {
                        "name": "Lyuye Zhang"
                    },
                    {
                        "name": "Chengwei Liu"
                    },
                    {
                        "name": "Jiahui Wu"
                    },
                    {
                        "name": "Shiyang Zhang"
                    },
                    {
                        "name": "Chengyue Liu"
                    },
                    {
                        "name": "Zhengzi Xu"
                    },
                    {
                        "name": "Sen Chen"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "6 pages, 1 figure, FSE25-IVR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22573v1",
                "updated": "2025-03-28T16:20:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    20,
                    57,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:20:57Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    20,
                    57,
                    4,
                    87,
                    0
                ],
                "title": "A Framework for Cryptographic Verifiability of End-to-End AI Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Cryptographic Verifiability of End-to-End AI Pipelines"
                },
                "summary": "The increasing integration of Artificial Intelligence across multiple\nindustry sectors necessitates robust mechanisms for ensuring transparency,\ntrust, and auditability of its development and deployment. This topic is\nparticularly important in light of recent calls in various jurisdictions to\nintroduce regulation and legislation on AI safety. In this paper, we propose a\nframework for complete verifiable AI pipelines, identifying key components and\nanalyzing existing cryptographic approaches that contribute to verifiability\nacross different stages of the AI lifecycle, from data sourcing to training,\ninference, and unlearning. This framework could be used to combat\nmisinformation by providing cryptographic proofs alongside AI-generated assets\nto allow downstream verification of their provenance and correctness. Our\nfindings underscore the importance of ongoing research to develop cryptographic\ntools that are not only efficient for isolated AI processes, but that are\nefficiently `linkable' across different processes within the AI pipeline, to\nsupport the development of end-to-end verifiable AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of Artificial Intelligence across multiple\nindustry sectors necessitates robust mechanisms for ensuring transparency,\ntrust, and auditability of its development and deployment. This topic is\nparticularly important in light of recent calls in various jurisdictions to\nintroduce regulation and legislation on AI safety. In this paper, we propose a\nframework for complete verifiable AI pipelines, identifying key components and\nanalyzing existing cryptographic approaches that contribute to verifiability\nacross different stages of the AI lifecycle, from data sourcing to training,\ninference, and unlearning. This framework could be used to combat\nmisinformation by providing cryptographic proofs alongside AI-generated assets\nto allow downstream verification of their provenance and correctness. Our\nfindings underscore the importance of ongoing research to develop cryptographic\ntools that are not only efficient for isolated AI processes, but that are\nefficiently `linkable' across different processes within the AI pipeline, to\nsupport the development of end-to-end verifiable AI technologies."
                },
                "authors": [
                    {
                        "name": "Kar Balan"
                    },
                    {
                        "name": "Robert Learney"
                    },
                    {
                        "name": "Tim Wood"
                    }
                ],
                "author_detail": {
                    "name": "Tim Wood"
                },
                "author": "Tim Wood",
                "arxiv_comment": "Accepted to 11th ACM International Workshop on Security and Privacy\n  Analytics (IWSPA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22562v1",
                "updated": "2025-03-28T16:04:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    4,
                    20,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T16:04:20Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    4,
                    20,
                    4,
                    87,
                    0
                ],
                "title": "Niyama : Breaking the Silos of LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Niyama : Breaking the Silos of LLM Inference Serving"
                },
                "summary": "The widespread adoption of Large Language Models (LLMs) has enabled diverse\napplications with very different latency requirements. Existing LLM serving\nframeworks rely on siloed infrastructure with coarse-grained workload\nsegregation -- interactive and batch -- leading to inefficient resource\nutilization and limited support for fine-grained Quality-of-Service (QoS)\ndifferentiation. This results in operational inefficiencies, over-provisioning\nand poor load management during traffic surges.\n  We present Niyama, a novel QoS-driven inference serving system that enables\nefficient co-scheduling of diverse workloads on shared infrastructure. Niyama\nintroduces fine-grained QoS classification allowing applications to specify\nprecise latency requirements, and dynamically adapts scheduling decisions based\non real-time system state. Leveraging the predictable execution characteristics\nof LLM inference, Niyama implements a dynamic chunking mechanism to improve\noverall throughput while maintaining strict QoS guarantees. Additionally,\nNiyama employs a hybrid prioritization policy that balances fairness and\nefficiency, and employs selective request relegation that enables graceful\nservice degradation during overload conditions. Our evaluation demonstrates\nthat Niyama increases serving capacity by 32% compared to current siloed\ndeployments, while maintaining QoS guarantees. Notably, under extreme load, our\nsystem reduces SLO violations by an order of magnitude compared to current\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs) has enabled diverse\napplications with very different latency requirements. Existing LLM serving\nframeworks rely on siloed infrastructure with coarse-grained workload\nsegregation -- interactive and batch -- leading to inefficient resource\nutilization and limited support for fine-grained Quality-of-Service (QoS)\ndifferentiation. This results in operational inefficiencies, over-provisioning\nand poor load management during traffic surges.\n  We present Niyama, a novel QoS-driven inference serving system that enables\nefficient co-scheduling of diverse workloads on shared infrastructure. Niyama\nintroduces fine-grained QoS classification allowing applications to specify\nprecise latency requirements, and dynamically adapts scheduling decisions based\non real-time system state. Leveraging the predictable execution characteristics\nof LLM inference, Niyama implements a dynamic chunking mechanism to improve\noverall throughput while maintaining strict QoS guarantees. Additionally,\nNiyama employs a hybrid prioritization policy that balances fairness and\nefficiency, and employs selective request relegation that enables graceful\nservice degradation during overload conditions. Our evaluation demonstrates\nthat Niyama increases serving capacity by 32% compared to current siloed\ndeployments, while maintaining QoS guarantees. Notably, under extreme load, our\nsystem reduces SLO violations by an order of magnitude compared to current\nstrategies."
                },
                "authors": [
                    {
                        "name": "Kanishk Goel"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Nipun Kwatra"
                    },
                    {
                        "name": "Ravi Shreyas Anupindi"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    }
                ],
                "author_detail": {
                    "name": "Ramachandran Ramjee"
                },
                "author": "Ramachandran Ramjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14582v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14582v4",
                "updated": "2025-03-28T15:50:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    50,
                    55,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-18T16:32:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    32,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "Do LLMs estimate uncertainty well in instruction-following?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs estimate uncertainty well in instruction-following?"
                },
                "summary": "Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents."
                },
                "authors": [
                    {
                        "name": "Juyeon Heo"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Christina Heinze-Deml"
                    },
                    {
                        "name": "Jaya Narain"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Narain"
                },
                "author": "Jaya Narain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14582v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14582v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22547v1",
                "updated": "2025-03-28T15:47:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    47,
                    30,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:47:30Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    47,
                    30,
                    4,
                    87,
                    0
                ],
                "title": "Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction\n  in Transformers through Token Correlation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction\n  in Transformers through Token Correlation"
                },
                "summary": "The geometric evolution of token representations in large language models\n(LLMs) presents a fundamental paradox: while human language inherently\norganizes semantic information in low-dimensional spaces ($\\sim 10^1$\ndimensions), modern LLMs employ high-dimensional embeddings ($\\sim 10^3$\ndimensions) processed through Transformer architectures. To resolve this\nparadox, this work bridges this conceptual gap by developing a geometric\nframework that tracks token dynamics across Transformers layers. Through\nlayer-wise analysis of intrinsic dimensions across multiple architectures, we\nreveal an expansion-contraction pattern where tokens diffuse to a \"working\nspace\" and then progressively project onto lower-dimensional submanifolds. Our\nfinding implies a negative correlation between the working space dimension and\nparameter-sensitive performance of the LLMs, and indicates that effective\nmodels tend to compress tokens into approximately 10-dimensional submanifolds,\nclosely resembling human semantic spaces. This work not only advances LLM\ninterpretability by reframing Transformers layers as projectors that mediate\nbetween high-dimensional computation and low-dimensional semantics, but also\nprovides practical tools for model diagnostics that do not rely on\ntask-specific evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The geometric evolution of token representations in large language models\n(LLMs) presents a fundamental paradox: while human language inherently\norganizes semantic information in low-dimensional spaces ($\\sim 10^1$\ndimensions), modern LLMs employ high-dimensional embeddings ($\\sim 10^3$\ndimensions) processed through Transformer architectures. To resolve this\nparadox, this work bridges this conceptual gap by developing a geometric\nframework that tracks token dynamics across Transformers layers. Through\nlayer-wise analysis of intrinsic dimensions across multiple architectures, we\nreveal an expansion-contraction pattern where tokens diffuse to a \"working\nspace\" and then progressively project onto lower-dimensional submanifolds. Our\nfinding implies a negative correlation between the working space dimension and\nparameter-sensitive performance of the LLMs, and indicates that effective\nmodels tend to compress tokens into approximately 10-dimensional submanifolds,\nclosely resembling human semantic spaces. This work not only advances LLM\ninterpretability by reframing Transformers layers as projectors that mediate\nbetween high-dimensional computation and low-dimensional semantics, but also\nprovides practical tools for model diagnostics that do not rely on\ntask-specific evaluations."
                },
                "authors": [
                    {
                        "name": "Zhuo-Yang Song"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Qing-Hong Cao"
                    },
                    {
                        "name": "Ming-xing Luo"
                    },
                    {
                        "name": "Hua Xing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xing Zhu"
                },
                "author": "Hua Xing Zhu",
                "arxiv_comment": "17 pages, 9 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05305v2",
                "updated": "2025-03-28T15:45:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    45,
                    58,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-04T18:18:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    18,
                    18,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "Output Scouting: Auditing Large Language Models for Catastrophic\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output Scouting: Auditing Large Language Models for Catastrophic\n  Responses"
                },
                "summary": "Recent high profile incidents in which the use of Large Language Models\n(LLMs) resulted in significant harm to individuals have brought about a growing\ninterest in AI safety. One reason LLM safety issues occur is that models often\nhave at least some non-zero probability of producing harmful outputs. In this\nwork, we explore the following scenario: imagine an AI safety auditor is\nsearching for catastrophic responses from an LLM (e.g. a \"yes\" responses to\n\"can I fire an employee for being pregnant?\"), and is able to query the model a\nlimited number times (e.g. 1000 times). What is a strategy for querying the\nmodel that would efficiently find those failure responses? To this end, we\npropose output scouting: an approach that aims to generate semantically fluent\noutputs to a given prompt matching any target probability distribution. We then\nrun experiments using two LLMs and find numerous examples of catastrophic\nresponses. We conclude with a discussion that includes advice for practitioners\nwho are looking to implement LLM auditing for catastrophic responses. We also\nrelease an open-source toolkit (https://github.com/joaopfonseca/outputscouting)\nthat implements our auditing framework using the Hugging Face transformers\nlibrary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent high profile incidents in which the use of Large Language Models\n(LLMs) resulted in significant harm to individuals have brought about a growing\ninterest in AI safety. One reason LLM safety issues occur is that models often\nhave at least some non-zero probability of producing harmful outputs. In this\nwork, we explore the following scenario: imagine an AI safety auditor is\nsearching for catastrophic responses from an LLM (e.g. a \"yes\" responses to\n\"can I fire an employee for being pregnant?\"), and is able to query the model a\nlimited number times (e.g. 1000 times). What is a strategy for querying the\nmodel that would efficiently find those failure responses? To this end, we\npropose output scouting: an approach that aims to generate semantically fluent\noutputs to a given prompt matching any target probability distribution. We then\nrun experiments using two LLMs and find numerous examples of catastrophic\nresponses. We conclude with a discussion that includes advice for practitioners\nwho are looking to implement LLM auditing for catastrophic responses. We also\nrelease an open-source toolkit (https://github.com/joaopfonseca/outputscouting)\nthat implements our auditing framework using the Hugging Face transformers\nlibrary."
                },
                "authors": [
                    {
                        "name": "Andrew Bell"
                    },
                    {
                        "name": "Joao Fonseca"
                    }
                ],
                "author_detail": {
                    "name": "Joao Fonseca"
                },
                "author": "Joao Fonseca",
                "arxiv_comment": "Work not ready, further experiments needed to validate the method",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14516v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14516v5",
                "updated": "2025-03-28T15:40:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    40,
                    49,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-18T14:55:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    55,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "Do LLMs \"know\" internally when they follow instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs \"know\" internally when they follow instructions?"
                },
                "summary": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. In this work, we investigate whether LLMs encode\ninformation in their representations that correlate with instruction-following\nsuccess - a property we term knowing internally. Our analysis identifies a\ndirection in the input embedding space, termed the instruction-following\ndimension, that predicts whether a response will comply with a given\ninstruction. We find that this dimension generalizes well across unseen tasks\nbut not across unseen instruction types. We demonstrate that modifying\nrepresentations along this dimension improves instruction-following success\nrates compared to random changes, without compromising response quality.\nFurther investigation reveals that this dimension is more closely related to\nthe phrasing of prompts rather than the inherent difficulty of the task or\ninstructions. This work provides insight into the internal workings of LLMs'\ninstruction-following, paving the way for reliable LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. In this work, we investigate whether LLMs encode\ninformation in their representations that correlate with instruction-following\nsuccess - a property we term knowing internally. Our analysis identifies a\ndirection in the input embedding space, termed the instruction-following\ndimension, that predicts whether a response will comply with a given\ninstruction. We find that this dimension generalizes well across unseen tasks\nbut not across unseen instruction types. We demonstrate that modifying\nrepresentations along this dimension improves instruction-following success\nrates compared to random changes, without compromising response quality.\nFurther investigation reveals that this dimension is more closely related to\nthe phrasing of prompts rather than the inherent difficulty of the task or\ninstructions. This work provides insight into the internal workings of LLMs'\ninstruction-following, paving the way for reliable LLM agents."
                },
                "authors": [
                    {
                        "name": "Juyeon Heo"
                    },
                    {
                        "name": "Christina Heinze-Deml"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Shirley Ren"
                    },
                    {
                        "name": "Udhay Nallasamy"
                    },
                    {
                        "name": "Andy Miller"
                    },
                    {
                        "name": "Jaya Narain"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Narain"
                },
                "author": "Jaya Narain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14516v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14516v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22541v1",
                "updated": "2025-03-28T15:38:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    38,
                    21,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:38:21Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    38,
                    21,
                    4,
                    87,
                    0
                ],
                "title": "SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles"
                },
                "summary": "Accurate motion forecasting is essential for the safety and reliability of\nautonomous driving (AD) systems. While existing methods have made significant\nprogress, they often overlook explicit safety constraints and struggle to\ncapture the complex interactions among traffic agents, environmental factors,\nand motion dynamics. To address these challenges, we present SafeCast, a\nrisk-responsive motion forecasting model that integrates safety-aware\ndecision-making with uncertainty-aware adaptability. SafeCast is the first to\nincorporate the Responsibility-Sensitive Safety (RSS) framework into motion\nforecasting, encoding interpretable safety rules--such as safe distances and\ncollision avoidance--based on traffic norms and physical principles. To further\nenhance robustness, we introduce the Graph Uncertainty Feature (GUF), a\ngraph-based module that injects learnable noise into Graph Attention Networks,\ncapturing real-world uncertainties and enhancing generalization across diverse\nscenarios. We evaluate SafeCast on four real-world benchmark datasets--Next\nGeneration Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the\nMacao Connected Autonomous Driving (MoCAD)--covering highway, urban, and\nmixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA)\naccuracy while maintaining a lightweight architecture and low inference\nlatency, underscoring its potential for real-time deployment in safety-critical\nAD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate motion forecasting is essential for the safety and reliability of\nautonomous driving (AD) systems. While existing methods have made significant\nprogress, they often overlook explicit safety constraints and struggle to\ncapture the complex interactions among traffic agents, environmental factors,\nand motion dynamics. To address these challenges, we present SafeCast, a\nrisk-responsive motion forecasting model that integrates safety-aware\ndecision-making with uncertainty-aware adaptability. SafeCast is the first to\nincorporate the Responsibility-Sensitive Safety (RSS) framework into motion\nforecasting, encoding interpretable safety rules--such as safe distances and\ncollision avoidance--based on traffic norms and physical principles. To further\nenhance robustness, we introduce the Graph Uncertainty Feature (GUF), a\ngraph-based module that injects learnable noise into Graph Attention Networks,\ncapturing real-world uncertainties and enhancing generalization across diverse\nscenarios. We evaluate SafeCast on four real-world benchmark datasets--Next\nGeneration Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the\nMacao Connected Autonomous Driving (MoCAD)--covering highway, urban, and\nmixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA)\naccuracy while maintaining a lightweight architecture and low inference\nlatency, underscoring its potential for real-time deployment in safety-critical\nAD systems."
                },
                "authors": [
                    {
                        "name": "Haicheng Liao"
                    },
                    {
                        "name": "Hanlin Kong"
                    },
                    {
                        "name": "Bin Rao"
                    },
                    {
                        "name": "Bonan Wang"
                    },
                    {
                        "name": "Chengyue Wang"
                    },
                    {
                        "name": "Guyang Yu"
                    },
                    {
                        "name": "Yuming Huang"
                    },
                    {
                        "name": "Ruru Tang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Zhenning Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhenning Li"
                },
                "author": "Zhenning Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05214v2",
                "updated": "2025-03-28T15:34:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    34,
                    58,
                    4,
                    87,
                    0
                ],
                "published": "2025-02-04T17:14:31Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    31,
                    1,
                    35,
                    0
                ],
                "title": "CoRPA: Adversarial Image Generation for Chest X-rays Using Concept\n  Vector Perturbations and Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoRPA: Adversarial Image Generation for Chest X-rays Using Concept\n  Vector Perturbations and Generative Models"
                },
                "summary": "Deep learning models for medical image classification tasks are becoming\nwidely implemented in AI-assisted diagnostic tools, aiming to enhance\ndiagnostic accuracy, reduce clinician workloads, and improve patient outcomes.\nHowever, their vulnerability to adversarial attacks poses significant risks to\npatient safety. Current attack methodologies use general techniques such as\nmodel querying or pixel value perturbations to generate adversarial examples\ndesigned to fool a model. These approaches may not adequately address the\nunique characteristics of clinical errors stemming from missed or incorrectly\nidentified clinical features. We propose the Concept-based Report Perturbation\nAttack (CoRPA), a clinically-focused black-box adversarial attack framework\ntailored to the medical imaging domain. CoRPA leverages clinical concepts to\ngenerate adversarial radiological reports and images that closely mirror\nrealistic clinical misdiagnosis scenarios. We demonstrate the utility of CoRPA\nusing the MIMIC-CXR-JPG dataset of chest X-rays and radiological reports. Our\nevaluation reveals that deep learning models exhibiting strong resilience to\nconventional adversarial attacks are significantly less robust when subjected\nto CoRPA's clinically-focused perturbations. This underscores the importance of\naddressing domain-specific vulnerabilities in medical AI systems. By\nintroducing a specialized adversarial attack framework, this study provides a\nfoundation for developing robust, real-world-ready AI models in healthcare,\nensuring their safe and reliable deployment in high-stakes clinical\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models for medical image classification tasks are becoming\nwidely implemented in AI-assisted diagnostic tools, aiming to enhance\ndiagnostic accuracy, reduce clinician workloads, and improve patient outcomes.\nHowever, their vulnerability to adversarial attacks poses significant risks to\npatient safety. Current attack methodologies use general techniques such as\nmodel querying or pixel value perturbations to generate adversarial examples\ndesigned to fool a model. These approaches may not adequately address the\nunique characteristics of clinical errors stemming from missed or incorrectly\nidentified clinical features. We propose the Concept-based Report Perturbation\nAttack (CoRPA), a clinically-focused black-box adversarial attack framework\ntailored to the medical imaging domain. CoRPA leverages clinical concepts to\ngenerate adversarial radiological reports and images that closely mirror\nrealistic clinical misdiagnosis scenarios. We demonstrate the utility of CoRPA\nusing the MIMIC-CXR-JPG dataset of chest X-rays and radiological reports. Our\nevaluation reveals that deep learning models exhibiting strong resilience to\nconventional adversarial attacks are significantly less robust when subjected\nto CoRPA's clinically-focused perturbations. This underscores the importance of\naddressing domain-specific vulnerabilities in medical AI systems. By\nintroducing a specialized adversarial attack framework, this study provides a\nfoundation for developing robust, real-world-ready AI models in healthcare,\nensuring their safe and reliable deployment in high-stakes clinical\nenvironments."
                },
                "authors": [
                    {
                        "name": "Amy Rafferty"
                    },
                    {
                        "name": "Rishi Ramaesh"
                    },
                    {
                        "name": "Ajitha Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Ajitha Rajan"
                },
                "author": "Ajitha Rajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22530v1",
                "updated": "2025-03-28T15:33:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    33,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:33:19Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    33,
                    19,
                    4,
                    87,
                    0
                ],
                "title": "Optimized Vehicular Antenna Placement for Phase-Coherent Positioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimized Vehicular Antenna Placement for Phase-Coherent Positioning"
                },
                "summary": "Distributed multi-antenna systems are an important enabling technology for\nfuture intelligent transportation systems (ITS), showing promising performance\nin vehicular communications and near-field (NF) localization applications. This\nwork investigates optimal deployments of phase-coherent sub-arrays on a vehicle\nfor NF localization in terms of a Cram\\'er-Rao lower bound (CRLB)-based metric.\nSub-array placements consider practical geometrical constraints on a\nthree-dimensional vehicle model accounting for self-occlusions. Results show\nthat, for coherent NF localization of the vehicle, the aperture spanned by the\nsub-arrays should be maximized and a larger number of sub-arrays results in\nmore even coverage over the vehicle orientations under a fixed total number of\nantenna elements, contrasting with the outcomes of incoherent localization.\nMoreover, while coherent NF processing significantly enhances accuracy, it also\nleads to more intricate cost functions, necessitating computationally more\ncomplex algorithms than incoherent processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed multi-antenna systems are an important enabling technology for\nfuture intelligent transportation systems (ITS), showing promising performance\nin vehicular communications and near-field (NF) localization applications. This\nwork investigates optimal deployments of phase-coherent sub-arrays on a vehicle\nfor NF localization in terms of a Cram\\'er-Rao lower bound (CRLB)-based metric.\nSub-array placements consider practical geometrical constraints on a\nthree-dimensional vehicle model accounting for self-occlusions. Results show\nthat, for coherent NF localization of the vehicle, the aperture spanned by the\nsub-arrays should be maximized and a larger number of sub-arrays results in\nmore even coverage over the vehicle orientations under a fixed total number of\nantenna elements, contrasting with the outcomes of incoherent localization.\nMoreover, while coherent NF processing significantly enhances accuracy, it also\nleads to more intricate cost functions, necessitating computationally more\ncomplex algorithms than incoherent processing."
                },
                "authors": [
                    {
                        "name": "Victor Pettersson"
                    },
                    {
                        "name": "Musa Furkan Keskin"
                    },
                    {
                        "name": "Carina Marcus"
                    },
                    {
                        "name": "Henk Wymeersch"
                    }
                ],
                "author_detail": {
                    "name": "Henk Wymeersch"
                },
                "author": "Henk Wymeersch",
                "arxiv_comment": "6 pages, 6 figures, accepted to IEEE International Conference on\n  Communications 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22522v1",
                "updated": "2025-03-28T15:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    28,
                    4,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    28,
                    4,
                    4,
                    87,
                    0
                ],
                "title": "A Centralized Planning and Distributed Execution Method for Shape\n  Filling with Homogeneous Mobile Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Centralized Planning and Distributed Execution Method for Shape\n  Filling with Homogeneous Mobile Robots"
                },
                "summary": "Nature has inspired humans in different ways. The formation behavior of\nanimals can perform tasks that exceed individual capability. For example, army\nants could transverse gaps by forming bridges, and fishes could group up to\nprotect themselves from predators. The pattern formation task is essential in a\nmultiagent robotic system because it usually serves as the initial\nconfiguration of downstream tasks, such as collective manipulation and\nadaptation to various environments. The formation of complex shapes, especially\nhollow shapes, remains an open question. Traditional approaches either require\nglobal coordinates for each robot or are prone to failure when attempting to\nclose the hole due to accumulated localization errors. Inspired by the ribbon\nidea introduced in the additive self-assembly algorithm by the Kilobot team, we\ndevelop a two-stage algorithm that does not require global coordinates\ninformation and effectively forms shapes with holes. In this paper, we\ninvestigate the partitioning of the shape using ribbons in a hexagonal lattice\nsetting and propose the add-subtract algorithm based on the movement sequence\ninduced by the ribbon structure. This advancement opens the door to tasks\nrequiring complex pattern formations, such as the assembly of nanobots for\nmedical applications involving intricate structures and the deployment of\nrobots along the boundaries of areas of interest. We also provide simulation\nresults on complex shapes, an analysis of the robustness as well as a proof of\ncorrectness of the proposed algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nature has inspired humans in different ways. The formation behavior of\nanimals can perform tasks that exceed individual capability. For example, army\nants could transverse gaps by forming bridges, and fishes could group up to\nprotect themselves from predators. The pattern formation task is essential in a\nmultiagent robotic system because it usually serves as the initial\nconfiguration of downstream tasks, such as collective manipulation and\nadaptation to various environments. The formation of complex shapes, especially\nhollow shapes, remains an open question. Traditional approaches either require\nglobal coordinates for each robot or are prone to failure when attempting to\nclose the hole due to accumulated localization errors. Inspired by the ribbon\nidea introduced in the additive self-assembly algorithm by the Kilobot team, we\ndevelop a two-stage algorithm that does not require global coordinates\ninformation and effectively forms shapes with holes. In this paper, we\ninvestigate the partitioning of the shape using ribbons in a hexagonal lattice\nsetting and propose the add-subtract algorithm based on the movement sequence\ninduced by the ribbon structure. This advancement opens the door to tasks\nrequiring complex pattern formations, such as the assembly of nanobots for\nmedical applications involving intricate structures and the deployment of\nrobots along the boundaries of areas of interest. We also provide simulation\nresults on complex shapes, an analysis of the robustness as well as a proof of\ncorrectness of the proposed algorithm."
                },
                "authors": [
                    {
                        "name": "Shuqing Liu"
                    },
                    {
                        "name": "Rong Su"
                    },
                    {
                        "name": "Karl H. Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Karl H. Johansson"
                },
                "author": "Karl H. Johansson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14739v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14739v4",
                "updated": "2025-03-28T15:21:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    21,
                    44,
                    4,
                    87,
                    0
                ],
                "published": "2025-02-20T17:05:58Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    5,
                    58,
                    3,
                    51,
                    0
                ],
                "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope."
                },
                "authors": [
                    {
                        "name": "M-A-P Team"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Bingli Wang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Zhenlin Wei"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Kaixin Deng"
                    },
                    {
                        "name": "Shawn Gavin"
                    },
                    {
                        "name": "Shian Jia"
                    },
                    {
                        "name": "Sichao Jiang"
                    },
                    {
                        "name": "Yiyan Liao"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Qinrui Li"
                    },
                    {
                        "name": "Sirun Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Yunwen Li"
                    },
                    {
                        "name": "David Ma"
                    },
                    {
                        "name": "Yuansheng Ni"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Qiyao Wang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Tyshawn Hsing"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Yuelin Bai"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Chengtuo Cheng"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Keyi Ding"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Yun Huang"
                    },
                    {
                        "name": "Yaoru Li"
                    },
                    {
                        "name": "Yizhe Li"
                    },
                    {
                        "name": "Zhaoqun Li"
                    },
                    {
                        "name": "Tianhao Liang"
                    },
                    {
                        "name": "Chengdong Lin"
                    },
                    {
                        "name": "Hongquan Lin"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Tianyang Pang"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Qige Qi"
                    },
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Yizhou Tan"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Chenqing Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yiya Wang"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Jiajun Xu"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Tianyang Zhan"
                    },
                    {
                        "name": "Chun Zhang"
                    },
                    {
                        "name": "Jinyang Zhang"
                    },
                    {
                        "name": "Xiyue Zhang"
                    },
                    {
                        "name": "Xingjian Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Xiangyu Zheng"
                    },
                    {
                        "name": "Chenghua Zhong"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Shi Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14739v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14739v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22517v1",
                "updated": "2025-03-28T15:21:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    21,
                    24,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:21:24Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    21,
                    24,
                    4,
                    87,
                    0
                ],
                "title": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative\n  Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative\n  Abilities"
                },
                "summary": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures."
                },
                "authors": [
                    {
                        "name": "Raman Dutt"
                    },
                    {
                        "name": "Harleen Hanspal"
                    },
                    {
                        "name": "Guoxuan Xia"
                    },
                    {
                        "name": "Petru-Daniel Tudosiu"
                    },
                    {
                        "name": "Alexander Black"
                    },
                    {
                        "name": "Yongxin Yang"
                    },
                    {
                        "name": "Steven McDonagh"
                    },
                    {
                        "name": "Sarah Parisot"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Parisot"
                },
                "author": "Sarah Parisot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22512v1",
                "updated": "2025-03-28T15:15:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    15,
                    56,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T15:15:56Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    15,
                    56,
                    4,
                    87,
                    0
                ],
                "title": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages\n  Through Cross-Language Translation and Multi-Agent Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages\n  Through Cross-Language Translation and Multi-Agent Refinement"
                },
                "summary": "Recent advances in leveraging LLMs for APR have demonstrated impressive\ncapabilities in fixing software defects. However, current LLM-based approaches\npredominantly focus on mainstream programming languages like Java and Python,\nneglecting less prevalent but emerging languages such as Rust due to expensive\ntraining resources, limited datasets, and insufficient community support. This\nnarrow focus creates a significant gap in repair capabilities across the\nprogramming language spectrum, where the full potential of LLMs for\ncomprehensive multilingual program repair remains largely unexplored. To\naddress this limitation, we introduce a novel cross-language program repair\napproach LANTERN that leverages LLMs' differential proficiency across languages\nthrough a multi-agent iterative repair paradigm. Our technique strategically\ntranslates defective code from languages where LLMs exhibit weaker repair\ncapabilities to languages where they demonstrate stronger performance, without\nrequiring additional training. A key innovation of our approach is an LLM-based\ndecision-making system that dynamically selects optimal target languages based\non bug characteristics and continuously incorporates feedback from previous\nrepair attempts. We evaluate our method on xCodeEval, a comprehensive\nmultilingual benchmark comprising 5,068 bugs across 11 programming languages.\nResults demonstrate significant enhancement in repair effectiveness,\nparticularly for underrepresented languages, with Rust showing a 22.09%\nimprovement in Pass@10 metrics. Our research provides the first empirical\nevidence that cross-language translation significantly expands the repair\ncapabilities of LLMs and effectively bridges the performance gap between\nprogramming languages with different levels of popularity, opening new avenues\nfor truly language-agnostic automated program repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in leveraging LLMs for APR have demonstrated impressive\ncapabilities in fixing software defects. However, current LLM-based approaches\npredominantly focus on mainstream programming languages like Java and Python,\nneglecting less prevalent but emerging languages such as Rust due to expensive\ntraining resources, limited datasets, and insufficient community support. This\nnarrow focus creates a significant gap in repair capabilities across the\nprogramming language spectrum, where the full potential of LLMs for\ncomprehensive multilingual program repair remains largely unexplored. To\naddress this limitation, we introduce a novel cross-language program repair\napproach LANTERN that leverages LLMs' differential proficiency across languages\nthrough a multi-agent iterative repair paradigm. Our technique strategically\ntranslates defective code from languages where LLMs exhibit weaker repair\ncapabilities to languages where they demonstrate stronger performance, without\nrequiring additional training. A key innovation of our approach is an LLM-based\ndecision-making system that dynamically selects optimal target languages based\non bug characteristics and continuously incorporates feedback from previous\nrepair attempts. We evaluate our method on xCodeEval, a comprehensive\nmultilingual benchmark comprising 5,068 bugs across 11 programming languages.\nResults demonstrate significant enhancement in repair effectiveness,\nparticularly for underrepresented languages, with Rust showing a 22.09%\nimprovement in Pass@10 metrics. Our research provides the first empirical\nevidence that cross-language translation significantly expands the repair\ncapabilities of LLMs and effectively bridges the performance gap between\nprogramming languages with different levels of popularity, opening new avenues\nfor truly language-agnostic automated program repair."
                },
                "authors": [
                    {
                        "name": "Wenqiang Luo"
                    },
                    {
                        "name": "Jacky Wai Keung"
                    },
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Tegawende F. Bissyande"
                    },
                    {
                        "name": "Haoye Tian"
                    },
                    {
                        "name": "Bach Le"
                    }
                ],
                "author_detail": {
                    "name": "Bach Le"
                },
                "author": "Bach Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19619v2",
                "updated": "2025-03-28T14:52:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    52,
                    31,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-25T13:08:26Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    8,
                    26,
                    1,
                    84,
                    0
                ],
                "title": "Exploring Next Token Prediction For Optimizing Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Next Token Prediction For Optimizing Databases"
                },
                "summary": "The Next Token Prediction paradigm (NTP, for short) lies at the forefront of\nmodern large foundational models that are pre-trained on diverse and large\ndatasets. These models generalize effectively and have proven to be very\nsuccessful in Natural Language Processing (NLP). Inspired by the generalization\ncapabilities of Large Language Models (LLMs), we investigate whether the same\nNTP paradigm can also be applied to DBMS design and optimization tasks.\nAdopting NTP directly for database optimization is non-trivial due to the\nfundamental differences between the domains. In this paper, we present a\nframework termed Probe and Learn (PoLe) for applying NTP to optimize database\nsystems. PoLe leverages Decision Transformers and hardware-generated tokens to\neffectively incorporate NTP into database systems. Preliminary results from the\nmain-memory index scheduling task demonstrate that adopting NTP can improve\nboth performance and generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Next Token Prediction paradigm (NTP, for short) lies at the forefront of\nmodern large foundational models that are pre-trained on diverse and large\ndatasets. These models generalize effectively and have proven to be very\nsuccessful in Natural Language Processing (NLP). Inspired by the generalization\ncapabilities of Large Language Models (LLMs), we investigate whether the same\nNTP paradigm can also be applied to DBMS design and optimization tasks.\nAdopting NTP directly for database optimization is non-trivial due to the\nfundamental differences between the domains. In this paper, we present a\nframework termed Probe and Learn (PoLe) for applying NTP to optimize database\nsystems. PoLe leverages Decision Transformers and hardware-generated tokens to\neffectively incorporate NTP into database systems. Preliminary results from the\nmain-memory index scheduling task demonstrate that adopting NTP can improve\nboth performance and generalizability."
                },
                "authors": [
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22473v1",
                "updated": "2025-03-28T14:33:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    33,
                    29,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T14:33:29Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    33,
                    29,
                    4,
                    87,
                    0
                ],
                "title": "WorkTeam: Constructing Workflows from Natural Language with Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorkTeam: Constructing Workflows from Natural Language with Multi-Agents"
                },
                "summary": "Workflows play a crucial role in enhancing enterprise efficiency by\norchestrating complex processes with multiple tools or components. However,\nhand-crafted workflow construction requires expert knowledge, presenting\nsignificant technical barriers. Recent advancements in Large Language Models\n(LLMs) have improved the generation of workflows from natural language\ninstructions (aka NL2Workflow), yet existing single LLM agent-based methods\nface performance degradation on complex tasks due to the need for specialized\nknowledge and the strain of task-switching. To tackle these challenges, we\npropose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor,\norchestrator, and filler agent, each with distinct roles that collaboratively\nenhance the conversion process. As there are currently no publicly available\nNL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which\nincludes 3,695 real-world business samples for training and evaluation.\nExperimental results show that our approach significantly increases the success\nrate of workflow construction, providing a novel and effective solution for\nenterprise NL2Workflow services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Workflows play a crucial role in enhancing enterprise efficiency by\norchestrating complex processes with multiple tools or components. However,\nhand-crafted workflow construction requires expert knowledge, presenting\nsignificant technical barriers. Recent advancements in Large Language Models\n(LLMs) have improved the generation of workflows from natural language\ninstructions (aka NL2Workflow), yet existing single LLM agent-based methods\nface performance degradation on complex tasks due to the need for specialized\nknowledge and the strain of task-switching. To tackle these challenges, we\npropose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor,\norchestrator, and filler agent, each with distinct roles that collaboratively\nenhance the conversion process. As there are currently no publicly available\nNL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which\nincludes 3,695 real-world business samples for training and evaluation.\nExperimental results show that our approach significantly increases the success\nrate of workflow construction, providing a novel and effective solution for\nenterprise NL2Workflow services."
                },
                "authors": [
                    {
                        "name": "Hanchao Liu"
                    },
                    {
                        "name": "Rongjun Li"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Ziyu Zhou"
                    },
                    {
                        "name": "Wei Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Peng"
                },
                "author": "Wei Peng",
                "arxiv_comment": "Accepted in NAACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06931v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06931v3",
                "updated": "2025-03-28T14:19:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    19,
                    33,
                    4,
                    87,
                    0
                ],
                "published": "2024-12-09T19:21:05Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    19,
                    21,
                    5,
                    0,
                    344,
                    0
                ],
                "title": "Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based\n  Planning and Manoeuvrability-Driven Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based\n  Planning and Manoeuvrability-Driven Controls"
                },
                "summary": "The ability to wield tools was once considered exclusive to human\nintelligence, but it's now known that many other animals, like crows, possess\nthis capability. Yet, robotic systems still fall short of matching biological\ndexterity. In this paper, we investigate the use of Large Language Models\n(LLMs), tool affordances, and object manoeuvrability for non-prehensile\ntool-based manipulation tasks. Our novel method leverages LLMs based on scene\ninformation and natural language instructions to enable symbolic task planning\nfor tool-object manipulation. This approach allows the system to convert the\nhuman language sentence into a sequence of feasible motion functions. We have\ndeveloped a novel manoeuvrability-driven controller using a new tool affordance\nmodel derived from visual feedback. This controller helps guide the robot's\ntool utilization and manipulation actions, even within confined areas, using a\nstepping incremental approach. The proposed methodology is evaluated with\nexperiments to prove its effectiveness under various manipulation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to wield tools was once considered exclusive to human\nintelligence, but it's now known that many other animals, like crows, possess\nthis capability. Yet, robotic systems still fall short of matching biological\ndexterity. In this paper, we investigate the use of Large Language Models\n(LLMs), tool affordances, and object manoeuvrability for non-prehensile\ntool-based manipulation tasks. Our novel method leverages LLMs based on scene\ninformation and natural language instructions to enable symbolic task planning\nfor tool-object manipulation. This approach allows the system to convert the\nhuman language sentence into a sequence of feasible motion functions. We have\ndeveloped a novel manoeuvrability-driven controller using a new tool affordance\nmodel derived from visual feedback. This controller helps guide the robot's\ntool utilization and manipulation actions, even within confined areas, using a\nstepping incremental approach. The proposed methodology is evaluated with\nexperiments to prove its effectiveness under various manipulation scenarios."
                },
                "authors": [
                    {
                        "name": "Hoi-Yin Lee"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Anqing Duan"
                    },
                    {
                        "name": "Wanyu Ma"
                    },
                    {
                        "name": "Chenguang Yang"
                    },
                    {
                        "name": "David Navarro-Alarcon"
                    }
                ],
                "author_detail": {
                    "name": "David Navarro-Alarcon"
                },
                "author": "David Navarro-Alarcon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06931v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06931v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22458v1",
                "updated": "2025-03-28T14:08:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    8,
                    40,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T14:08:40Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    8,
                    40,
                    4,
                    87,
                    0
                ],
                "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey"
                },
                "summary": "This survey examines evaluation methods for large language model (LLM)-based\nagents in multi-turn conversational settings. Using a PRISMA-inspired\nframework, we systematically reviewed nearly 250 scholarly sources, capturing\nthe state of the art from various venues of publication, and establishing a\nsolid foundation for our analysis. Our study offers a structured approach by\ndeveloping two interrelated taxonomy systems: one that defines \\emph{what to\nevaluate} and another that explains \\emph{how to evaluate}. The first taxonomy\nidentifies key components of LLM-based agents for multi-turn conversations and\ntheir evaluation dimensions, including task completion, response quality, user\nexperience, memory and context retention, as well as planning and tool\nintegration. These components ensure that the performance of conversational\nagents is assessed in a holistic and meaningful manner. The second taxonomy\nsystem focuses on the evaluation methodologies. It categorizes approaches into\nannotation-based evaluations, automated metrics, hybrid strategies that combine\nhuman assessments with quantitative measures, and self-judging methods\nutilizing LLMs. This framework not only captures traditional metrics derived\nfrom language understanding, such as BLEU and ROUGE scores, but also\nincorporates advanced techniques that reflect the dynamic, interactive nature\nof multi-turn dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey examines evaluation methods for large language model (LLM)-based\nagents in multi-turn conversational settings. Using a PRISMA-inspired\nframework, we systematically reviewed nearly 250 scholarly sources, capturing\nthe state of the art from various venues of publication, and establishing a\nsolid foundation for our analysis. Our study offers a structured approach by\ndeveloping two interrelated taxonomy systems: one that defines \\emph{what to\nevaluate} and another that explains \\emph{how to evaluate}. The first taxonomy\nidentifies key components of LLM-based agents for multi-turn conversations and\ntheir evaluation dimensions, including task completion, response quality, user\nexperience, memory and context retention, as well as planning and tool\nintegration. These components ensure that the performance of conversational\nagents is assessed in a holistic and meaningful manner. The second taxonomy\nsystem focuses on the evaluation methodologies. It categorizes approaches into\nannotation-based evaluations, automated metrics, hybrid strategies that combine\nhuman assessments with quantitative measures, and self-judging methods\nutilizing LLMs. This framework not only captures traditional metrics derived\nfrom language understanding, such as BLEU and ROUGE scores, but also\nincorporates advanced techniques that reflect the dynamic, interactive nature\nof multi-turn dialogues."
                },
                "authors": [
                    {
                        "name": "Shengyue Guan"
                    },
                    {
                        "name": "Haoyi Xiong"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Bin Zhu"
                    },
                    {
                        "name": "Jian-guang Lou"
                    }
                ],
                "author_detail": {
                    "name": "Jian-guang Lou"
                },
                "author": "Jian-guang Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22456v1",
                "updated": "2025-03-28T14:07:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    7,
                    51,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T14:07:51Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    7,
                    51,
                    4,
                    87,
                    0
                ],
                "title": "Entropy-guided sequence weighting for efficient exploration in RL-based\n  LLM fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-guided sequence weighting for efficient exploration in RL-based\n  LLM fine-tuning"
                },
                "summary": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Abdullah Vanlioglu"
                    }
                ],
                "author_detail": {
                    "name": "Abdullah Vanlioglu"
                },
                "author": "Abdullah Vanlioglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22451v1",
                "updated": "2025-03-28T14:03:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    3,
                    14,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T14:03:14Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    3,
                    14,
                    4,
                    87,
                    0
                ],
                "title": "STADE: Standard Deviation as a Pruning Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STADE: Standard Deviation as a Pruning Metric"
                },
                "summary": "Recently, Large Language Models (LLMs) have become very widespread and are\nused to solve a wide variety of tasks. To successfully handle these tasks, LLMs\nrequire longer training times and larger model sizes. This makes LLMs ideal\ncandidates for pruning methods that reduce computational demands while\nmaintaining performance. Previous methods require a retraining phase after\npruning to maintain the original model's performance. However, state-of-the-art\npruning methods, such as Wanda, prune the model without retraining, making the\npruning process faster and more efficient. Building upon Wanda's work, this\nstudy provides a theoretical explanation of why the method is effective and\nleverages these insights to enhance the pruning process. Specifically, a\ntheoretical analysis of the pruning problem reveals a common scenario in\nMachine Learning where Wanda is the optimal pruning method. Furthermore, this\nanalysis is extended to cases where Wanda is no longer optimal, leading to the\ndevelopment of a new method, STADE, based on the standard deviation of the\ninput. From a theoretical standpoint, STADE demonstrates better generality\nacross different scenarios. Finally, extensive experiments on Llama and Open\nPre-trained Transformers (OPT) models validate these theoretical findings,\nshowing that depending on the training conditions, Wanda's optimal performance\nvaries as predicted by the theoretical framework. These insights contribute to\na more robust understanding of pruning strategies and their practical\nimplications. Code is available at: https://github.com/Coello-dev/STADE/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have become very widespread and are\nused to solve a wide variety of tasks. To successfully handle these tasks, LLMs\nrequire longer training times and larger model sizes. This makes LLMs ideal\ncandidates for pruning methods that reduce computational demands while\nmaintaining performance. Previous methods require a retraining phase after\npruning to maintain the original model's performance. However, state-of-the-art\npruning methods, such as Wanda, prune the model without retraining, making the\npruning process faster and more efficient. Building upon Wanda's work, this\nstudy provides a theoretical explanation of why the method is effective and\nleverages these insights to enhance the pruning process. Specifically, a\ntheoretical analysis of the pruning problem reveals a common scenario in\nMachine Learning where Wanda is the optimal pruning method. Furthermore, this\nanalysis is extended to cases where Wanda is no longer optimal, leading to the\ndevelopment of a new method, STADE, based on the standard deviation of the\ninput. From a theoretical standpoint, STADE demonstrates better generality\nacross different scenarios. Finally, extensive experiments on Llama and Open\nPre-trained Transformers (OPT) models validate these theoretical findings,\nshowing that depending on the training conditions, Wanda's optimal performance\nvaries as predicted by the theoretical framework. These insights contribute to\na more robust understanding of pruning strategies and their practical\nimplications. Code is available at: https://github.com/Coello-dev/STADE/"
                },
                "authors": [
                    {
                        "name": "Diego Coello de Portugal Mecke"
                    },
                    {
                        "name": "Haya Alyoussef"
                    },
                    {
                        "name": "Ilia Koloiarov"
                    },
                    {
                        "name": "Maximilian Stubbemann"
                    },
                    {
                        "name": "Lars Schmidt-Thieme"
                    }
                ],
                "author_detail": {
                    "name": "Lars Schmidt-Thieme"
                },
                "author": "Lars Schmidt-Thieme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22436v1",
                "updated": "2025-03-28T13:55:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    55,
                    16,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:55:16Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    55,
                    16,
                    4,
                    87,
                    0
                ],
                "title": "NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous\n  Driving"
                },
                "summary": "Multi-view 3D visual grounding is critical for autonomous driving vehicles to\ninterpret natural languages and localize target objects in complex\nenvironments. However, existing datasets and methods suffer from coarse-grained\nlanguage instructions, and inadequate integration of 3D geometric reasoning\nwith linguistic comprehension. To this end, we introduce NuGrounding, the first\nlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.\nWe present a Hierarchy of Grounding (HoG) method to construct NuGrounding to\ngenerate hierarchical multi-level instructions, ensuring comprehensive coverage\nof human instruction patterns. To tackle this challenging dataset, we propose a\nnovel paradigm that seamlessly combines instruction comprehension abilities of\nmulti-modal LLMs (MLLMs) with precise localization abilities of specialist\ndetection models. Our approach introduces two decoupled task tokens and a\ncontext query to aggregate 3D geometric information and semantic instructions,\nfollowed by a fusion decoder to refine spatial-semantic feature fusion for\nprecise localization. Extensive experiments demonstrate that our method\nsignificantly outperforms the baselines adapted from representative 3D scene\nunderstanding methods by a significant margin and achieves 0.59 in precision\nand 0.64 in recall, with improvements of 50.8% and 54.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view 3D visual grounding is critical for autonomous driving vehicles to\ninterpret natural languages and localize target objects in complex\nenvironments. However, existing datasets and methods suffer from coarse-grained\nlanguage instructions, and inadequate integration of 3D geometric reasoning\nwith linguistic comprehension. To this end, we introduce NuGrounding, the first\nlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.\nWe present a Hierarchy of Grounding (HoG) method to construct NuGrounding to\ngenerate hierarchical multi-level instructions, ensuring comprehensive coverage\nof human instruction patterns. To tackle this challenging dataset, we propose a\nnovel paradigm that seamlessly combines instruction comprehension abilities of\nmulti-modal LLMs (MLLMs) with precise localization abilities of specialist\ndetection models. Our approach introduces two decoupled task tokens and a\ncontext query to aggregate 3D geometric information and semantic instructions,\nfollowed by a fusion decoder to refine spatial-semantic feature fusion for\nprecise localization. Extensive experiments demonstrate that our method\nsignificantly outperforms the baselines adapted from representative 3D scene\nunderstanding methods by a significant margin and achieves 0.59 in precision\nand 0.64 in recall, with improvements of 50.8% and 54.7%."
                },
                "authors": [
                    {
                        "name": "Fuhao Li"
                    },
                    {
                        "name": "Huan Jin"
                    },
                    {
                        "name": "Bin Gao"
                    },
                    {
                        "name": "Liaoyuan Fan"
                    },
                    {
                        "name": "Lihui Jiang"
                    },
                    {
                        "name": "Long Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Long Zeng"
                },
                "author": "Long Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22435v1",
                "updated": "2025-03-28T13:55:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    55,
                    7,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:55:07Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    55,
                    7,
                    4,
                    87,
                    0
                ],
                "title": "Numerical optimization of aviation decarbonization scenarios: balancing\n  traffic and emissions with maturing energy carriers and aircraft technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical optimization of aviation decarbonization scenarios: balancing\n  traffic and emissions with maturing energy carriers and aircraft technology"
                },
                "summary": "Despite being considered a hard-to-abate sector, aviation's emissions will\nplay an important role in long-term climate mitigation of transportation. The\nintroduction of low-carbon energy carriers and the deployment of new aircraft\nin the current fleet are modeled as a technology-centered decarbonization\npolicy, and supply constraints in targeted market segments are modeled as\ndemand-side policy. Shared socioeconomic pathways (SSP) are used to estimate\nthe trend traffic demand and limit the sectoral consumption of electricity and\nbiomass. Mitigation scenarios are formulated as optimization problems and three\napplications are demonstrated: single-policy optimization, scenario-robust\npolicy, and multiobjective policy trade-off. Overall, we find that the choice\nof energy carrier to embark is highly dependent on assumptions regarding\naircraft technology and background energy system, and that aligning trend\nscenarios with the Paris Agreement market-targeted traffic constraints are\nrequired to align trend scenarios with the Paris Agreement. The usual burdens\nassociated with nonlinear optimization with high-dimensional variables are\ndealt with by jointly using libraries for Multidisciplinary Optimization\n(GEMSEO) and Automatic Differentiation (JAX), which resulted in speedups of two\norders of magnitude at the optimization level, while reducing associated\nimplementation efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being considered a hard-to-abate sector, aviation's emissions will\nplay an important role in long-term climate mitigation of transportation. The\nintroduction of low-carbon energy carriers and the deployment of new aircraft\nin the current fleet are modeled as a technology-centered decarbonization\npolicy, and supply constraints in targeted market segments are modeled as\ndemand-side policy. Shared socioeconomic pathways (SSP) are used to estimate\nthe trend traffic demand and limit the sectoral consumption of electricity and\nbiomass. Mitigation scenarios are formulated as optimization problems and three\napplications are demonstrated: single-policy optimization, scenario-robust\npolicy, and multiobjective policy trade-off. Overall, we find that the choice\nof energy carrier to embark is highly dependent on assumptions regarding\naircraft technology and background energy system, and that aligning trend\nscenarios with the Paris Agreement market-targeted traffic constraints are\nrequired to align trend scenarios with the Paris Agreement. The usual burdens\nassociated with nonlinear optimization with high-dimensional variables are\ndealt with by jointly using libraries for Multidisciplinary Optimization\n(GEMSEO) and Automatic Differentiation (JAX), which resulted in speedups of two\norders of magnitude at the optimization level, while reducing associated\nimplementation efforts."
                },
                "authors": [
                    {
                        "name": "Ian Costa-Alves"
                    },
                    {
                        "name": "Nicolas Gourdain"
                    },
                    {
                        "name": "François Gallard"
                    },
                    {
                        "name": "Anne Gazaix"
                    },
                    {
                        "name": "Yri Amandine Kambiri"
                    },
                    {
                        "name": "Thierry Druot"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Druot"
                },
                "arxiv_affiliation": "ENAC",
                "author": "Thierry Druot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22424v1",
                "updated": "2025-03-28T13:36:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    36,
                    26,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:36:26Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    36,
                    26,
                    4,
                    87,
                    0
                ],
                "title": "CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph\n  Searching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph\n  Searching"
                },
                "summary": "Large language models (LLMs) have significantly advanced autonomous software\nengineering, leading to a growing number of software engineering agents that\nassist developers in automatic program repair. Issue localization forms the\nbasis for accurate patch generation. However, because of limitations caused by\nthe context window length of LLMs, existing issue localization methods face\nchallenges in balancing concise yet effective contexts and adequately\ncomprehensive search spaces. In this paper, we introduce CoSIL, an LLM driven,\nsimple yet powerful function level issue localization method without training\nor indexing. CoSIL reduces the search space through module call graphs,\niteratively searches the function call graph to obtain relevant contexts, and\nuses context pruning to control the search direction and manage contexts\neffectively. Importantly, the call graph is dynamically constructed by the LLM\nduring search, eliminating the need for pre-parsing. Experiment results\ndemonstrate that CoSIL achieves a Top-1 localization success rate of 43 percent\nand 44.6 percent on SWE bench Lite and SWE bench Verified, respectively, using\nQwen2.5 Coder 32B, outperforming existing methods by 8.6 to 98.2 percent. When\nCoSIL is applied to guide the patch generation stage, the resolved rate further\nimproves by 9.3 to 31.5 percent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced autonomous software\nengineering, leading to a growing number of software engineering agents that\nassist developers in automatic program repair. Issue localization forms the\nbasis for accurate patch generation. However, because of limitations caused by\nthe context window length of LLMs, existing issue localization methods face\nchallenges in balancing concise yet effective contexts and adequately\ncomprehensive search spaces. In this paper, we introduce CoSIL, an LLM driven,\nsimple yet powerful function level issue localization method without training\nor indexing. CoSIL reduces the search space through module call graphs,\niteratively searches the function call graph to obtain relevant contexts, and\nuses context pruning to control the search direction and manage contexts\neffectively. Importantly, the call graph is dynamically constructed by the LLM\nduring search, eliminating the need for pre-parsing. Experiment results\ndemonstrate that CoSIL achieves a Top-1 localization success rate of 43 percent\nand 44.6 percent on SWE bench Lite and SWE bench Verified, respectively, using\nQwen2.5 Coder 32B, outperforming existing methods by 8.6 to 98.2 percent. When\nCoSIL is applied to guide the patch generation stage, the resolved rate further\nimproves by 9.3 to 31.5 percent."
                },
                "authors": [
                    {
                        "name": "Zhonghao Jiang"
                    },
                    {
                        "name": "Xiaoxue Ren"
                    },
                    {
                        "name": "Meng Yan"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Zhongxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxin Liu"
                },
                "author": "Zhongxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22420v1",
                "updated": "2025-03-28T13:32:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    32,
                    29,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:32:29Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    32,
                    29,
                    4,
                    87,
                    0
                ],
                "title": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric\n  Evaluation with Chain-of-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric\n  Evaluation with Chain-of-Analysis"
                },
                "summary": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments."
                },
                "authors": [
                    {
                        "name": "Jiangyong Huang"
                    },
                    {
                        "name": "Baoxiong Jia"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Ziyu Zhu"
                    },
                    {
                        "name": "Xiongkun Linghu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Siyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Huang"
                },
                "author": "Siyuan Huang",
                "arxiv_comment": "CVPR 2025. Project page: https://beacon-3d.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16021v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16021v3",
                "updated": "2025-03-28T13:23:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    23,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-20T10:37:29Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    37,
                    29,
                    3,
                    79,
                    0
                ],
                "title": "Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems"
                },
                "summary": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's impact on the\ndiversity and democratic value of information ecosystems. We introduce a\nlarge-scale simulation framework to examine AI-based imitation within news, a\ncontext crucial for public discourse. By systematically testing two distinct\nimitation strategies across a range of information environments varying in\ninitial diversity, we demonstrate that AI-generated articles do not uniformly\nhomogenize content. Instead, AI's influence is strongly context-dependent:\nAI-generated content can introduce valuable diversity in originally homogeneous\nnews environments but diminish diversity in initially heterogeneous contexts.\nThese results illustrate that the initial diversity of an information\nenvironment critically shapes AI's impact, challenging assumptions that\nAI-driven imitation threatens diversity. Instead, when information is initially\nhomogeneous, AI-driven imitation can expand perspectives, styles, and topics.\nThis is especially important in news contexts, where information diversity\nfosters richer public debate by exposing citizens to alternative viewpoints,\nchallenging biases, and preventing narrative monopolies, which is essential for\na resilient democracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's impact on the\ndiversity and democratic value of information ecosystems. We introduce a\nlarge-scale simulation framework to examine AI-based imitation within news, a\ncontext crucial for public discourse. By systematically testing two distinct\nimitation strategies across a range of information environments varying in\ninitial diversity, we demonstrate that AI-generated articles do not uniformly\nhomogenize content. Instead, AI's influence is strongly context-dependent:\nAI-generated content can introduce valuable diversity in originally homogeneous\nnews environments but diminish diversity in initially heterogeneous contexts.\nThese results illustrate that the initial diversity of an information\nenvironment critically shapes AI's impact, challenging assumptions that\nAI-driven imitation threatens diversity. Instead, when information is initially\nhomogeneous, AI-driven imitation can expand perspectives, styles, and topics.\nThis is especially important in news contexts, where information diversity\nfosters richer public debate by exposing citizens to alternative viewpoints,\nchallenging biases, and preventing narrative monopolies, which is essential for\na resilient democracy."
                },
                "authors": [
                    {
                        "name": "Emil Bakkensen Johansen"
                    },
                    {
                        "name": "Oliver Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Baumann"
                },
                "author": "Oliver Baumann",
                "arxiv_comment": "42 pages, 11 figures, 4 tables; v2: corrected typographical errors,\n  streamlined language, updated abstract, added supplementary information; v3:\n  restructured appendix, added temperature and embeddings sensitivity checks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16021v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16021v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22406v1",
                "updated": "2025-03-28T13:16:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    16,
                    27,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:16:27Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    16,
                    27,
                    4,
                    87,
                    0
                ],
                "title": "Training Large Language Models for Advanced Typosquatting Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models for Advanced Typosquatting Detection"
                },
                "summary": "Typosquatting is a long-standing cyber threat that exploits human error in\ntyping URLs to deceive users, distribute malware, and conduct phishing attacks.\nWith the proliferation of domain names and new Top-Level Domains (TLDs),\ntyposquatting techniques have grown more sophisticated, posing significant\nrisks to individuals, businesses, and national cybersecurity infrastructure.\nTraditional detection methods primarily focus on well-known impersonation\npatterns, leaving gaps in identifying more complex attacks. This study\nintroduces a novel approach leveraging large language models (LLMs) to enhance\ntyposquatting detection. By training an LLM on character-level transformations\nand pattern-based heuristics rather than domain-specific data, a more adaptable\nand resilient detection mechanism develops. Experimental results indicate that\nthe Phi-4 14B model outperformed other tested models when properly fine tuned\nachieving a 98% accuracy rate with only a few thousand training samples. This\nresearch highlights the potential of LLMs in cybersecurity applications,\nspecifically in mitigating domain-based deception tactics, and provides\ninsights into optimizing machine learning strategies for threat detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typosquatting is a long-standing cyber threat that exploits human error in\ntyping URLs to deceive users, distribute malware, and conduct phishing attacks.\nWith the proliferation of domain names and new Top-Level Domains (TLDs),\ntyposquatting techniques have grown more sophisticated, posing significant\nrisks to individuals, businesses, and national cybersecurity infrastructure.\nTraditional detection methods primarily focus on well-known impersonation\npatterns, leaving gaps in identifying more complex attacks. This study\nintroduces a novel approach leveraging large language models (LLMs) to enhance\ntyposquatting detection. By training an LLM on character-level transformations\nand pattern-based heuristics rather than domain-specific data, a more adaptable\nand resilient detection mechanism develops. Experimental results indicate that\nthe Phi-4 14B model outperformed other tested models when properly fine tuned\nachieving a 98% accuracy rate with only a few thousand training samples. This\nresearch highlights the potential of LLMs in cybersecurity applications,\nspecifically in mitigating domain-based deception tactics, and provides\ninsights into optimizing machine learning strategies for threat detection."
                },
                "authors": [
                    {
                        "name": "Jackson Welch"
                    }
                ],
                "author_detail": {
                    "name": "Jackson Welch"
                },
                "author": "Jackson Welch",
                "arxiv_comment": "6 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22402v1",
                "updated": "2025-03-28T13:11:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    11,
                    27,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:11:27Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    11,
                    27,
                    4,
                    87,
                    0
                ],
                "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing"
                },
                "summary": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL."
                },
                "authors": [
                    {
                        "name": "Yizhang Zhu"
                    },
                    {
                        "name": "Runzhi Jiang"
                    },
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "arxiv_comment": "19 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22401v1",
                "updated": "2025-03-28T13:10:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    10,
                    4,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:10:04Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    10,
                    4,
                    4,
                    87,
                    0
                ],
                "title": "Generative Reliability-Based Design Optimization Using In-Context\n  Learning Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Reliability-Based Design Optimization Using In-Context\n  Learning Capabilities of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable in-context learning\ncapabilities, enabling flexible utilization of limited historical information\nto play pivotal roles in reasoning, problem-solving, and complex pattern\nrecognition tasks. Inspired by the successful applications of LLMs in multiple\ndomains, this paper proposes a generative design method by leveraging the\nin-context learning capabilities of LLMs with the iterative search mechanisms\nof metaheuristic algorithms for solving reliability-based design optimization\nproblems. In detail, reliability analysis is performed by engaging the LLMs and\nKriging surrogate modeling to overcome the computational burden. By dynamically\nproviding critical information of design points to the LLMs with prompt\nengineering, the method enables rapid generation of high-quality design\nalternatives that satisfy reliability constraints while achieving performance\noptimization. With the Deepseek-V3 model, three case studies are used to\ndemonstrated the performance of the proposed approach. Experimental results\nindicate that the proposed LLM-RBDO method successfully identifies feasible\nsolutions that meet reliability constraints while achieving a comparable\nconvergence rate compared to traditional genetic algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable in-context learning\ncapabilities, enabling flexible utilization of limited historical information\nto play pivotal roles in reasoning, problem-solving, and complex pattern\nrecognition tasks. Inspired by the successful applications of LLMs in multiple\ndomains, this paper proposes a generative design method by leveraging the\nin-context learning capabilities of LLMs with the iterative search mechanisms\nof metaheuristic algorithms for solving reliability-based design optimization\nproblems. In detail, reliability analysis is performed by engaging the LLMs and\nKriging surrogate modeling to overcome the computational burden. By dynamically\nproviding critical information of design points to the LLMs with prompt\nengineering, the method enables rapid generation of high-quality design\nalternatives that satisfy reliability constraints while achieving performance\noptimization. With the Deepseek-V3 model, three case studies are used to\ndemonstrated the performance of the proposed approach. Experimental results\nindicate that the proposed LLM-RBDO method successfully identifies feasible\nsolutions that meet reliability constraints while achieving a comparable\nconvergence rate compared to traditional genetic algorithms."
                },
                "authors": [
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Qian Tang"
                    },
                    {
                        "name": "Zequn Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zequn Wang"
                },
                "author": "Zequn Wang",
                "arxiv_comment": "17 pages, 11 figures, 4tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22395v1",
                "updated": "2025-03-28T13:04:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    4,
                    41,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T13:04:41Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    4,
                    41,
                    4,
                    87,
                    0
                ],
                "title": "Negation: A Pink Elephant in the Large Language Models' Room?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negation: A Pink Elephant in the Large Language Models' Room?"
                },
                "summary": "Negations are key to determining sentence meaning, making them essential for\nlogical reasoning. Despite their importance, negations pose a substantial\nchallenge for large language models (LLMs) and remain underexplored.\n  We construct two multilingual natural language inference (NLI) datasets with\n\\textit{paired} examples differing in negation. We investigate how model size\nand language impact its ability to handle negation correctly by evaluating\npopular LLMs.\n  Contrary to previous work, we show that increasing the model size\nconsistently improves the models' ability to handle negations. Furthermore, we\nfind that both the models' reasoning accuracy and robustness to negation are\nlanguage-dependent and that the length and explicitness of the premise have a\ngreater impact on robustness than language.\n  Our datasets can facilitate further research and improvements of language\nmodel reasoning in multilingual settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negations are key to determining sentence meaning, making them essential for\nlogical reasoning. Despite their importance, negations pose a substantial\nchallenge for large language models (LLMs) and remain underexplored.\n  We construct two multilingual natural language inference (NLI) datasets with\n\\textit{paired} examples differing in negation. We investigate how model size\nand language impact its ability to handle negation correctly by evaluating\npopular LLMs.\n  Contrary to previous work, we show that increasing the model size\nconsistently improves the models' ability to handle negations. Furthermore, we\nfind that both the models' reasoning accuracy and robustness to negation are\nlanguage-dependent and that the length and explicitness of the premise have a\ngreater impact on robustness than language.\n  Our datasets can facilitate further research and improvements of language\nmodel reasoning in multilingual settings."
                },
                "authors": [
                    {
                        "name": "Tereza Vrabcová"
                    },
                    {
                        "name": "Marek Kadlčík"
                    },
                    {
                        "name": "Petr Sojka"
                    },
                    {
                        "name": "Michal Štefánik"
                    },
                    {
                        "name": "Michal Spiegel"
                    }
                ],
                "author_detail": {
                    "name": "Michal Spiegel"
                },
                "author": "Michal Spiegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22388v1",
                "updated": "2025-03-28T12:46:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    46,
                    54,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T12:46:54Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    46,
                    54,
                    4,
                    87,
                    0
                ],
                "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers\n  for Multi-Hop and Multi-Bug Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers\n  for Multi-Hop and Multi-Bug Errors"
                },
                "summary": "LLMs are transforming software development, yet current code generation and\ncode repair benchmarks mainly assess syntactic and functional correctness in\nsimple, single-error cases. LLMs' capabilities to autonomously find and fix\nruntime logical errors in complex data science code remain largely unexplored.\nTo address this gap, we introduce DSDBench: the Data Science Debugging\nBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hop\nerror tracing and multi-bug detection in data science code debugging. DSDBench\nadapts datasets from existing data science task benchmarks, such as DABench and\nMatPlotBench, featuring realistic data science debugging tasks with\nautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes\n1,117 annotated samples with 741 cause-effect error pairs and runtime error\nmessages. Evaluations of state-of-the-art LLMs on DSDBench show significant\nperformance gaps, highlighting challenges in debugging logical runtime errors\nin data science code. DSDBench offers a crucial resource to evaluate and\nimprove LLMs' debugging and reasoning capabilities, enabling more reliable\nAI-assisted data science in the future.DSDBench is publicly available at\nhttps://github.com/KevinCL16/DSDBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are transforming software development, yet current code generation and\ncode repair benchmarks mainly assess syntactic and functional correctness in\nsimple, single-error cases. LLMs' capabilities to autonomously find and fix\nruntime logical errors in complex data science code remain largely unexplored.\nTo address this gap, we introduce DSDBench: the Data Science Debugging\nBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hop\nerror tracing and multi-bug detection in data science code debugging. DSDBench\nadapts datasets from existing data science task benchmarks, such as DABench and\nMatPlotBench, featuring realistic data science debugging tasks with\nautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes\n1,117 annotated samples with 741 cause-effect error pairs and runtime error\nmessages. Evaluations of state-of-the-art LLMs on DSDBench show significant\nperformance gaps, highlighting challenges in debugging logical runtime errors\nin data science code. DSDBench offers a crucial resource to evaluate and\nimprove LLMs' debugging and reasoning capabilities, enabling more reliable\nAI-assisted data science in the future.DSDBench is publicly available at\nhttps://github.com/KevinCL16/DSDBench."
                },
                "authors": [
                    {
                        "name": "Zhiyu Yang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Yang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Deng"
                },
                "author": "Yang Deng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21480v2",
                "updated": "2025-03-28T12:34:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    34,
                    25,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-27T13:12:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    12,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs"
                },
                "summary": "The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs."
                },
                "authors": [
                    {
                        "name": "John Murzaku"
                    },
                    {
                        "name": "Owen Rambow"
                    }
                ],
                "author_detail": {
                    "name": "Owen Rambow"
                },
                "author": "Owen Rambow",
                "arxiv_comment": "Submitted to COLM 2025. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22362v1",
                "updated": "2025-03-28T12:12:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    12,
                    38,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T12:12:38Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    12,
                    38,
                    4,
                    87,
                    0
                ],
                "title": "Supposedly Equivalent Facts That Aren't? Entity Frequency in\n  Pre-training Induces Asymmetry in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supposedly Equivalent Facts That Aren't? Entity Frequency in\n  Pre-training Induces Asymmetry in LLMs"
                },
                "summary": "Understanding and mitigating hallucinations in Large Language Models (LLMs)\nis crucial for ensuring reliable content generation. While previous research\nhas primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and\ndirectly links model behaviour to the pre-training data that forms their prior\nknowledge. Specifically, we demonstrate that an asymmetry exists in the\nrecognition of logically equivalent facts, which can be attributed to frequency\ndiscrepancies of entities appearing as subjects versus objects. Given that most\npre-training datasets are inaccessible, we leverage the fully open-source OLMo\nseries by indexing its Dolma dataset to estimate entity frequencies. Using\nrelational facts (represented as triples) from Wikidata5M, we construct probing\ndatasets to isolate this effect. Our experiments reveal that facts with a\nhigh-frequency subject and a low-frequency object are better recognised than\ntheir inverse, despite their logical equivalence. The pattern reverses in\nlow-to-high frequency settings, and no statistically significant asymmetry\nemerges when both entities are high-frequency. These findings highlight the\ninfluential role of pre-training data in shaping model predictions and provide\ninsights for inferring the characteristics of pre-training data in closed or\npartially closed LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and mitigating hallucinations in Large Language Models (LLMs)\nis crucial for ensuring reliable content generation. While previous research\nhas primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and\ndirectly links model behaviour to the pre-training data that forms their prior\nknowledge. Specifically, we demonstrate that an asymmetry exists in the\nrecognition of logically equivalent facts, which can be attributed to frequency\ndiscrepancies of entities appearing as subjects versus objects. Given that most\npre-training datasets are inaccessible, we leverage the fully open-source OLMo\nseries by indexing its Dolma dataset to estimate entity frequencies. Using\nrelational facts (represented as triples) from Wikidata5M, we construct probing\ndatasets to isolate this effect. Our experiments reveal that facts with a\nhigh-frequency subject and a low-frequency object are better recognised than\ntheir inverse, despite their logical equivalence. The pattern reverses in\nlow-to-high frequency settings, and no statistically significant asymmetry\nemerges when both entities are high-frequency. These findings highlight the\ninfluential role of pre-training data in shaping model predictions and provide\ninsights for inferring the characteristics of pre-training data in closed or\npartially closed LLMs."
                },
                "authors": [
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Bailan He"
                    },
                    {
                        "name": "Zifeng Ding"
                    },
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Yuqicheng Zhu"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Ian Horrocks"
                    }
                ],
                "author_detail": {
                    "name": "Ian Horrocks"
                },
                "author": "Ian Horrocks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22353v1",
                "updated": "2025-03-28T11:49:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    49,
                    56,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    49,
                    56,
                    4,
                    87,
                    0
                ],
                "title": "Firm or Fickle? Evaluating Large Language Models Consistency in\n  Sequential Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firm or Fickle? Evaluating Large Language Models Consistency in\n  Sequential Interactions"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications."
                },
                "authors": [
                    {
                        "name": "Yubo Li"
                    },
                    {
                        "name": "Yidi Miao"
                    },
                    {
                        "name": "Xueying Ding"
                    },
                    {
                        "name": "Ramayya Krishnan"
                    },
                    {
                        "name": "Rema Padman"
                    }
                ],
                "author_detail": {
                    "name": "Rema Padman"
                },
                "author": "Rema Padman",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22345v1",
                "updated": "2025-03-28T11:35:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    35,
                    43,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:35:43Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    35,
                    43,
                    4,
                    87,
                    0
                ],
                "title": "Using a Large Language Model as Design Material for an Interactive\n  Museum Installation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using a Large Language Model as Design Material for an Interactive\n  Museum Installation"
                },
                "summary": "We present a work in progress that explores using a Large Language Model\n(LLM) as a design material for an interactive museum installation. LLMs offer\nthe possibility of creating chatbots that can facilitate dynamic and human-like\nconversation, engaging in a form of role play to bring historical persons to\nlife for visitors. However, LLMs are prone to producing misinformation, which\nruns counter to museums' core mission to educate the public. We use\nResearch-through-Design to explore some approaches to navigating this dilemma\nthrough rapid prototyping and evaluation and propose some directions for\nfurther research. We suggest that designers may shape interactions with the\nchatbot to emphasize personal narratives and role play rather than historical\nfacts or to intentionally highlight the unreliability of the chatbot outputs to\nprovoke critical reflection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a work in progress that explores using a Large Language Model\n(LLM) as a design material for an interactive museum installation. LLMs offer\nthe possibility of creating chatbots that can facilitate dynamic and human-like\nconversation, engaging in a form of role play to bring historical persons to\nlife for visitors. However, LLMs are prone to producing misinformation, which\nruns counter to museums' core mission to educate the public. We use\nResearch-through-Design to explore some approaches to navigating this dilemma\nthrough rapid prototyping and evaluation and propose some directions for\nfurther research. We suggest that designers may shape interactions with the\nchatbot to emphasize personal narratives and role play rather than historical\nfacts or to intentionally highlight the unreliability of the chatbot outputs to\nprovoke critical reflection."
                },
                "authors": [
                    {
                        "name": "Maria Padilla Engstrøm"
                    },
                    {
                        "name": "Anders Sundnes Løvlie"
                    }
                ],
                "author_detail": {
                    "name": "Anders Sundnes Løvlie"
                },
                "author": "Anders Sundnes Løvlie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22338v1",
                "updated": "2025-03-28T11:25:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    25,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:25:05Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    25,
                    5,
                    4,
                    87,
                    0
                ],
                "title": "SKDU at De-Factify 4.0: Natural Language Features for AI-Generated\n  Text-Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKDU at De-Factify 4.0: Natural Language Features for AI-Generated\n  Text-Detection"
                },
                "summary": "The rapid advancement of large language models (LLMs) has introduced new\nchallenges in distinguishing human-written text from AI-generated content. In\nthis work, we explored a pipelined approach for AI-generated text detection\nthat includes a feature extraction step (i.e. prompt-based rewriting features\ninspired by RAIDAR and content-based features derived from the NELA toolkit)\nfollowed by a classification module. Comprehensive experiments were conducted\non the Defactify4.0 dataset, evaluating two tasks: binary classification to\ndifferentiate human-written and AI-generated text, and multi-class\nclassification to identify the specific generative model used to generate the\ninput text. Our findings reveal that NELA features significantly outperform\nRAIDAR features in both tasks, demonstrating their ability to capture nuanced\nlinguistic, stylistic, and content-based differences. Combining RAIDAR and NELA\nfeatures provided minimal improvement, highlighting the redundancy introduced\nby less discriminative features. Among the classifiers tested, XGBoost emerged\nas the most effective, leveraging the rich feature sets to achieve high\naccuracy and generalisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has introduced new\nchallenges in distinguishing human-written text from AI-generated content. In\nthis work, we explored a pipelined approach for AI-generated text detection\nthat includes a feature extraction step (i.e. prompt-based rewriting features\ninspired by RAIDAR and content-based features derived from the NELA toolkit)\nfollowed by a classification module. Comprehensive experiments were conducted\non the Defactify4.0 dataset, evaluating two tasks: binary classification to\ndifferentiate human-written and AI-generated text, and multi-class\nclassification to identify the specific generative model used to generate the\ninput text. Our findings reveal that NELA features significantly outperform\nRAIDAR features in both tasks, demonstrating their ability to capture nuanced\nlinguistic, stylistic, and content-based differences. Combining RAIDAR and NELA\nfeatures provided minimal improvement, highlighting the redundancy introduced\nby less discriminative features. Among the classifiers tested, XGBoost emerged\nas the most effective, leveraging the rich feature sets to achieve high\naccuracy and generalisation."
                },
                "authors": [
                    {
                        "name": "Shrikant Malviya"
                    },
                    {
                        "name": "Pablo Arnau-González"
                    },
                    {
                        "name": "Miguel Arevalillo-Herráez"
                    },
                    {
                        "name": "Stamos Katsigiannis"
                    }
                ],
                "author_detail": {
                    "name": "Stamos Katsigiannis"
                },
                "author": "Stamos Katsigiannis",
                "arxiv_comment": "De-Factify 4.0 Workshop at the 39th AAAI Conference on Artificial\n  Intelligence (AAAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Güra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Güra"
                },
                "author": "Fabian Güra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20316v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20316v2",
                "updated": "2025-03-28T11:08:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    2,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-26T08:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    33,
                    3,
                    2,
                    85,
                    0
                ],
                "title": "AI-Driven MRI Spine Pathology Detection: A Comprehensive Deep Learning\n  Approach for Automated Diagnosis in Diverse Clinical Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven MRI Spine Pathology Detection: A Comprehensive Deep Learning\n  Approach for Automated Diagnosis in Diverse Clinical Settings"
                },
                "summary": "Study Design: This study presents the development of an autonomous AI system\nfor MRI spine pathology detection, trained on a dataset of 2 million MRI spine\nscans sourced from diverse healthcare facilities across India. The AI system\nintegrates advanced architectures, including Vision Transformers, U-Net with\ncross-attention, MedSAM, and Cascade R-CNN, enabling comprehensive\nclassification, segmentation, and detection of 43 distinct spinal pathologies.\nThe dataset is balanced across age groups, genders, and scanner manufacturers\nto ensure robustness and adaptability. Subgroup analyses were conducted to\nvalidate the model's performance across different patient demographics, imaging\nconditions, and equipment types.\n  Performance: The AI system achieved up to 97.9 percent multi-pathology\ndetection, demonstrating consistent performance across age, gender, and\nmanufacturer subgroups. The normal vs. abnormal classification achieved 98.0\npercent accuracy, and the system was deployed across 13 major healthcare\nenterprises in India, encompassing diagnostic centers, large hospitals, and\ngovernment facilities. During deployment, it processed approximately 100,000\nplus MRI spine scans, leading to reduced reporting times and increased\ndiagnostic efficiency by automating the identification of common spinal\nconditions.\n  Conclusion: The AI system's high precision and recall validate its capability\nas a reliable tool for autonomous normal/abnormal classification, pathology\nsegmentation, and detection. Its scalability and adaptability address critical\ndiagnostic gaps, optimize radiology workflows, and improve patient care across\nvaried healthcare environments in India.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study Design: This study presents the development of an autonomous AI system\nfor MRI spine pathology detection, trained on a dataset of 2 million MRI spine\nscans sourced from diverse healthcare facilities across India. The AI system\nintegrates advanced architectures, including Vision Transformers, U-Net with\ncross-attention, MedSAM, and Cascade R-CNN, enabling comprehensive\nclassification, segmentation, and detection of 43 distinct spinal pathologies.\nThe dataset is balanced across age groups, genders, and scanner manufacturers\nto ensure robustness and adaptability. Subgroup analyses were conducted to\nvalidate the model's performance across different patient demographics, imaging\nconditions, and equipment types.\n  Performance: The AI system achieved up to 97.9 percent multi-pathology\ndetection, demonstrating consistent performance across age, gender, and\nmanufacturer subgroups. The normal vs. abnormal classification achieved 98.0\npercent accuracy, and the system was deployed across 13 major healthcare\nenterprises in India, encompassing diagnostic centers, large hospitals, and\ngovernment facilities. During deployment, it processed approximately 100,000\nplus MRI spine scans, leading to reduced reporting times and increased\ndiagnostic efficiency by automating the identification of common spinal\nconditions.\n  Conclusion: The AI system's high precision and recall validate its capability\nas a reliable tool for autonomous normal/abnormal classification, pathology\nsegmentation, and detection. Its scalability and adaptability address critical\ndiagnostic gaps, optimize radiology workflows, and improve patient care across\nvaried healthcare environments in India."
                },
                "authors": [
                    {
                        "name": "Bargava Subramanian"
                    },
                    {
                        "name": "Naveen Kumarasami"
                    },
                    {
                        "name": "Praveen Shastry"
                    },
                    {
                        "name": "Raghotham Sripadraj"
                    },
                    {
                        "name": "Kalyan Sivasailam"
                    },
                    {
                        "name": "Anandakumar D"
                    },
                    {
                        "name": "Abinaya Ramachandran"
                    },
                    {
                        "name": "Sudhir MP"
                    },
                    {
                        "name": "Gunakutti G"
                    },
                    {
                        "name": "Kishore Prasath Venkatesh"
                    }
                ],
                "author_detail": {
                    "name": "Kishore Prasath Venkatesh"
                },
                "author": "Kishore Prasath Venkatesh",
                "arxiv_comment": "20 pages , 3 figurea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20316v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21080v2",
                "updated": "2025-03-28T10:57:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    57,
                    38,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-27T01:41:34Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    1,
                    41,
                    34,
                    3,
                    86,
                    0
                ],
                "title": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues"
                },
                "summary": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yunbo Long"
                    }
                ],
                "author_detail": {
                    "name": "Yunbo Long"
                },
                "author": "Yunbo Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22315v1",
                "updated": "2025-03-28T10:43:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    43,
                    41,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T10:43:41Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    43,
                    41,
                    4,
                    87,
                    0
                ],
                "title": "Large Language Models Are Democracy Coders with Attitudes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Democracy Coders with Attitudes"
                },
                "summary": "Current political developments worldwide illustrate that research on\ndemocratic backsliding is as important as ever. A recent exchange in Political\nScience & Politics (2/2024) has highlighted again a fundamental challenge in\nthis literature: the measurement of democracy. With many democracy indicators\nconsisting of subjective assessments rather than factual observations, trends\nin democracy over time could be due to human biases in the coding of these\nindicators rather than empirical facts. In this paper, we leverage two\ncutting-edge Large Language Models (LLMs) for the coding of democracy\nindicators from the V-Dem project. With access to a huge amount of information,\nthese models may be able to rate the many \"soft\" characteristics of regimes\nwithout the cognitive biases that humans potentially possess. While\nLLM-generated codings largely align with expert coders for many countries, we\nshow that when these models deviate from human assessments, they do so in\ndifferent but consistent ways: Some LLMs are too pessimistic, while others\nconsistently overestimate the democratic quality of these countries. While the\ncombination of the two LLM codings can alleviate this concern, we conclude that\nit is difficult to replace human coders with LLMs, since the extent and\ndirection of these attitudes is not known a priori.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current political developments worldwide illustrate that research on\ndemocratic backsliding is as important as ever. A recent exchange in Political\nScience & Politics (2/2024) has highlighted again a fundamental challenge in\nthis literature: the measurement of democracy. With many democracy indicators\nconsisting of subjective assessments rather than factual observations, trends\nin democracy over time could be due to human biases in the coding of these\nindicators rather than empirical facts. In this paper, we leverage two\ncutting-edge Large Language Models (LLMs) for the coding of democracy\nindicators from the V-Dem project. With access to a huge amount of information,\nthese models may be able to rate the many \"soft\" characteristics of regimes\nwithout the cognitive biases that humans potentially possess. While\nLLM-generated codings largely align with expert coders for many countries, we\nshow that when these models deviate from human assessments, they do so in\ndifferent but consistent ways: Some LLMs are too pessimistic, while others\nconsistently overestimate the democratic quality of these countries. While the\ncombination of the two LLM codings can alleviate this concern, we conclude that\nit is difficult to replace human coders with LLMs, since the extent and\ndirection of these attitudes is not known a priori."
                },
                "authors": [
                    {
                        "name": "Nils B. Weidmann"
                    },
                    {
                        "name": "Mats Faulborn"
                    },
                    {
                        "name": "David García"
                    }
                ],
                "author_detail": {
                    "name": "David García"
                },
                "author": "David García",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22309v1",
                "updated": "2025-03-28T10:31:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    31,
                    1,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T10:31:01Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    31,
                    1,
                    4,
                    87,
                    0
                ],
                "title": "A Dataset for Semantic Segmentation in the Presence of Unknowns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset for Semantic Segmentation in the Presence of Unknowns"
                },
                "summary": "Before deployment in the real-world deep neural networks require thorough\nevaluation of how they handle both knowns, inputs represented in the training\ndata, and unknowns (anomalies). This is especially important for scene\nunderstanding tasks with safety critical applications, such as in autonomous\ndriving. Existing datasets allow evaluation of only knowns or unknowns - but\nnot both, which is required to establish \"in the wild\" suitability of deep\nneural network models. To bridge this gap, we propose a novel anomaly\nsegmentation dataset, ISSU, that features a diverse set of anomaly inputs from\ncluttered real-world environments. The dataset is twice larger than existing\nanomaly segmentation datasets, and provides a training, validation and test set\nfor controlled in-domain evaluation. The test set consists of a static and\ntemporal part, with the latter comprised of videos. The dataset provides\nannotations for both closed-set (knowns) and anomalies, enabling closed-set and\nopen-set evaluation. The dataset covers diverse conditions, such as domain and\ncross-sensor shift, illumination variation and allows ablation of anomaly\ndetection methods with respect to these variations. Evaluation results of\ncurrent state-of-the-art methods confirm the need for improvements especially\nin domain-generalization, small and large object segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Before deployment in the real-world deep neural networks require thorough\nevaluation of how they handle both knowns, inputs represented in the training\ndata, and unknowns (anomalies). This is especially important for scene\nunderstanding tasks with safety critical applications, such as in autonomous\ndriving. Existing datasets allow evaluation of only knowns or unknowns - but\nnot both, which is required to establish \"in the wild\" suitability of deep\nneural network models. To bridge this gap, we propose a novel anomaly\nsegmentation dataset, ISSU, that features a diverse set of anomaly inputs from\ncluttered real-world environments. The dataset is twice larger than existing\nanomaly segmentation datasets, and provides a training, validation and test set\nfor controlled in-domain evaluation. The test set consists of a static and\ntemporal part, with the latter comprised of videos. The dataset provides\nannotations for both closed-set (knowns) and anomalies, enabling closed-set and\nopen-set evaluation. The dataset covers diverse conditions, such as domain and\ncross-sensor shift, illumination variation and allows ablation of anomaly\ndetection methods with respect to these variations. Evaluation results of\ncurrent state-of-the-art methods confirm the need for improvements especially\nin domain-generalization, small and large object segmentation."
                },
                "authors": [
                    {
                        "name": "Zakaria Laskar"
                    },
                    {
                        "name": "Tomas Vojir"
                    },
                    {
                        "name": "Matej Grcic"
                    },
                    {
                        "name": "Iaroslav Melekhov"
                    },
                    {
                        "name": "Shankar Gangisettye"
                    },
                    {
                        "name": "Juho Kannala"
                    },
                    {
                        "name": "Jiri Matas"
                    },
                    {
                        "name": "Giorgos Tolias"
                    },
                    {
                        "name": "C. V. Jawahar"
                    }
                ],
                "author_detail": {
                    "name": "C. V. Jawahar"
                },
                "author": "C. V. Jawahar",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22303v1",
                "updated": "2025-03-28T10:26:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    26,
                    49,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T10:26:49Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    26,
                    49,
                    4,
                    87,
                    0
                ],
                "title": "Preference-based Learning with Retrieval Augmented Generation for\n  Conversational Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based Learning with Retrieval Augmented Generation for\n  Conversational Question Answering"
                },
                "summary": "Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines."
                },
                "authors": [
                    {
                        "name": "Magdalena Kaiser"
                    },
                    {
                        "name": "Gerhard Weikum"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Weikum"
                },
                "author": "Gerhard Weikum",
                "arxiv_doi": "10.1145/3701716.3715544",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715544",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.22303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "WWW 2025 Short Paper, 5 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22288v1",
                "updated": "2025-03-28T10:04:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    4,
                    40,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T10:04:40Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    4,
                    40,
                    4,
                    87,
                    0
                ],
                "title": "SimDC: A High-Fidelity Device Simulation Platform for Device-Cloud\n  Collaborative Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimDC: A High-Fidelity Device Simulation Platform for Device-Cloud\n  Collaborative Computing"
                },
                "summary": "The advent of edge intelligence and escalating concerns for data privacy\nprotection have sparked a surge of interest in device-cloud collaborative\ncomputing. Large-scale device deployments to validate prototype solutions are\noften prohibitively expensive and practically challenging, resulting in a\npronounced demand for simulation tools that can emulate realworld scenarios.\nHowever, existing simulators predominantly rely solely on high-performance\nservers to emulate edge computing devices, overlooking (1) the discrepancies\nbetween virtual computing units and actual heterogeneous computing devices and\n(2) the simulation of device behaviors in real-world environments. In this\npaper, we propose a high-fidelity device simulation platform, called SimDC,\nwhich uses a hybrid heterogeneous resource and integrates high-performance\nservers and physical mobile phones. Utilizing this platform, developers can\nsimulate numerous devices for functional testing cost-effectively and capture\nprecise operational responses from varied real devices. To simulate real\nbehaviors of heterogeneous devices, we offer a configurable device behavior\ntraffic controller that dispatches results on devices to the cloud using a\nuser-defined operation strategy. Comprehensive experiments on the public\ndataset show the effectiveness of our simulation platform and its great\npotential for application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of edge intelligence and escalating concerns for data privacy\nprotection have sparked a surge of interest in device-cloud collaborative\ncomputing. Large-scale device deployments to validate prototype solutions are\noften prohibitively expensive and practically challenging, resulting in a\npronounced demand for simulation tools that can emulate realworld scenarios.\nHowever, existing simulators predominantly rely solely on high-performance\nservers to emulate edge computing devices, overlooking (1) the discrepancies\nbetween virtual computing units and actual heterogeneous computing devices and\n(2) the simulation of device behaviors in real-world environments. In this\npaper, we propose a high-fidelity device simulation platform, called SimDC,\nwhich uses a hybrid heterogeneous resource and integrates high-performance\nservers and physical mobile phones. Utilizing this platform, developers can\nsimulate numerous devices for functional testing cost-effectively and capture\nprecise operational responses from varied real devices. To simulate real\nbehaviors of heterogeneous devices, we offer a configurable device behavior\ntraffic controller that dispatches results on devices to the cloud using a\nuser-defined operation strategy. Comprehensive experiments on the public\ndataset show the effectiveness of our simulation platform and its great\npotential for application."
                },
                "authors": [
                    {
                        "name": "Ruiguang Pei"
                    },
                    {
                        "name": "Junjie Wu"
                    },
                    {
                        "name": "Dan Peng"
                    },
                    {
                        "name": "Min Fang"
                    },
                    {
                        "name": "Jianan Zhang"
                    },
                    {
                        "name": "Zhihui Fu"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "Accepted by ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22283v1",
                "updated": "2025-03-28T09:56:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    56,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T09:56:05Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    56,
                    5,
                    4,
                    87,
                    0
                ],
                "title": "BanglAssist: A Bengali-English Generative AI Chatbot for Code-Switching\n  and Dialect-Handling in Customer Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanglAssist: A Bengali-English Generative AI Chatbot for Code-Switching\n  and Dialect-Handling in Customer Service"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated exponential\nimprovements that promise transformative opportunities across various\nindustries. Their ability to generate human-like text and ensure continuous\navailability facilitates the creation of interactive service chatbots aimed at\nenhancing customer experience and streamlining enterprise operations. Despite\ntheir potential, LLMs face critical challenges, such as a susceptibility to\nhallucinations and difficulties handling complex linguistic scenarios, notably\ncode switching and dialectal variations. To address these challenges, this\npaper describes the design of a multilingual chatbot for Bengali-English\ncustomer service interactions utilizing retrieval-augmented generation (RAG)\nand targeted prompt engineering. This research provides valuable insights for\nthe human-computer interaction (HCI) community, emphasizing the importance of\ndesigning systems that accommodate linguistic diversity to benefit both\ncustomers and businesses. By addressing the intersection of generative AI and\ncultural heterogeneity, this late-breaking work inspires future innovations in\nmultilingual and multicultural HCI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated exponential\nimprovements that promise transformative opportunities across various\nindustries. Their ability to generate human-like text and ensure continuous\navailability facilitates the creation of interactive service chatbots aimed at\nenhancing customer experience and streamlining enterprise operations. Despite\ntheir potential, LLMs face critical challenges, such as a susceptibility to\nhallucinations and difficulties handling complex linguistic scenarios, notably\ncode switching and dialectal variations. To address these challenges, this\npaper describes the design of a multilingual chatbot for Bengali-English\ncustomer service interactions utilizing retrieval-augmented generation (RAG)\nand targeted prompt engineering. This research provides valuable insights for\nthe human-computer interaction (HCI) community, emphasizing the importance of\ndesigning systems that accommodate linguistic diversity to benefit both\ncustomers and businesses. By addressing the intersection of generative AI and\ncultural heterogeneity, this late-breaking work inspires future innovations in\nmultilingual and multicultural HCI."
                },
                "authors": [
                    {
                        "name": "Francesco Kruk"
                    },
                    {
                        "name": "Savindu Herath"
                    },
                    {
                        "name": "Prithwiraj Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Prithwiraj Choudhury"
                },
                "author": "Prithwiraj Choudhury",
                "arxiv_comment": "Accepted at the 2025 Conference on Human Factors in Computing Systems\n  (CHI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22275v1",
                "updated": "2025-03-28T09:43:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    43,
                    47,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T09:43:47Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    43,
                    47,
                    4,
                    87,
                    0
                ],
                "title": "Make Some Noise: Towards LLM audio reasoning and generation using sound\n  tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Some Noise: Towards LLM audio reasoning and generation using sound\n  tokens"
                },
                "summary": "Integrating audio comprehension and generation into large language models\n(LLMs) remains challenging due to the continuous nature of audio and the\nresulting high sampling rates. Here, we introduce a novel approach that\ncombines Variational Quantization with Conditional Flow Matching to convert\naudio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless\nintegration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM\nusing Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true\nmultimodal capabilities, i.e., audio comprehension and generation. Our\ntokenizer outperforms a traditional VQ-VAE across various datasets with diverse\nacoustic events. Despite the substantial loss of fine-grained details through\naudio tokenization, our multimodal LLM trained with discrete tokens achieves\ncompetitive results in audio comprehension with state-of-the-art methods,\nthough audio generation is poor. Our results highlight the need for larger,\nmore diverse datasets and improved evaluation metrics to advance multimodal LLM\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating audio comprehension and generation into large language models\n(LLMs) remains challenging due to the continuous nature of audio and the\nresulting high sampling rates. Here, we introduce a novel approach that\ncombines Variational Quantization with Conditional Flow Matching to convert\naudio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless\nintegration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM\nusing Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true\nmultimodal capabilities, i.e., audio comprehension and generation. Our\ntokenizer outperforms a traditional VQ-VAE across various datasets with diverse\nacoustic events. Despite the substantial loss of fine-grained details through\naudio tokenization, our multimodal LLM trained with discrete tokens achieves\ncompetitive results in audio comprehension with state-of-the-art methods,\nthough audio generation is poor. Our results highlight the need for larger,\nmore diverse datasets and improved evaluation metrics to advance multimodal LLM\nperformance."
                },
                "authors": [
                    {
                        "name": "Shivam Mehta"
                    },
                    {
                        "name": "Nebojsa Jojic"
                    },
                    {
                        "name": "Hannes Gamper"
                    }
                ],
                "author_detail": {
                    "name": "Hannes Gamper"
                },
                "author": "Hannes Gamper",
                "arxiv_doi": "10.1109/ICASSP49660.2025.10888809",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICASSP49660.2025.10888809",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.22275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 2 figures, Accepted at ICASSP 2025",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; H.5.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22272v1",
                "updated": "2025-03-28T09:41:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    41,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T09:41:05Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    41,
                    5,
                    4,
                    87,
                    0
                ],
                "title": "Robust simultaneous UWB-anchor calibration and robot localization for\n  emergency situations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust simultaneous UWB-anchor calibration and robot localization for\n  emergency situations"
                },
                "summary": "In this work, we propose a factor graph optimization (FGO) framework to\nsimultaneously solve the calibration problem for Ultra-WideBand (UWB) anchors\nand the robot localization problem. Calibrating UWB anchors manually can be\ntime-consuming and even impossible in emergencies or those situations without\nspecial calibration tools. Therefore, automatic estimation of the anchor\npositions becomes a necessity. The proposed method enables the creation of a\nsoft sensor providing the position information of the anchors in a UWB network.\nThis soft sensor requires only UWB and LiDAR measurements measured from a\nmoving robot. The proposed FGO framework is suitable for the calibration of an\nextendable large UWB network. Moreover, the anchor calibration problem and\nrobot localization problem can be solved simultaneously, which saves time for\nUWB network deployment. The proposed framework also helps to avoid artificial\nerrors in the UWB-anchor position estimation and improves the accuracy and\nrobustness of the robot-pose. The experimental results of the robot\nlocalization using LiDAR and a UWB network in a 3D environment are discussed,\ndemonstrating the performance of the proposed method. More specifically, the\nanchor calibration problem with four anchors and the robot localization problem\ncan be solved simultaneously and automatically within 30 seconds by the\nproposed framework. The supplementary video and codes can be accessed via\nhttps://github.com/LiuxhRobotAI/Simultaneous_calibration_localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a factor graph optimization (FGO) framework to\nsimultaneously solve the calibration problem for Ultra-WideBand (UWB) anchors\nand the robot localization problem. Calibrating UWB anchors manually can be\ntime-consuming and even impossible in emergencies or those situations without\nspecial calibration tools. Therefore, automatic estimation of the anchor\npositions becomes a necessity. The proposed method enables the creation of a\nsoft sensor providing the position information of the anchors in a UWB network.\nThis soft sensor requires only UWB and LiDAR measurements measured from a\nmoving robot. The proposed FGO framework is suitable for the calibration of an\nextendable large UWB network. Moreover, the anchor calibration problem and\nrobot localization problem can be solved simultaneously, which saves time for\nUWB network deployment. The proposed framework also helps to avoid artificial\nerrors in the UWB-anchor position estimation and improves the accuracy and\nrobustness of the robot-pose. The experimental results of the robot\nlocalization using LiDAR and a UWB network in a 3D environment are discussed,\ndemonstrating the performance of the proposed method. More specifically, the\nanchor calibration problem with four anchors and the robot localization problem\ncan be solved simultaneously and automatically within 30 seconds by the\nproposed framework. The supplementary video and codes can be accessed via\nhttps://github.com/LiuxhRobotAI/Simultaneous_calibration_localization."
                },
                "authors": [
                    {
                        "name": "Xinghua Liu"
                    },
                    {
                        "name": "Ming Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ming Cao"
                },
                "author": "Ming Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22250v1",
                "updated": "2025-03-28T09:04:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    4,
                    10,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T09:04:10Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    4,
                    10,
                    4,
                    87,
                    0
                ],
                "title": "Beyond the Script: Testing LLMs for Authentic Patient Communication\n  Styles in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Script: Testing LLMs for Authentic Patient Communication\n  Styles in Healthcare"
                },
                "summary": "Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training."
                },
                "authors": [
                    {
                        "name": "Anna Bodonhelyi"
                    },
                    {
                        "name": "Christian Stegemann-Philipps"
                    },
                    {
                        "name": "Alessandra Sonanini"
                    },
                    {
                        "name": "Lea Herschbach"
                    },
                    {
                        "name": "Márton Szép"
                    },
                    {
                        "name": "Anne Herrmann-Werner"
                    },
                    {
                        "name": "Teresa Festl-Wietek"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Friederike Holderried"
                    }
                ],
                "author_detail": {
                    "name": "Friederike Holderried"
                },
                "author": "Friederike Holderried",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22241v1",
                "updated": "2025-03-28T08:45:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    45,
                    15,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T08:45:15Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    45,
                    15,
                    4,
                    87,
                    0
                ],
                "title": "Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs"
                },
                "summary": "Personalized multiple clustering aims to generate diverse partitions of a\ndataset based on different user-specific aspects, rather than a single\nclustering. It has recently drawn research interest for accommodating varying\nuser preferences. Recent approaches primarily use CLIP embeddings with proxy\nlearning to extract representations biased toward user clustering preferences.\nHowever, CLIP primarily focuses on coarse image-text alignment, lacking a deep\ncontextual understanding of user interests. To overcome these limitations, we\npropose an agent-centric personalized clustering framework that leverages\nmulti-modal large language models (MLLMs) as agents to comprehensively traverse\na relational graph to search for clusters based on user interests. Due to the\nadvanced reasoning mechanism of MLLMs, the obtained clusters align more closely\nwith user-defined criteria than those obtained from CLIP-based representations.\nTo reduce computational overhead, we shorten the agents' traversal path by\nconstructing a relational graph using user-interest-biased embeddings extracted\nby MLLMs. A large number of weakly connected edges can be filtered out based on\nembedding similarity, facilitating an efficient traversal search for agents.\nExperimental results show that the proposed method achieves NMI scores of\n0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively,\nlargely improving the SOTA model by over 140%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized multiple clustering aims to generate diverse partitions of a\ndataset based on different user-specific aspects, rather than a single\nclustering. It has recently drawn research interest for accommodating varying\nuser preferences. Recent approaches primarily use CLIP embeddings with proxy\nlearning to extract representations biased toward user clustering preferences.\nHowever, CLIP primarily focuses on coarse image-text alignment, lacking a deep\ncontextual understanding of user interests. To overcome these limitations, we\npropose an agent-centric personalized clustering framework that leverages\nmulti-modal large language models (MLLMs) as agents to comprehensively traverse\na relational graph to search for clusters based on user interests. Due to the\nadvanced reasoning mechanism of MLLMs, the obtained clusters align more closely\nwith user-defined criteria than those obtained from CLIP-based representations.\nTo reduce computational overhead, we shorten the agents' traversal path by\nconstructing a relational graph using user-interest-biased embeddings extracted\nby MLLMs. A large number of weakly connected edges can be filtered out based on\nembedding similarity, facilitating an efficient traversal search for agents.\nExperimental results show that the proposed method achieves NMI scores of\n0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively,\nlargely improving the SOTA model by over 140%."
                },
                "authors": [
                    {
                        "name": "Ziye Chen"
                    },
                    {
                        "name": "Yiqun Duan"
                    },
                    {
                        "name": "Riheng Zhu"
                    },
                    {
                        "name": "Zhenbang Sun"
                    },
                    {
                        "name": "Mingming Gong"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Gong"
                },
                "author": "Mingming Gong",
                "arxiv_comment": "10 pages, 7 figures, in submission to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T05, 05C82",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22238v1",
                "updated": "2025-03-28T08:41:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    41,
                    43,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T08:41:43Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    41,
                    43,
                    4,
                    87,
                    0
                ],
                "title": "Integrating LLMs in Software Engineering Education: Motivators,\n  Demotivators, and a Roadmap Towards a Framework for Finnish Higher Education\n  Institutes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLMs in Software Engineering Education: Motivators,\n  Demotivators, and a Roadmap Towards a Framework for Finnish Higher Education\n  Institutes"
                },
                "summary": "The increasing adoption of Large Language Models (LLMs) in software\nengineering education presents both opportunities and challenges. While LLMs\noffer benefits such as enhanced learning experiences, automated assessments,\nand personalized tutoring, their integration also raises concerns about\nacademic integrity, student over-reliance, and ethical considerations. In this\nstudy, we conducted a preliminary literature review to identify motivators and\ndemotivators for using LLMs in software engineering education. We applied a\nthematic mapping process to categorize and structure these factors (motivators\nand demotivators), offering a comprehensive view of their impact. In total, we\nidentified 25 motivators and 30 demotivators, which are further organized into\nfour high-level themes. This mapping provides a structured framework for\nunderstanding the factors that influence the integration of LLMs in software\nengineering education, both positively and negatively. As part of a larger\nresearch project, this study serves as a feasibility assessment, laying the\ngroundwork for future systematic literature review and empirical studies.\nUltimately, this project aims to develop a framework to assist Finnish higher\neducation institutions in effectively integrating LLMs into software\nengineering education while addressing potential risks and challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of Large Language Models (LLMs) in software\nengineering education presents both opportunities and challenges. While LLMs\noffer benefits such as enhanced learning experiences, automated assessments,\nand personalized tutoring, their integration also raises concerns about\nacademic integrity, student over-reliance, and ethical considerations. In this\nstudy, we conducted a preliminary literature review to identify motivators and\ndemotivators for using LLMs in software engineering education. We applied a\nthematic mapping process to categorize and structure these factors (motivators\nand demotivators), offering a comprehensive view of their impact. In total, we\nidentified 25 motivators and 30 demotivators, which are further organized into\nfour high-level themes. This mapping provides a structured framework for\nunderstanding the factors that influence the integration of LLMs in software\nengineering education, both positively and negatively. As part of a larger\nresearch project, this study serves as a feasibility assessment, laying the\ngroundwork for future systematic literature review and empirical studies.\nUltimately, this project aims to develop a framework to assist Finnish higher\neducation institutions in effectively integrating LLMs into software\nengineering education while addressing potential risks and challenges."
                },
                "authors": [
                    {
                        "name": "Maryam Khan"
                    },
                    {
                        "name": "Muhammad Azeem Akbar"
                    },
                    {
                        "name": "Jussi Kasurinen"
                    }
                ],
                "author_detail": {
                    "name": "Jussi Kasurinen"
                },
                "author": "Jussi Kasurinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16257v2",
                "updated": "2025-03-28T08:39:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    39,
                    46,
                    4,
                    87,
                    0
                ],
                "published": "2024-12-20T07:24:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    24,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "PromptLA: Towards Integrity Verification of Black-box Text-to-Image\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptLA: Towards Integrity Verification of Black-box Text-to-Image\n  Diffusion Models"
                },
                "summary": "Despite the impressive synthesis quality of text-to-image (T2I) diffusion\nmodels, their black-box deployment poses significant regulatory challenges:\nMalicious actors can fine-tune these models to generate illegal content,\ncircumventing existing safeguards through parameter manipulation. Therefore, it\nis essential to verify the integrity of T2I diffusion models. To this end,\nconsidering the randomness within the outputs of generative models and the high\ncosts in interacting with them, we discern model tampering via the KL\ndivergence between the distributions of the features of generated images. We\npropose a novel prompt selection algorithm based on learning automaton\n(PromptLA) for efficient and accurate verification. Evaluations on four\nadvanced T2I models (e.g., SDXL, FLUX.1) demonstrate that our method achieves a\nmean AUC of over 0.96 in integrity detection, exceeding baselines by more than\n0.2, showcasing strong effectiveness and generalization. Additionally, our\napproach achieves lower cost and is robust against image-level post-processing.\nTo the best of our knowledge, this paper is the first work addressing the\nintegrity verification of T2I diffusion models, which establishes quantifiable\nstandards for AI copyright litigation in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive synthesis quality of text-to-image (T2I) diffusion\nmodels, their black-box deployment poses significant regulatory challenges:\nMalicious actors can fine-tune these models to generate illegal content,\ncircumventing existing safeguards through parameter manipulation. Therefore, it\nis essential to verify the integrity of T2I diffusion models. To this end,\nconsidering the randomness within the outputs of generative models and the high\ncosts in interacting with them, we discern model tampering via the KL\ndivergence between the distributions of the features of generated images. We\npropose a novel prompt selection algorithm based on learning automaton\n(PromptLA) for efficient and accurate verification. Evaluations on four\nadvanced T2I models (e.g., SDXL, FLUX.1) demonstrate that our method achieves a\nmean AUC of over 0.96 in integrity detection, exceeding baselines by more than\n0.2, showcasing strong effectiveness and generalization. Additionally, our\napproach achieves lower cost and is robust against image-level post-processing.\nTo the best of our knowledge, this paper is the first work addressing the\nintegrity verification of T2I diffusion models, which establishes quantifiable\nstandards for AI copyright litigation in practice."
                },
                "authors": [
                    {
                        "name": "Zhuomeng Zhang"
                    },
                    {
                        "name": "Fangqi Li"
                    },
                    {
                        "name": "Chong Di"
                    },
                    {
                        "name": "Hongyu Zhu"
                    },
                    {
                        "name": "Hanyi Wang"
                    },
                    {
                        "name": "Shilin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shilin Wang"
                },
                "author": "Shilin Wang",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22235v1",
                "updated": "2025-03-28T08:37:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    37,
                    59,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T08:37:59Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    37,
                    59,
                    4,
                    87,
                    0
                ],
                "title": "WeatherMesh-3: Fast and accurate operational global weather forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeatherMesh-3: Fast and accurate operational global weather forecasting"
                },
                "summary": "We present WeatherMesh-3 (WM-3), an operational transformer-based global\nweather forecasting system that improves the state of the art in both accuracy\nand computational efficiency. We introduce the following advances: 1) a latent\nrollout that enables arbitrary-length predictions in latent space without\nintermediate encoding or decoding; and 2) a modular architecture that flexibly\nutilizes mixed-horizon processors and encodes multiple real-time analyses to\ncreate blended initial conditions. WM-3 generates 14-day global forecasts at\n0.25-degree resolution in 12 seconds on a single RTX 4090. This represents a\n>100,000-fold speedup over traditional NWP approaches while achieving superior\naccuracy with up to 37.7% improvement in RMSE over operational models,\nrequiring only a single consumer-grade GPU for deployment. We aim for WM-3 to\ndemocratize weather forecasting by providing an accessible, lightweight model\nfor operational use while pushing the performance boundaries of machine\nlearning-based weather prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present WeatherMesh-3 (WM-3), an operational transformer-based global\nweather forecasting system that improves the state of the art in both accuracy\nand computational efficiency. We introduce the following advances: 1) a latent\nrollout that enables arbitrary-length predictions in latent space without\nintermediate encoding or decoding; and 2) a modular architecture that flexibly\nutilizes mixed-horizon processors and encodes multiple real-time analyses to\ncreate blended initial conditions. WM-3 generates 14-day global forecasts at\n0.25-degree resolution in 12 seconds on a single RTX 4090. This represents a\n>100,000-fold speedup over traditional NWP approaches while achieving superior\naccuracy with up to 37.7% improvement in RMSE over operational models,\nrequiring only a single consumer-grade GPU for deployment. We aim for WM-3 to\ndemocratize weather forecasting by providing an accessible, lightweight model\nfor operational use while pushing the performance boundaries of machine\nlearning-based weather prediction."
                },
                "authors": [
                    {
                        "name": "Haoxing Du"
                    },
                    {
                        "name": "Lyna Kim"
                    },
                    {
                        "name": "Joan Creus-Costa"
                    },
                    {
                        "name": "Jack Michaels"
                    },
                    {
                        "name": "Anuj Shetty"
                    },
                    {
                        "name": "Todd Hutchinson"
                    },
                    {
                        "name": "Christopher Riedel"
                    },
                    {
                        "name": "John Dean"
                    }
                ],
                "author_detail": {
                    "name": "John Dean"
                },
                "author": "John Dean",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08127v2",
                "updated": "2025-03-28T08:33:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    33,
                    36,
                    4,
                    87,
                    0
                ],
                "published": "2025-02-12T05:13:04Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    13,
                    4,
                    2,
                    43,
                    0
                ],
                "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance"
                },
                "summary": "While large language models (LLMs) have shown strong general reasoning\ncapabilities, their effectiveness in financial reasoning, which is crucial for\nreal-world financial applications remains underexplored. In this study, we\nconduct a comprehensive evaluation of 24 state-of-the-art general and\nreasoning-focused LLMs across four complex financial reasoning tasks involving\nfinancial text, tabular data, and equations. We assess key capabilities such as\nnumerical reasoning, tabular interpretation, financial terminology\ncomprehension, long-context understanding, and equation-based problem solving.\nOur analysis reveals that while data quality and pretraining contribute to\nperformance, general techniques like chain-of-thought (CoT) fine-tuning offer\nlimited gains in financial tasks. To address this, we propose two\ndomain-adapted models, Fino1-8B and Fino1-14B, trained with CoT fine-tuning and\nreinforcement learning using domain-specific reasoning paths. Our models are\ntrained on a carefully curated dataset integrating high-quality examples from\ndiverse sources, covering financial reports, tables, equations, and structured\nXBRL texts. Despite limited training data, they achieve an 7-9% performance\nimprovement, outperforming several advanced LLMs, including GPT-o1,\nGPT-o3-mini, GPT-4.5, and comparable with DeepSeek models (V3 and R1),\ndemonstrating strong practical value in resource, constrained scenarios. Our\nfindings highlight the need for domain-specific adaptations in financial\nreasoning, and we release all datasets, models, and code for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown strong general reasoning\ncapabilities, their effectiveness in financial reasoning, which is crucial for\nreal-world financial applications remains underexplored. In this study, we\nconduct a comprehensive evaluation of 24 state-of-the-art general and\nreasoning-focused LLMs across four complex financial reasoning tasks involving\nfinancial text, tabular data, and equations. We assess key capabilities such as\nnumerical reasoning, tabular interpretation, financial terminology\ncomprehension, long-context understanding, and equation-based problem solving.\nOur analysis reveals that while data quality and pretraining contribute to\nperformance, general techniques like chain-of-thought (CoT) fine-tuning offer\nlimited gains in financial tasks. To address this, we propose two\ndomain-adapted models, Fino1-8B and Fino1-14B, trained with CoT fine-tuning and\nreinforcement learning using domain-specific reasoning paths. Our models are\ntrained on a carefully curated dataset integrating high-quality examples from\ndiverse sources, covering financial reports, tables, equations, and structured\nXBRL texts. Despite limited training data, they achieve an 7-9% performance\nimprovement, outperforming several advanced LLMs, including GPT-o1,\nGPT-o3-mini, GPT-4.5, and comparable with DeepSeek models (V3 and R1),\ndemonstrating strong practical value in resource, constrained scenarios. Our\nfindings highlight the need for domain-specific adaptations in financial\nreasoning, and we release all datasets, models, and code for future research."
                },
                "authors": [
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Weipeng Zhou"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Han Yi"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Jianyun Nie"
                    }
                ],
                "author_detail": {
                    "name": "Jianyun Nie"
                },
                "author": "Jianyun Nie",
                "arxiv_comment": "13 pages, 2 figures, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22232v1",
                "updated": "2025-03-28T08:27:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    27,
                    47,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T08:27:47Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    27,
                    47,
                    4,
                    87,
                    0
                ],
                "title": "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks"
                },
                "summary": "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis."
                },
                "authors": [
                    {
                        "name": "Ahmed Mohamed Hussain"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_comment": "10 pages, 6 figures. Author's version; accepted and presented at the\n  IEEE 23rd International Conference on Trust, Security and Privacy in\n  Computing and Communications (TrustCom) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22215v1",
                "updated": "2025-03-28T08:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    4,
                    51,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T08:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    4,
                    51,
                    4,
                    87,
                    0
                ],
                "title": "Learning to Instruct for Visual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Instruct for Visual Instruction Tuning"
                },
                "summary": "We propose LIT, an advancement of visual instruction tuning (VIT). While VIT\nequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, the\ncurrent design choices for VIT often result in overfitting and shortcut\nlearning, potentially degrading performance. This gap arises from an\noveremphasis on instruction-following abilities, while neglecting the proactive\nunderstanding of visual information. Inspired by this, LIT adopts a simple yet\neffective approach by incorporating the loss function into both the instruction\nand response sequences. It seamlessly expands the training data, and\nregularizes the MLLMs from overly relying on language priors. Based on this\nmerit, LIT achieves a significant relative improvement of up to 9% on\ncomprehensive multimodal benchmarks, requiring no additional training data and\nincurring negligible computational overhead. Surprisingly, LIT attains\nexceptional fundamental visual capabilities, yielding up to an 18% improvement\nin captioning performance, while simultaneously alleviating hallucination in\nMLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose LIT, an advancement of visual instruction tuning (VIT). While VIT\nequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, the\ncurrent design choices for VIT often result in overfitting and shortcut\nlearning, potentially degrading performance. This gap arises from an\noveremphasis on instruction-following abilities, while neglecting the proactive\nunderstanding of visual information. Inspired by this, LIT adopts a simple yet\neffective approach by incorporating the loss function into both the instruction\nand response sequences. It seamlessly expands the training data, and\nregularizes the MLLMs from overly relying on language priors. Based on this\nmerit, LIT achieves a significant relative improvement of up to 9% on\ncomprehensive multimodal benchmarks, requiring no additional training data and\nincurring negligible computational overhead. Surprisingly, LIT attains\nexceptional fundamental visual capabilities, yielding up to an 18% improvement\nin captioning performance, while simultaneously alleviating hallucination in\nMLLMs."
                },
                "authors": [
                    {
                        "name": "Zhihan Zhou"
                    },
                    {
                        "name": "Feng Hong"
                    },
                    {
                        "name": "Jiaan Luo"
                    },
                    {
                        "name": "Jiangchao Yao"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Wang"
                },
                "author": "Yanfeng Wang",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14883v2",
                "updated": "2025-03-28T07:56:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    56,
                    4,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-19T04:21:38Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    21,
                    38,
                    2,
                    78,
                    0
                ],
                "title": "Envisioning an AI-Enhanced Mental Health Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Envisioning an AI-Enhanced Mental Health Ecosystem"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs), reasoning models, and\nagentic AI approaches coincides with a growing global mental health crisis,\nwhere increasing demand has not translated into adequate access to professional\nsupport, particularly for underserved populations. This presents a unique\nopportunity for AI to complement human-led interventions, offering scalable and\ncontext-aware support while preserving human connection in this sensitive\ndomain. We explore various AI applications in peer support, self-help\ninterventions, proactive monitoring, and data-driven insights, using a\nhuman-centred approach that ensures AI supports rather than replaces human\ninteraction. However, AI deployment in mental health fields presents challenges\nsuch as ethical concerns, transparency, privacy risks, and risks of\nover-reliance. We propose a hybrid ecosystem where where AI assists but does\nnot replace human providers, emphasising responsible deployment and evaluation.\nWe also present some of our early work and findings in several of these AI\napplications. Finally, we outline future research directions for refining\nAI-enhanced interventions while adhering to ethical and culturally sensitive\nguidelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs), reasoning models, and\nagentic AI approaches coincides with a growing global mental health crisis,\nwhere increasing demand has not translated into adequate access to professional\nsupport, particularly for underserved populations. This presents a unique\nopportunity for AI to complement human-led interventions, offering scalable and\ncontext-aware support while preserving human connection in this sensitive\ndomain. We explore various AI applications in peer support, self-help\ninterventions, proactive monitoring, and data-driven insights, using a\nhuman-centred approach that ensures AI supports rather than replaces human\ninteraction. However, AI deployment in mental health fields presents challenges\nsuch as ethical concerns, transparency, privacy risks, and risks of\nover-reliance. We propose a hybrid ecosystem where where AI assists but does\nnot replace human providers, emphasising responsible deployment and evaluation.\nWe also present some of our early work and findings in several of these AI\napplications. Finally, we outline future research directions for refining\nAI-enhanced interventions while adhering to ethical and culturally sensitive\nguidelines."
                },
                "authors": [
                    {
                        "name": "Kellie Yu Hui Sim"
                    },
                    {
                        "name": "Kenny Tsu Wei Choo"
                    }
                ],
                "author_detail": {
                    "name": "Kenny Tsu Wei Choo"
                },
                "author": "Kenny Tsu Wei Choo",
                "arxiv_comment": "5 pages, 0 figures, accepted to the CHI'25 Envisioning the Future of\n  Interactive Health Workshop, to be published in HAL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22166v1",
                "updated": "2025-03-28T06:11:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    11,
                    4,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T06:11:04Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    11,
                    4,
                    4,
                    87,
                    0
                ],
                "title": "Reasoning of Large Language Models over Knowledge Graphs with\n  Super-Relations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning of Large Language Models over Knowledge Graphs with\n  Super-Relations"
                },
                "summary": "While large language models (LLMs) have made significant progress in\nprocessing and reasoning over knowledge graphs, current methods suffer from a\nhigh non-retrieval rate. This limitation reduces the accuracy of answering\nquestions based on these graphs. Our analysis reveals that the combination of\ngreedy search and forward reasoning is a major contributor to this issue. To\novercome these challenges, we introduce the concept of super-relations, which\nenables both forward and backward reasoning by summarizing and connecting\nvarious relational paths within the graph. This holistic approach not only\nexpands the search space, but also significantly improves retrieval efficiency.\nIn this paper, we propose the ReKnoS framework, which aims to Reason over\nKnowledge Graphs with Super-Relations. Our framework's key advantages include\nthe inclusion of multiple relation paths through super-relations, enhanced\nforward and backward reasoning capabilities, and increased efficiency in\nquerying LLMs. These enhancements collectively lead to a substantial\nimprovement in the successful retrieval rate and overall reasoning performance.\nWe conduct extensive experiments on nine real-world datasets to evaluate\nReKnoS, and the results demonstrate the superior performance of ReKnoS over\nexisting state-of-the-art baselines, with an average accuracy gain of 2.92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have made significant progress in\nprocessing and reasoning over knowledge graphs, current methods suffer from a\nhigh non-retrieval rate. This limitation reduces the accuracy of answering\nquestions based on these graphs. Our analysis reveals that the combination of\ngreedy search and forward reasoning is a major contributor to this issue. To\novercome these challenges, we introduce the concept of super-relations, which\nenables both forward and backward reasoning by summarizing and connecting\nvarious relational paths within the graph. This holistic approach not only\nexpands the search space, but also significantly improves retrieval efficiency.\nIn this paper, we propose the ReKnoS framework, which aims to Reason over\nKnowledge Graphs with Super-Relations. Our framework's key advantages include\nthe inclusion of multiple relation paths through super-relations, enhanced\nforward and backward reasoning capabilities, and increased efficiency in\nquerying LLMs. These enhancements collectively lead to a substantial\nimprovement in the successful retrieval rate and overall reasoning performance.\nWe conduct extensive experiments on nine real-world datasets to evaluate\nReKnoS, and the results demonstrate the superior performance of ReKnoS over\nexisting state-of-the-art baselines, with an average accuracy gain of 2.92%."
                },
                "authors": [
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Junhong Lin"
                    },
                    {
                        "name": "Xiaojie Guo"
                    },
                    {
                        "name": "Julian Shun"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Yada Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yada Zhu"
                },
                "author": "Yada Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22165v1",
                "updated": "2025-03-28T06:09:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    9,
                    51,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T06:09:51Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    9,
                    51,
                    4,
                    87,
                    0
                ],
                "title": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models"
                },
                "summary": "Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts-the first\nvisualization tool for users to inspect the reasoning paths of chain-of-thought\nand its derivatives on any multi-choice dataset. Specifically, we represent the\nstates in a reasoning path as feature vectors that quantify their distances to\nall answer choices. These features are then visualized in two-dimensional plots\nusing t-SNE. Qualitative and quantitative analysis with the landscape of\nthoughts effectively distinguishes between strong and weak models, correct and\nincorrect answers, as well as different reasoning tasks. It also uncovers\nundesirable reasoning patterns, such as low consistency and high uncertainty.\nAdditionally, users can adapt our tool to a model that predicts the property\nthey observe. We showcase this advantage by adapting our tool to a lightweight\nverifier that evaluates the correctness of reasoning paths. The code is\npublicly available at: https://github.com/tmlr-group/landscape-of-thoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts-the first\nvisualization tool for users to inspect the reasoning paths of chain-of-thought\nand its derivatives on any multi-choice dataset. Specifically, we represent the\nstates in a reasoning path as feature vectors that quantify their distances to\nall answer choices. These features are then visualized in two-dimensional plots\nusing t-SNE. Qualitative and quantitative analysis with the landscape of\nthoughts effectively distinguishes between strong and weak models, correct and\nincorrect answers, as well as different reasoning tasks. It also uncovers\nundesirable reasoning patterns, such as low consistency and high uncertainty.\nAdditionally, users can adapt our tool to a model that predicts the property\nthey observe. We showcase this advantage by adapting our tool to a lightweight\nverifier that evaluates the correctness of reasoning paths. The code is\npublicly available at: https://github.com/tmlr-group/landscape-of-thoughts."
                },
                "authors": [
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Zhaocheng Zhu"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Mikhail Galkin"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06220v2",
                "updated": "2025-03-28T06:08:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    8,
                    3,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-08T13:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    13,
                    44,
                    38,
                    5,
                    67,
                    0
                ],
                "title": "StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through\n  Event-Gated Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through\n  Event-Gated Cognition"
                },
                "summary": "With the rise of real-world human-AI interaction applications, such as AI\nassistants, the need for Streaming Video Dialogue is critical. To address this\nneed, we introduce StreamMind, a video LLM framework that achieves ultra-FPS\nstreaming video processing (100 fps on a single A100) and enables proactive,\nalways-on responses in real time, without explicit user intervention.\n  To solve the key challenge of the contradiction between linear video\nstreaming speed and quadratic transformer computation cost, we propose a novel\nperception-cognition interleaving paradigm named ''event-gated LLM\ninvocation'', in contrast to the existing per-time-step LLM invocation. By\nintroducing a Cognition Gate network between the video encoder and the LLM, LLM\nis only invoked when relevant events occur. To realize the event feature\nextraction with constant cost, we propose Event-Preserving Feature Extractor\n(EPFE) based on state-space method, generating a single perception token for\nspatiotemporal features. These techniques enable the video LLM with full-FPS\nperception and real-time cognition response.\n  Experiments on Ego4D and SoccerNet streaming tasks, as well as standard\noffline benchmarks, demonstrate state-of-the-art performance in both model\ncapability and real-time efficiency, paving the way for ultra-high-FPS\napplications, such as Game AI and interactive media. The code and data is\navailable at https://aka.ms/StreamMind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of real-world human-AI interaction applications, such as AI\nassistants, the need for Streaming Video Dialogue is critical. To address this\nneed, we introduce StreamMind, a video LLM framework that achieves ultra-FPS\nstreaming video processing (100 fps on a single A100) and enables proactive,\nalways-on responses in real time, without explicit user intervention.\n  To solve the key challenge of the contradiction between linear video\nstreaming speed and quadratic transformer computation cost, we propose a novel\nperception-cognition interleaving paradigm named ''event-gated LLM\ninvocation'', in contrast to the existing per-time-step LLM invocation. By\nintroducing a Cognition Gate network between the video encoder and the LLM, LLM\nis only invoked when relevant events occur. To realize the event feature\nextraction with constant cost, we propose Event-Preserving Feature Extractor\n(EPFE) based on state-space method, generating a single perception token for\nspatiotemporal features. These techniques enable the video LLM with full-FPS\nperception and real-time cognition response.\n  Experiments on Ego4D and SoccerNet streaming tasks, as well as standard\noffline benchmarks, demonstrate state-of-the-art performance in both model\ncapability and real-time efficiency, paving the way for ultra-high-FPS\napplications, such as Game AI and interactive media. The code and data is\navailable at https://aka.ms/StreamMind."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Donglin Bai"
                    },
                    {
                        "name": "Zhibo Chen"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22164v1",
                "updated": "2025-03-28T06:02:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    2,
                    53,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T06:02:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    2,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents"
                },
                "summary": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management."
                },
                "authors": [
                    {
                        "name": "Bowen Gao"
                    },
                    {
                        "name": "Yanwen Huang"
                    },
                    {
                        "name": "Yiqiao Liu"
                    },
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yanyan Lan"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Lan"
                },
                "author": "Yanyan Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22162v1",
                "updated": "2025-03-28T05:57:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    5,
                    57,
                    23,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T05:57:23Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    5,
                    57,
                    23,
                    4,
                    87,
                    0
                ],
                "title": "Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration\n  Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration\n  Maps"
                },
                "summary": "Multi-Agent Pathfinding is used in areas including multi-robot formations,\nwarehouse logistics, and intelligent vehicles. However, many environments are\nincomplete or frequently change, making it difficult for standard centralized\nplanning or pure reinforcement learning to maintain both global solution\nquality and local flexibility. This paper introduces a hybrid framework that\nintegrates D* Lite global search with multi-agent reinforcement learning, using\na switching mechanism and a freeze-prevention strategy to handle dynamic\nconditions and crowded settings. We evaluate the framework in the discrete\nPOGEMA environment and compare it with baseline methods. Experimental outcomes\nindicate that the proposed framework substantially improves success rate,\ncollision rate, and path efficiency. The model is further tested on the EyeSim\nplatform, where it maintains feasible Pathfinding under frequent changes and\nlarge-scale robot deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Pathfinding is used in areas including multi-robot formations,\nwarehouse logistics, and intelligent vehicles. However, many environments are\nincomplete or frequently change, making it difficult for standard centralized\nplanning or pure reinforcement learning to maintain both global solution\nquality and local flexibility. This paper introduces a hybrid framework that\nintegrates D* Lite global search with multi-agent reinforcement learning, using\na switching mechanism and a freeze-prevention strategy to handle dynamic\nconditions and crowded settings. We evaluate the framework in the discrete\nPOGEMA environment and compare it with baseline methods. Experimental outcomes\nindicate that the proposed framework substantially improves success rate,\ncollision rate, and path efficiency. The model is further tested on the EyeSim\nplatform, where it maintains feasible Pathfinding under frequent changes and\nlarge-scale robot deployments."
                },
                "authors": [
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Sen Shen"
                    },
                    {
                        "name": "Xiangrui Kong"
                    },
                    {
                        "name": "Hongtao Zhang"
                    },
                    {
                        "name": "Thomas Bräunl"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bräunl"
                },
                "arxiv_affiliation": "The University of Western Australia",
                "author": "Thomas Bräunl",
                "arxiv_comment": "22 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22161v1",
                "updated": "2025-03-28T05:54:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    5,
                    54,
                    17,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T05:54:17Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    5,
                    54,
                    17,
                    4,
                    87,
                    0
                ],
                "title": "Traffic Modeling for Network Security and Privacy: Challenges Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic Modeling for Network Security and Privacy: Challenges Ahead"
                },
                "summary": "Traffic analysis using machine learning and deep learning models has made\nsignificant progress over the past decades. These models address various tasks\nin network security and privacy, including detection of anomalies and attacks,\ncountering censorship, etc. They also reveal privacy risks to users as\ndemonstrated by the research on LLM token inference as well as fingerprinting\n(and counter-fingerprinting) of user-visiting websites, IoT devices, and\ndifferent applications. However, challenges remain in securing our networks\nfrom threats and attacks. After briefly reviewing the tasks and recent ML\nmodels in network security and privacy, we discuss the challenges that lie\nahead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic analysis using machine learning and deep learning models has made\nsignificant progress over the past decades. These models address various tasks\nin network security and privacy, including detection of anomalies and attacks,\ncountering censorship, etc. They also reveal privacy risks to users as\ndemonstrated by the research on LLM token inference as well as fingerprinting\n(and counter-fingerprinting) of user-visiting websites, IoT devices, and\ndifferent applications. However, challenges remain in securing our networks\nfrom threats and attacks. After briefly reviewing the tasks and recent ML\nmodels in network security and privacy, we discuss the challenges that lie\nahead."
                },
                "authors": [
                    {
                        "name": "Dinil Mon Divakaran"
                    }
                ],
                "author_detail": {
                    "name": "Dinil Mon Divakaran"
                },
                "author": "Dinil Mon Divakaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20776v2",
                "updated": "2025-03-28T04:48:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    48,
                    48,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-26T17:56:16Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    56,
                    16,
                    2,
                    85,
                    0
                ],
                "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields"
                },
                "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g., SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g., SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction."
                },
                "authors": [
                    {
                        "name": "Shijie Zhou"
                    },
                    {
                        "name": "Hui Ren"
                    },
                    {
                        "name": "Yijia Weng"
                    },
                    {
                        "name": "Shuwang Zhang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Dejia Xu"
                    },
                    {
                        "name": "Zhiwen Fan"
                    },
                    {
                        "name": "Suya You"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Achuta Kadambi"
                    }
                ],
                "author_detail": {
                    "name": "Achuta Kadambi"
                },
                "author": "Achuta Kadambi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22145v1",
                "updated": "2025-03-28T04:41:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    41,
                    9,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T04:41:09Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    41,
                    9,
                    4,
                    87,
                    0
                ],
                "title": "Tokenization of Gaze Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization of Gaze Data"
                },
                "summary": "A considerable part of the performance of today's large language models\n(LLM's) and multimodal large language models (MLLM's) depends on their\ntokenization strategies. While tokenizers are extensively researched for\ntextual and visual input, there is no research on tokenization strategies for\ngaze data due to its nature. However, a corresponding tokenization strategy\nwould allow using the vision capabilities of pre-trained MLLM's for gaze data,\nfor example, through fine-tuning.\n  In this paper, we aim to close this research gap by analyzing five different\ntokenizers for gaze data on three different datasets for the forecasting and\ngeneration of gaze data through LLMs (cf.~\\cref{fig:teaser}). We evaluate the\ntokenizers regarding their reconstruction and compression abilities. Further,\nwe train an LLM for each tokenization strategy, measuring its generative and\npredictive performance. Overall, we found that a quantile tokenizer outperforms\nall others in predicting the gaze positions and k-means is best when predicting\ngaze velocities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A considerable part of the performance of today's large language models\n(LLM's) and multimodal large language models (MLLM's) depends on their\ntokenization strategies. While tokenizers are extensively researched for\ntextual and visual input, there is no research on tokenization strategies for\ngaze data due to its nature. However, a corresponding tokenization strategy\nwould allow using the vision capabilities of pre-trained MLLM's for gaze data,\nfor example, through fine-tuning.\n  In this paper, we aim to close this research gap by analyzing five different\ntokenizers for gaze data on three different datasets for the forecasting and\ngeneration of gaze data through LLMs (cf.~\\cref{fig:teaser}). We evaluate the\ntokenizers regarding their reconstruction and compression abilities. Further,\nwe train an LLM for each tokenization strategy, measuring its generative and\npredictive performance. Overall, we found that a quantile tokenizer outperforms\nall others in predicting the gaze positions and k-means is best when predicting\ngaze velocities."
                },
                "authors": [
                    {
                        "name": "Tim Rolff"
                    },
                    {
                        "name": "Jurik Karimian"
                    },
                    {
                        "name": "Niklas Hypki"
                    },
                    {
                        "name": "Susanne Schmidt"
                    },
                    {
                        "name": "Markus Lappe"
                    },
                    {
                        "name": "Frank Steinicke"
                    }
                ],
                "author_detail": {
                    "name": "Frank Steinicke"
                },
                "author": "Frank Steinicke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01129v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01129v3",
                "updated": "2025-03-28T04:40:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    40,
                    20,
                    4,
                    87,
                    0
                ],
                "published": "2024-12-02T05:09:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    5,
                    9,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for\n  Boosting 2-bit Large Language Model Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for\n  Boosting 2-bit Large Language Model Accuracy"
                },
                "summary": "Low-rank adaptation (LoRA) has become the dominant method for\nparameter-efficient LLM fine-tuning, with LoRA-based quantization error\ncompensation (LQEC) emerging as a powerful tool for recovering accuracy in\ncompressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, with\nno prior investigation into understanding this limitation. We propose RILQ\n(Rank-Insensitive LoRA-based Quantization Error Compensation) to understand\nfundamental limitation and boost 2-bit LLM accuracy. Based on rank analysis\nrevealing model-wise activation discrepancy loss's rank-insensitive nature,\nRILQ employs this loss to adjust adapters cooperatively across layers, enabling\nrobust error compensation with low-rank adapters. Evaluations on LLaMA-2 and\nLLaMA-3 demonstrate RILQ's consistent improvements in 2-bit quantized inference\nacross various state-of-the-art quantizers and enhanced accuracy in\ntask-specific fine-tuning. RILQ maintains computational efficiency comparable\nto existing LoRA methods, enabling adapter-merged weight-quantized LLM\ninference with significantly enhanced accuracy, making it a promising approach\nfor boosting 2-bit LLM performance. Our code is available at\nhttps://github.com/aiha-lab/RILQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptation (LoRA) has become the dominant method for\nparameter-efficient LLM fine-tuning, with LoRA-based quantization error\ncompensation (LQEC) emerging as a powerful tool for recovering accuracy in\ncompressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, with\nno prior investigation into understanding this limitation. We propose RILQ\n(Rank-Insensitive LoRA-based Quantization Error Compensation) to understand\nfundamental limitation and boost 2-bit LLM accuracy. Based on rank analysis\nrevealing model-wise activation discrepancy loss's rank-insensitive nature,\nRILQ employs this loss to adjust adapters cooperatively across layers, enabling\nrobust error compensation with low-rank adapters. Evaluations on LLaMA-2 and\nLLaMA-3 demonstrate RILQ's consistent improvements in 2-bit quantized inference\nacross various state-of-the-art quantizers and enhanced accuracy in\ntask-specific fine-tuning. RILQ maintains computational efficiency comparable\nto existing LoRA methods, enabling adapter-merged weight-quantized LLM\ninference with significantly enhanced accuracy, making it a promising approach\nfor boosting 2-bit LLM performance. Our code is available at\nhttps://github.com/aiha-lab/RILQ."
                },
                "authors": [
                    {
                        "name": "Geonho Lee"
                    },
                    {
                        "name": "Janghwan Lee"
                    },
                    {
                        "name": "Sukjin Hong"
                    },
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Euijai Ahn"
                    },
                    {
                        "name": "Du-Seong Chang"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01129v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01129v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22144v1",
                "updated": "2025-03-28T04:39:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    39,
                    52,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T04:39:52Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    39,
                    52,
                    4,
                    87,
                    0
                ],
                "title": "FRASE: Structured Representations for Generalizable SPARQL Query\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRASE: Structured Representations for Generalizable SPARQL Query\n  Generation"
                },
                "summary": "Translating natural language questions into SPARQL queries enables Knowledge\nBase querying for factual and up-to-date responses. However, existing datasets\nfor this task are predominantly template-based, leading models to learn\nsuperficial mappings between question and query templates rather than\ndeveloping true generalization capabilities. As a result, models struggle when\nencountering naturally phrased, template-free questions. This paper introduces\nFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages Frame\nSemantic Role Labeling (FSRL) to address this limitation. We also present\nLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question is\nenriched using FRASE through frame detection and the mapping of frame-elements\nto their argument. We evaluate the impact of this approach through extensive\nexperiments on recent large language models (LLMs) under different fine-tuning\nconfigurations. Our results demonstrate that integrating frame-based structured\nrepresentations consistently improves SPARQL generation performance,\nparticularly in challenging generalization scenarios when test questions\nfeature unseen templates (unknown template splits) and when they are all\nnaturally phrased (reformulated questions).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating natural language questions into SPARQL queries enables Knowledge\nBase querying for factual and up-to-date responses. However, existing datasets\nfor this task are predominantly template-based, leading models to learn\nsuperficial mappings between question and query templates rather than\ndeveloping true generalization capabilities. As a result, models struggle when\nencountering naturally phrased, template-free questions. This paper introduces\nFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages Frame\nSemantic Role Labeling (FSRL) to address this limitation. We also present\nLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question is\nenriched using FRASE through frame detection and the mapping of frame-elements\nto their argument. We evaluate the impact of this approach through extensive\nexperiments on recent large language models (LLMs) under different fine-tuning\nconfigurations. Our results demonstrate that integrating frame-based structured\nrepresentations consistently improves SPARQL generation performance,\nparticularly in challenging generalization scenarios when test questions\nfeature unseen templates (unknown template splits) and when they are all\nnaturally phrased (reformulated questions)."
                },
                "authors": [
                    {
                        "name": "Papa Abdou Karim Karou Diallo"
                    },
                    {
                        "name": "Amal Zouaq"
                    }
                ],
                "author_detail": {
                    "name": "Amal Zouaq"
                },
                "author": "Amal Zouaq",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08723v2",
                "updated": "2025-03-28T04:38:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    38,
                    44,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-11T11:23:26Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    23,
                    26,
                    4,
                    285,
                    0
                ],
                "title": "Human-Computer Interaction and Visualization in Natural Language\n  Generation Models: Applications, Challenges, and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Computer Interaction and Visualization in Natural Language\n  Generation Models: Applications, Challenges, and Opportunities"
                },
                "summary": "Natural language generation (NLG) models have emerged as a focal point of\nresearch within natural language processing (NLP), exhibiting remarkable\nperformance in tasks such as text composition and dialogue generation. However,\ntheir intricate architectures and extensive model parameters pose significant\nchallenges to interpretability, limiting their applicability in high-stakes\ndecision-making scenarios. To address this issue, human-computer interaction\n(HCI) and visualization techniques offer promising avenues to enhance the\ntransparency and usability of NLG models by making their decision-making\nprocesses more interpretable. In this paper, we provide a comprehensive\ninvestigation into the roles, limitations, and impact of HCI and visualization\nin facilitating human understanding and control over NLG systems. We introduce\na taxonomy of interaction methods and visualization techniques, categorizing\nthree major research domains and their corresponding six key tasks in the\napplication of NLG models. Finally, we summarize the shortcomings in the\nexisting work and investigate the key challenges and emerging opportunities in\nthe era of large language models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language generation (NLG) models have emerged as a focal point of\nresearch within natural language processing (NLP), exhibiting remarkable\nperformance in tasks such as text composition and dialogue generation. However,\ntheir intricate architectures and extensive model parameters pose significant\nchallenges to interpretability, limiting their applicability in high-stakes\ndecision-making scenarios. To address this issue, human-computer interaction\n(HCI) and visualization techniques offer promising avenues to enhance the\ntransparency and usability of NLG models by making their decision-making\nprocesses more interpretable. In this paper, we provide a comprehensive\ninvestigation into the roles, limitations, and impact of HCI and visualization\nin facilitating human understanding and control over NLG systems. We introduce\na taxonomy of interaction methods and visualization techniques, categorizing\nthree major research domains and their corresponding six key tasks in the\napplication of NLG models. Finally, we summarize the shortcomings in the\nexisting work and investigate the key challenges and emerging opportunities in\nthe era of large language models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Yunchao Wang"
                    },
                    {
                        "name": "Guodao Sun"
                    },
                    {
                        "name": "Zihang Fu"
                    },
                    {
                        "name": "Ronghua Liang"
                    }
                ],
                "author_detail": {
                    "name": "Ronghua Liang"
                },
                "author": "Ronghua Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22137v1",
                "updated": "2025-03-28T04:22:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    22,
                    53,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T04:22:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    22,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has become a cornerstone of\nthe training and alignment pipeline for large language models (LLMs). Recent\nadvances, such as direct preference optimization (DPO), have simplified the\npreference learning step. However, collecting preference data remains a\nchallenging and costly process, often requiring expert annotation. This cost\ncan be mitigated by carefully selecting the data points presented for\nannotation. In this work, we propose an active learning approach to efficiently\nselect prompt and preference pairs using a risk assessment strategy based on\nthe Sharpe Ratio. To address the challenge of unknown preferences prior to\nannotation, our method evaluates the gradients of all potential preference\nannotations to assess their impact on model updates. These gradient-based\nevaluations enable risk assessment of data points regardless of the annotation\noutcome. By leveraging the DPO loss derivations, we derive a closed-form\nexpression for computing these Sharpe ratios on a per-tuple basis, ensuring our\napproach remains both tractable and computationally efficient. We also\nintroduce two variants of our method, each making different assumptions about\nprior information. Experimental results demonstrate that our method outperforms\nthe baseline by up to 5% in win rates against the chosen completion with\nlimited human preference data across several language models and real-world\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has become a cornerstone of\nthe training and alignment pipeline for large language models (LLMs). Recent\nadvances, such as direct preference optimization (DPO), have simplified the\npreference learning step. However, collecting preference data remains a\nchallenging and costly process, often requiring expert annotation. This cost\ncan be mitigated by carefully selecting the data points presented for\nannotation. In this work, we propose an active learning approach to efficiently\nselect prompt and preference pairs using a risk assessment strategy based on\nthe Sharpe Ratio. To address the challenge of unknown preferences prior to\nannotation, our method evaluates the gradients of all potential preference\nannotations to assess their impact on model updates. These gradient-based\nevaluations enable risk assessment of data points regardless of the annotation\noutcome. By leveraging the DPO loss derivations, we derive a closed-form\nexpression for computing these Sharpe ratios on a per-tuple basis, ensuring our\napproach remains both tractable and computationally efficient. We also\nintroduce two variants of our method, each making different assumptions about\nprior information. Experimental results demonstrate that our method outperforms\nthe baseline by up to 5% in win rates against the chosen completion with\nlimited human preference data across several language models and real-world\ndatasets."
                },
                "authors": [
                    {
                        "name": "Syrine Belakaria"
                    },
                    {
                        "name": "Joshua Kazdan"
                    },
                    {
                        "name": "Charles Marx"
                    },
                    {
                        "name": "Chris Cundy"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Barbara E. Engelhardt"
                    },
                    {
                        "name": "Stefano Ermon"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ermon"
                },
                "author": "Stefano Ermon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18288v2",
                "updated": "2025-03-28T03:35:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    3,
                    35,
                    17,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-24T02:17:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "Sun-Shine: A Large Language Model for Tibetan Culture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sun-Shine: A Large Language Model for Tibetan Culture"
                },
                "summary": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Nyima Tashi"
                    },
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Xiangxiang Wang"
                    },
                    {
                        "name": "Thupten Tsering"
                    },
                    {
                        "name": "Ban Ma-bao"
                    },
                    {
                        "name": "Renzeg Duojie"
                    },
                    {
                        "name": "Gadeng Luosang"
                    },
                    {
                        "name": "Rinchen Dongrub"
                    },
                    {
                        "name": "Dorje Tashi"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yongbin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Yu"
                },
                "author": "Yongbin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22115v1",
                "updated": "2025-03-28T03:31:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    3,
                    31,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T03:31:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    3,
                    31,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks\n  with Dialogues and Stories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks\n  with Dialogues and Stories"
                },
                "summary": "Evaluating the value alignment of large language models (LLMs) has\ntraditionally relied on single-sentence adversarial prompts, which directly\nprobe models with ethically sensitive or controversial questions. However, with\nthe rapid advancements in AI safety techniques, models have become increasingly\nadept at circumventing these straightforward tests, limiting their\neffectiveness in revealing underlying biases and ethical stances. To address\nthis limitation, we propose an upgraded value alignment benchmark that moves\nbeyond single-sentence prompts by incorporating multi-turn dialogues and\nnarrative-based scenarios. This approach enhances the stealth and adversarial\nnature of the evaluation, making it more robust against superficial safeguards\nimplemented in modern LLMs. We design and implement a dataset that includes\nconversational traps and ethically ambiguous storytelling, systematically\nassessing LLMs' responses in more nuanced and context-rich settings.\nExperimental results demonstrate that this enhanced methodology can effectively\nexpose latent biases that remain undetected in traditional single-shot\nevaluations. Our findings highlight the necessity of contextual and dynamic\ntesting for value alignment in LLMs, paving the way for more sophisticated and\nrealistic assessments of AI ethics and safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the value alignment of large language models (LLMs) has\ntraditionally relied on single-sentence adversarial prompts, which directly\nprobe models with ethically sensitive or controversial questions. However, with\nthe rapid advancements in AI safety techniques, models have become increasingly\nadept at circumventing these straightforward tests, limiting their\neffectiveness in revealing underlying biases and ethical stances. To address\nthis limitation, we propose an upgraded value alignment benchmark that moves\nbeyond single-sentence prompts by incorporating multi-turn dialogues and\nnarrative-based scenarios. This approach enhances the stealth and adversarial\nnature of the evaluation, making it more robust against superficial safeguards\nimplemented in modern LLMs. We design and implement a dataset that includes\nconversational traps and ethically ambiguous storytelling, systematically\nassessing LLMs' responses in more nuanced and context-rich settings.\nExperimental results demonstrate that this enhanced methodology can effectively\nexpose latent biases that remain undetected in traditional single-shot\nevaluations. Our findings highlight the necessity of contextual and dynamic\ntesting for value alignment in LLMs, paving the way for more sophisticated and\nrealistic assessments of AI ethics and safety."
                },
                "authors": [
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Qimeng Liu"
                    },
                    {
                        "name": "Qiuchi Li"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Jing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qin"
                },
                "author": "Jing Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03226v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03226v4",
                "updated": "2025-03-28T03:19:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    3,
                    19,
                    52,
                    4,
                    87,
                    0
                ],
                "published": "2024-10-04T08:26:06Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    8,
                    26,
                    6,
                    4,
                    278,
                    0
                ],
                "title": "Frame-Voyager: Learning to Query Frames for Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame-Voyager: Learning to Query Frames for Video Large Language Models"
                },
                "summary": "Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs."
                },
                "authors": [
                    {
                        "name": "Sicheng Yu"
                    },
                    {
                        "name": "Chengkai Jin"
                    },
                    {
                        "name": "Huanyu Wang"
                    },
                    {
                        "name": "Zhenghao Chen"
                    },
                    {
                        "name": "Sheng Jin"
                    },
                    {
                        "name": "Zhongrong Zuo"
                    },
                    {
                        "name": "Xiaolei Xu"
                    },
                    {
                        "name": "Zhenbang Sun"
                    },
                    {
                        "name": "Bingni Zhang"
                    },
                    {
                        "name": "Jiawei Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Qianru Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qianru Sun"
                },
                "author": "Qianru Sun",
                "arxiv_comment": "ICLR 2025, Camera-ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03226v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03226v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09464v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09464v3",
                "updated": "2025-03-28T03:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    3,
                    0,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-09-14T15:17:34Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    15,
                    17,
                    34,
                    5,
                    258,
                    0
                ],
                "title": "Measuring the Influence of Incorrect Code on Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the Influence of Incorrect Code on Test Generation"
                },
                "summary": "It is natural to suppose that a Large Language Model is more likely to\ngenerate correct test cases when prompted with correct code under test,\ncompared to incorrect code under test. However, the size of this effect has\nnever been previously measured, despite its obvious importance for both\npracticing software engineers and researchers. To answer the question, we\nconducted a comprehensive empirical study on 5 open source and 6 closed source\nlanguage models, with 3 widely-used benchmark data sets together with 41\nrepo-level real-world examples from two different real-world data sets. Our\nresults reveal that, when compared to incorrect code under test, LLMs prompted\nwith correct code achieve improvements in test accuracy, code coverage, and bug\ndetection of 57\\%, 12\\%, and 24\\% respectively. We further show that these\nscientific conclusions carry over from the three benchmark data sets to the\nreal-world code, where tests generated for incorrect code experience a 47\\%\nworse bug detection rate. Finally, we report that improvements of +18\\% in\naccuracy, +4\\% coverage, and +34\\% in bug detection can be achieved by\nproviding natural language code descriptions. These findings have actionable\nconclusions. For example, the 47\\% reduction in real-world bug detection is a\nclear concern. Fortunately, it is a concern for which our findings about the\nadded value of descriptions offer an immediately actionable remedy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to suppose that a Large Language Model is more likely to\ngenerate correct test cases when prompted with correct code under test,\ncompared to incorrect code under test. However, the size of this effect has\nnever been previously measured, despite its obvious importance for both\npracticing software engineers and researchers. To answer the question, we\nconducted a comprehensive empirical study on 5 open source and 6 closed source\nlanguage models, with 3 widely-used benchmark data sets together with 41\nrepo-level real-world examples from two different real-world data sets. Our\nresults reveal that, when compared to incorrect code under test, LLMs prompted\nwith correct code achieve improvements in test accuracy, code coverage, and bug\ndetection of 57\\%, 12\\%, and 24\\% respectively. We further show that these\nscientific conclusions carry over from the three benchmark data sets to the\nreal-world code, where tests generated for incorrect code experience a 47\\%\nworse bug detection rate. Finally, we report that improvements of +18\\% in\naccuracy, +4\\% coverage, and +34\\% in bug detection can be achieved by\nproviding natural language code descriptions. These findings have actionable\nconclusions. For example, the 47\\% reduction in real-world bug detection is a\nclear concern. Fortunately, it is a concern for which our findings about the\nadded value of descriptions offer an immediately actionable remedy."
                },
                "authors": [
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Mingzhe Du"
                    },
                    {
                        "name": "Heming Cui"
                    }
                ],
                "author_detail": {
                    "name": "Heming Cui"
                },
                "author": "Heming Cui",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09464v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09464v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12854v2",
                "updated": "2025-03-28T03:00:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    3,
                    0,
                    25,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-17T06:28:25Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    6,
                    28,
                    25,
                    0,
                    76,
                    0
                ],
                "title": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical\n  Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical\n  Investigation"
                },
                "summary": "Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations."
                },
                "authors": [
                    {
                        "name": "Songjun Tu"
                    },
                    {
                        "name": "Jiahao Lin"
                    },
                    {
                        "name": "Xiangyu Tian"
                    },
                    {
                        "name": "Qichao Zhang"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Xiangyuan Lan"
                    },
                    {
                        "name": "Dongmei Jiang"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22104v1",
                "updated": "2025-03-28T02:55:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    55,
                    39,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T02:55:39Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    55,
                    39,
                    4,
                    87,
                    0
                ],
                "title": "M2D2: Exploring General-purpose Audio-Language Representations Beyond\n  CLAP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2D2: Exploring General-purpose Audio-Language Representations Beyond\n  CLAP"
                },
                "summary": "Contrastive language-audio pre-training (CLAP) has addressed audio-language\ntasks such as audio-text retrieval by aligning audio and text in a common\nfeature space. While CLAP addresses general audio-language tasks, its audio\nfeatures do not generalize well in audio tasks. In contrast, self-supervised\nlearning (SSL) models learn general-purpose audio features that perform well in\ndiverse audio tasks. We pursue representation learning that can be widely used\nin audio applications and hypothesize that a method that learns both general\naudio features and CLAP features should achieve our goal, which we call a\ngeneral-purpose audio-language representation. To implement our hypothesis, we\npropose M2D2, a second-generation masked modeling duo (M2D) that combines an\nSSL M2D and CLAP. M2D2 learns two types of features using two modalities (audio\nand text) in a two-stage training process. It also utilizes advanced LLM-based\nsentence embeddings in CLAP training for powerful semantic supervision. In the\nfirst stage, M2D2 learns generalizable audio features from M2D and CLAP, where\nCLAP aligns the features with the fine LLM-based semantic embeddings. In the\nsecond stage, it learns CLAP features using the audio features learned from the\nLLM-based embeddings. Through these pre-training stages, M2D2 should enhance\ngeneralizability and performance in its audio and CLAP features. Experiments\nvalidated that M2D2 achieves effective general-purpose audio-language\nrepresentation, highlighted with SOTA fine-tuning mAP of 49.0 for AudioSet,\nSOTA performance in music tasks, and top-level performance in audio-language\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive language-audio pre-training (CLAP) has addressed audio-language\ntasks such as audio-text retrieval by aligning audio and text in a common\nfeature space. While CLAP addresses general audio-language tasks, its audio\nfeatures do not generalize well in audio tasks. In contrast, self-supervised\nlearning (SSL) models learn general-purpose audio features that perform well in\ndiverse audio tasks. We pursue representation learning that can be widely used\nin audio applications and hypothesize that a method that learns both general\naudio features and CLAP features should achieve our goal, which we call a\ngeneral-purpose audio-language representation. To implement our hypothesis, we\npropose M2D2, a second-generation masked modeling duo (M2D) that combines an\nSSL M2D and CLAP. M2D2 learns two types of features using two modalities (audio\nand text) in a two-stage training process. It also utilizes advanced LLM-based\nsentence embeddings in CLAP training for powerful semantic supervision. In the\nfirst stage, M2D2 learns generalizable audio features from M2D and CLAP, where\nCLAP aligns the features with the fine LLM-based semantic embeddings. In the\nsecond stage, it learns CLAP features using the audio features learned from the\nLLM-based embeddings. Through these pre-training stages, M2D2 should enhance\ngeneralizability and performance in its audio and CLAP features. Experiments\nvalidated that M2D2 achieves effective general-purpose audio-language\nrepresentation, highlighted with SOTA fine-tuning mAP of 49.0 for AudioSet,\nSOTA performance in music tasks, and top-level performance in audio-language\ntasks."
                },
                "authors": [
                    {
                        "name": "Daisuke Niizumi"
                    },
                    {
                        "name": "Daiki Takeuchi"
                    },
                    {
                        "name": "Masahiro Yasuda"
                    },
                    {
                        "name": "Binh Thien Nguyen"
                    },
                    {
                        "name": "Yasunori Ohishi"
                    },
                    {
                        "name": "Noboru Harada"
                    }
                ],
                "author_detail": {
                    "name": "Noboru Harada"
                },
                "author": "Noboru Harada",
                "arxiv_comment": "15 pages, 7 figures, 13 tables, under review at an IEEE journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20578v3",
                "updated": "2025-03-28T02:53:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    53,
                    43,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-26T14:25:01Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    1,
                    2,
                    85,
                    0
                ],
                "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation"
                },
                "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis."
                },
                "authors": [
                    {
                        "name": "Alif Al Hasan"
                    },
                    {
                        "name": "Subarna Saha"
                    },
                    {
                        "name": "Mia Mohammad Imran"
                    },
                    {
                        "name": "Tarannum Shaila Zaman"
                    }
                ],
                "author_detail": {
                    "name": "Tarannum Shaila Zaman"
                },
                "author": "Tarannum Shaila Zaman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22097v1",
                "updated": "2025-03-28T02:37:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    37,
                    18,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T02:37:18Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    37,
                    18,
                    4,
                    87,
                    0
                ],
                "title": "Few-Shot Graph Out-of-Distribution Detection with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Graph Out-of-Distribution Detection with LLMs"
                },
                "summary": "Existing methods for graph out-of-distribution (OOD) detection typically\ndepend on training graph neural network (GNN) classifiers using a substantial\namount of labeled in-distribution (ID) data. However, acquiring high-quality\nlabeled nodes in text-attributed graphs (TAGs) is challenging and costly due to\ntheir complex textual and structural characteristics. Large language models\n(LLMs), known for their powerful zero-shot capabilities in textual tasks, show\npromise but struggle to naturally capture the critical structural information\ninherent to TAGs, limiting their direct effectiveness.\n  To address these challenges, we propose LLM-GOOD, a general framework that\neffectively combines the strengths of LLMs and GNNs to enhance data efficiency\nin graph OOD detection. Specifically, we first leverage LLMs' strong zero-shot\ncapabilities to filter out likely OOD nodes, significantly reducing the human\nannotation burden. To minimize the usage and cost of the LLM, we employ it only\nto annotate a small subset of unlabeled nodes. We then train a lightweight GNN\nfilter using these noisy labels, enabling efficient predictions of ID status\nfor all other unlabeled nodes by leveraging both textual and structural\ninformation. After obtaining node embeddings from the GNN filter, we can apply\ninformativeness-based methods to select the most valuable nodes for precise\nhuman annotation. Finally, we train the target ID classifier using these\naccurately annotated ID nodes. Extensive experiments on four real-world TAG\ndatasets demonstrate that LLM-GOOD significantly reduces human annotation costs\nand outperforms state-of-the-art baselines in terms of both ID classification\naccuracy and OOD detection performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods for graph out-of-distribution (OOD) detection typically\ndepend on training graph neural network (GNN) classifiers using a substantial\namount of labeled in-distribution (ID) data. However, acquiring high-quality\nlabeled nodes in text-attributed graphs (TAGs) is challenging and costly due to\ntheir complex textual and structural characteristics. Large language models\n(LLMs), known for their powerful zero-shot capabilities in textual tasks, show\npromise but struggle to naturally capture the critical structural information\ninherent to TAGs, limiting their direct effectiveness.\n  To address these challenges, we propose LLM-GOOD, a general framework that\neffectively combines the strengths of LLMs and GNNs to enhance data efficiency\nin graph OOD detection. Specifically, we first leverage LLMs' strong zero-shot\ncapabilities to filter out likely OOD nodes, significantly reducing the human\nannotation burden. To minimize the usage and cost of the LLM, we employ it only\nto annotate a small subset of unlabeled nodes. We then train a lightweight GNN\nfilter using these noisy labels, enabling efficient predictions of ID status\nfor all other unlabeled nodes by leveraging both textual and structural\ninformation. After obtaining node embeddings from the GNN filter, we can apply\ninformativeness-based methods to select the most valuable nodes for precise\nhuman annotation. Finally, we train the target ID classifier using these\naccurately annotated ID nodes. Extensive experiments on four real-world TAG\ndatasets demonstrate that LLM-GOOD significantly reduces human annotation costs\nand outperforms state-of-the-art baselines in terms of both ID classification\naccuracy and OOD detection performance."
                },
                "authors": [
                    {
                        "name": "Haoyan Xu"
                    },
                    {
                        "name": "Zhengtao Yao"
                    },
                    {
                        "name": "Yushun Dong"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Mengyuan Li"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06787v2",
                "updated": "2025-03-28T02:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    27,
                    2,
                    4,
                    87,
                    0
                ],
                "published": "2025-02-10T18:59:35Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    35,
                    0,
                    41,
                    0
                ],
                "title": "Visual Agentic AI for Spatial Reasoning with a Dynamic API",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Agentic AI for Spatial Reasoning with a Dynamic API"
                },
                "summary": "Visual reasoning -- the ability to interpret the visual world -- is crucial\nfor embodied agents that operate within three-dimensional scenes. Progress in\nAI has led to vision and language models capable of answering questions from\nimages. However, their performance declines when tasked with 3D spatial\nreasoning. To tackle the complexity of such reasoning problems, we introduce an\nagentic program synthesis approach where LLM agents collaboratively generate a\nPythonic API with new functions to solve common subproblems. Our method\novercomes limitations of prior approaches that rely on a static, human-defined\nAPI, allowing it to handle a wider range of queries. To assess AI capabilities\nfor 3D understanding, we introduce a new benchmark of queries involving\nmultiple steps of grounding and inference. We show that our method outperforms\nprior zero-shot models for visual reasoning in 3D and empirically validate the\neffectiveness of our agentic framework for 3D spatial reasoning tasks. Project\nwebsite: https://glab-caltech.github.io/vadar/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning -- the ability to interpret the visual world -- is crucial\nfor embodied agents that operate within three-dimensional scenes. Progress in\nAI has led to vision and language models capable of answering questions from\nimages. However, their performance declines when tasked with 3D spatial\nreasoning. To tackle the complexity of such reasoning problems, we introduce an\nagentic program synthesis approach where LLM agents collaboratively generate a\nPythonic API with new functions to solve common subproblems. Our method\novercomes limitations of prior approaches that rely on a static, human-defined\nAPI, allowing it to handle a wider range of queries. To assess AI capabilities\nfor 3D understanding, we introduce a new benchmark of queries involving\nmultiple steps of grounding and inference. We show that our method outperforms\nprior zero-shot models for visual reasoning in 3D and empirically validate the\neffectiveness of our agentic framework for 3D spatial reasoning tasks. Project\nwebsite: https://glab-caltech.github.io/vadar/"
                },
                "authors": [
                    {
                        "name": "Damiano Marsili"
                    },
                    {
                        "name": "Rohun Agrawal"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Georgia Gkioxari"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Gkioxari"
                },
                "author": "Georgia Gkioxari",
                "arxiv_comment": "Project website: https://glab-caltech.github.io/vadar/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22092v1",
                "updated": "2025-03-28T02:15:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    15,
                    57,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T02:15:57Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    15,
                    57,
                    4,
                    87,
                    0
                ],
                "title": "Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes"
                },
                "summary": "Electronic Health Records (EHRs) often lack explicit links between\nmedications and diagnoses, making clinical decision-making and research more\ndifficult. Even when links exist, diagnosis lists may be incomplete, especially\nduring early patient visits. Discharge summaries tend to provide more complete\ninformation, which can help infer accurate diagnoses, especially with the help\nof large language models (LLMs). This study investigates whether LLMs can\npredict implicitly mentioned diagnoses from clinical notes and link them to\ncorresponding medications. We address two research questions: (1) Does majority\nvoting across diverse LLM configurations outperform the best single\nconfiguration in diagnosis prediction? (2) How sensitive is majority voting\naccuracy to LLM hyperparameters such as temperature, top-p, and summary length?\nTo evaluate, we created a new dataset of 240 expert-annotated\nmedication-diagnosis pairs from 20 MIMIC-IV notes. Using GPT-3.5 Turbo, we ran\n18 prompting configurations across short and long summary lengths, generating\n8568 test cases. Results show that majority voting achieved 75 percent\naccuracy, outperforming the best single configuration at 66 percent. No single\nhyperparameter setting dominated, but combining deterministic, balanced, and\nexploratory strategies improved performance. Shorter summaries generally led to\nhigher accuracy.In conclusion, ensemble-style majority voting with diverse LLM\nconfigurations improves diagnosis prediction in EHRs and offers a promising\nmethod to link medications and diagnoses in clinical texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) often lack explicit links between\nmedications and diagnoses, making clinical decision-making and research more\ndifficult. Even when links exist, diagnosis lists may be incomplete, especially\nduring early patient visits. Discharge summaries tend to provide more complete\ninformation, which can help infer accurate diagnoses, especially with the help\nof large language models (LLMs). This study investigates whether LLMs can\npredict implicitly mentioned diagnoses from clinical notes and link them to\ncorresponding medications. We address two research questions: (1) Does majority\nvoting across diverse LLM configurations outperform the best single\nconfiguration in diagnosis prediction? (2) How sensitive is majority voting\naccuracy to LLM hyperparameters such as temperature, top-p, and summary length?\nTo evaluate, we created a new dataset of 240 expert-annotated\nmedication-diagnosis pairs from 20 MIMIC-IV notes. Using GPT-3.5 Turbo, we ran\n18 prompting configurations across short and long summary lengths, generating\n8568 test cases. Results show that majority voting achieved 75 percent\naccuracy, outperforming the best single configuration at 66 percent. No single\nhyperparameter setting dominated, but combining deterministic, balanced, and\nexploratory strategies improved performance. Shorter summaries generally led to\nhigher accuracy.In conclusion, ensemble-style majority voting with diverse LLM\nconfigurations improves diagnosis prediction in EHRs and offers a promising\nmethod to link medications and diagnoses in clinical texts."
                },
                "authors": [
                    {
                        "name": "Dina Albassam"
                    },
                    {
                        "name": "Adam Cross"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Chengxiang Zhai"
                },
                "author": "Chengxiang Zhai",
                "arxiv_comment": "19 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19206v2",
                "updated": "2025-03-28T02:10:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    10,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-24T23:11:56Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    11,
                    56,
                    0,
                    83,
                    0
                ],
                "title": "Overtrained Language Models Are Harder to Fine-Tune",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overtrained Language Models Are Harder to Fine-Tune"
                },
                "summary": "Large language models are pre-trained on ever-growing token budgets under the\nassumption that better pre-training performance translates to improved\ndownstream models. In this work, we challenge this assumption and show that\nextended pre-training can make models harder to fine-tune, leading to degraded\nfinal performance. We term this phenomenon catastrophic overtraining. For\nexample, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads to\nover 2% worse performance on multiple standard LLM benchmarks than its 2.3T\ntoken counterpart. Through controlled experiments and theoretical analysis, we\nshow that catastrophic overtraining arises from a systematic increase in the\nbroad sensitivity of pre-trained parameters to modifications, including but not\nlimited to fine-tuning. Our findings call for a critical reassessment of\npre-training design that considers the downstream adaptability of the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are pre-trained on ever-growing token budgets under the\nassumption that better pre-training performance translates to improved\ndownstream models. In this work, we challenge this assumption and show that\nextended pre-training can make models harder to fine-tune, leading to degraded\nfinal performance. We term this phenomenon catastrophic overtraining. For\nexample, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads to\nover 2% worse performance on multiple standard LLM benchmarks than its 2.3T\ntoken counterpart. Through controlled experiments and theoretical analysis, we\nshow that catastrophic overtraining arises from a systematic increase in the\nbroad sensitivity of pre-trained parameters to modifications, including but not\nlimited to fine-tuning. Our findings call for a critical reassessment of\npre-training design that considers the downstream adaptability of the model."
                },
                "authors": [
                    {
                        "name": "Jacob Mitchell Springer"
                    },
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Kaiyue Wen"
                    },
                    {
                        "name": "Tanishq Kumar"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Sadhika Malladi"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Aditi Raghunathan"
                    }
                ],
                "author_detail": {
                    "name": "Aditi Raghunathan"
                },
                "author": "Aditi Raghunathan",
                "arxiv_comment": "72 pages, 65 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22081v1",
                "updated": "2025-03-28T01:57:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    1,
                    57,
                    35,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T01:57:35Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    1,
                    57,
                    35,
                    4,
                    87,
                    0
                ],
                "title": "A Survey on Remote Sensing Foundation Models: From Vision to\n  Multimodality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Remote Sensing Foundation Models: From Vision to\n  Multimodality"
                },
                "summary": "The rapid advancement of remote sensing foundation models, particularly\nvision and multimodal models, has significantly enhanced the capabilities of\nintelligent geospatial data interpretation. These models combine various data\nmodalities, such as optical, radar, and LiDAR imagery, with textual and\ngeographic information, enabling more comprehensive analysis and understanding\nof remote sensing data. The integration of multiple modalities allows for\nimproved performance in tasks like object detection, land cover classification,\nand change detection, which are often challenged by the complex and\nheterogeneous nature of remote sensing data. However, despite these\nadvancements, several challenges remain. The diversity in data types, the need\nfor large-scale annotated datasets, and the complexity of multimodal fusion\ntechniques pose significant obstacles to the effective deployment of these\nmodels. Moreover, the computational demands of training and fine-tuning\nmultimodal models require significant resources, further complicating their\npractical application in remote sensing image interpretation tasks. This paper\nprovides a comprehensive review of the state-of-the-art in vision and\nmultimodal foundation models for remote sensing, focusing on their\narchitecture, training methods, datasets and application scenarios. We discuss\nthe key challenges these models face, such as data alignment, cross-modal\ntransfer learning, and scalability, while also identifying emerging research\ndirections aimed at overcoming these limitations. Our goal is to provide a\nclear understanding of the current landscape of remote sensing foundation\nmodels and inspire future research that can push the boundaries of what these\nmodels can achieve in real-world applications. The list of resources collected\nby the paper can be found in the\nhttps://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of remote sensing foundation models, particularly\nvision and multimodal models, has significantly enhanced the capabilities of\nintelligent geospatial data interpretation. These models combine various data\nmodalities, such as optical, radar, and LiDAR imagery, with textual and\ngeographic information, enabling more comprehensive analysis and understanding\nof remote sensing data. The integration of multiple modalities allows for\nimproved performance in tasks like object detection, land cover classification,\nand change detection, which are often challenged by the complex and\nheterogeneous nature of remote sensing data. However, despite these\nadvancements, several challenges remain. The diversity in data types, the need\nfor large-scale annotated datasets, and the complexity of multimodal fusion\ntechniques pose significant obstacles to the effective deployment of these\nmodels. Moreover, the computational demands of training and fine-tuning\nmultimodal models require significant resources, further complicating their\npractical application in remote sensing image interpretation tasks. This paper\nprovides a comprehensive review of the state-of-the-art in vision and\nmultimodal foundation models for remote sensing, focusing on their\narchitecture, training methods, datasets and application scenarios. We discuss\nthe key challenges these models face, such as data alignment, cross-modal\ntransfer learning, and scalability, while also identifying emerging research\ndirections aimed at overcoming these limitations. Our goal is to provide a\nclear understanding of the current landscape of remote sensing foundation\nmodels and inspire future research that can push the boundaries of what these\nmodels can achieve in real-world applications. The list of resources collected\nby the paper can be found in the\nhttps://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models."
                },
                "authors": [
                    {
                        "name": "Ziyue Huang"
                    },
                    {
                        "name": "Hongxi Yan"
                    },
                    {
                        "name": "Qiqi Zhan"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Mingming Zhang"
                    },
                    {
                        "name": "Chenkai Zhang"
                    },
                    {
                        "name": "YiMing Lei"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Qingjie Liu"
                    },
                    {
                        "name": "Yunhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhong Wang"
                },
                "author": "Yunhong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22074v1",
                "updated": "2025-03-28T01:33:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    1,
                    33,
                    5,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T01:33:05Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    1,
                    33,
                    5,
                    4,
                    87,
                    0
                ],
                "title": "Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A\n  General Framework for Domain-Specific Large Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A\n  General Framework for Domain-Specific Large Language Model Adaptation"
                },
                "summary": "Large language models (LLMs) hold great promise for specialized scientific\ndomains such as materials science, yet adapting them efficiently and accurately\nto domain-specific knowledge remains challenging due to limited data and high\nknowledge density. We propose a two-stage framework that combines structured\nmodel compression with a scientific fine-tuning regimen to address this\nchallenge. In the compression stage, we decompose the LLM's weight matrices\ninto local low-rank \"rank blocks\" and arrange these blocks in a Penrose-like\nnon-periodic tiling pattern. Each block is then compacted via spectral\ntransformations (e.g., discrete cosine or Fourier transforms), and a\nKullback-Leibler (KL) divergence-based alignment loss preserves the\ndistributional similarity between the compressed model's representations and\nthose of the original full model. In the adaptation stage, the compressed model\nis further tuned using a human-like scientific reading protocol: it processes\ntechnical materials science documents section by section, engaging in a\nstructured question-and-answer routine for each section. This section-wise Q&A\nfine-tuning strategy extracts explicit reasoning traces and gradually injects\ndomain knowledge, while minimizing catastrophic forgetting of the model's\ngeneral language capabilities. By balancing efficient compression with targeted\nadaptation, our two-stage approach enables precise specialization of LLMs to\nhigh-value domains under data-scarce conditions. We present this principled yet\nexploratory pipeline and outline its potential for advancing materials science\nknowledge integration, laying the groundwork for comprehensive empirical\nevaluation in future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold great promise for specialized scientific\ndomains such as materials science, yet adapting them efficiently and accurately\nto domain-specific knowledge remains challenging due to limited data and high\nknowledge density. We propose a two-stage framework that combines structured\nmodel compression with a scientific fine-tuning regimen to address this\nchallenge. In the compression stage, we decompose the LLM's weight matrices\ninto local low-rank \"rank blocks\" and arrange these blocks in a Penrose-like\nnon-periodic tiling pattern. Each block is then compacted via spectral\ntransformations (e.g., discrete cosine or Fourier transforms), and a\nKullback-Leibler (KL) divergence-based alignment loss preserves the\ndistributional similarity between the compressed model's representations and\nthose of the original full model. In the adaptation stage, the compressed model\nis further tuned using a human-like scientific reading protocol: it processes\ntechnical materials science documents section by section, engaging in a\nstructured question-and-answer routine for each section. This section-wise Q&A\nfine-tuning strategy extracts explicit reasoning traces and gradually injects\ndomain knowledge, while minimizing catastrophic forgetting of the model's\ngeneral language capabilities. By balancing efficient compression with targeted\nadaptation, our two-stage approach enables precise specialization of LLMs to\nhigh-value domains under data-scarce conditions. We present this principled yet\nexploratory pipeline and outline its potential for advancing materials science\nknowledge integration, laying the groundwork for comprehensive empirical\nevaluation in future work."
                },
                "authors": [
                    {
                        "name": "Chuan-Wei Kuo"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Chenqi Yan"
                    },
                    {
                        "name": "Yu Yang Fredrik Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Yang Fredrik Liu"
                },
                "author": "Yu Yang Fredrik Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17266v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17266v2",
                "updated": "2025-03-28T01:02:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    1,
                    2,
                    11,
                    4,
                    87,
                    0
                ],
                "published": "2024-09-25T18:27:35Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    27,
                    35,
                    2,
                    269,
                    0
                ],
                "title": "Empirical Asset Pricing with Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Asset Pricing with Large Language Model Agents"
                },
                "summary": "In this study, we introduce a novel asset pricing model leveraging the Large\nLanguage Model (LLM) agents, which integrates qualitative discretionary\ninvestment evaluations from LLM agents with quantitative financial economic\nfactors manually curated, aiming to explain the excess asset returns. The\nexperimental results demonstrate that our methodology surpasses traditional\nmachine learning-based baselines in both portfolio optimization and asset\npricing errors. Notably, the Sharpe ratio for portfolio optimization and the\nmean magnitude of $|\\alpha|$ for anomaly portfolios experienced substantial\nenhancements of 10.6\\% and 10.0\\% respectively. Moreover, we performed\ncomprehensive ablation studies on our model and conducted a thorough analysis\nof the method to extract further insights into the proposed approach. Our\nresults show effective evidence of the feasibility of applying LLMs in\nempirical asset pricing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce a novel asset pricing model leveraging the Large\nLanguage Model (LLM) agents, which integrates qualitative discretionary\ninvestment evaluations from LLM agents with quantitative financial economic\nfactors manually curated, aiming to explain the excess asset returns. The\nexperimental results demonstrate that our methodology surpasses traditional\nmachine learning-based baselines in both portfolio optimization and asset\npricing errors. Notably, the Sharpe ratio for portfolio optimization and the\nmean magnitude of $|\\alpha|$ for anomaly portfolios experienced substantial\nenhancements of 10.6\\% and 10.0\\% respectively. Moreover, we performed\ncomprehensive ablation studies on our model and conducted a thorough analysis\nof the method to extract further insights into the proposed approach. Our\nresults show effective evidence of the feasibility of applying LLMs in\nempirical asset pricing."
                },
                "authors": [
                    {
                        "name": "Junyan Cheng"
                    },
                    {
                        "name": "Peter Chin"
                    }
                ],
                "author_detail": {
                    "name": "Peter Chin"
                },
                "author": "Peter Chin",
                "arxiv_comment": "ICLR 2025 Workshop on Advances in Financial AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17266v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22064v1",
                "updated": "2025-03-28T00:57:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    0,
                    57,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T00:57:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    0,
                    57,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "Multi-Task Semantic Communications via Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Task Semantic Communications via Large Models"
                },
                "summary": "Artificial intelligence (AI) promises to revolutionize the design,\noptimization and management of next-generation communication systems. In this\narticle, we explore the integration of large AI models (LAMs) into semantic\ncommunications (SemCom) by leveraging their multi-modal data processing and\ngeneration capabilities. Although LAMs bring unprecedented abilities to extract\nsemantics from raw data, this integration entails multifaceted challenges\nincluding high resource demands, model complexity, and the need for\nadaptability across diverse modalities and tasks. To overcome these challenges,\nwe propose a LAM-based multi-task SemCom (MTSC) architecture, which includes an\nadaptive model compression strategy and a federated split fine-tuning approach\nto facilitate the efficient deployment of LAM-based semantic models in\nresource-limited networks. Furthermore, a retrieval-augmented generation scheme\nis implemented to synthesize the most recent local and global knowledge bases\nto enhance the accuracy of semantic extraction and content generation, thereby\nimproving the inference performance. Finally, simulation results demonstrate\nthe efficacy of the proposed LAM-based MTSC architecture, highlighting the\nperformance enhancements across various downstream tasks under varying channel\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) promises to revolutionize the design,\noptimization and management of next-generation communication systems. In this\narticle, we explore the integration of large AI models (LAMs) into semantic\ncommunications (SemCom) by leveraging their multi-modal data processing and\ngeneration capabilities. Although LAMs bring unprecedented abilities to extract\nsemantics from raw data, this integration entails multifaceted challenges\nincluding high resource demands, model complexity, and the need for\nadaptability across diverse modalities and tasks. To overcome these challenges,\nwe propose a LAM-based multi-task SemCom (MTSC) architecture, which includes an\nadaptive model compression strategy and a federated split fine-tuning approach\nto facilitate the efficient deployment of LAM-based semantic models in\nresource-limited networks. Furthermore, a retrieval-augmented generation scheme\nis implemented to synthesize the most recent local and global knowledge bases\nto enhance the accuracy of semantic extraction and content generation, thereby\nimproving the inference performance. Finally, simulation results demonstrate\nthe efficacy of the proposed LAM-based MTSC architecture, highlighting the\nperformance enhancements across various downstream tasks under varying channel\nconditions."
                },
                "authors": [
                    {
                        "name": "Wanli Ni"
                    },
                    {
                        "name": "Zhijin Qin"
                    },
                    {
                        "name": "Haofeng Sun"
                    },
                    {
                        "name": "Xiaoming Tao"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22063v1",
                "updated": "2025-03-28T00:56:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    0,
                    56,
                    56,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T00:56:56Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    0,
                    56,
                    56,
                    4,
                    87,
                    0
                ],
                "title": "Arch-LLM: Taming LLMs for Neural Architecture Generation via\n  Unsupervised Discrete Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arch-LLM: Taming LLMs for Neural Architecture Generation via\n  Unsupervised Discrete Representation Learning"
                },
                "summary": "Unsupervised representation learning has been widely explored across various\nmodalities, including neural architectures, where it plays a key role in\ndownstream applications like Neural Architecture Search (NAS). These methods\ntypically learn an unsupervised representation space before generating/\nsampling architectures for the downstream search. A common approach involves\nthe use of Variational Autoencoders (VAEs) to map discrete architectures onto a\ncontinuous representation space, however, sampling from these spaces often\nleads to a high percentage of invalid or duplicate neural architectures. This\ncould be due to the unnatural mapping of inherently discrete architectural\nspace onto a continuous space, which emphasizes the need for a robust discrete\nrepresentation of these architectures. To address this, we introduce a Vector\nQuantized Variational Autoencoder (VQ-VAE) to learn a discrete latent space\nmore naturally aligned with the discrete neural architectures. In contrast to\nVAEs, VQ-VAEs (i) map each architecture into a discrete code sequence and (ii)\nallow the prior to be learned by any generative model rather than assuming a\nnormal distribution. We then represent these architecture latent codes as\nnumerical sequences and train a text-to-text model leveraging a Large Language\nModel to learn and generate sequences representing architectures. We experiment\nour method with Inception/ ResNet-like cell-based search spaces, namely\nNAS-Bench-101 and NAS-Bench-201. Compared to VAE-based methods, our approach\nimproves the generation of valid and unique architectures by over 80% on\nNASBench-101 and over 8% on NASBench-201. Finally, we demonstrate the\napplicability of our method in NAS employing a sequence-modeling-based NAS\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised representation learning has been widely explored across various\nmodalities, including neural architectures, where it plays a key role in\ndownstream applications like Neural Architecture Search (NAS). These methods\ntypically learn an unsupervised representation space before generating/\nsampling architectures for the downstream search. A common approach involves\nthe use of Variational Autoencoders (VAEs) to map discrete architectures onto a\ncontinuous representation space, however, sampling from these spaces often\nleads to a high percentage of invalid or duplicate neural architectures. This\ncould be due to the unnatural mapping of inherently discrete architectural\nspace onto a continuous space, which emphasizes the need for a robust discrete\nrepresentation of these architectures. To address this, we introduce a Vector\nQuantized Variational Autoencoder (VQ-VAE) to learn a discrete latent space\nmore naturally aligned with the discrete neural architectures. In contrast to\nVAEs, VQ-VAEs (i) map each architecture into a discrete code sequence and (ii)\nallow the prior to be learned by any generative model rather than assuming a\nnormal distribution. We then represent these architecture latent codes as\nnumerical sequences and train a text-to-text model leveraging a Large Language\nModel to learn and generate sequences representing architectures. We experiment\nour method with Inception/ ResNet-like cell-based search spaces, namely\nNAS-Bench-101 and NAS-Bench-201. Compared to VAE-based methods, our approach\nimproves the generation of valid and unique architectures by over 80% on\nNASBench-101 and over 8% on NASBench-201. Finally, we demonstrate the\napplicability of our method in NAS employing a sequence-modeling-based NAS\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Deshani Geethika Poddenige"
                    },
                    {
                        "name": "Sachith Seneviratne"
                    },
                    {
                        "name": "Damith Senanayake"
                    },
                    {
                        "name": "Mahesan Niranjan"
                    },
                    {
                        "name": "PN Suganthan"
                    },
                    {
                        "name": "Saman Halgamuge"
                    }
                ],
                "author_detail": {
                    "name": "Saman Halgamuge"
                },
                "author": "Saman Halgamuge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19820v3",
                "updated": "2025-03-28T00:37:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    0,
                    37,
                    10,
                    4,
                    87,
                    0
                ],
                "published": "2025-02-27T06:49:16Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    49,
                    16,
                    3,
                    58,
                    0
                ],
                "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs"
                },
                "summary": "Ensuring AI safety is crucial as large language models become increasingly\nintegrated into real-world applications. A key challenge is jailbreak, where\nadversarial prompts bypass built-in safeguards to elicit harmful disallowed\noutputs. Inspired by psychological foot-in-the-door principles, we introduce\nFITD,a novel multi-turn jailbreak method that leverages the phenomenon where\nminor initial commitments lower resistance to more significant or more\nunethical transgressions. Our approach progressively escalates the malicious\nintent of user queries through intermediate bridge prompts and aligns the\nmodel's response by itself to induce toxic responses. Extensive experimental\nresults on two jailbreak benchmarks demonstrate that FITD achieves an average\nattack success rate of 94% across seven widely used models, outperforming\nexisting state-of-the-art methods. Additionally, we provide an in-depth\nanalysis of LLM self-corruption, highlighting vulnerabilities in current\nalignment strategies and emphasizing the risks inherent in multi-turn\ninteractions. The code is available at\nhttps://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring AI safety is crucial as large language models become increasingly\nintegrated into real-world applications. A key challenge is jailbreak, where\nadversarial prompts bypass built-in safeguards to elicit harmful disallowed\noutputs. Inspired by psychological foot-in-the-door principles, we introduce\nFITD,a novel multi-turn jailbreak method that leverages the phenomenon where\nminor initial commitments lower resistance to more significant or more\nunethical transgressions. Our approach progressively escalates the malicious\nintent of user queries through intermediate bridge prompts and aligns the\nmodel's response by itself to induce toxic responses. Extensive experimental\nresults on two jailbreak benchmarks demonstrate that FITD achieves an average\nattack success rate of 94% across seven widely used models, outperforming\nexisting state-of-the-art methods. Additionally, we provide an in-depth\nanalysis of LLM self-corruption, highlighting vulnerabilities in current\nalignment strategies and emphasizing the risks inherent in multi-turn\ninteractions. The code is available at\nhttps://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak."
                },
                "authors": [
                    {
                        "name": "Zixuan Weng"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22055v1",
                "updated": "2025-03-28T00:23:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    0,
                    23,
                    4,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T00:23:04Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    0,
                    23,
                    4,
                    4,
                    87,
                    0
                ],
                "title": "Large eddy simulation of a utility-scale vertical-axis marine\n  hydrokinetic turbine under live-bed conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large eddy simulation of a utility-scale vertical-axis marine\n  hydrokinetic turbine under live-bed conditions"
                },
                "summary": "We present a coupled large-eddy simulation (LES) and bed morphodynamics study\nto investigate the impact of sediment dynamics on the wake flow, wake recovery\nand power production of a utility-scale marine hydrokinetic vertical-axis\nturbine (VAT). A geometry-resolving immersed boundary method is employed to\ncapture the turbine components, the waterway, and the sediment layer. Our\nnumerical findings reveal that increasing the turbine tip speed ratio (TSR)\nwould intensify turbulence, accelerate wake recovery, and increase erosion at\nthe base of the device. Furthermore, it is found that the deformation of the\nbed around the turbine induces a jet-like flow near the bed beneath the\nturbine, which enhances wake recovery. Analyzing the interactions between\nturbulent flow and bed morphodynamics, this study seeks to provide physical\ninformation on the environmental and operational implications of VAT deployment\nin natural riverine and marine environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a coupled large-eddy simulation (LES) and bed morphodynamics study\nto investigate the impact of sediment dynamics on the wake flow, wake recovery\nand power production of a utility-scale marine hydrokinetic vertical-axis\nturbine (VAT). A geometry-resolving immersed boundary method is employed to\ncapture the turbine components, the waterway, and the sediment layer. Our\nnumerical findings reveal that increasing the turbine tip speed ratio (TSR)\nwould intensify turbulence, accelerate wake recovery, and increase erosion at\nthe base of the device. Furthermore, it is found that the deformation of the\nbed around the turbine induces a jet-like flow near the bed beneath the\nturbine, which enhances wake recovery. Analyzing the interactions between\nturbulent flow and bed morphodynamics, this study seeks to provide physical\ninformation on the environmental and operational implications of VAT deployment\nin natural riverine and marine environments."
                },
                "authors": [
                    {
                        "name": "Mehrshad Gholami Anjiraki"
                    },
                    {
                        "name": "Mustafa Meriç Aksen"
                    },
                    {
                        "name": "Jonathan Craig"
                    },
                    {
                        "name": "Hossein Seyedzadeh"
                    },
                    {
                        "name": "Ali Khosronejad"
                    }
                ],
                "author_detail": {
                    "name": "Ali Khosronejad"
                },
                "author": "Ali Khosronejad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10020v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10020v3",
                "updated": "2025-03-28T00:06:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    0,
                    6,
                    51,
                    4,
                    87,
                    0
                ],
                "published": "2024-01-18T14:43:47Z",
                "published_parsed": [
                    2024,
                    1,
                    18,
                    14,
                    43,
                    47,
                    3,
                    18,
                    0
                ],
                "title": "Self-Rewarding Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Rewarding Language Models"
                },
                "summary": "We posit that to achieve superhuman agents, future models require superhuman\nfeedback in order to provide an adequate training signal. Current approaches\ncommonly train reward models from human preferences, which may then be\nbottlenecked by human performance level, and secondly these separate frozen\nreward models cannot then learn to improve during LLM training. In this work,\nwe study Self-Rewarding Language Models, where the language model itself is\nused via LLM-as-a-Judge prompting to provide its own rewards during training.\nWe show that during Iterative DPO training that not only does instruction\nfollowing ability improve, but also the ability to provide high-quality rewards\nto itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a\nmodel that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,\nincluding Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still\nto explore, this work opens the door to the possibility of models that can\ncontinually improve in both axes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We posit that to achieve superhuman agents, future models require superhuman\nfeedback in order to provide an adequate training signal. Current approaches\ncommonly train reward models from human preferences, which may then be\nbottlenecked by human performance level, and secondly these separate frozen\nreward models cannot then learn to improve during LLM training. In this work,\nwe study Self-Rewarding Language Models, where the language model itself is\nused via LLM-as-a-Judge prompting to provide its own rewards during training.\nWe show that during Iterative DPO training that not only does instruction\nfollowing ability improve, but also the ability to provide high-quality rewards\nto itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a\nmodel that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,\nincluding Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still\nto explore, this work opens the door to the possibility of models that can\ncontinually improve in both axes."
                },
                "authors": [
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Richard Yuanzhe Pang"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jason Weston"
                    }
                ],
                "author_detail": {
                    "name": "Jason Weston"
                },
                "author": "Jason Weston",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10020v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10020v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15206v2",
                "updated": "2025-03-28T00:04:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    0,
                    4,
                    4,
                    4,
                    87,
                    0
                ],
                "published": "2025-01-25T13:20:11Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    13,
                    20,
                    11,
                    5,
                    25,
                    0
                ],
                "title": "Engineering-Oriented Design of Drift-Resilient MTJ Random Number\n  Generator via Hybrid Control Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering-Oriented Design of Drift-Resilient MTJ Random Number\n  Generator via Hybrid Control Strategies"
                },
                "summary": "Magnetic Tunnel Junctions (MTJs) have shown great promise as hardware sources\nfor true random number generation (TRNG) due to their intrinsic stochastic\nswitching behavior. However, practical deployment remains challenged by drift\nin switching probability caused by thermal fluctuations, device aging, and\nenvironmental instability. This work presents an engineering-oriented,\ndrift-resilient MTJ-based TRNG architecture, enabled by a hybrid control\nstrategy that combines self-stabilizing feedback with pulse width modulation. A\nkey component is the Downcalibration-2 scheme, which updates the control\nparameter every two steps using only integer-resolution timing, ensuring\nexcellent statistical quality without requiring bit discarding,\npre-characterization, or external calibration. Extensive experimental\nmeasurements and numerical simulations demonstrate that this approach maintains\nstable randomness under dynamic temperature drift, using only simple digital\nlogic. The proposed architecture offers high throughput, robustness, and\nscalability, making it well-suited for secure hardware applications, embedded\nsystems, and edge computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic Tunnel Junctions (MTJs) have shown great promise as hardware sources\nfor true random number generation (TRNG) due to their intrinsic stochastic\nswitching behavior. However, practical deployment remains challenged by drift\nin switching probability caused by thermal fluctuations, device aging, and\nenvironmental instability. This work presents an engineering-oriented,\ndrift-resilient MTJ-based TRNG architecture, enabled by a hybrid control\nstrategy that combines self-stabilizing feedback with pulse width modulation. A\nkey component is the Downcalibration-2 scheme, which updates the control\nparameter every two steps using only integer-resolution timing, ensuring\nexcellent statistical quality without requiring bit discarding,\npre-characterization, or external calibration. Extensive experimental\nmeasurements and numerical simulations demonstrate that this approach maintains\nstable randomness under dynamic temperature drift, using only simple digital\nlogic. The proposed architecture offers high throughput, robustness, and\nscalability, making it well-suited for secure hardware applications, embedded\nsystems, and edge computing environments."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Caihua Wan"
                    },
                    {
                        "name": "Yingqian Xu"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Raik Hoffmann"
                    },
                    {
                        "name": "Meike Hindenberg"
                    },
                    {
                        "name": "Shiqiang Liu"
                    },
                    {
                        "name": "Dehao Kong"
                    },
                    {
                        "name": "Shilong Xiong"
                    },
                    {
                        "name": "Shikun He"
                    },
                    {
                        "name": "Alptekin Vardar"
                    },
                    {
                        "name": "Qiang Dai"
                    },
                    {
                        "name": "Junlu Gong"
                    },
                    {
                        "name": "Yihui Sun"
                    },
                    {
                        "name": "Zejie Zheng"
                    },
                    {
                        "name": "Thomas Kämpfe"
                    },
                    {
                        "name": "Guoqiang Yu"
                    },
                    {
                        "name": "Xiufeng Han"
                    }
                ],
                "author_detail": {
                    "name": "Xiufeng Han"
                },
                "author": "Xiufeng Han",
                "arxiv_comment": "16 pages, 9 figures, data shared at\n  https://doi.org/10.6084/m9.figshare.28680899.v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]