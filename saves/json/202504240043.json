[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Ho√üfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v1",
                "updated": "2025-04-21T21:01:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology.Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 {\\mu}s\nper slide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology.Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 {\\mu}s\nper slide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v1",
                "updated": "2025-04-21T18:12:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v1",
                "updated": "2025-04-21T09:41:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max L√ºbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v5",
                "updated": "2025-04-19T05:57:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    57,
                    44,
                    5,
                    109,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v2",
                "updated": "2025-04-17T21:19:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    21,
                    19,
                    19,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v1",
                "updated": "2025-04-16T18:03:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13915v1",
                "updated": "2025-04-10T17:13:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:13:08Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "title": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding"
                },
                "summary": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets."
                },
                "authors": [
                    {
                        "name": "Dibyadip Chatterjee"
                    },
                    {
                        "name": "Edoardo Remelli"
                    },
                    {
                        "name": "Yale Song"
                    },
                    {
                        "name": "Bugra Tekin"
                    },
                    {
                        "name": "Abhay Mittal"
                    },
                    {
                        "name": "Bharat Bhatnagar"
                    },
                    {
                        "name": "Necati Cihan Camg√∂z"
                    },
                    {
                        "name": "Shreyas Hampali"
                    },
                    {
                        "name": "Eric Sauser"
                    },
                    {
                        "name": "Shugao Ma"
                    },
                    {
                        "name": "Angela Yao"
                    },
                    {
                        "name": "Fadime Sener"
                    }
                ],
                "author_detail": {
                    "name": "Fadime Sener"
                },
                "author": "Fadime Sener",
                "arxiv_comment": "13 pages, 5 figures; https://dibschat.github.io/ProVideLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Sch√∂ne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Sch√∂ne"
                },
                "author": "Robert Sch√∂ne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-Andr√© No√´l"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Andr√© No√´l"
                },
                "author": "Pierre-Andr√© No√´l",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.16084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16084v1",
                "updated": "2025-04-22T17:59:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    56,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    56,
                    1,
                    112,
                    0
                ],
                "title": "TTRL: Test-Time Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTRL: Test-Time Reinforcement Learning"
                },
                "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16083v1",
                "updated": "2025-04-22T17:59:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    51,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:59:51Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    51,
                    1,
                    112,
                    0
                ],
                "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention"
                },
                "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16082v1",
                "updated": "2025-04-22T17:59:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    41,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    41,
                    1,
                    112,
                    0
                ],
                "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding"
                },
                "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video"
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16080v1",
                "updated": "2025-04-22T17:58:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    58,
                    7,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:58:07Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    58,
                    7,
                    1,
                    112,
                    0
                ],
                "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning"
                },
                "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks."
                },
                "authors": [
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Liangbing Zhao"
                    },
                    {
                        "name": "Sayak Paul"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Mohamed Elhoseiny"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "All code, checkpoints, and datasets are available at\n  \\url{https://diffusion-cot.github.io/reflection2perfection}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16078v1",
                "updated": "2025-04-22T17:57:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    57,
                    14,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:57:14Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    57,
                    14,
                    1,
                    112,
                    0
                ],
                "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities"
                },
                "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Thomas Schmied"
                    },
                    {
                        "name": "J√∂rg Bornschein"
                    },
                    {
                        "name": "Jordi Grau-Moya"
                    },
                    {
                        "name": "Markus Wulfmeier"
                    },
                    {
                        "name": "Razvan Pascanu"
                    }
                ],
                "author_detail": {
                    "name": "Razvan Pascanu"
                },
                "author": "Razvan Pascanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09572v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09572v3",
                "updated": "2025-04-22T17:56:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    56,
                    22,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-12T17:40:52Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    40,
                    52,
                    2,
                    71,
                    0
                ],
                "title": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks"
                },
                "summary": "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a\nstate-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as\na text-only state-of-the-art 81.36% success rate on WebVoyager.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a\nstate-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as\na text-only state-of-the-art 81.36% success rate on WebVoyager."
                },
                "authors": [
                    {
                        "name": "Lutfi Eren Erdogan"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09572v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09572v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16074v1",
                "updated": "2025-04-22T17:53:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    53,
                    29,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:53:29Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    53,
                    29,
                    1,
                    112,
                    0
                ],
                "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in\n  Large Language Models"
                },
                "summary": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/."
                },
                "authors": [
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Shaoyang Guo"
                    },
                    {
                        "name": "Zhuo-Yang Song"
                    },
                    {
                        "name": "Yunbo Sun"
                    },
                    {
                        "name": "Zeyu Cai"
                    },
                    {
                        "name": "Jiashen Wei"
                    },
                    {
                        "name": "Tianyu Luo"
                    },
                    {
                        "name": "Yixuan Yin"
                    },
                    {
                        "name": "Haoxu Zhang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Chencheng Tang"
                    },
                    {
                        "name": "Haoling Chang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Ziheng Zhou"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Jingtian Zhang"
                    },
                    {
                        "name": "Zhangyi Liu"
                    },
                    {
                        "name": "Minghao Li"
                    },
                    {
                        "name": "Yuku Zhang"
                    },
                    {
                        "name": "Boxuan Jing"
                    },
                    {
                        "name": "Xianqi Yin"
                    },
                    {
                        "name": "Yutong Ren"
                    },
                    {
                        "name": "Zizhuo Fu"
                    },
                    {
                        "name": "Weike Wang"
                    },
                    {
                        "name": "Xudong Tian"
                    },
                    {
                        "name": "Anqi Lv"
                    },
                    {
                        "name": "Laifu Man"
                    },
                    {
                        "name": "Jianxiang Li"
                    },
                    {
                        "name": "Feiyu Tao"
                    },
                    {
                        "name": "Qihua Sun"
                    },
                    {
                        "name": "Zhou Liang"
                    },
                    {
                        "name": "Yushu Mu"
                    },
                    {
                        "name": "Zhongxuan Li"
                    },
                    {
                        "name": "Jing-Jun Zhang"
                    },
                    {
                        "name": "Shutao Zhang"
                    },
                    {
                        "name": "Xiaotian Li"
                    },
                    {
                        "name": "Xingqi Xia"
                    },
                    {
                        "name": "Jiawei Lin"
                    },
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Jiahang Chen"
                    },
                    {
                        "name": "Qiuhao Xiong"
                    },
                    {
                        "name": "Binran Wang"
                    },
                    {
                        "name": "Fengyuan Wang"
                    },
                    {
                        "name": "Ziyang Ni"
                    },
                    {
                        "name": "Bohan Zhang"
                    },
                    {
                        "name": "Fan Cui"
                    },
                    {
                        "name": "Changkun Shao"
                    },
                    {
                        "name": "Qing-Hong Cao"
                    },
                    {
                        "name": "Ming-xing Luo"
                    },
                    {
                        "name": "Muhan Zhang"
                    },
                    {
                        "name": "Hua Xing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xing Zhu"
                },
                "author": "Hua Xing Zhu",
                "arxiv_comment": "21 pages ,8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16073v1",
                "updated": "2025-04-22T17:52:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    52,
                    42,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:52:42Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    52,
                    42,
                    1,
                    112,
                    0
                ],
                "title": "Guiding VLM Agents with Process Rewards at Inference Time for GUI\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding VLM Agents with Process Rewards at Inference Time for GUI\n  Navigation"
                },
                "summary": "Recent advancements in visual language models (VLMs) have notably enhanced\ntheir capabilities in handling complex Graphical User Interface (GUI)\ninteraction tasks. Despite these improvements, current frameworks often\nstruggle to generate correct actions in challenging GUI environments.\nState-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source\nVLMs for GUI tasks requires significant resources. Additionally, existing\ntrajectory-level evaluation and refinement techniques frequently fall short due\nto delayed feedback and local optimization issues. To address these challenges,\nwe propose an approach that guides VLM agents with process supervision by a\nreward model during GUI navigation and control at inference time. This guidance\nallows the VLM agent to optimize actions at each inference step, thereby\nimproving performance in both static and dynamic environments. In particular,\nour method demonstrates significant performance gains in three GUI navigation\ntasks, achieving a 3.4% improvement in single step action accuracy for static\nenvironments, along with a around 33% increase in task success rate in one\ndynamic environment. With further integration of trajectory reflection and\nretry mechanisms, we also demonstrate even greater enhancement in task success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in visual language models (VLMs) have notably enhanced\ntheir capabilities in handling complex Graphical User Interface (GUI)\ninteraction tasks. Despite these improvements, current frameworks often\nstruggle to generate correct actions in challenging GUI environments.\nState-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source\nVLMs for GUI tasks requires significant resources. Additionally, existing\ntrajectory-level evaluation and refinement techniques frequently fall short due\nto delayed feedback and local optimization issues. To address these challenges,\nwe propose an approach that guides VLM agents with process supervision by a\nreward model during GUI navigation and control at inference time. This guidance\nallows the VLM agent to optimize actions at each inference step, thereby\nimproving performance in both static and dynamic environments. In particular,\nour method demonstrates significant performance gains in three GUI navigation\ntasks, achieving a 3.4% improvement in single step action accuracy for static\nenvironments, along with a around 33% increase in task success rate in one\ndynamic environment. With further integration of trajectory reflection and\nretry mechanisms, we also demonstrate even greater enhancement in task success."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Shiyun Xiong"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13065v2",
                "updated": "2025-04-22T17:51:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    51,
                    42,
                    1,
                    112,
                    0
                ],
                "published": "2025-02-18T17:08:59Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    59,
                    1,
                    49,
                    0
                ],
                "title": "Improving Algorithmic Efficiency using Cryptography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Algorithmic Efficiency using Cryptography"
                },
                "summary": "Cryptographic primitives have been used for various non-cryptographic\nobjectives, such as eliminating or reducing randomness and interaction. We show\nhow to use cryptography to improve the time complexity of solving computational\nproblems. Specifically, we show that under standard cryptographic assumptions,\nwe can design algorithms that are asymptotically faster than existing ones\nwhile maintaining correctness. As a concrete demonstration, we construct a\ndistribution of trapdoored matrices with the following properties: (a)\ncomputationally bounded adversaries cannot distinguish a random matrix from one\ndrawn from this distribution (under computational hardness assumptions), and\n(b) given a trapdoor, we can multiply such an $n \\times n$ matrix with any\nvector in near-linear (in $n$) time. We provide constructions both over finite\nfields and over the reals. This enables a broad speedup technique: any\nalgorithm relying on a random matrix -- such as those that use various notions\nof dimensionality reduction -- can replace it with a matrix from our\ndistribution, achieving computational speedups while preserving correctness.\nUsing these trapdoored matrices, we present the first uniform reduction from\nworst-case to approximate and average-case matrix multiplication with optimal\nparameters (improving on Hirahara--Shimizu STOC 2025, albeit under\ncomputational assumptions), the first worst-case to average-case reductions for\nmatrix inversion, solving a linear system, and computing a determinant, as well\nas a speedup of inference time in classification models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptographic primitives have been used for various non-cryptographic\nobjectives, such as eliminating or reducing randomness and interaction. We show\nhow to use cryptography to improve the time complexity of solving computational\nproblems. Specifically, we show that under standard cryptographic assumptions,\nwe can design algorithms that are asymptotically faster than existing ones\nwhile maintaining correctness. As a concrete demonstration, we construct a\ndistribution of trapdoored matrices with the following properties: (a)\ncomputationally bounded adversaries cannot distinguish a random matrix from one\ndrawn from this distribution (under computational hardness assumptions), and\n(b) given a trapdoor, we can multiply such an $n \\times n$ matrix with any\nvector in near-linear (in $n$) time. We provide constructions both over finite\nfields and over the reals. This enables a broad speedup technique: any\nalgorithm relying on a random matrix -- such as those that use various notions\nof dimensionality reduction -- can replace it with a matrix from our\ndistribution, achieving computational speedups while preserving correctness.\nUsing these trapdoored matrices, we present the first uniform reduction from\nworst-case to approximate and average-case matrix multiplication with optimal\nparameters (improving on Hirahara--Shimizu STOC 2025, albeit under\ncomputational assumptions), the first worst-case to average-case reductions for\nmatrix inversion, solving a linear system, and computing a determinant, as well\nas a speedup of inference time in classification models."
                },
                "authors": [
                    {
                        "name": "Vinod Vaikuntanathan"
                    },
                    {
                        "name": "Or Zamir"
                    }
                ],
                "author_detail": {
                    "name": "Or Zamir"
                },
                "author": "Or Zamir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16068v1",
                "updated": "2025-04-22T17:47:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    47,
                    1,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:47:01Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    47,
                    1,
                    1,
                    112,
                    0
                ],
                "title": "High-performance training and inference for deep equivariant interatomic\n  potentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-performance training and inference for deep equivariant interatomic\n  potentials"
                },
                "summary": "Machine learning interatomic potentials, particularly those based on deep\nequivariant neural networks, have demonstrated state-of-the-art accuracy and\ncomputational efficiency in atomistic modeling tasks like molecular dynamics\nand high-throughput screening. The size of datasets and demands of downstream\nworkflows are growing rapidly, making robust and scalable software essential.\nThis work presents a major overhaul of the NequIP framework focusing on\nmulti-node parallelism, computational performance, and extensibility. The\nredesigned framework supports distributed training on large datasets and\nremoves barriers preventing full utilization of the PyTorch 2.0 compiler at\ntrain time. We demonstrate this acceleration in a case study by training\nAllegro models on the SPICE 2 dataset of organic molecular systems. For\ninference, we introduce the first end-to-end infrastructure that uses the\nPyTorch Ahead-of-Time Inductor compiler for machine learning interatomic\npotentials. Additionally, we implement a custom kernel for the Allegro model's\nmost expensive operation, the tensor product. Together, these advancements\nspeed up molecular dynamics calculations on system sizes of practical relevance\nby up to a factor of 18.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning interatomic potentials, particularly those based on deep\nequivariant neural networks, have demonstrated state-of-the-art accuracy and\ncomputational efficiency in atomistic modeling tasks like molecular dynamics\nand high-throughput screening. The size of datasets and demands of downstream\nworkflows are growing rapidly, making robust and scalable software essential.\nThis work presents a major overhaul of the NequIP framework focusing on\nmulti-node parallelism, computational performance, and extensibility. The\nredesigned framework supports distributed training on large datasets and\nremoves barriers preventing full utilization of the PyTorch 2.0 compiler at\ntrain time. We demonstrate this acceleration in a case study by training\nAllegro models on the SPICE 2 dataset of organic molecular systems. For\ninference, we introduce the first end-to-end infrastructure that uses the\nPyTorch Ahead-of-Time Inductor compiler for machine learning interatomic\npotentials. Additionally, we implement a custom kernel for the Allegro model's\nmost expensive operation, the tensor product. Together, these advancements\nspeed up molecular dynamics calculations on system sizes of practical relevance\nby up to a factor of 18."
                },
                "authors": [
                    {
                        "name": "Chuin Wei Tan"
                    },
                    {
                        "name": "Marc L. Descoteaux"
                    },
                    {
                        "name": "Mit Kotak"
                    },
                    {
                        "name": "Gabriel de Miranda Nascimento"
                    },
                    {
                        "name": "Se√°n R. Kavanagh"
                    },
                    {
                        "name": "Laura Zichi"
                    },
                    {
                        "name": "Menghang Wang"
                    },
                    {
                        "name": "Aadit Saluja"
                    },
                    {
                        "name": "Yizhong R. Hu"
                    },
                    {
                        "name": "Tess Smidt"
                    },
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "William C. Witt"
                    },
                    {
                        "name": "Boris Kozinsky"
                    },
                    {
                        "name": "Albert Musaelian"
                    }
                ],
                "author_detail": {
                    "name": "Albert Musaelian"
                },
                "author": "Albert Musaelian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14354v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14354v3",
                "updated": "2025-04-22T17:44:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    44,
                    39,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-18T21:42:15Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    42,
                    15,
                    2,
                    353,
                    0
                ],
                "title": "State Space Models are Strong Text Rerankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Space Models are Strong Text Rerankers"
                },
                "summary": "Transformers dominate NLP and IR; but their inference inefficiencies and\nchallenges in extrapolating to longer contexts have sparked interest in\nalternative model architectures. Among these, state space models (SSMs) like\nMamba offer promising advantages, particularly $O(1)$ time complexity in\ninference. Despite their potential, SSMs' effectiveness at text reranking -- a\ntask requiring fine-grained query-document interaction and long-context\nunderstanding -- remains underexplored. This study benchmarks SSM-based\narchitectures (specifically, Mamba-1 and Mamba-2) against transformer-based\nmodels across various scales, architectures, and pre-training objectives,\nfocusing on performance and efficiency in text reranking tasks. We find that\n(1) Mamba architectures achieve competitive text ranking performance,\ncomparable to transformer-based models of similar size; (2) they are less\nefficient in training and inference compared to transformers with flash\nattention; and (3) Mamba-2 outperforms Mamba-1 in both performance and\nefficiency. These results underscore the potential of state space models as a\ntransformer alternative and highlight areas for improvement in future IR\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers dominate NLP and IR; but their inference inefficiencies and\nchallenges in extrapolating to longer contexts have sparked interest in\nalternative model architectures. Among these, state space models (SSMs) like\nMamba offer promising advantages, particularly $O(1)$ time complexity in\ninference. Despite their potential, SSMs' effectiveness at text reranking -- a\ntask requiring fine-grained query-document interaction and long-context\nunderstanding -- remains underexplored. This study benchmarks SSM-based\narchitectures (specifically, Mamba-1 and Mamba-2) against transformer-based\nmodels across various scales, architectures, and pre-training objectives,\nfocusing on performance and efficiency in text reranking tasks. We find that\n(1) Mamba architectures achieve competitive text ranking performance,\ncomparable to transformer-based models of similar size; (2) they are less\nefficient in training and inference compared to transformers with flash\nattention; and (3) Mamba-2 outperforms Mamba-1 in both performance and\nefficiency. These results underscore the potential of state space models as a\ntransformer alternative and highlight areas for improvement in future IR\napplications."
                },
                "authors": [
                    {
                        "name": "Zhichao Xu"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Ashim Gupta"
                    },
                    {
                        "name": "Vivek Srikumar"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Srikumar"
                },
                "author": "Vivek Srikumar",
                "arxiv_comment": "Accepted to RepL4NLP 2025. The first two authors contributed equally,\n  order decided randomly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14354v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14354v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16064v1",
                "updated": "2025-04-22T17:41:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    41,
                    42,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:41:42Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    41,
                    42,
                    1,
                    112,
                    0
                ],
                "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis"
                },
                "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling."
                },
                "authors": [
                    {
                        "name": "Theodoros Kouzelis"
                    },
                    {
                        "name": "Efstathios Karypidis"
                    },
                    {
                        "name": "Ioannis Kakogeorgiou"
                    },
                    {
                        "name": "Spyros Gidaris"
                    },
                    {
                        "name": "Nikos Komodakis"
                    }
                ],
                "author_detail": {
                    "name": "Nikos Komodakis"
                },
                "author": "Nikos Komodakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.15795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.15795v3",
                "updated": "2025-04-22T17:37:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    37,
                    42,
                    1,
                    112,
                    0
                ],
                "published": "2023-10-24T12:47:13Z",
                "published_parsed": [
                    2023,
                    10,
                    24,
                    12,
                    47,
                    13,
                    1,
                    297,
                    0
                ],
                "title": "Thermodynamics sheds light on the nature of dark matter galactic halos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermodynamics sheds light on the nature of dark matter galactic halos"
                },
                "summary": "In spherical symmetry, the gravitational potential is uniquely determined by\nthe rotational velocity profile. Numerous galaxies exhibit a universal velocity\nprofile from which a universal gravitational profile is inferred. When treating\ndark matter as either an ideal gas, a Fermi gas, or a Bose gas, only the latter\ncan produce a gravitational profile consistent with observations, along with a\ntemperature profile that decreases outward. This requires the mass of the boson\nto be below a certain threshold of $43\\mbox{ eV}/c^2$. Additionally, ensuring\nthat the speed of sound is less than the speed of light yields a lower bound on\nthe boson's mass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In spherical symmetry, the gravitational potential is uniquely determined by\nthe rotational velocity profile. Numerous galaxies exhibit a universal velocity\nprofile from which a universal gravitational profile is inferred. When treating\ndark matter as either an ideal gas, a Fermi gas, or a Bose gas, only the latter\ncan produce a gravitational profile consistent with observations, along with a\ntemperature profile that decreases outward. This requires the mass of the boson\nto be below a certain threshold of $43\\mbox{ eV}/c^2$. Additionally, ensuring\nthat the speed of sound is less than the speed of light yields a lower bound on\nthe boson's mass."
                },
                "authors": [
                    {
                        "name": "Andr√©s Ace√±a"
                    },
                    {
                        "name": "Juan Barranco"
                    },
                    {
                        "name": "Argelia Bernal"
                    },
                    {
                        "name": "Ericson L√≥pez"
                    }
                ],
                "author_detail": {
                    "name": "Ericson L√≥pez"
                },
                "author": "Ericson L√≥pez",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.15795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.15795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16057v1",
                "updated": "2025-04-22T17:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    33,
                    53,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:33:53Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    33,
                    53,
                    1,
                    112,
                    0
                ],
                "title": "Automated Static Vulnerability Detection via a Holistic Neuro-symbolic\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Static Vulnerability Detection via a Holistic Neuro-symbolic\n  Approach"
                },
                "summary": "Static vulnerability detection is still a challenging problem and demands\nexcessive human efforts, e.g., manual curation of good vulnerability patterns.\nNone of prior works, including classic program analysis or Large Language Model\n(LLM)-based approaches, have fully automated such vulnerability pattern\ngenerations with reasonable detection accuracy. In this paper, we design and\nimplement, MoCQ, a novel holistic neuro-symbolic framework that combines the\ncomplementary strengths of LLMs and classical static analysis to enable\nscalable vulnerability detection. The key insight is that MoCQ leverages an LLM\nto automatically extract vulnerability patterns and translate them into\ndetection queries, and then on static analysis to refine such queries in a\nfeedback loop and eventually execute them for analyzing large codebases and\nmining vulnerabilities. We evaluate MoCQ on seven types of vulnerabilities\nspanning two programming languages. We found MoCQ-generated queries uncovered\nat least 12 patterns that were missed by experts. On a ground truth dataset,\nMoCQ achieved comparable precision and recall compared to expert-crafted\nqueries. Moreover, MoCQ has identified seven previously unknown vulnerabilities\nin real-world applications, demonstrating its practical effectiveness. We have\nresponsibly disclosed them to the corresponding developers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static vulnerability detection is still a challenging problem and demands\nexcessive human efforts, e.g., manual curation of good vulnerability patterns.\nNone of prior works, including classic program analysis or Large Language Model\n(LLM)-based approaches, have fully automated such vulnerability pattern\ngenerations with reasonable detection accuracy. In this paper, we design and\nimplement, MoCQ, a novel holistic neuro-symbolic framework that combines the\ncomplementary strengths of LLMs and classical static analysis to enable\nscalable vulnerability detection. The key insight is that MoCQ leverages an LLM\nto automatically extract vulnerability patterns and translate them into\ndetection queries, and then on static analysis to refine such queries in a\nfeedback loop and eventually execute them for analyzing large codebases and\nmining vulnerabilities. We evaluate MoCQ on seven types of vulnerabilities\nspanning two programming languages. We found MoCQ-generated queries uncovered\nat least 12 patterns that were missed by experts. On a ground truth dataset,\nMoCQ achieved comparable precision and recall compared to expert-crafted\nqueries. Moreover, MoCQ has identified seven previously unknown vulnerabilities\nin real-world applications, demonstrating its practical effectiveness. We have\nresponsibly disclosed them to the corresponding developers."
                },
                "authors": [
                    {
                        "name": "Penghui Li"
                    },
                    {
                        "name": "Songchen Yao"
                    },
                    {
                        "name": "Josef Sarfati Korich"
                    },
                    {
                        "name": "Changhua Luo"
                    },
                    {
                        "name": "Jianjia Yu"
                    },
                    {
                        "name": "Yinzhi Cao"
                    },
                    {
                        "name": "Junfeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Yang"
                },
                "author": "Junfeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16056v1",
                "updated": "2025-04-22T17:32:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    32,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:32:48Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    32,
                    48,
                    1,
                    112,
                    0
                ],
                "title": "Honey, I Shrunk the Language Model: Impact of Knowledge Distillation\n  Methods on Performance and Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honey, I Shrunk the Language Model: Impact of Knowledge Distillation\n  Methods on Performance and Explainability"
                },
                "summary": "Artificial Intelligence (AI) has increasingly influenced modern society,\nrecently in particular through significant advancements in Large Language\nModels (LLMs). However, high computational and storage demands of LLMs still\nlimit their deployment in resource-constrained environments. Knowledge\ndistillation addresses this challenge by training a small student model from a\nlarger teacher model. Previous research has introduced several distillation\nmethods for both generating training data and for training the student model.\nDespite their relevance, the effects of state-of-the-art distillation methods\non model performance and explainability have not been thoroughly investigated\nand compared. In this work, we enlarge the set of available methods by applying\ncritique-revision prompting to distillation for data generation and by\nsynthesizing existing methods for training. For these methods, we provide a\nsystematic comparison based on the widely used Commonsense Question-Answering\n(CQA) dataset. While we measure performance via student model accuracy, we\nemploy a human-grounded study to evaluate explainability. We contribute new\ndistillation methods and their comparison in terms of both performance and\nexplainability. This should further advance the distillation of small language\nmodels and, thus, contribute to broader applicability and faster diffusion of\nLLM technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has increasingly influenced modern society,\nrecently in particular through significant advancements in Large Language\nModels (LLMs). However, high computational and storage demands of LLMs still\nlimit their deployment in resource-constrained environments. Knowledge\ndistillation addresses this challenge by training a small student model from a\nlarger teacher model. Previous research has introduced several distillation\nmethods for both generating training data and for training the student model.\nDespite their relevance, the effects of state-of-the-art distillation methods\non model performance and explainability have not been thoroughly investigated\nand compared. In this work, we enlarge the set of available methods by applying\ncritique-revision prompting to distillation for data generation and by\nsynthesizing existing methods for training. For these methods, we provide a\nsystematic comparison based on the widely used Commonsense Question-Answering\n(CQA) dataset. While we measure performance via student model accuracy, we\nemploy a human-grounded study to evaluate explainability. We contribute new\ndistillation methods and their comparison in terms of both performance and\nexplainability. This should further advance the distillation of small language\nmodels and, thus, contribute to broader applicability and faster diffusion of\nLLM technology."
                },
                "authors": [
                    {
                        "name": "Daniel Hendriks"
                    },
                    {
                        "name": "Philipp Spitzer"
                    },
                    {
                        "name": "Niklas K√ºhl"
                    },
                    {
                        "name": "Gerhard Satzger"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Satzger"
                },
                "author": "Gerhard Satzger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Ho√üfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16046v1",
                "updated": "2025-04-22T17:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    16,
                    53,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    16,
                    53,
                    1,
                    112,
                    0
                ],
                "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certified Mitigation of Worst-Case LLM Copyright Infringement"
                },
                "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Jiacan Yu"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16032v1",
                "updated": "2025-04-22T16:56:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    56,
                    59,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T16:56:59Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    56,
                    59,
                    1,
                    112,
                    0
                ],
                "title": "LLMs meet Federated Learning for Scalable and Secure IoT Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs meet Federated Learning for Scalable and Secure IoT Management"
                },
                "summary": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions."
                },
                "authors": [
                    {
                        "name": "Yazan Otoum"
                    },
                    {
                        "name": "Arghavan Asad"
                    },
                    {
                        "name": "Amiya Nayak"
                    }
                ],
                "author_detail": {
                    "name": "Amiya Nayak"
                },
                "author": "Amiya Nayak",
                "arxiv_comment": "This work has been submitted to the IEEE Global Communications\n  Conference (GLOBECOM) 2025 for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20507v2",
                "updated": "2025-04-22T16:55:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    55,
                    45,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-26T12:47:52Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    47,
                    52,
                    2,
                    85,
                    0
                ],
                "title": "Harmonia: A Multi-Agent Reinforcement Learning Approach to Data\n  Placement and Migration in Hybrid Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmonia: A Multi-Agent Reinforcement Learning Approach to Data\n  Placement and Migration in Hybrid Storage Systems"
                },
                "summary": "Hybrid storage systems (HSS) combine multiple storage devices with diverse\ncharacteristics to achieve high performance and capacity at low cost. The\nperformance of an HSS highly depends on the effectiveness of two key policies:\n(1) the data-placement policy, which determines the best-fit storage device for\nincoming data, and (2) the data-migration policy, which rearranges stored data\nacross the devices to sustain high HSS performance. Prior works focus on\nimproving only data placement or only data migration in HSS, which leads to\nrelatively low HSS performance. Unfortunately, no prior work tries to optimize\nboth policies together. Our goal is to design a holistic data-management\ntechnique that optimizes both data-placement and data-migration policies to\nfully exploit the potential of an HSS, and thus significantly improve system\nperformance. We demonstrate the need for multiple reinforcement learning (RL)\nagents to accomplish our goal. We propose Harmonia, a multi-agent RL-based\ndata-management technique that employs two lightweight autonomous RL agents, a\ndata-placement agent and a data-migration agent, which adapt their policies for\nthe current workload and HSS configuration, and coordinate with each other to\nimprove overall HSS performance. We evaluate Harmonia on a real HSS with up to\nfour heterogeneous and diverse storage devices. Our evaluation using 17\ndata-intensive workloads on performance-optimized (cost-optimized) HSS with two\nstorage devices shows that, on average, Harmonia outperforms the\nbest-performing prior approach by 49.5% (31.7%). On an HSS with three (four)\ndevices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%).\nHarmonia's performance benefits come with low latency (240ns for inference) and\nstorage overheads (206 KiB in DRAM for both RL agents together). We will\nopen-source Harmonia's implementation to aid future research on HSS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid storage systems (HSS) combine multiple storage devices with diverse\ncharacteristics to achieve high performance and capacity at low cost. The\nperformance of an HSS highly depends on the effectiveness of two key policies:\n(1) the data-placement policy, which determines the best-fit storage device for\nincoming data, and (2) the data-migration policy, which rearranges stored data\nacross the devices to sustain high HSS performance. Prior works focus on\nimproving only data placement or only data migration in HSS, which leads to\nrelatively low HSS performance. Unfortunately, no prior work tries to optimize\nboth policies together. Our goal is to design a holistic data-management\ntechnique that optimizes both data-placement and data-migration policies to\nfully exploit the potential of an HSS, and thus significantly improve system\nperformance. We demonstrate the need for multiple reinforcement learning (RL)\nagents to accomplish our goal. We propose Harmonia, a multi-agent RL-based\ndata-management technique that employs two lightweight autonomous RL agents, a\ndata-placement agent and a data-migration agent, which adapt their policies for\nthe current workload and HSS configuration, and coordinate with each other to\nimprove overall HSS performance. We evaluate Harmonia on a real HSS with up to\nfour heterogeneous and diverse storage devices. Our evaluation using 17\ndata-intensive workloads on performance-optimized (cost-optimized) HSS with two\nstorage devices shows that, on average, Harmonia outperforms the\nbest-performing prior approach by 49.5% (31.7%). On an HSS with three (four)\ndevices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%).\nHarmonia's performance benefits come with low latency (240ns for inference) and\nstorage overheads (206 KiB in DRAM for both RL agents together). We will\nopen-source Harmonia's implementation to aid future research on HSS."
                },
                "authors": [
                    {
                        "name": "Rakesh Nadig"
                    },
                    {
                        "name": "Vamanan Arulchelvan"
                    },
                    {
                        "name": "Rahul Bera"
                    },
                    {
                        "name": "Taha Shahroodi"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Andreas Kakolyris"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Jisung Park"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04791v3",
                "updated": "2025-04-22T16:55:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    55,
                    24,
                    1,
                    112,
                    0
                ],
                "published": "2025-01-08T19:14:22Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    19,
                    14,
                    22,
                    2,
                    8,
                    0
                ],
                "title": "Approximating non-Gaussian Bayesian partitions with normalising flows:\n  statistics, inference and application to cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximating non-Gaussian Bayesian partitions with normalising flows:\n  statistics, inference and application to cosmology"
                },
                "summary": "Subject of this paper is the simplification of Markov chain Monte Carlo\nsampling as used in Bayesian statistical inference by means of normalising\nflows, a machine learning method which is able to construct an invertible and\ndifferentiable transformation between Gaussian and non-Gaussian random\ndistributions. We use normalising flows to compute Bayesian partition functions\nfor non-Gaussian distributions and show how normalising flows can be employed\nin finding analytical expressions for posterior distributions beyond the\nGaussian limit. Flows offer advantages for the numerical evaluation of the\npartition function itself, as well as for cumulants and for the information\nentropy. We demonstrate how normalising flows in conjunction with Bayes\npartitions can be used in inference problems in cosmology and apply them to the\nposterior distribution for the matter density $\\Omega_m$ and a dark energy\nequation of state parameter $w_0$ on the basis of supernova data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subject of this paper is the simplification of Markov chain Monte Carlo\nsampling as used in Bayesian statistical inference by means of normalising\nflows, a machine learning method which is able to construct an invertible and\ndifferentiable transformation between Gaussian and non-Gaussian random\ndistributions. We use normalising flows to compute Bayesian partition functions\nfor non-Gaussian distributions and show how normalising flows can be employed\nin finding analytical expressions for posterior distributions beyond the\nGaussian limit. Flows offer advantages for the numerical evaluation of the\npartition function itself, as well as for cumulants and for the information\nentropy. We demonstrate how normalising flows in conjunction with Bayes\npartitions can be used in inference problems in cosmology and apply them to the\nposterior distribution for the matter density $\\Omega_m$ and a dark energy\nequation of state parameter $w_0$ on the basis of supernova data."
                },
                "authors": [
                    {
                        "name": "Tobias R√∂spel"
                    },
                    {
                        "name": "Adrian Schlosser"
                    },
                    {
                        "name": "Bj√∂rn Malte Sch√§fer"
                    }
                ],
                "author_detail": {
                    "name": "Bj√∂rn Malte Sch√§fer"
                },
                "author": "Bj√∂rn Malte Sch√§fer",
                "arxiv_doi": "10.33232/001c.137057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.33232/001c.137057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.04791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02584v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02584v4",
                "updated": "2025-04-22T16:53:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    53,
                    30,
                    1,
                    112,
                    0
                ],
                "published": "2024-05-30T20:48:10Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    20,
                    48,
                    10,
                    3,
                    151,
                    0
                ],
                "title": "A Scoping Review of Earth Observation and Machine Learning for Causal\n  Inference: Implications for the Geography of Poverty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scoping Review of Earth Observation and Machine Learning for Causal\n  Inference: Implications for the Geography of Poverty"
                },
                "summary": "Earth observation (EO) data such as satellite imagery can have far-reaching\nimpacts on our understanding of the geography of poverty, especially when\ncoupled with machine learning (ML) and computer vision. Early research used\ncomputer vision to predict living conditions in areas with limited data, but\nrecent studies increasingly focus on causal analysis. Despite this shift, the\nuse of EO-ML methods for causal inference lacks thorough documentation, and\nbest practices are still developing. Through a comprehensive scoping review, we\ncatalog the current literature on EO-ML methods in causal analysis. We\nsynthesize five principal approaches to incorporating EO data in causal\nworkflows: (1) outcome imputation for downstream causal analysis, (2) EO image\ndeconfounding, (3) EO-based treatment effect heterogeneity, (4) EO-based\ntransportability analysis, and (5) image-informed causal discovery. Building on\nthese findings, we provide a detailed protocol guiding researchers in\nintegrating EO data into causal analysis -- covering data requirements,\ncomputer vision model selection, and evaluation metrics. While our focus\ncenters on health and living conditions outcomes, our protocol is adaptable to\nother sustainable development domains utilizing EO data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earth observation (EO) data such as satellite imagery can have far-reaching\nimpacts on our understanding of the geography of poverty, especially when\ncoupled with machine learning (ML) and computer vision. Early research used\ncomputer vision to predict living conditions in areas with limited data, but\nrecent studies increasingly focus on causal analysis. Despite this shift, the\nuse of EO-ML methods for causal inference lacks thorough documentation, and\nbest practices are still developing. Through a comprehensive scoping review, we\ncatalog the current literature on EO-ML methods in causal analysis. We\nsynthesize five principal approaches to incorporating EO data in causal\nworkflows: (1) outcome imputation for downstream causal analysis, (2) EO image\ndeconfounding, (3) EO-based treatment effect heterogeneity, (4) EO-based\ntransportability analysis, and (5) image-informed causal discovery. Building on\nthese findings, we provide a detailed protocol guiding researchers in\nintegrating EO data into causal analysis -- covering data requirements,\ncomputer vision model selection, and evaluation metrics. While our focus\ncenters on health and living conditions outcomes, our protocol is adaptable to\nother sustainable development domains utilizing EO data."
                },
                "authors": [
                    {
                        "name": "Kazuki Sakamoto"
                    },
                    {
                        "name": "Connor T. Jerzak"
                    },
                    {
                        "name": "Adel Daoud"
                    }
                ],
                "author_detail": {
                    "name": "Adel Daoud"
                },
                "author": "Adel Daoud",
                "arxiv_comment": "To appear as: Sakamoto, Kazuki, Connor T. Jerzak, and Adel Daoud. \"A\n  Scoping Review of Earth Observation and Machine Learning for Causal\n  Inference: Implications for the Geography of Poverty.\" In Geography of\n  Poverty, edited by Ola Hall and Ibrahim Wahab. Edward Elgar Publishing\n  (Cheltenham, UK), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02584v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02584v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14287v2",
                "updated": "2025-04-22T16:52:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    52,
                    27,
                    1,
                    112,
                    0
                ],
                "published": "2024-08-26T14:16:22Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    16,
                    22,
                    0,
                    239,
                    0
                ],
                "title": "Cosmology and nuclear-physics implications of a subsolar\n  gravitational-wave event",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmology and nuclear-physics implications of a subsolar\n  gravitational-wave event"
                },
                "summary": "Detecting a compact subsolar object would have profound implications in\nphysics, the reach of which depends on the nature of the object. Here we\nexplore such consequences for a putative subsolar-mass gravitational wave event\ndetected by the LIGO-Virgo-KAGRA Collaboration. We forecast that the nature of\na subsolar binary (made of light neutron stars, primordial black holes, or more\nexotic compact objects) can be inferred with a great statistical confidence\nlevel already during the ongoing fourth observing run, based on the large tidal\ndeformability effects on the signal. The detection of a primordial black hole\nwould have implications for cosmology and dark matter scenarios, while the\nmeasurement of the tidal deformability of a subsolar neutron star could rule\nout or confirm the existence of strange stars made of quarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting a compact subsolar object would have profound implications in\nphysics, the reach of which depends on the nature of the object. Here we\nexplore such consequences for a putative subsolar-mass gravitational wave event\ndetected by the LIGO-Virgo-KAGRA Collaboration. We forecast that the nature of\na subsolar binary (made of light neutron stars, primordial black holes, or more\nexotic compact objects) can be inferred with a great statistical confidence\nlevel already during the ongoing fourth observing run, based on the large tidal\ndeformability effects on the signal. The detection of a primordial black hole\nwould have implications for cosmology and dark matter scenarios, while the\nmeasurement of the tidal deformability of a subsolar neutron star could rule\nout or confirm the existence of strange stars made of quarks."
                },
                "authors": [
                    {
                        "name": "Francesco Crescimbeni"
                    },
                    {
                        "name": "Gabriele Franciolini"
                    },
                    {
                        "name": "Paolo Pani"
                    },
                    {
                        "name": "Massimo Vaglio"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Vaglio"
                },
                "author": "Massimo Vaglio",
                "arxiv_doi": "10.1103/PhysRevD.111.083538",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.083538",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.14287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5+4 pages, 3 figures; v2: matching published version",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16030v1",
                "updated": "2025-04-22T16:52:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    52,
                    9,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T16:52:09Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    52,
                    9,
                    1,
                    112,
                    0
                ],
                "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale"
                },
                "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc."
                },
                "authors": [
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Ziyun Zeng"
                    },
                    {
                        "name": "Yiqi Lin"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "CVPR 2025. If any references are missing, please contact\n  joyachen@u.nus.edu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16027v1",
                "updated": "2025-04-22T16:44:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    44,
                    39,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T16:44:39Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    44,
                    39,
                    1,
                    112,
                    0
                ],
                "title": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs\n  DeepSeek-V3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs\n  DeepSeek-V3"
                },
                "summary": "Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection"
                },
                "authors": [
                    {
                        "name": "Ahmed R. Sadik"
                    },
                    {
                        "name": "Siddhata Govind"
                    }
                ],
                "author_detail": {
                    "name": "Siddhata Govind"
                },
                "author": "Siddhata Govind",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07959v2",
                "updated": "2025-04-22T16:33:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    33,
                    55,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-10T22:52:44Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    52,
                    44,
                    1,
                    345,
                    0
                ],
                "title": "Deep-Learning Control of Lower-Limb Exoskeletons via simplified\n  Therapist Input",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep-Learning Control of Lower-Limb Exoskeletons via simplified\n  Therapist Input"
                },
                "summary": "Partial-assistance exoskeletons hold significant potential for gait\nrehabilitation by promoting active participation during (re)learning of\nnormative walking patterns. Typically, the control of interaction torques in\npartial-assistance exoskeletons relies on a hierarchical control structure.\nThese approaches require extensive calibration due to the complexity of the\ncontroller and user-specific parameter tuning, especially for activities like\nstair or ramp navigation. To address the limitations of hierarchical control in\nexoskeletons, this work proposes a three-step, data-driven approach: (1) using\nrecent sensor data to probabilistically infer locomotion states (landing step\nlength, landing step height, walking velocity, step clearance, gait phase), (2)\nallowing therapists to modify these features via a user interface, and (3)\nusing the adjusted locomotion features to predict the desired joint posture and\nmodel stiffness in a spring-damper system based on prediction uncertainty. We\nevaluated the proposed approach with two healthy participants engaging in\ntreadmill walking and stair ascent and descent at varying speeds, with and\nwithout external modification of the gait features through a user interface.\nResults showed a variation in kinematics according to the gait characteristics\nand a negative interaction power suggesting exoskeleton assistance across the\ndifferent conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial-assistance exoskeletons hold significant potential for gait\nrehabilitation by promoting active participation during (re)learning of\nnormative walking patterns. Typically, the control of interaction torques in\npartial-assistance exoskeletons relies on a hierarchical control structure.\nThese approaches require extensive calibration due to the complexity of the\ncontroller and user-specific parameter tuning, especially for activities like\nstair or ramp navigation. To address the limitations of hierarchical control in\nexoskeletons, this work proposes a three-step, data-driven approach: (1) using\nrecent sensor data to probabilistically infer locomotion states (landing step\nlength, landing step height, walking velocity, step clearance, gait phase), (2)\nallowing therapists to modify these features via a user interface, and (3)\nusing the adjusted locomotion features to predict the desired joint posture and\nmodel stiffness in a spring-damper system based on prediction uncertainty. We\nevaluated the proposed approach with two healthy participants engaging in\ntreadmill walking and stair ascent and descent at varying speeds, with and\nwithout external modification of the gait features through a user interface.\nResults showed a variation in kinematics according to the gait characteristics\nand a negative interaction power suggesting exoskeleton assistance across the\ndifferent conditions."
                },
                "authors": [
                    {
                        "name": "Lorenzo Vianello"
                    },
                    {
                        "name": "Cl√©ment Lhoste"
                    },
                    {
                        "name": "Emek Barƒ±≈ü K√º√ß√ºktabak"
                    },
                    {
                        "name": "Matthew Short"
                    },
                    {
                        "name": "Levi Hargrove"
                    },
                    {
                        "name": "Jose L. Pons"
                    }
                ],
                "author_detail": {
                    "name": "Jose L. Pons"
                },
                "author": "Jose L. Pons",
                "arxiv_comment": "Accepted to the INTERNATIONAL CONSORTIUM FOR REHABILITATION ROBOTICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16015v1",
                "updated": "2025-04-22T16:26:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    26,
                    29,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T16:26:29Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    26,
                    29,
                    1,
                    112,
                    0
                ],
                "title": "From observed transitions to hidden paths in Markov networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From observed transitions to hidden paths in Markov networks"
                },
                "summary": "The number of observable degrees of freedom is typically limited in\nexperiments. Here, we consider discrete Markov networks in which an observer\nhas access to a few visible transitions and the waiting times between these\ntransitions. Focusing on the underlying structure of a discrete network, we\npresent methods to infer local and global properties of the network from\nobserved data. First, we derive bounds on the microscopic entropy production\nalong the hidden paths between two visible transitions, which complement extant\nbounds on mean entropy production and affinities of hidden cycles. Second, we\ndemonstrate how the operationally accessible data encodes information about the\ntopology of shortest hidden paths, which can be used to identify potential\nclusters of states or exclude their existence. Finally, we outline a systematic\nway to combine the inferred data, resulting in an algorithm that finds the\ncandidates for a minimal graph of the underlying network, i.e., a graph that is\npart of the original one and compatible with the observations. Our results\nhighlight the interplay between thermodynamic methods, waiting-time\ndistributions and topological aspects like network structure, which can be\nexpected to provide novel insights in other set-ups of coarse graining as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The number of observable degrees of freedom is typically limited in\nexperiments. Here, we consider discrete Markov networks in which an observer\nhas access to a few visible transitions and the waiting times between these\ntransitions. Focusing on the underlying structure of a discrete network, we\npresent methods to infer local and global properties of the network from\nobserved data. First, we derive bounds on the microscopic entropy production\nalong the hidden paths between two visible transitions, which complement extant\nbounds on mean entropy production and affinities of hidden cycles. Second, we\ndemonstrate how the operationally accessible data encodes information about the\ntopology of shortest hidden paths, which can be used to identify potential\nclusters of states or exclude their existence. Finally, we outline a systematic\nway to combine the inferred data, resulting in an algorithm that finds the\ncandidates for a minimal graph of the underlying network, i.e., a graph that is\npart of the original one and compatible with the observations. Our results\nhighlight the interplay between thermodynamic methods, waiting-time\ndistributions and topological aspects like network structure, which can be\nexpected to provide novel insights in other set-ups of coarse graining as well."
                },
                "authors": [
                    {
                        "name": "Alexander M. Maier"
                    },
                    {
                        "name": "Udo Seifert"
                    },
                    {
                        "name": "Jann van der Meer"
                    }
                ],
                "author_detail": {
                    "name": "Jann van der Meer"
                },
                "author": "Jann van der Meer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02355v4",
                "updated": "2025-04-22T16:15:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    15,
                    47,
                    1,
                    112,
                    0
                ],
                "published": "2024-10-03T10:06:27Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    6,
                    27,
                    3,
                    277,
                    0
                ],
                "title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models"
                },
                "summary": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.7%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.7%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit."
                },
                "authors": [
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yunshan Ma"
                    },
                    {
                        "name": "Shi Jie"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Tat-seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-seng Chua"
                },
                "author": "Tat-seng Chua",
                "arxiv_journal_ref": "13th International Conference on Learning Representations (ICLR\n  2025 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16005v2",
                "updated": "2025-04-23T09:59:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    59,
                    31,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T16:14:31Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    14,
                    31,
                    1,
                    112,
                    0
                ],
                "title": "CAPO: Cost-Aware Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Cost-Aware Prompt Optimization"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."
                },
                "authors": [
                    {
                        "name": "Tom Zehle"
                    },
                    {
                        "name": "Moritz Schlager"
                    },
                    {
                        "name": "Timo Hei√ü"
                    },
                    {
                        "name": "Matthias Feurer"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Feurer"
                },
                "author": "Matthias Feurer",
                "arxiv_comment": "Submitted to AutoML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16000v1",
                "updated": "2025-04-22T16:05:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    5,
                    26,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T16:05:26Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    5,
                    26,
                    1,
                    112,
                    0
                ],
                "title": "How Private is Your Attention? Bridging Privacy with In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Private is Your Attention? Bridging Privacy with In-Context Learning"
                },
                "summary": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings."
                },
                "authors": [
                    {
                        "name": "Soham Bonnerjee"
                    },
                    {
                        "name": "Zhen Wei"
                    },
                    {
                        "name": "Yeon"
                    },
                    {
                        "name": "Anna Asch"
                    },
                    {
                        "name": "Sagnik Nandy"
                    },
                    {
                        "name": "Promit Ghosal"
                    }
                ],
                "author_detail": {
                    "name": "Promit Ghosal"
                },
                "arxiv_affiliation": "Kingsley",
                "author": "Promit Ghosal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15995v1",
                "updated": "2025-04-22T16:00:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    0,
                    11,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T16:00:11Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    0,
                    11,
                    1,
                    112,
                    0
                ],
                "title": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical\n  Federated Learning"
                },
                "summary": "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL."
                },
                "authors": [
                    {
                        "name": "Sindhuja Madabushi"
                    },
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Haider Ali"
                    },
                    {
                        "name": "Jin-Hee Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jin-Hee Cho"
                },
                "author": "Jin-Hee Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15991v1",
                "updated": "2025-04-22T15:53:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    53,
                    59,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T15:53:59Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    53,
                    59,
                    1,
                    112,
                    0
                ],
                "title": "Efficient Adaptation of Deep Neural Networks for Semantic Segmentation\n  in Space Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Adaptation of Deep Neural Networks for Semantic Segmentation\n  in Space Applications"
                },
                "summary": "In recent years, the application of Deep Learning techniques has shown\nremarkable success in various computer vision tasks, paving the way for their\ndeployment in extraterrestrial exploration. Transfer learning has emerged as a\npowerful strategy for addressing the scarcity of labeled data in these novel\nenvironments. This paper represents one of the first efforts in evaluating the\nfeasibility of employing adapters toward efficient transfer learning for rock\nsegmentation in extraterrestrial landscapes, mainly focusing on lunar and\nmartian terrains. Our work suggests that the use of adapters, strategically\nintegrated into a pre-trained backbone model, can be successful in reducing\nboth bandwidth and memory requirements for the target extraterrestrial device.\nIn this study, we considered two memory-saving strategies: layer fusion (to\nreduce to zero the inference overhead) and an ``adapter ranking'' (to also\nreduce the transmission cost). Finally, we evaluate these results in terms of\ntask performance, memory, and computation on embedded devices, evidencing\ntrade-offs that open the road to more research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the application of Deep Learning techniques has shown\nremarkable success in various computer vision tasks, paving the way for their\ndeployment in extraterrestrial exploration. Transfer learning has emerged as a\npowerful strategy for addressing the scarcity of labeled data in these novel\nenvironments. This paper represents one of the first efforts in evaluating the\nfeasibility of employing adapters toward efficient transfer learning for rock\nsegmentation in extraterrestrial landscapes, mainly focusing on lunar and\nmartian terrains. Our work suggests that the use of adapters, strategically\nintegrated into a pre-trained backbone model, can be successful in reducing\nboth bandwidth and memory requirements for the target extraterrestrial device.\nIn this study, we considered two memory-saving strategies: layer fusion (to\nreduce to zero the inference overhead) and an ``adapter ranking'' (to also\nreduce the transmission cost). Finally, we evaluate these results in terms of\ntask performance, memory, and computation on embedded devices, evidencing\ntrade-offs that open the road to more research in the field."
                },
                "authors": [
                    {
                        "name": "Leonardo Olivi"
                    },
                    {
                        "name": "Edoardo Santero Mormile"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Tartaglione"
                },
                "author": "Enzo Tartaglione",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15989v1",
                "updated": "2025-04-22T15:51:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    51,
                    0,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T15:51:00Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    51,
                    0,
                    1,
                    112,
                    0
                ],
                "title": "Token-Aware Coding Flow: A Study with Nano Surge in Reasoning Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Aware Coding Flow: A Study with Nano Surge in Reasoning Model"
                },
                "summary": "With the widespread application of large-scale language models (LLMs) in\nsoftware engineering, the Chain of Thought (CoT) approach has emerged as a\ncrucial tool for driving automated code generation and optimization. However,\ndespite the significant success of CoT methods in generating high-quality code,\nthe issue of token inflation during the reasoning process remains a formidable\nchallenge to model performance and efficiency, particularly when dealing with\ncomplex code smells. Code smells not only affect the maintainability and\nscalability of code but also significantly increase the computational burden\nduring LLM inference, leading to excessive token consumption and, consequently,\nreduced reasoning efficiency. This paper introduces an innovative Token-Aware\nCoding Flow method, aimed at addressing the token inflation problem caused by\nsmelly code in the CoT process. Through experimentation, we validate the\nsynergistic effect of code refactoring and prompt engineering strategies,\ndemonstrating that after eliminating code smells, token consumption during\nmodel inference is significantly reduced. The experimental results show that\nrefactored code, while maintaining functional consistency, can reduce token\nconsumption by up to 50\\%. Additionally, by explicitly prompting the type of\ncode smells in the prompt and incorporating strategies such as context\nawareness and role constraints, we further optimize the reasoning process,\nachieving a 24.5\\% to 30\\% reduction in token consumption. These optimizations\nnot only significantly enhance the model's reasoning efficiency and improve\ncode generation quality but also provide new insights for addressing\nperformance bottlenecks in complex code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of large-scale language models (LLMs) in\nsoftware engineering, the Chain of Thought (CoT) approach has emerged as a\ncrucial tool for driving automated code generation and optimization. However,\ndespite the significant success of CoT methods in generating high-quality code,\nthe issue of token inflation during the reasoning process remains a formidable\nchallenge to model performance and efficiency, particularly when dealing with\ncomplex code smells. Code smells not only affect the maintainability and\nscalability of code but also significantly increase the computational burden\nduring LLM inference, leading to excessive token consumption and, consequently,\nreduced reasoning efficiency. This paper introduces an innovative Token-Aware\nCoding Flow method, aimed at addressing the token inflation problem caused by\nsmelly code in the CoT process. Through experimentation, we validate the\nsynergistic effect of code refactoring and prompt engineering strategies,\ndemonstrating that after eliminating code smells, token consumption during\nmodel inference is significantly reduced. The experimental results show that\nrefactored code, while maintaining functional consistency, can reduce token\nconsumption by up to 50\\%. Additionally, by explicitly prompting the type of\ncode smells in the prompt and incorporating strategies such as context\nawareness and role constraints, we further optimize the reasoning process,\nachieving a 24.5\\% to 30\\% reduction in token consumption. These optimizations\nnot only significantly enhance the model's reasoning efficiency and improve\ncode generation quality but also provide new insights for addressing\nperformance bottlenecks in complex code generation tasks."
                },
                "authors": [
                    {
                        "name": "Junwei Hu"
                    },
                    {
                        "name": "Weicheng Zheng"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Yihan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Liu"
                },
                "author": "Yihan Liu",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03621v2",
                "updated": "2025-04-22T15:45:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    45,
                    39,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-04T15:26:10Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    26,
                    10,
                    2,
                    339,
                    0
                ],
                "title": "Network-aided Efficient LLM Services With Denoising-inspired Prompt\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network-aided Efficient LLM Services With Denoising-inspired Prompt\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, leading to their increasing adoption in diverse services\ndelivered through wireless networks. There is a growing trend toward longer\nprompts to better leverage LLMs' capabilities and address difficult tasks.\nHowever, longer prompts not only increase data transmission costs but also\nrequire more computing resources and processing time, which impacts overall\nsystem efficiency and user experience. To address this challenge, we propose\nJoint Power and Prompt Optimization (JPPO), a framework that combines Small\nLanguage Model (SLM)-based prompt compression with wireless power allocation\noptimization. By deploying SLM at edge devices for prompt compression and\nemploying Deep Reinforcement Learning (DRL) for joint optimization of\ncompression ratio and transmission power, JPPO effectively balances service\nquality with resource efficiency. Furthermore, inspired by denoising diffusion\nmodels, we design a denoising-inspired prompt compression approach that\niteratively compresses prompts by gradually removing non-critical information,\nfurther enhancing the framework's performance. Experimental results with long\nprompt tokens demonstrate that our framework achieves high service fidelity\nwhile optimizing power usage in wireless LLM services, significantly reducing\nthe total service response time. With our DRL-based JPPO, the framework\nmaintains fidelity comparable to the no-compression baseline while still\nachieving a 17% service time reduction through adaptive compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, leading to their increasing adoption in diverse services\ndelivered through wireless networks. There is a growing trend toward longer\nprompts to better leverage LLMs' capabilities and address difficult tasks.\nHowever, longer prompts not only increase data transmission costs but also\nrequire more computing resources and processing time, which impacts overall\nsystem efficiency and user experience. To address this challenge, we propose\nJoint Power and Prompt Optimization (JPPO), a framework that combines Small\nLanguage Model (SLM)-based prompt compression with wireless power allocation\noptimization. By deploying SLM at edge devices for prompt compression and\nemploying Deep Reinforcement Learning (DRL) for joint optimization of\ncompression ratio and transmission power, JPPO effectively balances service\nquality with resource efficiency. Furthermore, inspired by denoising diffusion\nmodels, we design a denoising-inspired prompt compression approach that\niteratively compresses prompts by gradually removing non-critical information,\nfurther enhancing the framework's performance. Experimental results with long\nprompt tokens demonstrate that our framework achieves high service fidelity\nwhile optimizing power usage in wireless LLM services, significantly reducing\nthe total service response time. With our DRL-based JPPO, the framework\nmaintains fidelity comparable to the no-compression baseline while still\nachieving a 17% service time reduction through adaptive compression."
                },
                "authors": [
                    {
                        "name": "Feiran You"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "arxiv_comment": "This paper has been accepted by SCIENCE CHINA Information Sciences.\n  arXiv admin note: substantial text overlap with arXiv:2411.18010",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15986v1",
                "updated": "2025-04-22T15:42:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    42,
                    31,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T15:42:31Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    42,
                    31,
                    1,
                    112,
                    0
                ],
                "title": "Charting the Uncharted: The Landscape of Monero Peer-to-Peer Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the Uncharted: The Landscape of Monero Peer-to-Peer Network"
                },
                "summary": "The Monero blockchain enables anonymous transactions through advanced\ncryptography in its peer-to-peer network, which underpins decentralization,\nsecurity, and trustless interactions. However, privacy measures obscure peer\nconnections, complicating network analysis. This study proposes a method to\ninfer peer connections in Monero's latest protocol version, where timestamp\ndata is unavailable. We collect peerlist data from TCP flows, validate our\ninference algorithm, and map the network structure. Our results show high\naccuracy, improving with longer observation periods. This work is the first to\nreveal connectivity patterns in Monero's updated protocol, providing\nvisualizations and insights into its topology. Our findings enhance the\nunderstanding of Monero's P2P network, including the role of supernodes, and\nhighlight potential protocol and security improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Monero blockchain enables anonymous transactions through advanced\ncryptography in its peer-to-peer network, which underpins decentralization,\nsecurity, and trustless interactions. However, privacy measures obscure peer\nconnections, complicating network analysis. This study proposes a method to\ninfer peer connections in Monero's latest protocol version, where timestamp\ndata is unavailable. We collect peerlist data from TCP flows, validate our\ninference algorithm, and map the network structure. Our results show high\naccuracy, improving with longer observation periods. This work is the first to\nreveal connectivity patterns in Monero's updated protocol, providing\nvisualizations and insights into its topology. Our findings enhance the\nunderstanding of Monero's P2P network, including the role of supernodes, and\nhighlight potential protocol and security improvements."
                },
                "authors": [
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Matija Pi≈°korec"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Nicol√≤ Vallarano"
                    },
                    {
                        "name": "Claudio J. Tessone"
                    }
                ],
                "author_detail": {
                    "name": "Claudio J. Tessone"
                },
                "author": "Claudio J. Tessone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15975v1",
                "updated": "2025-04-22T15:23:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    23,
                    37,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T15:23:37Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    23,
                    37,
                    1,
                    112,
                    0
                ],
                "title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition"
                },
                "summary": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches."
                },
                "authors": [
                    {
                        "name": "Peter Fletcher"
                    }
                ],
                "author_detail": {
                    "name": "Peter Fletcher"
                },
                "author": "Peter Fletcher",
                "arxiv_comment": "64 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.2; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15965v1",
                "updated": "2025-04-22T15:05:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    5,
                    4,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T15:05:04Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    5,
                    4,
                    1,
                    112,
                    0
                ],
                "title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era\n  of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era\n  of LLMs"
                },
                "summary": "Memory is the process of encoding, storing, and retrieving information,\nallowing humans to retain experiences, knowledge, skills, and facts over time,\nand serving as the foundation for growth and effective interaction with the\nworld. It plays a crucial role in shaping our identity, making decisions,\nlearning from past experiences, building relationships, and adapting to\nchanges. In the era of large language models (LLMs), memory refers to the\nability of an AI system to retain, recall, and use information from past\ninteractions to improve future responses and interactions. Although previous\nresearch and reviews have provided detailed descriptions of memory mechanisms,\nthere is still a lack of a systematic review that summarizes and analyzes the\nrelationship between the memory of LLM-driven AI systems and human memory, as\nwell as how we can be inspired by human memory to construct more powerful\nmemory systems. To achieve this, in this paper, we propose a comprehensive\nsurvey on the memory of LLM-driven AI systems. In particular, we first conduct\na detailed analysis of the categories of human memory and relate them to the\nmemory of AI systems. Second, we systematically organize existing\nmemory-related work and propose a categorization method based on three\ndimensions (object, form, and time) and eight quadrants. Finally, we illustrate\nsome open problems regarding the memory of current AI systems and outline\npossible future directions for memory in the era of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is the process of encoding, storing, and retrieving information,\nallowing humans to retain experiences, knowledge, skills, and facts over time,\nand serving as the foundation for growth and effective interaction with the\nworld. It plays a crucial role in shaping our identity, making decisions,\nlearning from past experiences, building relationships, and adapting to\nchanges. In the era of large language models (LLMs), memory refers to the\nability of an AI system to retain, recall, and use information from past\ninteractions to improve future responses and interactions. Although previous\nresearch and reviews have provided detailed descriptions of memory mechanisms,\nthere is still a lack of a systematic review that summarizes and analyzes the\nrelationship between the memory of LLM-driven AI systems and human memory, as\nwell as how we can be inspired by human memory to construct more powerful\nmemory systems. To achieve this, in this paper, we propose a comprehensive\nsurvey on the memory of LLM-driven AI systems. In particular, we first conduct\na detailed analysis of the categories of human memory and relate them to the\nmemory of AI systems. Second, we systematically organize existing\nmemory-related work and propose a categorization method based on three\ndimensions (object, form, and time) and eight quadrants. Finally, we illustrate\nsome open problems regarding the memory of current AI systems and outline\npossible future directions for memory in the era of large language models."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "26 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03901v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03901v3",
                "updated": "2025-04-22T14:51:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    51,
                    17,
                    1,
                    112,
                    0
                ],
                "published": "2024-09-05T20:21:49Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    49,
                    3,
                    249,
                    0
                ],
                "title": "Onboard Satellite Image Classification for Earth Observation: A\n  Comparative Study of ViT Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Onboard Satellite Image Classification for Earth Observation: A\n  Comparative Study of ViT Models"
                },
                "summary": "This study focuses on identifying the most effective pre-trained model for\nland use classification in onboard satellite processing, emphasizing achieving\nhigh accuracy, computational efficiency, and robustness against noisy data\nconditions commonly encountered during satellite-based inference. Through\nextensive experimentation, we compare the performance of traditional CNN-based,\nResNet-based, and various pre-trained vision Transformer models. Our findings\ndemonstrate that pre-trained Vision Transformer (ViT) models, particularly\nMobileViTV2 and EfficientViT-M2, outperform models trained from scratch in\nterms of accuracy and efficiency. These models achieve high performance with\nreduced computational requirements and exhibit greater resilience during\ninference under noisy conditions. While MobileViTV2 has excelled on clean\nvalidation data, EfficientViT-M2 has proved more robust when handling noise,\nmaking it the most suitable model for onboard satellite EO tasks. Our\nexperimental results demonstrate that EfficientViT-M2 is the optimal choice for\nreliable and efficient RS-IC in satellite operations, achieving 98.76 % of\naccuracy, precision, and recall. Precisely, EfficientViT-M2 delivers the\nhighest performance across all metrics, excels in training efficiency (1,000s)\nand inference time (10s), and demonstrates greater robustness (overall\nrobustness score of 0.79). Consequently, EfficientViT-M2 consumes 63.93 % less\npower than MobileViTV2 (79.23 W) and 73.26 % less power than SwinTransformer\n(108.90 W). This highlights its significant advantage in energy efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study focuses on identifying the most effective pre-trained model for\nland use classification in onboard satellite processing, emphasizing achieving\nhigh accuracy, computational efficiency, and robustness against noisy data\nconditions commonly encountered during satellite-based inference. Through\nextensive experimentation, we compare the performance of traditional CNN-based,\nResNet-based, and various pre-trained vision Transformer models. Our findings\ndemonstrate that pre-trained Vision Transformer (ViT) models, particularly\nMobileViTV2 and EfficientViT-M2, outperform models trained from scratch in\nterms of accuracy and efficiency. These models achieve high performance with\nreduced computational requirements and exhibit greater resilience during\ninference under noisy conditions. While MobileViTV2 has excelled on clean\nvalidation data, EfficientViT-M2 has proved more robust when handling noise,\nmaking it the most suitable model for onboard satellite EO tasks. Our\nexperimental results demonstrate that EfficientViT-M2 is the optimal choice for\nreliable and efficient RS-IC in satellite operations, achieving 98.76 % of\naccuracy, precision, and recall. Precisely, EfficientViT-M2 delivers the\nhighest performance across all metrics, excels in training efficiency (1,000s)\nand inference time (10s), and demonstrates greater robustness (overall\nrobustness score of 0.79). Consequently, EfficientViT-M2 consumes 63.93 % less\npower than MobileViTV2 (79.23 W) and 73.26 % less power than SwinTransformer\n(108.90 W). This highlights its significant advantage in energy efficiency."
                },
                "authors": [
                    {
                        "name": "Thanh-Dung Le"
                    },
                    {
                        "name": "Vu Nguyen Ha"
                    },
                    {
                        "name": "Ti Ti Nguyen"
                    },
                    {
                        "name": "Geoffrey Eappen"
                    },
                    {
                        "name": "Prabhu Thiruvasagam"
                    },
                    {
                        "name": "Hong-fu Chou"
                    },
                    {
                        "name": "Duc-Dung Tran"
                    },
                    {
                        "name": "Hung Nguyen-Kha"
                    },
                    {
                        "name": "Luis M. Garces-Socarras"
                    },
                    {
                        "name": "Jorge L. Gonzalez-Rios"
                    },
                    {
                        "name": "Juan Carlos Merlano-Duncan"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    }
                ],
                "author_detail": {
                    "name": "Symeon Chatzinotas"
                },
                "author": "Symeon Chatzinotas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03901v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03901v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19503v2",
                "updated": "2025-04-22T14:41:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    41,
                    35,
                    1,
                    112,
                    0
                ],
                "published": "2024-10-25T12:10:49Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    10,
                    49,
                    4,
                    299,
                    0
                ],
                "title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWITCH: Studying with Teacher for Knowledge Distillation of Large\n  Language Models"
                },
                "summary": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) as training data being\nparticularly notable for reducing the mismatch between training and inference.\nHowever, SGOs often produce noisy and biased sequences, which can lead to\nmisguidance from the teacher model, especially in long sequences. To mitigate\nthese challenges, we propose SWITCH (Studying WIth TeaCHer for Knowledge\nDistillation), a novel approach that strategically incorporates the teacher\nmodel during the student's sequence generation. SWITCH identifies discrepancies\nbetween the token probabilities of the teacher and student models, allowing the\nteacher to intervene selectively, particularly in long sequences that are more\nprone to teacher misguidance. Extensive experimental results across three model\nfamilies and five instruction-following datasets show that SWITCH surpasses\ntraditional KD methods, particularly excelling in the generation of long\nsequential data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) as training data being\nparticularly notable for reducing the mismatch between training and inference.\nHowever, SGOs often produce noisy and biased sequences, which can lead to\nmisguidance from the teacher model, especially in long sequences. To mitigate\nthese challenges, we propose SWITCH (Studying WIth TeaCHer for Knowledge\nDistillation), a novel approach that strategically incorporates the teacher\nmodel during the student's sequence generation. SWITCH identifies discrepancies\nbetween the token probabilities of the teacher and student models, allowing the\nteacher to intervene selectively, particularly in long sequences that are more\nprone to teacher misguidance. Extensive experimental results across three model\nfamilies and five instruction-following datasets show that SWITCH surpasses\ntraditional KD methods, particularly excelling in the generation of long\nsequential data."
                },
                "authors": [
                    {
                        "name": "Jahyun Koo"
                    },
                    {
                        "name": "Yerin Hwang"
                    },
                    {
                        "name": "Yongil Kim"
                    },
                    {
                        "name": "Taegwan Kang"
                    },
                    {
                        "name": "Hyunkyung Bae"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "arxiv_comment": "NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15941v1",
                "updated": "2025-04-22T14:35:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    35,
                    16,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:35:16Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    35,
                    16,
                    1,
                    112,
                    0
                ],
                "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in\n  Machine Translation by Overcoming Gender Binarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in\n  Machine Translation by Overcoming Gender Binarity"
                },
                "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub."
                },
                "authors": [
                    {
                        "name": "Fanny Jourdan"
                    },
                    {
                        "name": "Yannick Chevalier"
                    },
                    {
                        "name": "C√©cile Favre"
                    }
                ],
                "author_detail": {
                    "name": "C√©cile Favre"
                },
                "author": "C√©cile Favre",
                "arxiv_comment": "FAccT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13221v3",
                "updated": "2025-04-22T14:32:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    32,
                    59,
                    1,
                    112,
                    0
                ],
                "published": "2024-09-20T05:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    5,
                    15,
                    38,
                    4,
                    264,
                    0
                ],
                "title": "Optimizing RLHF Training for Large Language Models with Stage Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing RLHF Training for Large Language Models with Stage Fusion"
                },
                "summary": "We present RLHFuse, an efficient training system with stage fusion for\nReinforcement Learning from Human Feedback (RLHF). Due to the intrinsic nature\nof RLHF training, i.e., the data skewness in the generation stage and the\npipeline bubbles in the training stage, existing RLHF systems suffer from low\nGPU utilization. RLHFuse breaks the traditional view of RLHF workflow as a\ncomposition of individual tasks, splitting each task into finer-grained\nsubtasks, and performing stage fusion to improve GPU utilization. RLHFuse\ncontains two key ideas. First, for generation and inference tasks, RLHFuse\nsplits them into sample-level subtasks, enabling efficient inter-stage fusion\nto overlap the execution of generation and inference stages, thus mitigating\nthe original generation bottleneck dominated by long-tailed samples. Second,\nfor training tasks, RLHFuse breaks them into subtasks of micro-batches and\nperforms intra-stage fusion to concurrently execute these subtasks in the\ntraining stage with a fused pipeline schedule, effectively mitigating the\npipeline bubbles. The experiments show that RLHFuse increases the training\nthroughput by up to $3.7\\times$, compared to existing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RLHFuse, an efficient training system with stage fusion for\nReinforcement Learning from Human Feedback (RLHF). Due to the intrinsic nature\nof RLHF training, i.e., the data skewness in the generation stage and the\npipeline bubbles in the training stage, existing RLHF systems suffer from low\nGPU utilization. RLHFuse breaks the traditional view of RLHF workflow as a\ncomposition of individual tasks, splitting each task into finer-grained\nsubtasks, and performing stage fusion to improve GPU utilization. RLHFuse\ncontains two key ideas. First, for generation and inference tasks, RLHFuse\nsplits them into sample-level subtasks, enabling efficient inter-stage fusion\nto overlap the execution of generation and inference stages, thus mitigating\nthe original generation bottleneck dominated by long-tailed samples. Second,\nfor training tasks, RLHFuse breaks them into subtasks of micro-batches and\nperforms intra-stage fusion to concurrently execute these subtasks in the\ntraining stage with a fused pipeline schedule, effectively mitigating the\npipeline bubbles. The experiments show that RLHFuse increases the training\nthroughput by up to $3.7\\times$, compared to existing systems."
                },
                "authors": [
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15938v1",
                "updated": "2025-04-22T14:28:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    28,
                    30,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:28:30Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    28,
                    30,
                    1,
                    112,
                    0
                ],
                "title": "Differentiable graph neural network simulator for forward and inverse\n  modeling of multi-layered slope system with multiple material properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable graph neural network simulator for forward and inverse\n  modeling of multi-layered slope system with multiple material properties"
                },
                "summary": "Graph neural network simulators (GNS) have emerged as a computationally\nefficient tool for simulating granular flows. Previous efforts have been\nlimited to simplified geometries and material characterizations, typically\nconsidering only friction angle, which does not reflect the complexity of\nrealistic geotechnical systems such as slopes encountered in engineering\npractice. This study introduces a differentiable GNS framework designed for\nmulti-layered slope systems comprising both forward and inverse modeling\ncomponents. The forward component relies on a fine-tuned GNS that incorporates\nboth friction angle and cohesion. Its performance is demonstrated through\ncolumn collapse and multi-layered slope runout simulations, where GNS\nreplicates multi-material flow dynamics while achieving up to 145x\ncomputational speedup over the Material Point Method (MPM). The inverse\nmodeling component leverages the trained GNS, reverse-mode automatic\ndifferentiation, and L-BFGS-B optimization to infer material properties from a\ntarget runout geometry. Its performance is demonstrated by back-calculating the\nmaterial strengths that led to failure-induced runout in a dam system composed\nof multiple materials. Results are obtained within minutes and show good\nagreement with the target strength values. The framework introduced in this\nstudy provides an efficient approach for forward runout assessments and inverse\nstrength back-calculation in realistic slope systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network simulators (GNS) have emerged as a computationally\nefficient tool for simulating granular flows. Previous efforts have been\nlimited to simplified geometries and material characterizations, typically\nconsidering only friction angle, which does not reflect the complexity of\nrealistic geotechnical systems such as slopes encountered in engineering\npractice. This study introduces a differentiable GNS framework designed for\nmulti-layered slope systems comprising both forward and inverse modeling\ncomponents. The forward component relies on a fine-tuned GNS that incorporates\nboth friction angle and cohesion. Its performance is demonstrated through\ncolumn collapse and multi-layered slope runout simulations, where GNS\nreplicates multi-material flow dynamics while achieving up to 145x\ncomputational speedup over the Material Point Method (MPM). The inverse\nmodeling component leverages the trained GNS, reverse-mode automatic\ndifferentiation, and L-BFGS-B optimization to infer material properties from a\ntarget runout geometry. Its performance is demonstrated by back-calculating the\nmaterial strengths that led to failure-induced runout in a dam system composed\nof multiple materials. Results are obtained within minutes and show good\nagreement with the target strength values. The framework introduced in this\nstudy provides an efficient approach for forward runout assessments and inverse\nstrength back-calculation in realistic slope systems."
                },
                "authors": [
                    {
                        "name": "Yongjin Choi"
                    },
                    {
                        "name": "Jorge Macedo"
                    },
                    {
                        "name": "Chenying Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenying Liu"
                },
                "author": "Chenying Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15933v1",
                "updated": "2025-04-22T14:21:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    21,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:21:34Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    21,
                    34,
                    1,
                    112,
                    0
                ],
                "title": "Low-Rank Adaptation of Neural Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation of Neural Fields"
                },
                "summary": "Processing visual data often involves small adjustments or sequences of\nchanges, such as in image filtering, surface smoothing, and video storage.\nWhile established graphics techniques like normal mapping and video compression\nexploit redundancy to encode such small changes efficiently, the problem of\nencoding small changes to neural fields (NF) -- neural network\nparameterizations of visual or physical functions -- has received less\nattention.\n  We propose a parameter-efficient strategy for updating neural fields using\nlow-rank adaptations (LoRA). LoRA, a method from the parameter-efficient\nfine-tuning LLM community, encodes small updates to pre-trained models with\nminimal computational overhead. We adapt LoRA to instance-specific neural\nfields, avoiding the need for large pre-trained models yielding a pipeline\nsuitable for low-compute hardware.\n  We validate our approach with experiments in image filtering, video\ncompression, and geometry editing, demonstrating its effectiveness and\nversatility for representing neural field updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing visual data often involves small adjustments or sequences of\nchanges, such as in image filtering, surface smoothing, and video storage.\nWhile established graphics techniques like normal mapping and video compression\nexploit redundancy to encode such small changes efficiently, the problem of\nencoding small changes to neural fields (NF) -- neural network\nparameterizations of visual or physical functions -- has received less\nattention.\n  We propose a parameter-efficient strategy for updating neural fields using\nlow-rank adaptations (LoRA). LoRA, a method from the parameter-efficient\nfine-tuning LLM community, encodes small updates to pre-trained models with\nminimal computational overhead. We adapt LoRA to instance-specific neural\nfields, avoiding the need for large pre-trained models yielding a pipeline\nsuitable for low-compute hardware.\n  We validate our approach with experiments in image filtering, video\ncompression, and geometry editing, demonstrating its effectiveness and\nversatility for representing neural field updates."
                },
                "authors": [
                    {
                        "name": "Anh Truong"
                    },
                    {
                        "name": "Ahmed H. Mahmoud"
                    },
                    {
                        "name": "Mina Konakoviƒá Lukoviƒá"
                    },
                    {
                        "name": "Justin Solomon"
                    }
                ],
                "author_detail": {
                    "name": "Justin Solomon"
                },
                "author": "Justin Solomon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15930v1",
                "updated": "2025-04-22T14:19:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    19,
                    6,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:19:06Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    19,
                    6,
                    1,
                    112,
                    0
                ],
                "title": "StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with\n  Disaggregated Stream Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with\n  Disaggregated Stream Generation"
                },
                "summary": "Reinforcement learning (RL) has become the core post-training technique for\nlarge language models (LLMs). RL for LLMs involves two stages: generation and\ntraining. The LLM first generates samples online, which are then used to derive\nrewards for training. The conventional view holds that the colocated\narchitecture, where the two stages share resources via temporal multiplexing,\noutperforms the disaggregated architecture, in which dedicated resources are\nassigned to each stage. However, in real-world deployments, we observe that the\ncolocated architecture suffers from resource coupling, where the two stages are\nconstrained to use the same resources. This coupling compromises the\nscalability and cost-efficiency of colocated RL in large-scale training. In\ncontrast, the disaggregated architecture allows for flexible resource\nallocation, supports heterogeneous training setups, and facilitates\ncross-datacenter deployment.\n  StreamRL is designed with disaggregation from first principles and fully\nunlocks its potential by addressing two types of performance bottlenecks in\nexisting disaggregated RL frameworks: pipeline bubbles, caused by stage\ndependencies, and skewness bubbles, resulting from long-tail output length\ndistributions. To address pipeline bubbles, StreamRL breaks the traditional\nstage boundary in synchronous RL algorithms through stream generation and\nachieves full overlapping in asynchronous RL. To address skewness bubbles,\nStreamRL employs an output-length ranker model to identify long-tail samples\nand reduces generation time via skewness-aware dispatching and scheduling.\nExperiments show that StreamRL improves throughput by up to 2.66x compared to\nexisting state-of-the-art systems, and improves cost-effectiveness by up to\n1.33x in a heterogeneous, cross-datacenter setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become the core post-training technique for\nlarge language models (LLMs). RL for LLMs involves two stages: generation and\ntraining. The LLM first generates samples online, which are then used to derive\nrewards for training. The conventional view holds that the colocated\narchitecture, where the two stages share resources via temporal multiplexing,\noutperforms the disaggregated architecture, in which dedicated resources are\nassigned to each stage. However, in real-world deployments, we observe that the\ncolocated architecture suffers from resource coupling, where the two stages are\nconstrained to use the same resources. This coupling compromises the\nscalability and cost-efficiency of colocated RL in large-scale training. In\ncontrast, the disaggregated architecture allows for flexible resource\nallocation, supports heterogeneous training setups, and facilitates\ncross-datacenter deployment.\n  StreamRL is designed with disaggregation from first principles and fully\nunlocks its potential by addressing two types of performance bottlenecks in\nexisting disaggregated RL frameworks: pipeline bubbles, caused by stage\ndependencies, and skewness bubbles, resulting from long-tail output length\ndistributions. To address pipeline bubbles, StreamRL breaks the traditional\nstage boundary in synchronous RL algorithms through stream generation and\nachieves full overlapping in asynchronous RL. To address skewness bubbles,\nStreamRL employs an output-length ranker model to identify long-tail samples\nand reduces generation time via skewness-aware dispatching and scheduling.\nExperiments show that StreamRL improves throughput by up to 2.66x compared to\nexisting state-of-the-art systems, and improves cost-effectiveness by up to\n1.33x in a heterogeneous, cross-datacenter setting."
                },
                "authors": [
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Chao Jin"
                    },
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21131v3",
                "updated": "2025-04-22T14:15:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    15,
                    38,
                    1,
                    112,
                    0
                ],
                "published": "2024-10-28T15:33:37Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    33,
                    37,
                    0,
                    302,
                    0
                ],
                "title": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments"
                },
                "summary": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks."
                },
                "authors": [
                    {
                        "name": "Marharyta Domnich"
                    },
                    {
                        "name": "Julius V√§lja"
                    },
                    {
                        "name": "Rasmus Moorits Veski"
                    },
                    {
                        "name": "Giacomo Magnifico"
                    },
                    {
                        "name": "Kadi Tulver"
                    },
                    {
                        "name": "Eduard Barbu"
                    },
                    {
                        "name": "Raul Vicente"
                    }
                ],
                "author_detail": {
                    "name": "Raul Vicente"
                },
                "author": "Raul Vicente",
                "arxiv_doi": "10.1609/aaai.v39i15.33791",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1609/aaai.v39i15.33791",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.21131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper extends the AAAI-2025 version by including the Appendix",
                "arxiv_journal_ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  39(15), 16308-16316, 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09597v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09597v4",
                "updated": "2025-04-22T14:11:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    11,
                    33,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-13T14:31:52Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    31,
                    52,
                    6,
                    103,
                    0
                ],
                "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions."
                },
                "authors": [
                    {
                        "name": "Zhixuan Pan"
                    },
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09597v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09597v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15923v1",
                "updated": "2025-04-22T14:07:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    7,
                    5,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:07:05Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    7,
                    5,
                    1,
                    112,
                    0
                ],
                "title": "Bayesian sample size calculations for external validation studies of\n  risk prediction models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian sample size calculations for external validation studies of\n  risk prediction models"
                },
                "summary": "Summary: Contemporary sample size calculations for external validation of\nrisk prediction models require users to specify fixed values of assumed model\nperformance metrics alongside target precision levels (e.g., 95% CI widths).\nHowever, due to the finite samples of previous studies, our knowledge of true\nmodel performance in the target population is uncertain, and so choosing fixed\nvalues represents an incomplete picture. As well, for net benefit (NB) as a\nmeasure of clinical utility, the relevance of conventional precision-based\ninference is doubtful. In this work, we propose a general Bayesian algorithm\nfor constructing the joint distribution of predicted risks and response values\nbased on summary statistics of model performance in previous studies. For\nstatistical metrics of performance, we propose sample size determination rules\nthat either target desired expected precision, or a desired assurance\nprobability that the precision criteria will be satisfied. For NB, we propose\nrules based on optimality assurance (the probability that the planned study\ncorrectly identifies the most beneficial strategy) and the Expected Value of\nSample Information (EVSI), the expected gain in NB from the planned validation\nstudy. We showcase these developments in a case study on the validation of a\nrisk prediction model for deterioration of hospitalized COVID-19 patients.\nCompared to the conventional sample size calculation methods, a Bayesian\napproach requires explicit quantification of uncertainty around model\nperformance, but thereby enables various sample size rules based on expected\nprecision, assurance probabilities, and value of information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summary: Contemporary sample size calculations for external validation of\nrisk prediction models require users to specify fixed values of assumed model\nperformance metrics alongside target precision levels (e.g., 95% CI widths).\nHowever, due to the finite samples of previous studies, our knowledge of true\nmodel performance in the target population is uncertain, and so choosing fixed\nvalues represents an incomplete picture. As well, for net benefit (NB) as a\nmeasure of clinical utility, the relevance of conventional precision-based\ninference is doubtful. In this work, we propose a general Bayesian algorithm\nfor constructing the joint distribution of predicted risks and response values\nbased on summary statistics of model performance in previous studies. For\nstatistical metrics of performance, we propose sample size determination rules\nthat either target desired expected precision, or a desired assurance\nprobability that the precision criteria will be satisfied. For NB, we propose\nrules based on optimality assurance (the probability that the planned study\ncorrectly identifies the most beneficial strategy) and the Expected Value of\nSample Information (EVSI), the expected gain in NB from the planned validation\nstudy. We showcase these developments in a case study on the validation of a\nrisk prediction model for deterioration of hospitalized COVID-19 patients.\nCompared to the conventional sample size calculation methods, a Bayesian\napproach requires explicit quantification of uncertainty around model\nperformance, but thereby enables various sample size rules based on expected\nprecision, assurance probabilities, and value of information."
                },
                "authors": [
                    {
                        "name": "Mohsen Sadatsafavi"
                    },
                    {
                        "name": "Paul Gustafson"
                    },
                    {
                        "name": "Solmaz Setayeshgar"
                    },
                    {
                        "name": "Laure Wynants"
                    },
                    {
                        "name": "Richard Riley"
                    }
                ],
                "author_detail": {
                    "name": "Richard Riley"
                },
                "author": "Richard Riley",
                "arxiv_comment": "21 pages, 4 tables, 4 figures, 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15922v2",
                "updated": "2025-04-23T07:24:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    24,
                    40,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T14:06:02Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    6,
                    2,
                    1,
                    112,
                    0
                ],
                "title": "Language Models to Support Multi-Label Classification of Industrial Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models to Support Multi-Label Classification of Industrial Data"
                },
                "summary": "Multi-label requirements classification is a challenging task, especially\nwhen dealing with numerous classes at varying levels of abstraction. The\ndifficulties increases when a limited number of requirements is available to\ntrain a supervised classifier. Zero-shot learning (ZSL) does not require\ntraining data and can potentially address this problem. This paper investigates\nthe performance of zero-shot classifiers (ZSCs) on a multi-label industrial\ndataset. We focuse on classifying requirements according to a taxonomy designed\nto support requirements tracing. We compare multiple variants of ZSCs using\ndifferent embeddings, including 9 language models (LMs) with a reduced number\nof parameters (up to 3B), e.g., BERT, and 5 large LMs (LLMs) with a large\nnumber of parameters (up to 70B), e.g., Llama. Our ground truth includes 377\nrequirements and 1968 labels from 6 output spaces. For the evaluation, we adopt\ntraditional metrics, i.e., precision, recall, F1, and $F_\\beta$, as well as a\nnovel label distance metric Dn. This aims to better capture the\nclassification's hierarchical nature and provides a more nuanced evaluation of\nhow far the results are from the ground truth. 1) The top-performing model on 5\nout of 6 output spaces is T5-xl, with maximum $F_\\beta$ = 0.78 and Dn = 0.04,\nwhile BERT base outperformed the other models in one case, with maximum\n$F_\\beta$ = 0.83 and Dn = 0.04. 2) LMs with smaller parameter size produce the\nbest classification results compared to LLMs. Thus, addressing the problem in\npractice is feasible as limited computing power is needed. 3) The model\narchitecture (autoencoding, autoregression, and sentence-to-sentence)\nsignificantly affects the classifier's performance. We conclude that using ZSL\nfor multi-label requirements classification offers promising results. We also\npresent a novel metric that can be used to select the top-performing model for\nthis problem",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-label requirements classification is a challenging task, especially\nwhen dealing with numerous classes at varying levels of abstraction. The\ndifficulties increases when a limited number of requirements is available to\ntrain a supervised classifier. Zero-shot learning (ZSL) does not require\ntraining data and can potentially address this problem. This paper investigates\nthe performance of zero-shot classifiers (ZSCs) on a multi-label industrial\ndataset. We focuse on classifying requirements according to a taxonomy designed\nto support requirements tracing. We compare multiple variants of ZSCs using\ndifferent embeddings, including 9 language models (LMs) with a reduced number\nof parameters (up to 3B), e.g., BERT, and 5 large LMs (LLMs) with a large\nnumber of parameters (up to 70B), e.g., Llama. Our ground truth includes 377\nrequirements and 1968 labels from 6 output spaces. For the evaluation, we adopt\ntraditional metrics, i.e., precision, recall, F1, and $F_\\beta$, as well as a\nnovel label distance metric Dn. This aims to better capture the\nclassification's hierarchical nature and provides a more nuanced evaluation of\nhow far the results are from the ground truth. 1) The top-performing model on 5\nout of 6 output spaces is T5-xl, with maximum $F_\\beta$ = 0.78 and Dn = 0.04,\nwhile BERT base outperformed the other models in one case, with maximum\n$F_\\beta$ = 0.83 and Dn = 0.04. 2) LMs with smaller parameter size produce the\nbest classification results compared to LLMs. Thus, addressing the problem in\npractice is feasible as limited computing power is needed. 3) The model\narchitecture (autoencoding, autoregression, and sentence-to-sentence)\nsignificantly affects the classifier's performance. We conclude that using ZSL\nfor multi-label requirements classification offers promising results. We also\npresent a novel metric that can be used to select the top-performing model for\nthis problem"
                },
                "authors": [
                    {
                        "name": "Waleed Abdeen"
                    },
                    {
                        "name": "Michael Unterkalmsteiner"
                    },
                    {
                        "name": "Krzysztof Wnuk"
                    },
                    {
                        "name": "Alessio Ferrari"
                    },
                    {
                        "name": "Panagiota Chatzipetrou"
                    }
                ],
                "author_detail": {
                    "name": "Panagiota Chatzipetrou"
                },
                "author": "Panagiota Chatzipetrou",
                "arxiv_comment": "Accepted at SANER Conference 2025. Awaiting publication by IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15921v1",
                "updated": "2025-04-22T14:06:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    6,
                    1,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:06:01Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    6,
                    1,
                    1,
                    112,
                    0
                ],
                "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting"
                },
                "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication."
                },
                "authors": [
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Dimitrios Korkinof"
                    },
                    {
                        "name": "Shaogang Gong"
                    },
                    {
                        "name": "Mariano Beguerisse-Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Mariano Beguerisse-Diaz"
                },
                "author": "Mariano Beguerisse-Diaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15920v1",
                "updated": "2025-04-22T14:05:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    5,
                    11,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:05:11Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    5,
                    11,
                    1,
                    112,
                    0
                ],
                "title": "ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order\n  Neighboring Feature Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order\n  Neighboring Feature Fusion"
                },
                "summary": "Graph Neural Networks (GNNs) have demonstrated strong performance across\nvarious graph-based tasks by effectively capturing relational information\nbetween nodes. These models rely on iterative message passing to propagate node\nfeatures, enabling nodes to aggregate information from their neighbors. Recent\nresearch has significantly improved the message-passing mechanism, enhancing\nGNN scalability on large-scale graphs. However, GNNs still face two main\nchallenges: over-smoothing, where excessive message passing results in\nindistinguishable node representations, especially in deep networks\nincorporating high-order neighbors; and scalability issues, as traditional\narchitectures suffer from high model complexity and increased inference time\ndue to redundant information aggregation. This paper proposes a novel framework\nfor large-scale graphs named ScaleGNN that simultaneously addresses both\nchallenges by adaptively fusing multi-level graph features. We first construct\nneighbor matrices for each order, learning their relative information through\ntrainable weights through an adaptive high-order feature fusion module. This\nallows the model to selectively emphasize informative high-order neighbors\nwhile reducing unnecessary computational costs. Additionally, we introduce a\nHigh-order redundant feature masking mechanism based on a Local Contribution\nScore (LCS), which enables the model to retain only the most relevant neighbors\nat each order, preventing redundant information propagation. Furthermore,\nlow-order enhanced feature aggregation adaptively integrates low-order and\nhigh-order features based on task relevance, ensuring effective capture of both\nlocal and global structural information without excessive complexity. Extensive\nexperiments on real-world datasets demonstrate that our approach consistently\noutperforms state-of-the-art GNN models in both accuracy and computational\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have demonstrated strong performance across\nvarious graph-based tasks by effectively capturing relational information\nbetween nodes. These models rely on iterative message passing to propagate node\nfeatures, enabling nodes to aggregate information from their neighbors. Recent\nresearch has significantly improved the message-passing mechanism, enhancing\nGNN scalability on large-scale graphs. However, GNNs still face two main\nchallenges: over-smoothing, where excessive message passing results in\nindistinguishable node representations, especially in deep networks\nincorporating high-order neighbors; and scalability issues, as traditional\narchitectures suffer from high model complexity and increased inference time\ndue to redundant information aggregation. This paper proposes a novel framework\nfor large-scale graphs named ScaleGNN that simultaneously addresses both\nchallenges by adaptively fusing multi-level graph features. We first construct\nneighbor matrices for each order, learning their relative information through\ntrainable weights through an adaptive high-order feature fusion module. This\nallows the model to selectively emphasize informative high-order neighbors\nwhile reducing unnecessary computational costs. Additionally, we introduce a\nHigh-order redundant feature masking mechanism based on a Local Contribution\nScore (LCS), which enables the model to retain only the most relevant neighbors\nat each order, preventing redundant information propagation. Furthermore,\nlow-order enhanced feature aggregation adaptively integrates low-order and\nhigh-order features based on task relevance, ensuring effective capture of both\nlocal and global structural information without excessive complexity. Extensive\nexperiments on real-world datasets demonstrate that our approach consistently\noutperforms state-of-the-art GNN models in both accuracy and computational\nefficiency."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Haobing Liu"
                    },
                    {
                        "name": "Jianpeng Qi"
                    },
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Guoqing Chao"
                    },
                    {
                        "name": "Yanwei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Yu"
                },
                "author": "Yanwei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15917v1",
                "updated": "2025-04-22T14:02:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    2,
                    57,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:02:57Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    2,
                    57,
                    1,
                    112,
                    0
                ],
                "title": "Towards Test Generation from Task Description for Mobile Testing with\n  Multi-modal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Test Generation from Task Description for Mobile Testing with\n  Multi-modal Reasoning"
                },
                "summary": "In Android GUI testing, generating an action sequence for a task that can be\nreplayed as a test script is common. Generating sequences of actions and\nrespective test scripts from task goals described in natural language can\neliminate the need for manually writing test scripts. However, existing\napproaches based on large language models (LLM) often struggle with identifying\nthe final action, and either end prematurely or continue past the final screen.\nIn this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent\nframework that iteratively determines the next action and leverages visual\nimages of screens to detect the task's completeness. The multi-modal approach\nenhances our model in two significant ways. First, this approach enables it to\navoid prematurely terminating a task when textual content alone provides\nmisleading indications of task completion. Additionally, visual input helps the\ntool avoid errors when changes in the GUI do not directly affect functionality\ntoward task completion, such as adjustments to font sizes or colors. Second,\nthe multi-modal approach also ensures the tool not progress beyond the final\nscreen, which might lack explicit textual indicators of task completion but\ncould display a visual element indicating task completion, which is common in\nGUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%,\noutperforming the best baseline relatively by 23.5%. We also demonstrate that\nour multi-modal framework with images and texts enables the LLM to better\ndetermine when a task is completed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Android GUI testing, generating an action sequence for a task that can be\nreplayed as a test script is common. Generating sequences of actions and\nrespective test scripts from task goals described in natural language can\neliminate the need for manually writing test scripts. However, existing\napproaches based on large language models (LLM) often struggle with identifying\nthe final action, and either end prematurely or continue past the final screen.\nIn this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent\nframework that iteratively determines the next action and leverages visual\nimages of screens to detect the task's completeness. The multi-modal approach\nenhances our model in two significant ways. First, this approach enables it to\navoid prematurely terminating a task when textual content alone provides\nmisleading indications of task completion. Additionally, visual input helps the\ntool avoid errors when changes in the GUI do not directly affect functionality\ntoward task completion, such as adjustments to font sizes or colors. Second,\nthe multi-modal approach also ensures the tool not progress beyond the final\nscreen, which might lack explicit textual indicators of task completion but\ncould display a visual element indicating task completion, which is common in\nGUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%,\noutperforming the best baseline relatively by 23.5%. We also demonstrate that\nour multi-modal framework with images and texts enables the LLM to better\ndetermine when a task is completed."
                },
                "authors": [
                    {
                        "name": "Hieu Huynh"
                    },
                    {
                        "name": "Hai Phung"
                    },
                    {
                        "name": "Hao Pham"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Vu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Vu Nguyen"
                },
                "author": "Vu Nguyen",
                "arxiv_comment": "Under review for a conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09385v2",
                "updated": "2025-04-22T13:56:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    56,
                    32,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-12T15:52:41Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    52,
                    41,
                    3,
                    347,
                    0
                ],
                "title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore\n  LLMs' Complex Reasoning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore\n  LLMs' Complex Reasoning Capabilities"
                },
                "summary": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Fabrizio Davide"
                    },
                    {
                        "name": "Pietro Torre"
                    },
                    {
                        "name": "Leonardo Ercolani"
                    },
                    {
                        "name": "Andrea Gaggioli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Gaggioli"
                },
                "author": "Andrea Gaggioli",
                "arxiv_comment": "47 pages, 8 figures, 17 tables, appendix with data and code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15909v1",
                "updated": "2025-04-22T13:55:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    55,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:55:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    55,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "Synergizing RAG and Reasoning: A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing RAG and Reasoning: A Systematic Review"
                },
                "summary": "Recent breakthroughs in large language models (LLMs), particularly in\nreasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to\nunprecedented levels. By synergizing retrieval mechanisms with advanced\nreasoning, LLMs can now tackle increasingly complex problems. This paper\npresents a systematic review of the collaborative interplay between RAG and\nreasoning, clearly defining \"reasoning\" within the RAG context. It construct a\ncomprehensive taxonomy encompassing multi-dimensional collaborative objectives,\nrepresentative paradigms, and technical implementations, and analyze the\nbidirectional synergy methods. Additionally, we critically evaluate current\nlimitations in RAG assessment, including the absence of intermediate\nsupervision for multi-step reasoning and practical challenges related to\ncost-risk trade-offs. To bridge theory and practice, we provide practical\nguidelines tailored to diverse real-world applications. Finally, we identify\npromising research directions, such as graph-based knowledge integration,\nhybrid model collaboration, and RL-driven optimization. Overall, this work\npresents a theoretical framework and practical foundation to advance RAG\nsystems in academia and industry, fostering the next generation of RAG\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large language models (LLMs), particularly in\nreasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to\nunprecedented levels. By synergizing retrieval mechanisms with advanced\nreasoning, LLMs can now tackle increasingly complex problems. This paper\npresents a systematic review of the collaborative interplay between RAG and\nreasoning, clearly defining \"reasoning\" within the RAG context. It construct a\ncomprehensive taxonomy encompassing multi-dimensional collaborative objectives,\nrepresentative paradigms, and technical implementations, and analyze the\nbidirectional synergy methods. Additionally, we critically evaluate current\nlimitations in RAG assessment, including the absence of intermediate\nsupervision for multi-step reasoning and practical challenges related to\ncost-risk trade-offs. To bridge theory and practice, we provide practical\nguidelines tailored to diverse real-world applications. Finally, we identify\npromising research directions, such as graph-based knowledge integration,\nhybrid model collaboration, and RL-driven optimization. Overall, this work\npresents a theoretical framework and practical foundation to advance RAG\nsystems in academia and industry, fostering the next generation of RAG\nsolutions."
                },
                "authors": [
                    {
                        "name": "Yunfan Gao"
                    },
                    {
                        "name": "Yun Xiong"
                    },
                    {
                        "name": "Yijie Zhong"
                    },
                    {
                        "name": "Yuxi Bi"
                    },
                    {
                        "name": "Ming Xue"
                    },
                    {
                        "name": "Haofen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haofen Wang"
                },
                "author": "Haofen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15905v1",
                "updated": "2025-04-22T13:45:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    45,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:45:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    45,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs\n  Computing in Edge Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs\n  Computing in Edge Network"
                },
                "summary": "With the exponential growth of Internet of Things (IoT) devices, edge\ncomputing (EC) is gradually playing an important role in providing\ncost-effective services. However, existing approaches struggle to perform well\nin graph-structured scenarios where user data is correlated, such as traffic\nflow prediction and social relationship recommender systems. In particular,\ngraph neural network (GNN)-based approaches lead to expensive server\ncommunication cost. To address this problem, we propose GraphEdge, an efficient\nGNN-based EC architecture. It considers the EC system of GNN tasks, where there\nare associations between users and it needs to take into account the task data\nof its neighbors when processing the tasks of a user. Specifically, the\narchitecture first perceives the user topology and represents their data\nassociations as a graph layout at each time step. Then the graph layout is\noptimized by calling our proposed hierarchical traversal graph cut algorithm\n(HiCut), which cuts the graph layout into multiple weakly associated subgraphs\nbased on the aggregation characteristics of GNN, and the communication cost\nbetween different subgraphs during GNN inference is minimized. Finally, based\non the optimized graph layout, our proposed deep reinforcement learning (DRL)\nbased graph offloading algorithm (DRLGO) is executed to obtain the optimal\noffloading strategy for the tasks of users, the offloading strategy is\nsubgraph-based, it tries to offload user tasks in a subgraph to the same edge\nserver as possible while minimizing the task processing time and energy\nconsumption of the EC system. Experimental results show the good effectiveness\nand dynamic adaptation of our proposed architecture and it also performs well\neven in dynamic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential growth of Internet of Things (IoT) devices, edge\ncomputing (EC) is gradually playing an important role in providing\ncost-effective services. However, existing approaches struggle to perform well\nin graph-structured scenarios where user data is correlated, such as traffic\nflow prediction and social relationship recommender systems. In particular,\ngraph neural network (GNN)-based approaches lead to expensive server\ncommunication cost. To address this problem, we propose GraphEdge, an efficient\nGNN-based EC architecture. It considers the EC system of GNN tasks, where there\nare associations between users and it needs to take into account the task data\nof its neighbors when processing the tasks of a user. Specifically, the\narchitecture first perceives the user topology and represents their data\nassociations as a graph layout at each time step. Then the graph layout is\noptimized by calling our proposed hierarchical traversal graph cut algorithm\n(HiCut), which cuts the graph layout into multiple weakly associated subgraphs\nbased on the aggregation characteristics of GNN, and the communication cost\nbetween different subgraphs during GNN inference is minimized. Finally, based\non the optimized graph layout, our proposed deep reinforcement learning (DRL)\nbased graph offloading algorithm (DRLGO) is executed to obtain the optimal\noffloading strategy for the tasks of users, the offloading strategy is\nsubgraph-based, it tries to offload user tasks in a subgraph to the same edge\nserver as possible while minimizing the task processing time and energy\nconsumption of the EC system. Experimental results show the good effectiveness\nand dynamic adaptation of our proposed architecture and it also performs well\neven in dynamic scenarios."
                },
                "authors": [
                    {
                        "name": "Wenjing Xiao"
                    },
                    {
                        "name": "Chenglong Shi"
                    },
                    {
                        "name": "Miaojiang Chen"
                    },
                    {
                        "name": "Zhiquan Liu"
                    },
                    {
                        "name": "Min Chen"
                    },
                    {
                        "name": "H. Herbert Song"
                    }
                ],
                "author_detail": {
                    "name": "H. Herbert Song"
                },
                "author": "H. Herbert Song",
                "arxiv_comment": "17 pages,12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15903v1",
                "updated": "2025-04-22T13:43:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    43,
                    58,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:43:58Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    43,
                    58,
                    1,
                    112,
                    0
                ],
                "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning\n  Corpus (ARC) Tasks with Model Temperature Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning\n  Corpus (ARC) Tasks with Model Temperature Considerations"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility."
                },
                "authors": [
                    {
                        "name": "Nikhil Khandalkar"
                    },
                    {
                        "name": "Pavan Yadav"
                    },
                    {
                        "name": "Krishna Shinde"
                    },
                    {
                        "name": "Lokesh B. Ramegowda"
                    },
                    {
                        "name": "Rajarshi Das"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Das"
                },
                "author": "Rajarshi Das",
                "arxiv_comment": "60 pages, 25 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15900v1",
                "updated": "2025-04-22T13:41:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    41,
                    26,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    41,
                    26,
                    1,
                    112,
                    0
                ],
                "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement\n  Learning"
                },
                "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding."
                },
                "authors": [
                    {
                        "name": "Cheng Wen"
                    },
                    {
                        "name": "Tingwei Guo"
                    },
                    {
                        "name": "Shuaijiang Zhao"
                    },
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15895v1",
                "updated": "2025-04-22T13:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    36,
                    53,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:36:53Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    36,
                    53,
                    1,
                    112,
                    0
                ],
                "title": "Dynamic Early Exit in Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Early Exit in Reasoning Models"
                },
                "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%."
                },
                "authors": [
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Yongjie Duan"
                    },
                    {
                        "name": "Zheliang Zhu"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15893v1",
                "updated": "2025-04-22T13:35:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    35,
                    22,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:35:22Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    35,
                    22,
                    1,
                    112,
                    0
                ],
                "title": "Leveraging differentiable programming in the inverse problem of neutron\n  stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging differentiable programming in the inverse problem of neutron\n  stars"
                },
                "summary": "Neutron stars (NSs) probe the high-density regime of the nuclear equation of\nstate (EOS). However, inferring the EOS from observations of NSs is a\ncomputationally challenging task. In this work, we efficiently solve this\ninverse problem by leveraging differential programming in two ways. First, we\nenable full Bayesian inference in under one hour of wall time on a GPU by using\ngradient-based samplers, without requiring pre-trained machine learning\nemulators. Moreover, we demonstrate efficient scaling to high-dimensional\nparameter spaces. Second, we introduce a novel gradient-based optimization\nscheme that recovers the EOS of a given NS mass-radius curve. We demonstrate\nhow our framework can reveal consistencies or tensions between nuclear physics\nand astrophysics. First, we show how the breakdown density of a metamodel\ndescription of the EOS can be determined from NS observations. Second, we\ndemonstrate how degeneracies in EOS modeling using nuclear empirical parameters\ncan influence the inverse problem during gradient-based optimization. Looking\nahead, our approach opens up new theoretical studies of the relation between NS\nproperties and the EOS, while effectively tackling the data analysis challenges\nbrought by future detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron stars (NSs) probe the high-density regime of the nuclear equation of\nstate (EOS). However, inferring the EOS from observations of NSs is a\ncomputationally challenging task. In this work, we efficiently solve this\ninverse problem by leveraging differential programming in two ways. First, we\nenable full Bayesian inference in under one hour of wall time on a GPU by using\ngradient-based samplers, without requiring pre-trained machine learning\nemulators. Moreover, we demonstrate efficient scaling to high-dimensional\nparameter spaces. Second, we introduce a novel gradient-based optimization\nscheme that recovers the EOS of a given NS mass-radius curve. We demonstrate\nhow our framework can reveal consistencies or tensions between nuclear physics\nand astrophysics. First, we show how the breakdown density of a metamodel\ndescription of the EOS can be determined from NS observations. Second, we\ndemonstrate how degeneracies in EOS modeling using nuclear empirical parameters\ncan influence the inverse problem during gradient-based optimization. Looking\nahead, our approach opens up new theoretical studies of the relation between NS\nproperties and the EOS, while effectively tackling the data analysis challenges\nbrought by future detectors."
                },
                "authors": [
                    {
                        "name": "Thibeau Wouters"
                    },
                    {
                        "name": "Peter T. H. Pang"
                    },
                    {
                        "name": "Hauke Koehn"
                    },
                    {
                        "name": "Henrik Rose"
                    },
                    {
                        "name": "Rahul Somasundaram"
                    },
                    {
                        "name": "Ingo Tews"
                    },
                    {
                        "name": "Tim Dietrich"
                    },
                    {
                        "name": "Chris Van Den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Chris Van Den Broeck"
                },
                "author": "Chris Van Den Broeck",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14350v2",
                "updated": "2025-04-22T13:31:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    31,
                    25,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-19T16:32:28Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    16,
                    32,
                    28,
                    5,
                    109,
                    0
                ],
                "title": "Time's Up! An Empirical Study of LLM Reasoning Ability Under Output\n  Length Constraint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time's Up! An Empirical Study of LLM Reasoning Ability Under Output\n  Length Constraint"
                },
                "summary": "Recent work has demonstrated the remarkable potential of Large Language\nModels (LLMs) in test-time scaling. By making the models think before\nanswering, they are able to achieve much higher accuracy with extra inference\ncomputation. However, in many real-world scenarios, models are used under time\nconstraints, where an answer should be given to the user within a certain\noutput length. It is unclear whether and how the reasoning abilities of LLMs\nremain effective under such constraints. We take a first look at this problem\nby conducting an in-depth empirical study. Specifically, we test more than 25\nLLMs on common reasoning datasets under a wide range of output length budgets,\nand we analyze the correlation between the inference accuracy and various\nproperties including model type, model size, prompt style, etc. We also\nconsider the mappings between the token budgets and the actual on-device\nlatency budgets. The results have demonstrated several interesting findings\nregarding the budget-aware LLM reasoning that differ from the unconstrained\nsituation, e.g. the optimal choices of model sizes and prompts change under\ndifferent budgets. These findings offer practical guidance for users to deploy\nLLMs under real-world latency constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated the remarkable potential of Large Language\nModels (LLMs) in test-time scaling. By making the models think before\nanswering, they are able to achieve much higher accuracy with extra inference\ncomputation. However, in many real-world scenarios, models are used under time\nconstraints, where an answer should be given to the user within a certain\noutput length. It is unclear whether and how the reasoning abilities of LLMs\nremain effective under such constraints. We take a first look at this problem\nby conducting an in-depth empirical study. Specifically, we test more than 25\nLLMs on common reasoning datasets under a wide range of output length budgets,\nand we analyze the correlation between the inference accuracy and various\nproperties including model type, model size, prompt style, etc. We also\nconsider the mappings between the token budgets and the actual on-device\nlatency budgets. The results have demonstrated several interesting findings\nregarding the budget-aware LLM reasoning that differ from the unconstrained\nsituation, e.g. the optimal choices of model sizes and prompts change under\ndifferent budgets. These findings offer practical guidance for users to deploy\nLLMs under real-world latency constraints."
                },
                "authors": [
                    {
                        "name": "Yi Sun"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jiaqiang Li"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Huiwen Zheng"
                    },
                    {
                        "name": "Yan Liang"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15881v1",
                "updated": "2025-04-22T13:25:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    25,
                    40,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:25:40Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    25,
                    40,
                    1,
                    112,
                    0
                ],
                "title": "Measurement of the time-integrated $CP$ asymmetry in $D^0 \\to K^0_{\\rm\n  S} K^0_{\\rm S}$ decays using opposite-side flavor tagging at Belle and Belle\n  II",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of the time-integrated $CP$ asymmetry in $D^0 \\to K^0_{\\rm\n  S} K^0_{\\rm S}$ decays using opposite-side flavor tagging at Belle and Belle\n  II"
                },
                "summary": "We measure the time-integrated $CP$ asymmetry in $D^0 \\to K^0_{\\rm S}\nK^0_{\\rm S}$ decays reconstructed in $e^+e^-\\to c{\\overline c}$ events\ncollected by the Belle and Belle II experiments. The corresponding data samples\nhave integrated luminosities of 980 and 428 fb${}^{-1}$, respectively. To infer\nthe flavor of the $D^0$ meson, we exploit the correlation between the flavor of\nthe reconstructed decay and the electric charges of particles reconstructed in\nthe rest of the $e^+e^-\\to c{\\overline c}$ event. This results in a sample\nwhich is independent from any other previously used at Belle or Belle II. The\nresult, $A_{CP}(D^0 \\to K^0_{\\rm S} K^0_{\\rm S}) = (1.3 \\pm 2.0 \\pm 0.2)\\%$,\nwhere the first uncertainty is statistical and the second systematic, is\nconsistent with previous determinations and with $CP$ symmetry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We measure the time-integrated $CP$ asymmetry in $D^0 \\to K^0_{\\rm S}\nK^0_{\\rm S}$ decays reconstructed in $e^+e^-\\to c{\\overline c}$ events\ncollected by the Belle and Belle II experiments. The corresponding data samples\nhave integrated luminosities of 980 and 428 fb${}^{-1}$, respectively. To infer\nthe flavor of the $D^0$ meson, we exploit the correlation between the flavor of\nthe reconstructed decay and the electric charges of particles reconstructed in\nthe rest of the $e^+e^-\\to c{\\overline c}$ event. This results in a sample\nwhich is independent from any other previously used at Belle or Belle II. The\nresult, $A_{CP}(D^0 \\to K^0_{\\rm S} K^0_{\\rm S}) = (1.3 \\pm 2.0 \\pm 0.2)\\%$,\nwhere the first uncertainty is statistical and the second systematic, is\nconsistent with previous determinations and with $CP$ symmetry."
                },
                "authors": [
                    {
                        "name": "Belle"
                    },
                    {
                        "name": "Belle II Collaborations"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "I. Adachi"
                    },
                    {
                        "name": "Y. Ahn"
                    },
                    {
                        "name": "N. Akopov"
                    },
                    {
                        "name": "S. Alghamdi"
                    },
                    {
                        "name": "M. Alhakami"
                    },
                    {
                        "name": "A. Aloisio"
                    },
                    {
                        "name": "N. Althubiti"
                    },
                    {
                        "name": "K. Amos"
                    },
                    {
                        "name": "M. Angelsmark"
                    },
                    {
                        "name": "N. Anh Ky"
                    },
                    {
                        "name": "C. Antonioli"
                    },
                    {
                        "name": "D. M. Asner"
                    },
                    {
                        "name": "H. Atmacan"
                    },
                    {
                        "name": "T. Aushev"
                    },
                    {
                        "name": "M. Aversano"
                    },
                    {
                        "name": "R. Ayad"
                    },
                    {
                        "name": "V. Babu"
                    },
                    {
                        "name": "H. Bae"
                    },
                    {
                        "name": "N. K. Baghel"
                    },
                    {
                        "name": "S. Bahinipati"
                    },
                    {
                        "name": "P. Bambade"
                    },
                    {
                        "name": "Sw. Banerjee"
                    },
                    {
                        "name": "M. Barrett"
                    },
                    {
                        "name": "M. Bartl"
                    },
                    {
                        "name": "J. Baudot"
                    },
                    {
                        "name": "A. Baur"
                    },
                    {
                        "name": "A. Beaubien"
                    },
                    {
                        "name": "F. Becherer"
                    },
                    {
                        "name": "J. Becker"
                    },
                    {
                        "name": "J. V. Bennett"
                    },
                    {
                        "name": "F. U. Bernlochner"
                    },
                    {
                        "name": "V. Bertacchi"
                    },
                    {
                        "name": "M. Bertemes"
                    },
                    {
                        "name": "E. Bertholet"
                    },
                    {
                        "name": "M. Bessner"
                    },
                    {
                        "name": "S. Bettarini"
                    },
                    {
                        "name": "B. Bhuyan"
                    },
                    {
                        "name": "F. Bianchi"
                    },
                    {
                        "name": "T. Bilka"
                    },
                    {
                        "name": "D. Biswas"
                    },
                    {
                        "name": "A. Bobrov"
                    },
                    {
                        "name": "D. Bodrov"
                    },
                    {
                        "name": "A. Bondar"
                    },
                    {
                        "name": "J. Borah"
                    },
                    {
                        "name": "A. Boschetti"
                    },
                    {
                        "name": "A. Bozek"
                    },
                    {
                        "name": "M. Braƒçko"
                    },
                    {
                        "name": "P. Branchini"
                    },
                    {
                        "name": "R. A. Briere"
                    },
                    {
                        "name": "T. E. Browder"
                    },
                    {
                        "name": "A. Budano"
                    },
                    {
                        "name": "S. Bussino"
                    },
                    {
                        "name": "M. Campajola"
                    },
                    {
                        "name": "L. Cao"
                    },
                    {
                        "name": "G. Casarosa"
                    },
                    {
                        "name": "C. Cecchi"
                    },
                    {
                        "name": "P. Cheema"
                    },
                    {
                        "name": "B. G. Cheon"
                    },
                    {
                        "name": "K. Chilikin"
                    },
                    {
                        "name": "J. Chin"
                    },
                    {
                        "name": "K. Chirapatpimol"
                    },
                    {
                        "name": "H. -E. Cho"
                    },
                    {
                        "name": "K. Cho"
                    },
                    {
                        "name": "S. -J. Cho"
                    },
                    {
                        "name": "S. -K. Choi"
                    },
                    {
                        "name": "S. Choudhury"
                    },
                    {
                        "name": "I. Consigny"
                    },
                    {
                        "name": "L. Corona"
                    },
                    {
                        "name": "J. X. Cui"
                    },
                    {
                        "name": "E. De La Cruz-Burelo"
                    },
                    {
                        "name": "S. A. De La Motte"
                    },
                    {
                        "name": "G. de Marino"
                    },
                    {
                        "name": "G. De Pietro"
                    },
                    {
                        "name": "R. de Sangro"
                    },
                    {
                        "name": "M. Destefanis"
                    },
                    {
                        "name": "A. Di Canto"
                    },
                    {
                        "name": "J. Dingfelder"
                    },
                    {
                        "name": "Z. Dole≈æal"
                    },
                    {
                        "name": "I. Dom√≠nguez Jim√©nez"
                    },
                    {
                        "name": "T. V. Dong"
                    },
                    {
                        "name": "M. Dorigo"
                    },
                    {
                        "name": "G. Dujany"
                    },
                    {
                        "name": "P. Ecker"
                    },
                    {
                        "name": "D. Epifanov"
                    },
                    {
                        "name": "R. Farkas"
                    },
                    {
                        "name": "P. Feichtinger"
                    },
                    {
                        "name": "T. Ferber"
                    },
                    {
                        "name": "T. Fillinger"
                    },
                    {
                        "name": "C. Finck"
                    },
                    {
                        "name": "G. Finocchiaro"
                    },
                    {
                        "name": "A. Fodor"
                    },
                    {
                        "name": "F. Forti"
                    },
                    {
                        "name": "B. G. Fulsom"
                    },
                    {
                        "name": "A. Gabrielli"
                    },
                    {
                        "name": "A. Gale"
                    },
                    {
                        "name": "E. Ganiev"
                    },
                    {
                        "name": "M. Garcia-Hernandez"
                    },
                    {
                        "name": "R. Garg"
                    },
                    {
                        "name": "G. Gaudino"
                    },
                    {
                        "name": "V. Gaur"
                    },
                    {
                        "name": "V. Gautam"
                    },
                    {
                        "name": "A. Gaz"
                    },
                    {
                        "name": "A. Gellrich"
                    },
                    {
                        "name": "D. Ghosh"
                    },
                    {
                        "name": "H. Ghumaryan"
                    },
                    {
                        "name": "G. Giakoustidis"
                    },
                    {
                        "name": "R. Giordano"
                    },
                    {
                        "name": "A. Giri"
                    },
                    {
                        "name": "P. Gironella Gironell"
                    },
                    {
                        "name": "B. Gobbo"
                    },
                    {
                        "name": "R. Godang"
                    },
                    {
                        "name": "O. Gogota"
                    },
                    {
                        "name": "P. Goldenzweig"
                    },
                    {
                        "name": "W. Gradl"
                    },
                    {
                        "name": "E. Graziani"
                    },
                    {
                        "name": "D. Greenwald"
                    },
                    {
                        "name": "Z. Gruberov√°"
                    },
                    {
                        "name": "Y. Guan"
                    },
                    {
                        "name": "K. Gudkova"
                    },
                    {
                        "name": "I. Haide"
                    },
                    {
                        "name": "Y. Han"
                    },
                    {
                        "name": "H. Hayashii"
                    },
                    {
                        "name": "S. Hazra"
                    },
                    {
                        "name": "C. Hearty"
                    },
                    {
                        "name": "M. T. Hedges"
                    },
                    {
                        "name": "A. Heidelbach"
                    },
                    {
                        "name": "G. Heine"
                    },
                    {
                        "name": "I. Heredia de la Cruz"
                    },
                    {
                        "name": "M. Hern√°ndez Villanueva"
                    },
                    {
                        "name": "T. Higuchi"
                    },
                    {
                        "name": "M. Hoek"
                    },
                    {
                        "name": "M. Hohmann"
                    },
                    {
                        "name": "P. Horak"
                    },
                    {
                        "name": "C. -L. Hsu"
                    },
                    {
                        "name": "T. Humair"
                    },
                    {
                        "name": "T. Iijima"
                    },
                    {
                        "name": "K. Inami"
                    },
                    {
                        "name": "G. Inguglia"
                    },
                    {
                        "name": "N. Ipsita"
                    },
                    {
                        "name": "A. Ishikawa"
                    },
                    {
                        "name": "R. Itoh"
                    },
                    {
                        "name": "M. Iwasaki"
                    },
                    {
                        "name": "P. Jackson"
                    },
                    {
                        "name": "D. Jacobi"
                    },
                    {
                        "name": "W. W. Jacobs"
                    },
                    {
                        "name": "D. E. Jaffe"
                    },
                    {
                        "name": "Q. P. Ji"
                    },
                    {
                        "name": "S. Jia"
                    },
                    {
                        "name": "Y. Jin"
                    },
                    {
                        "name": "A. Johnson"
                    },
                    {
                        "name": "J. Kandra"
                    },
                    {
                        "name": "K. H. Kang"
                    },
                    {
                        "name": "G. Karyan"
                    },
                    {
                        "name": "T. Kawasaki"
                    },
                    {
                        "name": "F. Keil"
                    },
                    {
                        "name": "C. Ketter"
                    },
                    {
                        "name": "M. Khan"
                    },
                    {
                        "name": "C. Kiesling"
                    },
                    {
                        "name": "D. Y. Kim"
                    },
                    {
                        "name": "J. -Y. Kim"
                    },
                    {
                        "name": "K. -H. Kim"
                    },
                    {
                        "name": "K. Kinoshita"
                    },
                    {
                        "name": "P. Kody≈°"
                    },
                    {
                        "name": "T. Koga"
                    },
                    {
                        "name": "S. Kohani"
                    },
                    {
                        "name": "K. Kojima"
                    },
                    {
                        "name": "A. Korobov"
                    },
                    {
                        "name": "S. Korpar"
                    },
                    {
                        "name": "E. Kovalenko"
                    },
                    {
                        "name": "R. Kowalewski"
                    },
                    {
                        "name": "P. Kri≈æan"
                    },
                    {
                        "name": "P. Krokovny"
                    },
                    {
                        "name": "K. Kumara"
                    },
                    {
                        "name": "T. Kunigo"
                    },
                    {
                        "name": "A. Kuzmin"
                    },
                    {
                        "name": "Y. -J. Kwon"
                    },
                    {
                        "name": "K. Lalwani"
                    },
                    {
                        "name": "T. Lam"
                    },
                    {
                        "name": "J. S. Lange"
                    },
                    {
                        "name": "T. S. Lau"
                    },
                    {
                        "name": "M. Laurenza"
                    },
                    {
                        "name": "R. Leboucher"
                    },
                    {
                        "name": "F. R. Le Diberder"
                    },
                    {
                        "name": "M. J. Lee"
                    },
                    {
                        "name": "C. Lemettais"
                    },
                    {
                        "name": "P. Leo"
                    },
                    {
                        "name": "P. M. Lewis"
                    },
                    {
                        "name": "H. -J. Li"
                    },
                    {
                        "name": "L. K. Li"
                    },
                    {
                        "name": "Q. M. Li"
                    },
                    {
                        "name": "W. Z. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "Y. B. Li"
                    },
                    {
                        "name": "Y. P. Liao"
                    },
                    {
                        "name": "J. Libby"
                    },
                    {
                        "name": "J. Lin"
                    },
                    {
                        "name": "S. Lin"
                    },
                    {
                        "name": "V. Lisovskyi"
                    },
                    {
                        "name": "M. H. Liu"
                    },
                    {
                        "name": "Q. Y. Liu"
                    },
                    {
                        "name": "Y. Liu"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "D. Liventsev"
                    },
                    {
                        "name": "S. Longo"
                    },
                    {
                        "name": "C. Lyu"
                    },
                    {
                        "name": "Y. Ma"
                    },
                    {
                        "name": "C. Madaan"
                    },
                    {
                        "name": "M. Maggiora"
                    },
                    {
                        "name": "S. P. Maharana"
                    },
                    {
                        "name": "R. Maiti"
                    },
                    {
                        "name": "G. Mancinelli"
                    },
                    {
                        "name": "R. Manfredi"
                    },
                    {
                        "name": "E. Manoni"
                    },
                    {
                        "name": "M. Mantovano"
                    },
                    {
                        "name": "D. Marcantonio"
                    },
                    {
                        "name": "S. Marcello"
                    },
                    {
                        "name": "C. Marinas"
                    },
                    {
                        "name": "C. Martellini"
                    },
                    {
                        "name": "A. Martens"
                    },
                    {
                        "name": "T. Martinov"
                    },
                    {
                        "name": "L. Massaccesi"
                    },
                    {
                        "name": "M. Masuda"
                    },
                    {
                        "name": "S. K. Maurya"
                    },
                    {
                        "name": "M. Maushart"
                    },
                    {
                        "name": "J. A. McKenna"
                    },
                    {
                        "name": "F. Meier"
                    },
                    {
                        "name": "M. Merola"
                    },
                    {
                        "name": "C. Miller"
                    },
                    {
                        "name": "M. Mirra"
                    },
                    {
                        "name": "S. Mitra"
                    },
                    {
                        "name": "K. Miyabayashi"
                    },
                    {
                        "name": "R. Mizuk"
                    },
                    {
                        "name": "G. B. Mohanty"
                    },
                    {
                        "name": "S. Moneta"
                    },
                    {
                        "name": "H. -G. Moser"
                    },
                    {
                        "name": "M. Nakao"
                    },
                    {
                        "name": "H. Nakazawa"
                    },
                    {
                        "name": "Y. Nakazawa"
                    },
                    {
                        "name": "M. Naruki"
                    },
                    {
                        "name": "Z. Natkaniec"
                    },
                    {
                        "name": "A. Natochii"
                    },
                    {
                        "name": "M. Nayak"
                    },
                    {
                        "name": "M. Neu"
                    },
                    {
                        "name": "S. Nishida"
                    },
                    {
                        "name": "S. Ogawa"
                    },
                    {
                        "name": "R. Okubo"
                    },
                    {
                        "name": "H. Ono"
                    },
                    {
                        "name": "E. R. Oxford"
                    },
                    {
                        "name": "G. Pakhlova"
                    },
                    {
                        "name": "S. Pardi"
                    },
                    {
                        "name": "K. Parham"
                    },
                    {
                        "name": "H. Park"
                    },
                    {
                        "name": "J. Park"
                    },
                    {
                        "name": "K. Park"
                    },
                    {
                        "name": "S. -H. Park"
                    },
                    {
                        "name": "A. Passeri"
                    },
                    {
                        "name": "S. Patra"
                    },
                    {
                        "name": "R. Pestotnik"
                    },
                    {
                        "name": "L. E. Piilonen"
                    },
                    {
                        "name": "P. L. M. Podesta-Lerma"
                    },
                    {
                        "name": "T. Podobnik"
                    },
                    {
                        "name": "A. Prakash"
                    },
                    {
                        "name": "C. Praz"
                    },
                    {
                        "name": "S. Prell"
                    },
                    {
                        "name": "E. Prencipe"
                    },
                    {
                        "name": "M. T. Prim"
                    },
                    {
                        "name": "S. Privalov"
                    },
                    {
                        "name": "H. Purwar"
                    },
                    {
                        "name": "P. Rados"
                    },
                    {
                        "name": "G. Raeuber"
                    },
                    {
                        "name": "S. Raiz"
                    },
                    {
                        "name": "V. Raj"
                    },
                    {
                        "name": "K. Ravindran"
                    },
                    {
                        "name": "J. U. Rehman"
                    },
                    {
                        "name": "M. Reif"
                    },
                    {
                        "name": "S. Reiter"
                    },
                    {
                        "name": "M. Remnev"
                    },
                    {
                        "name": "L. Reuter"
                    },
                    {
                        "name": "D. Ricalde Herrmann"
                    },
                    {
                        "name": "I. Ripp-Baudot"
                    },
                    {
                        "name": "G. Rizzo"
                    },
                    {
                        "name": "J. M. Roney"
                    },
                    {
                        "name": "A. Rostomyan"
                    },
                    {
                        "name": "N. Rout"
                    },
                    {
                        "name": "L. Salutari"
                    },
                    {
                        "name": "D. A. Sanders"
                    },
                    {
                        "name": "S. Sandilya"
                    },
                    {
                        "name": "L. Santelj"
                    },
                    {
                        "name": "V. Savinov"
                    },
                    {
                        "name": "B. Scavino"
                    },
                    {
                        "name": "C. Schmitt"
                    },
                    {
                        "name": "J. Schmitz"
                    },
                    {
                        "name": "S. Schneider"
                    },
                    {
                        "name": "G. Schnell"
                    },
                    {
                        "name": "M. Schnepf"
                    },
                    {
                        "name": "C. Schwanda"
                    },
                    {
                        "name": "A. J. Schwartz"
                    },
                    {
                        "name": "Y. Seino"
                    },
                    {
                        "name": "A. Selce"
                    },
                    {
                        "name": "K. Senyo"
                    },
                    {
                        "name": "J. Serrano"
                    },
                    {
                        "name": "M. E. Sevior"
                    },
                    {
                        "name": "C. Sfienti"
                    },
                    {
                        "name": "W. Shan"
                    },
                    {
                        "name": "X. D. Shi"
                    },
                    {
                        "name": "T. Shillington"
                    },
                    {
                        "name": "J. -G. Shiu"
                    },
                    {
                        "name": "D. Shtol"
                    },
                    {
                        "name": "B. Shwartz"
                    },
                    {
                        "name": "A. Sibidanov"
                    },
                    {
                        "name": "F. Simon"
                    },
                    {
                        "name": "J. Skorupa"
                    },
                    {
                        "name": "R. J. Sobie"
                    },
                    {
                        "name": "M. Sobotzik"
                    },
                    {
                        "name": "A. Soffer"
                    },
                    {
                        "name": "A. Sokolov"
                    },
                    {
                        "name": "E. Solovieva"
                    },
                    {
                        "name": "S. Spataro"
                    },
                    {
                        "name": "B. Spruck"
                    },
                    {
                        "name": "M. Stariƒç"
                    },
                    {
                        "name": "P. Stavroulakis"
                    },
                    {
                        "name": "S. Stefkova"
                    },
                    {
                        "name": "L. Stoetzer"
                    },
                    {
                        "name": "R. Stroili"
                    },
                    {
                        "name": "Y. Sue"
                    },
                    {
                        "name": "M. Sumihama"
                    },
                    {
                        "name": "N. Suwonjandee"
                    },
                    {
                        "name": "H. Svidras"
                    },
                    {
                        "name": "M. Takizawa"
                    },
                    {
                        "name": "K. Tanida"
                    },
                    {
                        "name": "F. Tenchini"
                    },
                    {
                        "name": "F. Testa"
                    },
                    {
                        "name": "O. Tittel"
                    },
                    {
                        "name": "R. Tiwary"
                    },
                    {
                        "name": "E. Torassa"
                    },
                    {
                        "name": "K. Trabelsi"
                    },
                    {
                        "name": "F. F. Trantou"
                    },
                    {
                        "name": "I. Tsaklidis"
                    },
                    {
                        "name": "M. Uchida"
                    },
                    {
                        "name": "I. Ueda"
                    },
                    {
                        "name": "T. Uglov"
                    },
                    {
                        "name": "K. Unger"
                    },
                    {
                        "name": "Y. Unno"
                    },
                    {
                        "name": "K. Uno"
                    },
                    {
                        "name": "S. Uno"
                    },
                    {
                        "name": "Y. Ushiroda"
                    },
                    {
                        "name": "R. van Tonder"
                    },
                    {
                        "name": "K. E. Varvell"
                    },
                    {
                        "name": "M. Veronesi"
                    },
                    {
                        "name": "A. Vinokurova"
                    },
                    {
                        "name": "V. S. Vismaya"
                    },
                    {
                        "name": "L. Vitale"
                    },
                    {
                        "name": "R. Volpe"
                    },
                    {
                        "name": "A. Vossen"
                    },
                    {
                        "name": "S. Wallner"
                    },
                    {
                        "name": "M. -Z. Wang"
                    },
                    {
                        "name": "A. Warburton"
                    },
                    {
                        "name": "M. Watanabe"
                    },
                    {
                        "name": "S. Watanuki"
                    },
                    {
                        "name": "C. Wessel"
                    },
                    {
                        "name": "E. Won"
                    },
                    {
                        "name": "B. D. Yabsley"
                    },
                    {
                        "name": "S. Yamada"
                    },
                    {
                        "name": "W. Yan"
                    },
                    {
                        "name": "S. B. Yang"
                    },
                    {
                        "name": "J. Yelton"
                    },
                    {
                        "name": "J. H. Yin"
                    },
                    {
                        "name": "K. Yoshihara"
                    },
                    {
                        "name": "J. Yuan"
                    },
                    {
                        "name": "Y. Yusa"
                    },
                    {
                        "name": "L. Zani"
                    },
                    {
                        "name": "M. Zeyrek"
                    },
                    {
                        "name": "B. Zhang"
                    },
                    {
                        "name": "V. Zhilich"
                    },
                    {
                        "name": "J. S. Zhou"
                    },
                    {
                        "name": "Q. D. Zhou"
                    },
                    {
                        "name": "L. Zhu"
                    },
                    {
                        "name": "R. ≈Ωlebƒç√≠k"
                    }
                ],
                "author_detail": {
                    "name": "R. ≈Ωlebƒç√≠k"
                },
                "author": "R. ≈Ωlebƒç√≠k",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15867v1",
                "updated": "2025-04-22T13:09:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    9,
                    20,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:09:20Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    9,
                    20,
                    1,
                    112,
                    0
                ],
                "title": "Inducing Vulnerable Code Generation in LLM Coding Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inducing Vulnerable Code Generation in LLM Coding Assistants"
                },
                "summary": "Due to insufficient domain knowledge, LLM coding assistants often reference\nrelated solutions from the Internet to address programming problems. However,\nincorporating external information into LLMs' code generation process\nintroduces new security risks. In this paper, we reveal a real-world threat,\nnamed HACKODE, where attackers exploit referenced external information to embed\nattack sequences, causing LLMs to produce code with vulnerabilities such as\nbuffer overflows and incomplete validations. We designed a prototype of the\nattack, which generates effective attack sequences for potential diverse inputs\nwith various user queries and prompt templates. Through the evaluation on two\ngeneral LLMs and two code LLMs, we demonstrate that the attack is effective,\nachieving an 84.29% success rate. Additionally, on a real-world application,\nHACKODE achieves 75.92% ASR, demonstrating its real-world impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to insufficient domain knowledge, LLM coding assistants often reference\nrelated solutions from the Internet to address programming problems. However,\nincorporating external information into LLMs' code generation process\nintroduces new security risks. In this paper, we reveal a real-world threat,\nnamed HACKODE, where attackers exploit referenced external information to embed\nattack sequences, causing LLMs to produce code with vulnerabilities such as\nbuffer overflows and incomplete validations. We designed a prototype of the\nattack, which generates effective attack sequences for potential diverse inputs\nwith various user queries and prompt templates. Through the evaluation on two\ngeneral LLMs and two code LLMs, we demonstrate that the attack is effective,\nachieving an 84.29% success rate. Additionally, on a real-world application,\nHACKODE achieves 75.92% ASR, demonstrating its real-world impact."
                },
                "authors": [
                    {
                        "name": "Binqi Zeng"
                    },
                    {
                        "name": "Quan Zhang"
                    },
                    {
                        "name": "Chijin Zhou"
                    },
                    {
                        "name": "Gwihwan Go"
                    },
                    {
                        "name": "Yu Jiang"
                    },
                    {
                        "name": "Heyuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Heyuan Shi"
                },
                "author": "Heyuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15863v1",
                "updated": "2025-04-22T12:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    58,
                    5,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T12:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    58,
                    5,
                    1,
                    112,
                    0
                ],
                "title": "DERD-Net: Learning Depth from Event-based Ray Densities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DERD-Net: Learning Depth from Event-based Ray Densities"
                },
                "summary": "Event cameras offer a promising avenue for multi-view stereo depth estimation\nand Simultaneous Localization And Mapping (SLAM) due to their ability to detect\nblur-free 3D edges at high-speed and over broad illumination conditions.\nHowever, traditional deep learning frameworks designed for conventional cameras\nstruggle with the asynchronous, stream-like nature of event data, as their\narchitectures are optimized for discrete, image-like inputs. We propose a\nscalable, flexible and adaptable framework for pixel-wise depth estimation with\nevent cameras in both monocular and stereo setups. The 3D scene structure is\nencoded into disparity space images (DSIs), representing spatial densities of\nrays obtained by back-projecting events into space via known camera poses. Our\nneural network processes local subregions of the DSIs combining 3D convolutions\nand a recurrent structure to recognize valuable patterns for depth prediction.\nLocal processing enables fast inference with full parallelization and ensures\nconstant ultra-low model complexity and memory costs, regardless of camera\nresolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)\ndemonstrate unprecedented effectiveness: (i) using purely monocular data, our\nmethod achieves comparable results to existing stereo methods; (ii) when\napplied to stereo data, it strongly outperforms all state-of-the-art (SOTA)\napproaches, reducing the mean absolute error by at least 42%; (iii) our method\nalso allows for increases in depth completeness by more than 3-fold while still\nyielding a reduction in median absolute error of at least 30%. Given its\nremarkable performance and effective processing of event-data, our framework\nholds strong potential to become a standard approach for using deep learning\nfor event-based depth estimation and SLAM. Project page:\nhttps://github.com/tub-rip/DERD-Net",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras offer a promising avenue for multi-view stereo depth estimation\nand Simultaneous Localization And Mapping (SLAM) due to their ability to detect\nblur-free 3D edges at high-speed and over broad illumination conditions.\nHowever, traditional deep learning frameworks designed for conventional cameras\nstruggle with the asynchronous, stream-like nature of event data, as their\narchitectures are optimized for discrete, image-like inputs. We propose a\nscalable, flexible and adaptable framework for pixel-wise depth estimation with\nevent cameras in both monocular and stereo setups. The 3D scene structure is\nencoded into disparity space images (DSIs), representing spatial densities of\nrays obtained by back-projecting events into space via known camera poses. Our\nneural network processes local subregions of the DSIs combining 3D convolutions\nand a recurrent structure to recognize valuable patterns for depth prediction.\nLocal processing enables fast inference with full parallelization and ensures\nconstant ultra-low model complexity and memory costs, regardless of camera\nresolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)\ndemonstrate unprecedented effectiveness: (i) using purely monocular data, our\nmethod achieves comparable results to existing stereo methods; (ii) when\napplied to stereo data, it strongly outperforms all state-of-the-art (SOTA)\napproaches, reducing the mean absolute error by at least 42%; (iii) our method\nalso allows for increases in depth completeness by more than 3-fold while still\nyielding a reduction in median absolute error of at least 30%. Given its\nremarkable performance and effective processing of event-data, our framework\nholds strong potential to become a standard approach for using deep learning\nfor event-based depth estimation and SLAM. Project page:\nhttps://github.com/tub-rip/DERD-Net"
                },
                "authors": [
                    {
                        "name": "Diego de Oliveira Hitzges"
                    },
                    {
                        "name": "Suman Ghosh"
                    },
                    {
                        "name": "Guillermo Gallego"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Gallego"
                },
                "author": "Guillermo Gallego",
                "arxiv_comment": "13 pages, 3 figures, 14 tables. Project page:\n  https://github.com/tub-rip/DERD-Net",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15856v1",
                "updated": "2025-04-22T12:52:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    52,
                    11,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T12:52:11Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    52,
                    11,
                    1,
                    112,
                    0
                ],
                "title": "FailLite: Failure-Resilient Model Serving for Resource-Constrained Edge\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FailLite: Failure-Resilient Model Serving for Resource-Constrained Edge\n  Environments"
                },
                "summary": "Model serving systems have become popular for deploying deep learning models\nfor various latency-sensitive inference tasks. While traditional\nreplication-based methods have been used for failure-resilient model serving in\nthe cloud, such methods are often infeasible in edge environments due to\nsignificant resource constraints that preclude full replication. To address\nthis problem, this paper presents FailLite, a failure-resilient model serving\nsystem that employs (i) a heterogeneous replication where failover models are\nsmaller variants of the original model, (ii) an intelligent approach that uses\nwarm replicas to ensure quick failover for critical applications while using\ncold replicas, and (iii) progressive failover to provide low mean time to\nrecovery (MTTR) for the remaining applications. We implement a full prototype\nof our system and demonstrate its efficacy on an experimental edge testbed. Our\nresults using 27 models show that FailLite can recover all failed applications\nwith 175.5ms MTTR and only a 0.6% reduction in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model serving systems have become popular for deploying deep learning models\nfor various latency-sensitive inference tasks. While traditional\nreplication-based methods have been used for failure-resilient model serving in\nthe cloud, such methods are often infeasible in edge environments due to\nsignificant resource constraints that preclude full replication. To address\nthis problem, this paper presents FailLite, a failure-resilient model serving\nsystem that employs (i) a heterogeneous replication where failover models are\nsmaller variants of the original model, (ii) an intelligent approach that uses\nwarm replicas to ensure quick failover for critical applications while using\ncold replicas, and (iii) progressive failover to provide low mean time to\nrecovery (MTTR) for the remaining applications. We implement a full prototype\nof our system and demonstrate its efficacy on an experimental edge testbed. Our\nresults using 27 models show that FailLite can recover all failed applications\nwith 175.5ms MTTR and only a 0.6% reduction in accuracy."
                },
                "authors": [
                    {
                        "name": "Li Wu"
                    },
                    {
                        "name": "Walid A. Hanafy"
                    },
                    {
                        "name": "Tarek Abdelzaher"
                    },
                    {
                        "name": "David Irwin"
                    },
                    {
                        "name": "Jesse Milzman"
                    },
                    {
                        "name": "Prashant Shenoy"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Shenoy"
                },
                "author": "Prashant Shenoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15854v1",
                "updated": "2025-04-22T12:51:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    51,
                    7,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T12:51:07Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    51,
                    7,
                    1,
                    112,
                    0
                ],
                "title": "Consistent Causal Inference of Group Effects in Non-Targeted Trials with\n  Finitely Many Effect Levels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistent Causal Inference of Group Effects in Non-Targeted Trials with\n  Finitely Many Effect Levels"
                },
                "summary": "A treatment may be appropriate for some group (the ``sick\" group) on whom it\nhas a positive effect, but it can also have a detrimental effect on subjects\nfrom another group (the ``healthy\" group). In a non-targeted trial both sick\nand healthy subjects may be treated, producing heterogeneous effects within the\ntreated group. Inferring the correct treatment effect on the sick population is\nthen difficult, because the effects on the different groups get tangled. We\npropose an efficient nonparametric approach to estimating the group effects,\ncalled {\\bf PCM} (pre-cluster and merge). We prove its asymptotic consistency\nin a general setting and show, on synthetic data, more than a 10x improvement\nin accuracy over existing state-of-the-art. Our approach applies more generally\nto consistent estimation of functions with a finite range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A treatment may be appropriate for some group (the ``sick\" group) on whom it\nhas a positive effect, but it can also have a detrimental effect on subjects\nfrom another group (the ``healthy\" group). In a non-targeted trial both sick\nand healthy subjects may be treated, producing heterogeneous effects within the\ntreated group. Inferring the correct treatment effect on the sick population is\nthen difficult, because the effects on the different groups get tangled. We\npropose an efficient nonparametric approach to estimating the group effects,\ncalled {\\bf PCM} (pre-cluster and merge). We prove its asymptotic consistency\nin a general setting and show, on synthetic data, more than a 10x improvement\nin accuracy over existing state-of-the-art. Our approach applies more generally\nto consistent estimation of functions with a finite range."
                },
                "authors": [
                    {
                        "name": "Georgios Mavroudeas"
                    },
                    {
                        "name": "Malik Magdon-Ismail"
                    },
                    {
                        "name": "Kristin P. Bennett"
                    },
                    {
                        "name": "Jason Kuruzovich"
                    }
                ],
                "author_detail": {
                    "name": "Jason Kuruzovich"
                },
                "author": "Jason Kuruzovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15848v1",
                "updated": "2025-04-22T12:43:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    43,
                    37,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T12:43:37Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    43,
                    37,
                    1,
                    112,
                    0
                ],
                "title": "Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based\n  Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based\n  Sentiment Analysis"
                },
                "summary": "Multimodal aspect-based sentiment classification (MASC) is an emerging task\ndue to an increase in user-generated multimodal content on social platforms,\naimed at predicting sentiment polarity toward specific aspect targets (i.e.,\nentities or attributes explicitly mentioned in text-image pairs). Despite\nextensive efforts and significant achievements in existing MASC, substantial\ngaps remain in understanding fine-grained visual content and the cognitive\nrationales derived from semantic content and impressions (cognitive\ninterpretations of emotions evoked by image content). In this study, we present\nChimera: a cognitive and aesthetic sentiment causality understanding framework\nto derive fine-grained holistic features of aspects and infer the fundamental\ndrivers of sentiment expression from both semantic perspectives and\naffective-cognitive resonance (the synergistic effect between emotional\nresponses and cognitive interpretations). Specifically, this framework first\nincorporates visual patch features for patch-word alignment. Meanwhile, it\nextracts coarse-grained visual features (e.g., overall image representation)\nand fine-grained visual regions (e.g., aspect-related regions) and translates\nthem into corresponding textual descriptions (e.g., facial, aesthetic).\nFinally, we leverage the sentimental causes and impressions generated by a\nlarge language model (LLM) to enhance the model's awareness of sentimental cues\nevoked by semantic content and affective-cognitive resonance. Experimental\nresults on standard MASC datasets demonstrate the effectiveness of the proposed\nmodel, which also exhibits greater flexibility to MASC compared to LLMs such as\nGPT-4o. We have publicly released the complete implementation and dataset at\nhttps://github.com/Xillv/Chimera",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal aspect-based sentiment classification (MASC) is an emerging task\ndue to an increase in user-generated multimodal content on social platforms,\naimed at predicting sentiment polarity toward specific aspect targets (i.e.,\nentities or attributes explicitly mentioned in text-image pairs). Despite\nextensive efforts and significant achievements in existing MASC, substantial\ngaps remain in understanding fine-grained visual content and the cognitive\nrationales derived from semantic content and impressions (cognitive\ninterpretations of emotions evoked by image content). In this study, we present\nChimera: a cognitive and aesthetic sentiment causality understanding framework\nto derive fine-grained holistic features of aspects and infer the fundamental\ndrivers of sentiment expression from both semantic perspectives and\naffective-cognitive resonance (the synergistic effect between emotional\nresponses and cognitive interpretations). Specifically, this framework first\nincorporates visual patch features for patch-word alignment. Meanwhile, it\nextracts coarse-grained visual features (e.g., overall image representation)\nand fine-grained visual regions (e.g., aspect-related regions) and translates\nthem into corresponding textual descriptions (e.g., facial, aesthetic).\nFinally, we leverage the sentimental causes and impressions generated by a\nlarge language model (LLM) to enhance the model's awareness of sentimental cues\nevoked by semantic content and affective-cognitive resonance. Experimental\nresults on standard MASC datasets demonstrate the effectiveness of the proposed\nmodel, which also exhibits greater flexibility to MASC compared to LLMs such as\nGPT-4o. We have publicly released the complete implementation and dataset at\nhttps://github.com/Xillv/Chimera"
                },
                "authors": [
                    {
                        "name": "Luwei Xiao"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Yanhao Jia"
                    },
                    {
                        "name": "Liang He"
                    },
                    {
                        "name": "Erik Cambria"
                    }
                ],
                "author_detail": {
                    "name": "Erik Cambria"
                },
                "author": "Erik Cambria",
                "arxiv_comment": "Accepted by TAFFC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15843v1",
                "updated": "2025-04-22T12:39:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    39,
                    30,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T12:39:30Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    39,
                    30,
                    1,
                    112,
                    0
                ],
                "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model"
                },
                "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data."
                },
                "authors": [
                    {
                        "name": "Junshu Pan"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Shulin Huang"
                    },
                    {
                        "name": "Qiji Zhou"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10852v2",
                "updated": "2025-04-22T12:31:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    31,
                    32,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-15T04:21:50Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    4,
                    21,
                    50,
                    1,
                    105,
                    0
                ],
                "title": "Enhancing Features in Long-tailed Data Using Large Vision Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Features in Long-tailed Data Using Large Vision Model"
                },
                "summary": "Language-based foundation models, such as large language models (LLMs) or\nlarge vision-language models (LVLMs), have been widely studied in long-tailed\nrecognition. However, the need for linguistic data is not applicable to all\npractical tasks. In this study, we aim to explore using large vision models\n(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features\nwithout any language information. Specifically, we extract features from the\nLVM and fuse them with features in the baseline network's map and latent space\nto obtain the augmented features. Moreover, we design several prototype-based\nlosses in the latent space to further exploit the potential of the augmented\nfeatures. In the experimental section, we validate our approach on two\nbenchmark datasets: ImageNet-LT and iNaturalist2018.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-based foundation models, such as large language models (LLMs) or\nlarge vision-language models (LVLMs), have been widely studied in long-tailed\nrecognition. However, the need for linguistic data is not applicable to all\npractical tasks. In this study, we aim to explore using large vision models\n(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features\nwithout any language information. Specifically, we extract features from the\nLVM and fuse them with features in the baseline network's map and latent space\nto obtain the augmented features. Moreover, we design several prototype-based\nlosses in the latent space to further exploit the potential of the augmented\nfeatures. In the experimental section, we validate our approach on two\nbenchmark datasets: ImageNet-LT and iNaturalist2018."
                },
                "authors": [
                    {
                        "name": "Pengxiao Han"
                    },
                    {
                        "name": "Changkun Ye"
                    },
                    {
                        "name": "Jinguang Tong"
                    },
                    {
                        "name": "Cuicui Jiang"
                    },
                    {
                        "name": "Jie Hong"
                    },
                    {
                        "name": "Li Fang"
                    },
                    {
                        "name": "Xuesong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuesong Li"
                },
                "author": "Xuesong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15822v1",
                "updated": "2025-04-22T12:09:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    9,
                    3,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T12:09:03Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    9,
                    3,
                    1,
                    112,
                    0
                ],
                "title": "Quantifying Source Speaker Leakage in One-to-One Voice Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Source Speaker Leakage in One-to-One Voice Conversion"
                },
                "summary": "Using a multi-accented corpus of parallel utterances for use with commercial\nspeech devices, we present a case study to show that it is possible to quantify\na degree of confidence about a source speaker's identity in the case of\none-to-one voice conversion. Following voice conversion using a HiFi-GAN\nvocoder, we compare information leakage for a range speaker characteristics;\nassuming a \"worst-case\" white-box scenario, we quantify our confidence to\nperform inference and narrow the pool of likely source speakers, reinforcing\nthe regulatory obligation and moral duty that providers of synthetic voices\nhave to ensure the privacy of their speakers' data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using a multi-accented corpus of parallel utterances for use with commercial\nspeech devices, we present a case study to show that it is possible to quantify\na degree of confidence about a source speaker's identity in the case of\none-to-one voice conversion. Following voice conversion using a HiFi-GAN\nvocoder, we compare information leakage for a range speaker characteristics;\nassuming a \"worst-case\" white-box scenario, we quantify our confidence to\nperform inference and narrow the pool of likely source speakers, reinforcing\nthe regulatory obligation and moral duty that providers of synthetic voices\nhave to ensure the privacy of their speakers' data."
                },
                "authors": [
                    {
                        "name": "Scott Wellington"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Junichi Yamagishi"
                    }
                ],
                "author_detail": {
                    "name": "Junichi Yamagishi"
                },
                "author": "Junichi Yamagishi",
                "arxiv_doi": "10.1109/BIOSIG61931.2024.10786731",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/BIOSIG61931.2024.10786731",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at IEEE 23rd International Conference of the Biometrics\n  Special Interest Group (BIOSIG 2024)",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03101v2",
                "updated": "2025-04-22T11:58:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    58,
                    54,
                    1,
                    112,
                    0
                ],
                "published": "2024-08-06T11:04:37Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    11,
                    4,
                    37,
                    1,
                    219,
                    0
                ],
                "title": "LogUpdater: Automated Detection and Repair of Specific Defects in\n  Logging Statements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogUpdater: Automated Detection and Repair of Specific Defects in\n  Logging Statements"
                },
                "summary": "Developers use logging statements to track software runtime behaviors and\nsystem status. Yet, unclear or misleading logs can hide true execution patterns\nand hinder software maintenance. Current research on logging statement issues\nis limited, often only spotting one defect type and relying on manual\ncorrections instead of automation. To bridge this gap, we conduct a study to\nidentify four logging statement defect types by analyzing log-centric changes.\nThen we introduce LogUpdater, a two-stage framework for automatically detecting\nand updating these log defects. In the offline phase, LogUpdater builds a\nclassifier using synthetic defective logs to spot defect types. During online\ntesting, this classifier assesses if and how logs in code snippets need\nimprovement. LogUpdater then uses type-aware prompts from past logging updates\nto suggest fixes via a recommendation framework based on LLMs. Results show\nstrong defect detection with an F1 score of 0.625. It also greatly improves\nstatic text and dynamic variable suggestions by 48.12% and 24.90%,\nrespectively. LogUpdater successfully recommends updates 61.49% of the time on\nnew projects. We reported 40 problematic logs and their fixes on GitHub,\nleading to 25 merged changes across 11 projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developers use logging statements to track software runtime behaviors and\nsystem status. Yet, unclear or misleading logs can hide true execution patterns\nand hinder software maintenance. Current research on logging statement issues\nis limited, often only spotting one defect type and relying on manual\ncorrections instead of automation. To bridge this gap, we conduct a study to\nidentify four logging statement defect types by analyzing log-centric changes.\nThen we introduce LogUpdater, a two-stage framework for automatically detecting\nand updating these log defects. In the offline phase, LogUpdater builds a\nclassifier using synthetic defective logs to spot defect types. During online\ntesting, this classifier assesses if and how logs in code snippets need\nimprovement. LogUpdater then uses type-aware prompts from past logging updates\nto suggest fixes via a recommendation framework based on LLMs. Results show\nstrong defect detection with an F1 score of 0.625. It also greatly improves\nstatic text and dynamic variable suggestions by 48.12% and 24.90%,\nrespectively. LogUpdater successfully recommends updates 61.49% of the time on\nnew projects. We reported 40 problematic logs and their fixes on GitHub,\nleading to 25 merged changes across 11 projects."
                },
                "authors": [
                    {
                        "name": "Renyi Zhong"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Jinxi Kuang"
                    },
                    {
                        "name": "Wenwei Gu"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_doi": "10.1145/3731754",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731754",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.03101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14325v2",
                "updated": "2025-04-22T11:56:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    56,
                    45,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-19T15:29:04Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    15,
                    29,
                    4,
                    5,
                    109,
                    0
                ],
                "title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory"
                },
                "summary": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents."
                },
                "authors": [
                    {
                        "name": "Alessio Buscemi"
                    },
                    {
                        "name": "Daniele Proverbio"
                    },
                    {
                        "name": "Alessandro Di Stefano"
                    },
                    {
                        "name": "The Anh Han"
                    },
                    {
                        "name": "German Castignani"
                    },
                    {
                        "name": "Pietro Li√≤"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Li√≤"
                },
                "author": "Pietro Li√≤",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15804v1",
                "updated": "2025-04-22T11:38:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    38,
                    14,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T11:38:14Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    38,
                    14,
                    1,
                    112,
                    0
                ],
                "title": "Insights from Verification: Training a Verilog Generation LLM with\n  Reinforcement Learning with Testbench Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights from Verification: Training a Verilog Generation LLM with\n  Reinforcement Learning with Testbench Feedback"
                },
                "summary": "Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B."
                },
                "authors": [
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Bingkun Yao"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15801v1",
                "updated": "2025-04-22T11:31:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    31,
                    50,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T11:31:50Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    31,
                    50,
                    1,
                    112,
                    0
                ],
                "title": "A closer look at how large language models trust humans: patterns and\n  biases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A closer look at how large language models trust humans: patterns and\n  biases"
                },
                "summary": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI."
                },
                "authors": [
                    {
                        "name": "Valeria Lerman"
                    },
                    {
                        "name": "Yaniv Dover"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Dover"
                },
                "author": "Yaniv Dover",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11761v2",
                "updated": "2025-04-22T11:24:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    24,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-16T13:25:42Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    25,
                    42,
                    0,
                    351,
                    0
                ],
                "title": "Harnessing Language for Coordination: A Framework and Benchmark for\n  LLM-Driven Multi-Agent Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Language for Coordination: A Framework and Benchmark for\n  LLM-Driven Multi-Agent Control"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. Their potential to facilitate human coordination with many\nagents is a promising but largely under-explored area. Such capabilities would\nbe helpful in disaster response, urban planning, and real-time strategy\nscenarios. In this work, we introduce (1) a real-time strategy game benchmark\ndesigned to evaluate these abilities and (2) a novel framework we term HIVE.\nHIVE empowers a single human to coordinate swarms of up to 2,000 agents through\na natural language dialog with an LLM. We present promising results on this\nmulti-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. Our findings also\nhighlight critical limitations of current models, including difficulties in\nprocessing spatial visual information and challenges in formulating long-term\nstrategic plans. This work sheds light on the potential and limitations of LLMs\nin human-swarm coordination, paving the way for future research in this area.\nThe HIVE project page, hive.syrkis.com, includes videos of the system in\naction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. Their potential to facilitate human coordination with many\nagents is a promising but largely under-explored area. Such capabilities would\nbe helpful in disaster response, urban planning, and real-time strategy\nscenarios. In this work, we introduce (1) a real-time strategy game benchmark\ndesigned to evaluate these abilities and (2) a novel framework we term HIVE.\nHIVE empowers a single human to coordinate swarms of up to 2,000 agents through\na natural language dialog with an LLM. We present promising results on this\nmulti-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. Our findings also\nhighlight critical limitations of current models, including difficulties in\nprocessing spatial visual information and challenges in formulating long-term\nstrategic plans. This work sheds light on the potential and limitations of LLMs\nin human-swarm coordination, paving the way for future research in this area.\nThe HIVE project page, hive.syrkis.com, includes videos of the system in\naction."
                },
                "authors": [
                    {
                        "name": "Timoth√©e Anne"
                    },
                    {
                        "name": "Noah Syrkis"
                    },
                    {
                        "name": "Meriem Elhosni"
                    },
                    {
                        "name": "Florian Turati"
                    },
                    {
                        "name": "Franck Legendre"
                    },
                    {
                        "name": "Alain Jaquier"
                    },
                    {
                        "name": "Sebastian Risi"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Risi"
                },
                "author": "Sebastian Risi",
                "arxiv_doi": "10.1109/TG.2025.3564042",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TG.2025.3564042",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.11761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06276v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06276v4",
                "updated": "2025-04-22T11:18:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    18,
                    16,
                    1,
                    112,
                    0
                ],
                "published": "2024-08-12T16:39:03Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    3,
                    0,
                    225,
                    0
                ],
                "title": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Buru Chang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_doi": "10.1145/3726302.3730055",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730055",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06276v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06276v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12915v2",
                "updated": "2025-04-22T11:11:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    11,
                    50,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-17T13:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    5,
                    14,
                    3,
                    107,
                    0
                ],
                "title": "ConExion: Concept Extraction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConExion: Concept Extraction with Large Language Models"
                },
                "summary": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction."
                },
                "authors": [
                    {
                        "name": "Ebrahim Norouzi"
                    },
                    {
                        "name": "Sven Hertling"
                    },
                    {
                        "name": "Harald Sack"
                    }
                ],
                "author_detail": {
                    "name": "Harald Sack"
                },
                "author": "Harald Sack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15045v2",
                "updated": "2025-04-22T11:07:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    7,
                    54,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-19T09:33:44Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    33,
                    44,
                    2,
                    78,
                    0
                ],
                "title": "Semiparametric plug-in estimation, sup-norm risk bounds, marginal\n  optimization, and inference in BTL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric plug-in estimation, sup-norm risk bounds, marginal\n  optimization, and inference in BTL model"
                },
                "summary": "The recent paper \\cite{GSZ2023} on estimation and inference for top-ranking\nproblem in Bradley-Terry-Lice (BTL) model presented a surprising result:\ncomponent-wise estimation and inference can be done under much weaker\nconditions on the number of comparison then it is required for the full\ndimensional estimation. The present paper revisits this finding from completely\ndifferent viewpoint. Namely, we show how a theoretical study of\n\\emph{estimation in sup-norm} can be reduced to the analysis of \\emph{plug-in\nsemiparametric estimation}. For the latter, we adopt and extend the general\napproach from \\cite{Sp2024} to high-dimensional estimation and inference. The\nmain tool of the analysis is a theory of \\emph{perturbed marginal optimization}\nwhen an objective function depends on a low-dimensional target parameter along\nwith a high-dimensional nuisance parameter. A particular focus of the study is\nthe critical dimension condition. Full-dimensional estimation requires in\ngeneral the condition \\( \\mathbbmsl{N} \\gg \\mathbb{p} \\) between the effective\nparameter dimension \\( \\mathbb{p} \\) and the effective sample size \\(\n\\mathbbmsl{N} \\) corresponding to the smallest eigenvalue of the Fisher\ninformation matrix \\( \\mathbbmsl{F} \\). Inference on the estimated parameter is\neven more demanding: the condition \\( \\mathbbmsl{N} \\gg \\mathbb{p}^{2} \\)\ncannot be generally avoided; see \\cite{Sp2024}. However, for the sup-norm\nestimation, the critical dimension condition can be reduced to \\( \\mathbbmsl{N}\n\\geq C \\log p \\).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent paper \\cite{GSZ2023} on estimation and inference for top-ranking\nproblem in Bradley-Terry-Lice (BTL) model presented a surprising result:\ncomponent-wise estimation and inference can be done under much weaker\nconditions on the number of comparison then it is required for the full\ndimensional estimation. The present paper revisits this finding from completely\ndifferent viewpoint. Namely, we show how a theoretical study of\n\\emph{estimation in sup-norm} can be reduced to the analysis of \\emph{plug-in\nsemiparametric estimation}. For the latter, we adopt and extend the general\napproach from \\cite{Sp2024} to high-dimensional estimation and inference. The\nmain tool of the analysis is a theory of \\emph{perturbed marginal optimization}\nwhen an objective function depends on a low-dimensional target parameter along\nwith a high-dimensional nuisance parameter. A particular focus of the study is\nthe critical dimension condition. Full-dimensional estimation requires in\ngeneral the condition \\( \\mathbbmsl{N} \\gg \\mathbb{p} \\) between the effective\nparameter dimension \\( \\mathbb{p} \\) and the effective sample size \\(\n\\mathbbmsl{N} \\) corresponding to the smallest eigenvalue of the Fisher\ninformation matrix \\( \\mathbbmsl{F} \\). Inference on the estimated parameter is\neven more demanding: the condition \\( \\mathbbmsl{N} \\gg \\mathbb{p}^{2} \\)\ncannot be generally avoided; see \\cite{Sp2024}. However, for the sup-norm\nestimation, the critical dimension condition can be reduced to \\( \\mathbbmsl{N}\n\\geq C \\log p \\)."
                },
                "authors": [
                    {
                        "name": "Vladimir Spokoiny"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Spokoiny"
                },
                "author": "Vladimir Spokoiny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F10, 62E17, 62J12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15785v1",
                "updated": "2025-04-22T10:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    58,
                    27,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T10:58:27Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    58,
                    27,
                    1,
                    112,
                    0
                ],
                "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents"
                },
                "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations."
                },
                "authors": [
                    {
                        "name": "Siyu Zhou"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Guodong Long"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Jing Jiang"
                    },
                    {
                        "name": "Chengqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chengqi Zhang"
                },
                "author": "Chengqi Zhang",
                "arxiv_comment": "Code is available at https://github.com/elated-sawyer/WALL-E",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15784v1",
                "updated": "2025-04-22T10:52:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    52,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T10:52:23Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    52,
                    23,
                    1,
                    112,
                    0
                ],
                "title": "Automated Creativity Evaluation for Large Language Models: A\n  Reference-Based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Creativity Evaluation for Large Language Models: A\n  Reference-Based Approach"
                },
                "summary": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%)."
                },
                "authors": [
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Chiwei Zhu"
                    },
                    {
                        "name": "Benfeng Xu"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15781v1",
                "updated": "2025-04-22T10:45:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    45,
                    58,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T10:45:58Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    45,
                    58,
                    1,
                    112,
                    0
                ],
                "title": "Interacting Immediate Neighbour Interpolation for Geoscientific Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interacting Immediate Neighbour Interpolation for Geoscientific Data"
                },
                "summary": "A diverse range of interpolation methods, including Kriging, spline/minimum\ncurvature and radial basis function interpolation exist for interpolating\nspatially incomplete geoscientific data. Such methods use various spatial\nproperties of the observed data to infer its local and global behaviour. In\nthis study, we exploit the adaptability of locally interacting systems from\nstatistical physics and develop an interpolation framework for numerical\ngeoscientific data called Interacting Immediate Neighbour Interpolation (IINI),\nwhich solely relies on local and immediate neighbour correlations. In the IINI\nmethod, medium-to-long range correlations are constructed from the collective\nlocal interactions of grid centroids. To demonstrate the functionality and\nstrengths of IINI, we apply our methodology to the interpolation of ground\ngravity, airborne magnetic and airborne radiometric datasets. We further\ncompare the performance of IINI to conventional methods such as minimum\ncurvature surface fitting. Results show that IINI is competitive with\nconventional interpolation techniques in terms of validation accuracy, while\nbeing significantly simpler in terms of algorithmic complexity and data\npre-processing requirements. IINI demonstrates the broader applicability of\nstatistical physics concepts within the field of geostatistics, highlighting\ntheir potential to enrich and expand traditional geostatistical methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A diverse range of interpolation methods, including Kriging, spline/minimum\ncurvature and radial basis function interpolation exist for interpolating\nspatially incomplete geoscientific data. Such methods use various spatial\nproperties of the observed data to infer its local and global behaviour. In\nthis study, we exploit the adaptability of locally interacting systems from\nstatistical physics and develop an interpolation framework for numerical\ngeoscientific data called Interacting Immediate Neighbour Interpolation (IINI),\nwhich solely relies on local and immediate neighbour correlations. In the IINI\nmethod, medium-to-long range correlations are constructed from the collective\nlocal interactions of grid centroids. To demonstrate the functionality and\nstrengths of IINI, we apply our methodology to the interpolation of ground\ngravity, airborne magnetic and airborne radiometric datasets. We further\ncompare the performance of IINI to conventional methods such as minimum\ncurvature surface fitting. Results show that IINI is competitive with\nconventional interpolation techniques in terms of validation accuracy, while\nbeing significantly simpler in terms of algorithmic complexity and data\npre-processing requirements. IINI demonstrates the broader applicability of\nstatistical physics concepts within the field of geostatistics, highlighting\ntheir potential to enrich and expand traditional geostatistical methods."
                },
                "authors": [
                    {
                        "name": "Arya Kimiaghalam"
                    },
                    {
                        "name": "Andrei Swidinsky"
                    },
                    {
                        "name": "Mohammad Parsa"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Parsa"
                },
                "author": "Mohammad Parsa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15780v1",
                "updated": "2025-04-22T10:45:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    45,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T10:45:23Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    45,
                    23,
                    1,
                    112,
                    0
                ],
                "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving"
                },
                "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen"
                },
                "authors": [
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Zijun Chen"
                    },
                    {
                        "name": "Renqiu Xia"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Hongbin Zhou"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Shiyang Feng"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15771v1",
                "updated": "2025-04-22T10:28:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    28,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T10:28:23Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    28,
                    23,
                    1,
                    112,
                    0
                ],
                "title": "Grounded in Context: Retrieval-Based Method for Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded in Context: Retrieval-Based Method for Hallucination Detection"
                },
                "summary": "Despite advancements in grounded content generation, production Large\nLanguage Models (LLMs) based applications still suffer from hallucinated\nanswers. We present \"Grounded in Context\" - Deepchecks' hallucination detection\nframework, designed for production-scale long-context data and tailored to\ndiverse use cases, including summarization, data extraction, and RAG. Inspired\nby RAG architecture, our method integrates retrieval and Natural Language\nInference (NLI) models to predict factual consistency between premises and\nhypotheses using an encoder-based model with only a 512-token context window.\nOur framework identifies unsupported claims with an F1 score of 0.83 in\nRAGTruth's response-level classification task, matching methods that trained on\nthe dataset, and outperforming all comparable frameworks using similar-sized\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in grounded content generation, production Large\nLanguage Models (LLMs) based applications still suffer from hallucinated\nanswers. We present \"Grounded in Context\" - Deepchecks' hallucination detection\nframework, designed for production-scale long-context data and tailored to\ndiverse use cases, including summarization, data extraction, and RAG. Inspired\nby RAG architecture, our method integrates retrieval and Natural Language\nInference (NLI) models to predict factual consistency between premises and\nhypotheses using an encoder-based model with only a 512-token context window.\nOur framework identifies unsupported claims with an F1 score of 0.83 in\nRAGTruth's response-level classification task, matching methods that trained on\nthe dataset, and outperforming all comparable frameworks using similar-sized\nmodels."
                },
                "authors": [
                    {
                        "name": "Assaf Gerner"
                    },
                    {
                        "name": "Netta Madvil"
                    },
                    {
                        "name": "Nadav Barak"
                    },
                    {
                        "name": "Alex Zaikman"
                    },
                    {
                        "name": "Jonatan Liberman"
                    },
                    {
                        "name": "Liron Hamra"
                    },
                    {
                        "name": "Rotem Brazilay"
                    },
                    {
                        "name": "Shay Tsadok"
                    },
                    {
                        "name": "Yaron Friedman"
                    },
                    {
                        "name": "Neal Harow"
                    },
                    {
                        "name": "Noam Bresler"
                    },
                    {
                        "name": "Shir Chorev"
                    },
                    {
                        "name": "Philip Tannor"
                    }
                ],
                "author_detail": {
                    "name": "Philip Tannor"
                },
                "author": "Philip Tannor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15993v2",
                "updated": "2025-04-22T10:20:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    20,
                    16,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-20T15:41:47Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    41,
                    47,
                    4,
                    355,
                    0
                ],
                "title": "Fearful Falcons and Angry Llamas: Emotion Category Annotations of\n  Arguments by Humans and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fearful Falcons and Angry Llamas: Emotion Category Annotations of\n  Arguments by Humans and LLMs"
                },
                "summary": "Arguments evoke emotions, influencing the effect of the argument itself. Not\nonly the emotional intensity but also the category influence the argument's\neffects, for instance, the willingness to adapt stances. While binary\nemotionality has been studied in arguments, there is no work on discrete\nemotion categories (e.g., \"Anger\") in such data. To fill this gap, we\ncrowdsource subjective annotations of emotion categories in a German argument\ncorpus and evaluate automatic LLM-based labeling methods. Specifically, we\ncompare three prompting strategies (zero-shot, one-shot, chain-of-thought) on\nthree large instruction-tuned language models (Falcon-7b-instruct,\nLlama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the\noutput space to be binary (is there emotionality in the argument?),\nclosed-domain (which emotion from a given label set is in the argument?), or\nopen-domain (which emotion is in the argument?). We find that emotion\ncategories enhance the prediction of emotionality in arguments, emphasizing the\nneed for discrete emotion annotations in arguments. Across all prompt settings\nand models, automatic predictions show a high recall but low precision for\npredicting anger and fear, indicating a strong bias toward negative emotions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arguments evoke emotions, influencing the effect of the argument itself. Not\nonly the emotional intensity but also the category influence the argument's\neffects, for instance, the willingness to adapt stances. While binary\nemotionality has been studied in arguments, there is no work on discrete\nemotion categories (e.g., \"Anger\") in such data. To fill this gap, we\ncrowdsource subjective annotations of emotion categories in a German argument\ncorpus and evaluate automatic LLM-based labeling methods. Specifically, we\ncompare three prompting strategies (zero-shot, one-shot, chain-of-thought) on\nthree large instruction-tuned language models (Falcon-7b-instruct,\nLlama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the\noutput space to be binary (is there emotionality in the argument?),\nclosed-domain (which emotion from a given label set is in the argument?), or\nopen-domain (which emotion is in the argument?). We find that emotion\ncategories enhance the prediction of emotionality in arguments, emphasizing the\nneed for discrete emotion annotations in arguments. Across all prompt settings\nand models, automatic predictions show a high recall but low precision for\npredicting anger and fear, indicating a strong bias toward negative emotions."
                },
                "authors": [
                    {
                        "name": "Lynn Greschner"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "arxiv_comment": "accepted to NLP4DH 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15756v1",
                "updated": "2025-04-22T10:09:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    9,
                    33,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T10:09:33Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    9,
                    33,
                    1,
                    112,
                    0
                ],
                "title": "DSDNet: Raw Domain Demoir√©ing via Dual Color-Space Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSDNet: Raw Domain Demoir√©ing via Dual Color-Space Synergy"
                },
                "summary": "With the rapid advancement of mobile imaging, capturing screens using\nsmartphones has become a prevalent practice in distance learning and conference\nrecording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain\ndemoir\\'eing methods struggle with irreversible information loss, while recent\ntwo-stage raw domain approaches suffer from information bottlenecks and\ninference inefficiency. To address these limitations, we propose a single-stage\nraw domain demoir\\'eing framework, Dual-Stream Demoir\\'eing Network (DSDNet),\nwhich leverages the synergy of raw and YCbCr images to remove moir\\'e while\npreserving luminance and color fidelity. Specifically, to guide luminance\ncorrection and moir\\'e removal, we design a raw-to-YCbCr mapping pipeline and\nintroduce the Synergic Attention with Dynamic Modulation (SADM) module. This\nmodule enriches the raw-to-sRGB conversion with cross-domain contextual\nfeatures. Furthermore, to better guide color fidelity, we develop a\nLuminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance\nand chrominance representations. Extensive experiments demonstrate that DSDNet\noutperforms state-of-the-art methods in both visual quality and quantitative\nevaluation, and achieves an inference speed $\\mathrm{\\textbf{2.4x}}$ faster\nthan the second-best method, highlighting its practical advantages. We provide\nan anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of mobile imaging, capturing screens using\nsmartphones has become a prevalent practice in distance learning and conference\nrecording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain\ndemoir\\'eing methods struggle with irreversible information loss, while recent\ntwo-stage raw domain approaches suffer from information bottlenecks and\ninference inefficiency. To address these limitations, we propose a single-stage\nraw domain demoir\\'eing framework, Dual-Stream Demoir\\'eing Network (DSDNet),\nwhich leverages the synergy of raw and YCbCr images to remove moir\\'e while\npreserving luminance and color fidelity. Specifically, to guide luminance\ncorrection and moir\\'e removal, we design a raw-to-YCbCr mapping pipeline and\nintroduce the Synergic Attention with Dynamic Modulation (SADM) module. This\nmodule enriches the raw-to-sRGB conversion with cross-domain contextual\nfeatures. Furthermore, to better guide color fidelity, we develop a\nLuminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance\nand chrominance representations. Extensive experiments demonstrate that DSDNet\noutperforms state-of-the-art methods in both visual quality and quantitative\nevaluation, and achieves an inference speed $\\mathrm{\\textbf{2.4x}}$ faster\nthan the second-best method, highlighting its practical advantages. We provide\nan anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/."
                },
                "authors": [
                    {
                        "name": "Qirui Yang"
                    },
                    {
                        "name": "Fangpu Zhang"
                    },
                    {
                        "name": "Yeying Jin"
                    },
                    {
                        "name": "Qihua Cheng"
                    },
                    {
                        "name": "Pengtao Jiang"
                    },
                    {
                        "name": "Huanjing Yue"
                    },
                    {
                        "name": "Jingyu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyu Yang"
                },
                "author": "Jingyu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14286v2",
                "updated": "2025-04-22T10:07:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    7,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-19T13:06:03Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    13,
                    6,
                    3,
                    5,
                    109,
                    0
                ],
                "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement\n  Learning on LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement\n  Learning on LLM"
                },
                "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and\nDeepSeek's R1, highlight the significant potential of Reinforcement Learning\n(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).\nHowever, replicating these advancements across diverse domains remains\nchallenging due to limited methodological transparency. In this work, we\npresent two-Staged history-Resampling Policy Optimization (SRPO), which\nsurpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and\nLiveCodeBench benchmarks. SRPO achieves this using the same base model as\nDeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps\nrequired by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building\nupon Group Relative Policy Optimization (GRPO), we introduce two key\nmethodological innovations: (1) a two-stage cross-domain training paradigm\ndesigned to balance the development of mathematical reasoning and coding\nproficiency, and (2) History Resampling (HR), a technique to address\nineffective samples. Our comprehensive experiments validate the effectiveness\nof our approach, offering valuable insights into scaling LLM reasoning\ncapabilities across diverse tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances of reasoning models, exemplified by OpenAI's o1 and\nDeepSeek's R1, highlight the significant potential of Reinforcement Learning\n(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).\nHowever, replicating these advancements across diverse domains remains\nchallenging due to limited methodological transparency. In this work, we\npresent two-Staged history-Resampling Policy Optimization (SRPO), which\nsurpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and\nLiveCodeBench benchmarks. SRPO achieves this using the same base model as\nDeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps\nrequired by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building\nupon Group Relative Policy Optimization (GRPO), we introduce two key\nmethodological innovations: (1) a two-stage cross-domain training paradigm\ndesigned to balance the development of mathematical reasoning and coding\nproficiency, and (2) History Resampling (HR), a technique to address\nineffective samples. Our comprehensive experiments validate the effectiveness\nof our approach, offering valuable insights into scaling LLM reasoning\ncapabilities across diverse tasks."
                },
                "authors": [
                    {
                        "name": "Xiaojiang Zhang"
                    },
                    {
                        "name": "Jinghui Wang"
                    },
                    {
                        "name": "Zifei Cheng"
                    },
                    {
                        "name": "Wenhao Zhuang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Minglei Zhang"
                    },
                    {
                        "name": "Shaojie Wang"
                    },
                    {
                        "name": "Yinghan Cui"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Junyi Peng"
                    },
                    {
                        "name": "Shimiao Jiang"
                    },
                    {
                        "name": "Shiqi Kuang"
                    },
                    {
                        "name": "Shouyu Yin"
                    },
                    {
                        "name": "Chaohang Wen"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Bing Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Yu"
                },
                "author": "Bing Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15739v1",
                "updated": "2025-04-22T09:36:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    36,
                    7,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:36:07Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    36,
                    7,
                    1,
                    112,
                    0
                ],
                "title": "Development of an Ultra-fast, Likelihood-based, Distance Inference\n  Framework for the Next Generation of Type Ia Supernova Surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of an Ultra-fast, Likelihood-based, Distance Inference\n  Framework for the Next Generation of Type Ia Supernova Surveys"
                },
                "summary": "In this work, we present EDRIS (French for Distance Estimator for Incomplete\nSupernova Surveys), a cosmological inference framework tailored to reconstruct\nunbiased cosmological distances from type Ia supernovae light-curve parameters.\nThis goal is achieved by including data truncation directly in the statistical\nmodel which takes care of the standardization of luminosity distances. It\nallows us to build a single-step distance estimate by maximizing the\ncorresponding likelihood, free from the biases the survey detection limits\nwould introduce otherwise. Moreover, we expect the current worldwide statistics\nto be multiplied by O(10) in the upcoming years. This provides a new challenge\nto handle as the cosmological analysis must stay computationally towable. We\nshow that the optimization methods used in EDRIS allow for a reasonable time\ncomplexity of O($N^2$) resulting in a very fast inference process (O(10s) for\n1500 supernovae).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present EDRIS (French for Distance Estimator for Incomplete\nSupernova Surveys), a cosmological inference framework tailored to reconstruct\nunbiased cosmological distances from type Ia supernovae light-curve parameters.\nThis goal is achieved by including data truncation directly in the statistical\nmodel which takes care of the standardization of luminosity distances. It\nallows us to build a single-step distance estimate by maximizing the\ncorresponding likelihood, free from the biases the survey detection limits\nwould introduce otherwise. Moreover, we expect the current worldwide statistics\nto be multiplied by O(10) in the upcoming years. This provides a new challenge\nto handle as the cosmological analysis must stay computationally towable. We\nshow that the optimization methods used in EDRIS allow for a reasonable time\ncomplexity of O($N^2$) resulting in a very fast inference process (O(10s) for\n1500 supernovae)."
                },
                "authors": [
                    {
                        "name": "Dylan Kuhn"
                    },
                    {
                        "name": "Marc Betoule"
                    }
                ],
                "author_detail": {
                    "name": "Marc Betoule"
                },
                "author": "Marc Betoule",
                "arxiv_comment": "5 pages, 3 figures, contribution to the 2024 Cosmology session of the\n  58th Rencontres de Moriond",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15733v1",
                "updated": "2025-04-22T09:25:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    25,
                    36,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:25:36Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    25,
                    36,
                    1,
                    112,
                    0
                ],
                "title": "Operator Inference for Elliptic Eigenvalue Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operator Inference for Elliptic Eigenvalue Problems"
                },
                "summary": "Eigenvalue problems for elliptic operators play an important role in science\nand engineering applications, where efficient and accurate numerical\ncomputation is essential. In this work, we propose a novel operator inference\napproach for elliptic eigenvalue problems based on neural network\napproximations that directly maps computational domains to their associated\neigenvalues and eigenfunctions. Motivated by existing neural network\narchitectures and the mathematical characteristics of eigenvalue problems, we\nrepresent computational domains as pixelated images and decompose the task into\ntwo subtasks: eigenvalue prediction and eigenfunction prediction. For the\neigenvalue prediction, we design a convolutional neural network (CNN), while\nfor the eigenfunction prediction, we employ a Fourier Neural Operator (FNO).\nAdditionally, we introduce a critical preprocessing module that integrates\ndomain scaling, detailed boundary pixelization, and main-axis alignment. This\npreprocessing step not only simplifies the learning task but also enhances the\nperformance of the neural networks. Finally, we present numerical results to\ndemonstrate the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigenvalue problems for elliptic operators play an important role in science\nand engineering applications, where efficient and accurate numerical\ncomputation is essential. In this work, we propose a novel operator inference\napproach for elliptic eigenvalue problems based on neural network\napproximations that directly maps computational domains to their associated\neigenvalues and eigenfunctions. Motivated by existing neural network\narchitectures and the mathematical characteristics of eigenvalue problems, we\nrepresent computational domains as pixelated images and decompose the task into\ntwo subtasks: eigenvalue prediction and eigenfunction prediction. For the\neigenvalue prediction, we design a convolutional neural network (CNN), while\nfor the eigenfunction prediction, we employ a Fourier Neural Operator (FNO).\nAdditionally, we introduce a critical preprocessing module that integrates\ndomain scaling, detailed boundary pixelization, and main-axis alignment. This\npreprocessing step not only simplifies the learning task but also enhances the\nperformance of the neural networks. Finally, we present numerical results to\ndemonstrate the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Haoqian Li"
                    },
                    {
                        "name": "Jiguang Sun"
                    },
                    {
                        "name": "Zhiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwen Zhang"
                },
                "author": "Zhiwen Zhang",
                "arxiv_comment": "21 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "35J15, 65N25, 68T07, 65T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15722v1",
                "updated": "2025-04-22T09:11:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    11,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:11:48Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    11,
                    48,
                    1,
                    112,
                    0
                ],
                "title": "From predictions to confidence intervals: an empirical study of\n  conformal prediction methods for in-context learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From predictions to confidence intervals: an empirical study of\n  conformal prediction methods for in-context learning"
                },
                "summary": "Transformers have become a standard architecture in machine learning,\ndemonstrating strong in-context learning (ICL) abilities that allow them to\nlearn from the prompt at inference time. However, uncertainty quantification\nfor ICL remains an open challenge, particularly in noisy regression tasks. This\npaper investigates whether ICL can be leveraged for distribution-free\nuncertainty estimation, proposing a method based on conformal prediction to\nconstruct prediction intervals with guaranteed coverage. While traditional\nconformal methods are computationally expensive due to repeated model fitting,\nwe exploit ICL to efficiently generate confidence intervals in a single forward\npass. Our empirical analysis compares this approach against ridge\nregression-based conformal methods, showing that conformal prediction with\nin-context learning (CP with ICL) achieves robust and scalable uncertainty\nestimates. Additionally, we evaluate its performance under distribution shifts\nand establish scaling laws to guide model training. These findings bridge ICL\nand conformal prediction, providing a theoretically grounded and new framework\nfor uncertainty quantification in transformer-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become a standard architecture in machine learning,\ndemonstrating strong in-context learning (ICL) abilities that allow them to\nlearn from the prompt at inference time. However, uncertainty quantification\nfor ICL remains an open challenge, particularly in noisy regression tasks. This\npaper investigates whether ICL can be leveraged for distribution-free\nuncertainty estimation, proposing a method based on conformal prediction to\nconstruct prediction intervals with guaranteed coverage. While traditional\nconformal methods are computationally expensive due to repeated model fitting,\nwe exploit ICL to efficiently generate confidence intervals in a single forward\npass. Our empirical analysis compares this approach against ridge\nregression-based conformal methods, showing that conformal prediction with\nin-context learning (CP with ICL) achieves robust and scalable uncertainty\nestimates. Additionally, we evaluate its performance under distribution shifts\nand establish scaling laws to guide model training. These findings bridge ICL\nand conformal prediction, providing a theoretically grounded and new framework\nfor uncertainty quantification in transformer-based models."
                },
                "authors": [
                    {
                        "name": "Zhe Huang"
                    },
                    {
                        "name": "Simone Rossi"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Thomas Hannagan"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hannagan"
                },
                "author": "Thomas Hannagan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15721v1",
                "updated": "2025-04-22T09:11:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    11,
                    21,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:11:21Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    11,
                    21,
                    1,
                    112,
                    0
                ],
                "title": "BBAL: A Bidirectional Block Floating Point-Based Quantisation\n  Accelerator for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BBAL: A Bidirectional Block Floating Point-Based Quantisation\n  Accelerator for Large Language Models"
                },
                "summary": "Large language models (LLMs), with their billions of parameters, pose\nsubstantial challenges for deployment on edge devices, straining both memory\ncapacity and computational resources. Block Floating Point (BFP) quantisation\nreduces memory and computational overhead by converting high-overhead floating\npoint operations into low-bit fixed point operations. However, BFP requires\naligning all data to the maximum exponent, which causes loss of small and\nmoderate values, resulting in quantisation error and degradation in the\naccuracy of LLMs. To address this issue, we propose a Bidirectional Block\nFloating Point (BBFP) data format, which reduces the probability of selecting\nthe maximum as shared exponent, thereby reducing quantisation error. By\nutilizing the features in BBFP, we present a full-stack Bidirectional Block\nFloating Point-Based Quantisation Accelerator for LLMs (BBAL), primarily\ncomprising a processing element array based on BBFP, paired with proposed\ncost-effective nonlinear computation unit. Experimental results show BBAL\nachieves a 22% improvement in accuracy compared to an outlier-aware accelerator\nat similar efficiency, and a 40% efficiency improvement over a BFP-based\naccelerator at similar accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with their billions of parameters, pose\nsubstantial challenges for deployment on edge devices, straining both memory\ncapacity and computational resources. Block Floating Point (BFP) quantisation\nreduces memory and computational overhead by converting high-overhead floating\npoint operations into low-bit fixed point operations. However, BFP requires\naligning all data to the maximum exponent, which causes loss of small and\nmoderate values, resulting in quantisation error and degradation in the\naccuracy of LLMs. To address this issue, we propose a Bidirectional Block\nFloating Point (BBFP) data format, which reduces the probability of selecting\nthe maximum as shared exponent, thereby reducing quantisation error. By\nutilizing the features in BBFP, we present a full-stack Bidirectional Block\nFloating Point-Based Quantisation Accelerator for LLMs (BBAL), primarily\ncomprising a processing element array based on BBFP, paired with proposed\ncost-effective nonlinear computation unit. Experimental results show BBAL\nachieves a 22% improvement in accuracy compared to an outlier-aware accelerator\nat similar efficiency, and a 40% efficiency improvement over a BFP-based\naccelerator at similar accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Junyang Lu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "X. x. Zhang"
                    },
                    {
                        "name": "Ning Xu"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15719v1",
                "updated": "2025-04-22T09:08:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    21,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:21Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    21,
                    1,
                    112,
                    0
                ],
                "title": "Implementing Rational Choice Functions with LLMs and Measuring their\n  Alignment with User Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing Rational Choice Functions with LLMs and Measuring their\n  Alignment with User Preferences"
                },
                "summary": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain."
                },
                "authors": [
                    {
                        "name": "Anna Karnysheva"
                    },
                    {
                        "name": "Christian Drescher"
                    },
                    {
                        "name": "Dietrich Klakow"
                    }
                ],
                "author_detail": {
                    "name": "Dietrich Klakow"
                },
                "author": "Dietrich Klakow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15716v1",
                "updated": "2025-04-22T09:01:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    1,
                    4,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:01:04Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    1,
                    4,
                    1,
                    112,
                    0
                ],
                "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models"
                },
                "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Jie Zhu"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Huaixia Dou"
                    },
                    {
                        "name": "Junhui Li"
                    },
                    {
                        "name": "Lifan Guo"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09093v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09093v3",
                "updated": "2025-04-22T08:46:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    46,
                    20,
                    1,
                    112,
                    0
                ],
                "published": "2024-08-17T04:43:26Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    4,
                    43,
                    26,
                    5,
                    230,
                    0
                ],
                "title": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language\n  Models by Treating Harmful Instruction as Backdoor Trigger",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language\n  Models by Treating Harmful Instruction as Backdoor Trigger"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have showcased impressive\nperformance in a variety of multimodal tasks. On the other hand, the\nintegration of additional image modality may allow the malicious users to\ninject harmful content inside the images for jailbreaking. Unlike text-based\nLLMs, where adversaries need to select discrete tokens to conceal their\nmalicious intent using specific algorithms, the continuous nature of image\nsignals provides a direct opportunity for adversaries to inject harmful\nintentions. In this work, we propose $\\textbf{BaThe}$ ($\\textbf{Ba}$ckdoor\n$\\textbf{T}$rigger S$\\textbf{h}$i$\\textbf{e}$ld), a simple yet effective\njailbreak defense mechanism. Our work is motivated by recent research on\njailbreak backdoor attack and virtual prompt backdoor attack in generative\nlanguage models. Jailbreak backdoor attack uses harmful instructions combined\nwith manually crafted strings as triggers to make the backdoored model generate\nprohibited responses. We assume that harmful instructions can function as\ntriggers, and if we alternatively set rejection responses as the triggered\nresponse, the backdoored model then can defend against jailbreak attacks. We\nachieve this by utilizing virtual rejection prompt, similar to the virtual\nprompt backdoor attack. We embed the virtual rejection prompt into the soft\ntext embeddings, which we call ``wedge''. Our comprehensive experiments\ndemonstrate that BaThe effectively mitigates various types of jailbreak attacks\nand is adaptable to defend against unseen attacks, with minimal impact on\nMLLMs' performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have showcased impressive\nperformance in a variety of multimodal tasks. On the other hand, the\nintegration of additional image modality may allow the malicious users to\ninject harmful content inside the images for jailbreaking. Unlike text-based\nLLMs, where adversaries need to select discrete tokens to conceal their\nmalicious intent using specific algorithms, the continuous nature of image\nsignals provides a direct opportunity for adversaries to inject harmful\nintentions. In this work, we propose $\\textbf{BaThe}$ ($\\textbf{Ba}$ckdoor\n$\\textbf{T}$rigger S$\\textbf{h}$i$\\textbf{e}$ld), a simple yet effective\njailbreak defense mechanism. Our work is motivated by recent research on\njailbreak backdoor attack and virtual prompt backdoor attack in generative\nlanguage models. Jailbreak backdoor attack uses harmful instructions combined\nwith manually crafted strings as triggers to make the backdoored model generate\nprohibited responses. We assume that harmful instructions can function as\ntriggers, and if we alternatively set rejection responses as the triggered\nresponse, the backdoored model then can defend against jailbreak attacks. We\nachieve this by utilizing virtual rejection prompt, similar to the virtual\nprompt backdoor attack. We embed the virtual rejection prompt into the soft\ntext embeddings, which we call ``wedge''. Our comprehensive experiments\ndemonstrate that BaThe effectively mitigates various types of jailbreak attacks\nand is adaptable to defend against unseen attacks, with minimal impact on\nMLLMs' performance."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yirui Zhang"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09093v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09093v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15694v1",
                "updated": "2025-04-22T08:26:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    26,
                    35,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T08:26:35Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    26,
                    35,
                    1,
                    112,
                    0
                ],
                "title": "You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object\n  Detection"
                },
                "summary": "Despite the remarkable achievements in object detection, the model's accuracy\nand efficiency still require further improvement under challenging underwater\nconditions, such as low image quality and limited computational resources. To\naddress this, we propose an Ultra-Light Real-Time Underwater Object Detection\nframework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a\nMulti-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on\nthe input image, minimizing the semantic loss caused by underwater optical\ncolor distortion. Furthermore, we revisit the unique characteristics of\neven-sized and transposed convolutions, allowing the model to dynamically\nselect and enhance key information during the resampling process, thereby\nimproving its generalization ability. Finally, we eliminate model redundancy\nthrough a simple yet effective channel compression and reconstructed large\nkernel convolution (RLKC) to achieve model lightweight. As a result, forms a\nhigh-performance underwater object detector YSOOB with only 1.2 million\nparameters. Extensive experimental results demonstrate that, with the fewest\nparameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO\ndatasets, respectively, comparable to the current SOTA detectors. The inference\nspeed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge\ncomputing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by\n28.1% and 22.5%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable achievements in object detection, the model's accuracy\nand efficiency still require further improvement under challenging underwater\nconditions, such as low image quality and limited computational resources. To\naddress this, we propose an Ultra-Light Real-Time Underwater Object Detection\nframework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a\nMulti-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on\nthe input image, minimizing the semantic loss caused by underwater optical\ncolor distortion. Furthermore, we revisit the unique characteristics of\neven-sized and transposed convolutions, allowing the model to dynamically\nselect and enhance key information during the resampling process, thereby\nimproving its generalization ability. Finally, we eliminate model redundancy\nthrough a simple yet effective channel compression and reconstructed large\nkernel convolution (RLKC) to achieve model lightweight. As a result, forms a\nhigh-performance underwater object detector YSOOB with only 1.2 million\nparameters. Extensive experimental results demonstrate that, with the fewest\nparameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO\ndatasets, respectively, comparable to the current SOTA detectors. The inference\nspeed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge\ncomputing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by\n28.1% and 22.5%, respectively."
                },
                "authors": [
                    {
                        "name": "Jun Dong"
                    },
                    {
                        "name": "Wenli Wu"
                    },
                    {
                        "name": "Jintao Cheng"
                    },
                    {
                        "name": "Xiaoyu Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Tang"
                },
                "author": "Xiaoyu Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15691v1",
                "updated": "2025-04-22T08:15:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    15,
                    59,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T08:15:59Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    15,
                    59,
                    1,
                    112,
                    0
                ],
                "title": "Transfer Learning for High-dimensional Reduced Rank Time Series Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Learning for High-dimensional Reduced Rank Time Series Models"
                },
                "summary": "The objective of transfer learning is to enhance estimation and inference in\na target data by leveraging knowledge gained from additional sources. Recent\nstudies have explored transfer learning for independent observations in\ncomplex, high-dimensional models assuming sparsity, yet research on time series\nmodels remains limited. Our focus is on transfer learning for sequences of\nobservations with temporal dependencies and a more intricate model parameter\nstructure. Specifically, we investigate the vector autoregressive model (VAR),\na widely recognized model for time series data, where the transition matrix can\nbe deconstructed into a combination of a sparse matrix and a low-rank one. We\npropose a new transfer learning algorithm tailored for estimating\nhigh-dimensional VAR models characterized by low-rank and sparse structures.\nAdditionally, we present a novel approach for selecting informative\nobservations from auxiliary datasets. Theoretical guarantees are established,\nencompassing model parameter consistency, informative set selection, and the\nasymptotic distribution of estimators under mild conditions. The latter\nfacilitates the construction of entry-wise confidence intervals for model\nparameters. Finally, we demonstrate the empirical efficacy of our methodologies\nthrough both simulated and real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The objective of transfer learning is to enhance estimation and inference in\na target data by leveraging knowledge gained from additional sources. Recent\nstudies have explored transfer learning for independent observations in\ncomplex, high-dimensional models assuming sparsity, yet research on time series\nmodels remains limited. Our focus is on transfer learning for sequences of\nobservations with temporal dependencies and a more intricate model parameter\nstructure. Specifically, we investigate the vector autoregressive model (VAR),\na widely recognized model for time series data, where the transition matrix can\nbe deconstructed into a combination of a sparse matrix and a low-rank one. We\npropose a new transfer learning algorithm tailored for estimating\nhigh-dimensional VAR models characterized by low-rank and sparse structures.\nAdditionally, we present a novel approach for selecting informative\nobservations from auxiliary datasets. Theoretical guarantees are established,\nencompassing model parameter consistency, informative set selection, and the\nasymptotic distribution of estimators under mild conditions. The latter\nfacilitates the construction of entry-wise confidence intervals for model\nparameters. Finally, we demonstrate the empirical efficacy of our methodologies\nthrough both simulated and real-world datasets."
                },
                "authors": [
                    {
                        "name": "Mingliang Ma Abolfazl Safikhani"
                    }
                ],
                "author_detail": {
                    "name": "Mingliang Ma Abolfazl Safikhani"
                },
                "author": "Mingliang Ma Abolfazl Safikhani",
                "arxiv_comment": "29 pages accepted by AISTATS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15689v1",
                "updated": "2025-04-22T08:13:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    13,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T08:13:34Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    13,
                    34,
                    1,
                    112,
                    0
                ],
                "title": "The Viability of Crowdsourcing for RAG Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Viability of Crowdsourcing for RAG Evaluation"
                },
                "summary": "How good are humans at writing and judging responses in retrieval-augmented\ngeneration (RAG) scenarios? To answer this question, we investigate the\nefficacy of crowdsourcing for RAG through two complementary studies: response\nwriting and response utility judgment. We present the Crowd RAG Corpus 2025\n(CrowdRAG-25), which consists of 903 human-written and 903 LLM-generated\nresponses for the 301 topics of the TREC RAG'24 track, across the three\ndiscourse styles 'bulleted list', 'essay', and 'news'. For a selection of 65\ntopics, the corpus further contains 47,320 pairwise human judgments and 10,556\npairwise LLM judgments across seven utility dimensions (e.g., coverage and\ncoherence). Our analyses give insights into human writing behavior for RAG and\nthe viability of crowdsourcing for RAG evaluation. Human pairwise judgments\nprovide reliable and cost-effective results compared to LLM-based pairwise or\nhuman/LLM-based pointwise judgments, as well as automated comparisons with\nhuman-written reference responses. All our data and tools are freely available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How good are humans at writing and judging responses in retrieval-augmented\ngeneration (RAG) scenarios? To answer this question, we investigate the\nefficacy of crowdsourcing for RAG through two complementary studies: response\nwriting and response utility judgment. We present the Crowd RAG Corpus 2025\n(CrowdRAG-25), which consists of 903 human-written and 903 LLM-generated\nresponses for the 301 topics of the TREC RAG'24 track, across the three\ndiscourse styles 'bulleted list', 'essay', and 'news'. For a selection of 65\ntopics, the corpus further contains 47,320 pairwise human judgments and 10,556\npairwise LLM judgments across seven utility dimensions (e.g., coverage and\ncoherence). Our analyses give insights into human writing behavior for RAG and\nthe viability of crowdsourcing for RAG evaluation. Human pairwise judgments\nprovide reliable and cost-effective results compared to LLM-based pairwise or\nhuman/LLM-based pointwise judgments, as well as automated comparisons with\nhuman-written reference responses. All our data and tools are freely available."
                },
                "authors": [
                    {
                        "name": "Lukas Gienapp"
                    },
                    {
                        "name": "Tim Hagen"
                    },
                    {
                        "name": "Maik Fr√∂be"
                    },
                    {
                        "name": "Matthias Hagen"
                    },
                    {
                        "name": "Benno Stein"
                    },
                    {
                        "name": "Martin Potthast"
                    },
                    {
                        "name": "Harrisen Scells"
                    }
                ],
                "author_detail": {
                    "name": "Harrisen Scells"
                },
                "author": "Harrisen Scells",
                "arxiv_doi": "10.1145/3726302.3730093",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730093",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 9 tables, 5 figures. Accepted at SIGIR'25",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15686v1",
                "updated": "2025-04-22T08:10:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    10,
                    6,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T08:10:06Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    10,
                    6,
                    1,
                    112,
                    0
                ],
                "title": "Invariant Learning with Annotation-free Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariant Learning with Annotation-free Environments"
                },
                "summary": "Invariant learning is a promising approach to improve domain generalization\ncompared to Empirical Risk Minimization (ERM). However, most invariant learning\nmethods rely on the assumption that training examples are pre-partitioned into\ndifferent known environments. We instead infer environments without the need\nfor additional annotations, motivated by observations of the properties within\nthe representation space of a trained ERM model. We show the preliminary\neffectiveness of our approach on the ColoredMNIST benchmark, achieving\nperformance comparable to methods requiring explicit environment labels and on\npar with an annotation-free method that poses strong restrictions on the ERM\nreference model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariant learning is a promising approach to improve domain generalization\ncompared to Empirical Risk Minimization (ERM). However, most invariant learning\nmethods rely on the assumption that training examples are pre-partitioned into\ndifferent known environments. We instead infer environments without the need\nfor additional annotations, motivated by observations of the properties within\nthe representation space of a trained ERM model. We show the preliminary\neffectiveness of our approach on the ColoredMNIST benchmark, achieving\nperformance comparable to methods requiring explicit environment labels and on\npar with an annotation-free method that poses strong restrictions on the ERM\nreference model."
                },
                "authors": [
                    {
                        "name": "Phuong Quynh Le"
                    },
                    {
                        "name": "Christin Seifert"
                    },
                    {
                        "name": "J√∂rg Schl√∂tterer"
                    }
                ],
                "author_detail": {
                    "name": "J√∂rg Schl√∂tterer"
                },
                "author": "J√∂rg Schl√∂tterer",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop UniReps",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06223v2",
                "updated": "2025-04-22T08:07:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    7,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-08T13:51:40Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    13,
                    51,
                    40,
                    5,
                    67,
                    0
                ],
                "title": "Red Team Diffuser: Exposing Toxic Continuation Vulnerabilities in\n  Vision-Language Models via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red Team Diffuser: Exposing Toxic Continuation Vulnerabilities in\n  Vision-Language Models via Reinforcement Learning"
                },
                "summary": "The growing deployment of large Vision-Language Models (VLMs) exposes\ncritical safety gaps in their alignment mechanisms. While existing jailbreak\nstudies primarily focus on VLMs' susceptibility to harmful instructions, we\nreveal a fundamental yet overlooked vulnerability: toxic text continuation,\nwhere VLMs produce highly toxic completions when prompted with harmful text\nprefixes paired with semantically adversarial images. To systematically study\nthis threat, we propose Red Team Diffuser (RTD), the first red teaming\ndiffusion model that coordinates adversarial image generation and toxic\ncontinuation through reinforcement learning. Our key innovations include\ndynamic cross-modal attack and stealth-aware optimization. For toxic text\nprefixes from an LLM safety benchmark, we conduct greedy search to identify\noptimal image prompts that maximally induce toxic completions. The discovered\nimage prompts then drive RL-based diffusion model fine-tuning, producing\nsemantically aligned adversarial images that boost toxicity rates.\nStealth-aware optimization introduces joint adversarial rewards that balance\ntoxicity maximization (via Detoxify classifier) and stealthiness (via\nBERTScore), circumventing traditional noise-based adversarial patterns.\nExperimental results demonstrate the effectiveness of RTD, increasing the\ntoxicity rate of LLaVA outputs by 10.69% over text-only baselines on the\noriginal attack set and 8.91% on an unseen set, proving generalization\ncapability. Moreover, RTD exhibits strong cross-model transferability, raising\nthe toxicity rate by 5.1% on Gemini and 26.83% on LLaMA. Our findings expose\ntwo critical flaws in current VLM alignment: (1) failure to prevent toxic\ncontinuation from harmful prefixes, and (2) overlooking cross-modal attack\nvectors. These results necessitate a paradigm shift toward multimodal red\nteaming in safety evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing deployment of large Vision-Language Models (VLMs) exposes\ncritical safety gaps in their alignment mechanisms. While existing jailbreak\nstudies primarily focus on VLMs' susceptibility to harmful instructions, we\nreveal a fundamental yet overlooked vulnerability: toxic text continuation,\nwhere VLMs produce highly toxic completions when prompted with harmful text\nprefixes paired with semantically adversarial images. To systematically study\nthis threat, we propose Red Team Diffuser (RTD), the first red teaming\ndiffusion model that coordinates adversarial image generation and toxic\ncontinuation through reinforcement learning. Our key innovations include\ndynamic cross-modal attack and stealth-aware optimization. For toxic text\nprefixes from an LLM safety benchmark, we conduct greedy search to identify\noptimal image prompts that maximally induce toxic completions. The discovered\nimage prompts then drive RL-based diffusion model fine-tuning, producing\nsemantically aligned adversarial images that boost toxicity rates.\nStealth-aware optimization introduces joint adversarial rewards that balance\ntoxicity maximization (via Detoxify classifier) and stealthiness (via\nBERTScore), circumventing traditional noise-based adversarial patterns.\nExperimental results demonstrate the effectiveness of RTD, increasing the\ntoxicity rate of LLaVA outputs by 10.69% over text-only baselines on the\noriginal attack set and 8.91% on an unseen set, proving generalization\ncapability. Moreover, RTD exhibits strong cross-model transferability, raising\nthe toxicity rate by 5.1% on Gemini and 26.83% on LLaMA. Our findings expose\ntwo critical flaws in current VLM alignment: (1) failure to prevent toxic\ncontinuation from harmful prefixes, and (2) overlooking cross-modal attack\nvectors. These results necessitate a paradigm shift toward multimodal red\nteaming in safety evaluations."
                },
                "authors": [
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Xiaosen Wang"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xingjun Ma"
                },
                "author": "Xingjun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04060v2",
                "updated": "2025-04-22T07:59:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    59,
                    31,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-05T04:57:12Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    4,
                    57,
                    12,
                    5,
                    95,
                    0
                ],
                "title": "VocalNet: Speech LLM with Multi-Token Prediction for Faster and\n  High-Quality Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VocalNet: Speech LLM with Multi-Token Prediction for Faster and\n  High-Quality Generation"
                },
                "summary": "Speech large language models (LLMs) have emerged as a prominent research\nfocus in speech processing. We introduce VocalNet-1B and VocalNet-8B, a series\nof high-performance, low-latency speech LLMs enabled by a scalable and\nmodel-agnostic training framework designed for real-time voice interaction.\nCentral to our contribution is the first application of multi-token prediction\n(MTP) to speech LLMs. This approach represents a paradigm shift from standard\nnext-token prediction (NTP), offering simultaneous improvements in generation\nspeed and quality. Informed by analysis of MTP's effect on speech generation\nand experimental comparisons, we designed a straightforward and highly\neffective MTP implementation. Experiments demonstrate that VocalNet performs on\npar with mainstream Omni LLMs even with limited training data, and\nsignificantly surpasses existing open-source speech LLMs. To foster\nreproducibility and community advancement, all model weights, inference code,\ntraining data, and framework implementations have been made publicly available\nat https://github.com/SJTU-OmniAgent/VocalNet",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech large language models (LLMs) have emerged as a prominent research\nfocus in speech processing. We introduce VocalNet-1B and VocalNet-8B, a series\nof high-performance, low-latency speech LLMs enabled by a scalable and\nmodel-agnostic training framework designed for real-time voice interaction.\nCentral to our contribution is the first application of multi-token prediction\n(MTP) to speech LLMs. This approach represents a paradigm shift from standard\nnext-token prediction (NTP), offering simultaneous improvements in generation\nspeed and quality. Informed by analysis of MTP's effect on speech generation\nand experimental comparisons, we designed a straightforward and highly\neffective MTP implementation. Experiments demonstrate that VocalNet performs on\npar with mainstream Omni LLMs even with limited training data, and\nsignificantly surpasses existing open-source speech LLMs. To foster\nreproducibility and community advancement, all model weights, inference code,\ntraining data, and framework implementations have been made publicly available\nat https://github.com/SJTU-OmniAgent/VocalNet"
                },
                "authors": [
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Heyang Liu"
                    },
                    {
                        "name": "Ziyang Cheng"
                    },
                    {
                        "name": "Ronghua Wu"
                    },
                    {
                        "name": "Qunshan Gu"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10603v2",
                "updated": "2025-04-22T07:48:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    48,
                    5,
                    1,
                    112,
                    0
                ],
                "published": "2024-10-14T15:12:15Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    12,
                    15,
                    0,
                    288,
                    0
                ],
                "title": "Testing interacting dark energy with Stage IV cosmic shear surveys\n  through differentiable neural emulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing interacting dark energy with Stage IV cosmic shear surveys\n  through differentiable neural emulators"
                },
                "summary": "We employ a novel framework for accelerated cosmological inference, based on\nneural emulators and gradient-based sampling methods, to forecast constraints\non dark energy models from Stage IV cosmic shear surveys. We focus on dark\nscattering (DS), an interacting dark energy model with pure momentum exchange\nin the dark sector, and train COSMOPOWER emulators to accurately and\nefficiently model the DS non-linear matter power spectrum produced by the halo\nmodel reaction framework, including the effects of baryon feedback and massive\nneutrinos. We embed the emulators within a fully-differentiable pipeline for\ngradient-based cosmological inference for which the batch likelihood call is up\nto $O(10^5)$ times faster than with traditional approaches, producing parameter\nconstraints from simulated Stage IV cosmic shear data running on a single\ngraphics processing unit (GPU). We also perform model comparison on the output\nchains from the inference process, employing the learnt harmonic mean estimator\nimplemented in the software HARMONIC. We investigate degeneracies between dark\nenergy and systematics parameters and assess the impact of scale cuts on the\nfinal constraints. Assuming a DS model for the mock data vector, we find that a\nStage IV survey cosmic shear analysis can constrain the DS amplitude parameter\n$A_{\\mathrm{ds}}$ with an uncertainty roughly an order of magnitude smaller\nthan current constraints from Stage III surveys, even after marginalising over\nbaryonic feedback, intrinsic alignments and redshift distribution\nuncertainties. These results show great promise for constraining DS with Stage\nIV data; furthermore, our methodology can be straightforwardly extended to a\nwide range of dark energy and modified gravity models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We employ a novel framework for accelerated cosmological inference, based on\nneural emulators and gradient-based sampling methods, to forecast constraints\non dark energy models from Stage IV cosmic shear surveys. We focus on dark\nscattering (DS), an interacting dark energy model with pure momentum exchange\nin the dark sector, and train COSMOPOWER emulators to accurately and\nefficiently model the DS non-linear matter power spectrum produced by the halo\nmodel reaction framework, including the effects of baryon feedback and massive\nneutrinos. We embed the emulators within a fully-differentiable pipeline for\ngradient-based cosmological inference for which the batch likelihood call is up\nto $O(10^5)$ times faster than with traditional approaches, producing parameter\nconstraints from simulated Stage IV cosmic shear data running on a single\ngraphics processing unit (GPU). We also perform model comparison on the output\nchains from the inference process, employing the learnt harmonic mean estimator\nimplemented in the software HARMONIC. We investigate degeneracies between dark\nenergy and systematics parameters and assess the impact of scale cuts on the\nfinal constraints. Assuming a DS model for the mock data vector, we find that a\nStage IV survey cosmic shear analysis can constrain the DS amplitude parameter\n$A_{\\mathrm{ds}}$ with an uncertainty roughly an order of magnitude smaller\nthan current constraints from Stage III surveys, even after marginalising over\nbaryonic feedback, intrinsic alignments and redshift distribution\nuncertainties. These results show great promise for constraining DS with Stage\nIV data; furthermore, our methodology can be straightforwardly extended to a\nwide range of dark energy and modified gravity models."
                },
                "authors": [
                    {
                        "name": "Karim Carrion"
                    },
                    {
                        "name": "Alessio Spurio Mancini"
                    },
                    {
                        "name": "Davide Piras"
                    },
                    {
                        "name": "Juan Carlos Hidalgo"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Hidalgo"
                },
                "author": "Juan Carlos Hidalgo",
                "arxiv_comment": "9 pages, 6 figures. COSMOPOWER available at\n  https://github.com/alessiospuriomancini/cosmopower, COSMOPOWER-JAX available\n  at https://github.com/dpiras/cosmopower-jax, emulators for Dark Scattering\n  available at https://github.com/karimpsi22/DS-emulators. V2: Minor\n  corrections. Version accepted for publication at MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.16084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16084v1",
                "updated": "2025-04-22T17:59:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    56,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    56,
                    1,
                    112,
                    0
                ],
                "title": "TTRL: Test-Time Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTRL: Test-Time Reinforcement Learning"
                },
                "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16083v1",
                "updated": "2025-04-22T17:59:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    51,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:59:51Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    51,
                    1,
                    112,
                    0
                ],
                "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention"
                },
                "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16082v1",
                "updated": "2025-04-22T17:59:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    41,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    41,
                    1,
                    112,
                    0
                ],
                "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding"
                },
                "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video"
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16078v1",
                "updated": "2025-04-22T17:57:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    57,
                    14,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:57:14Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    57,
                    14,
                    1,
                    112,
                    0
                ],
                "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities"
                },
                "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Thomas Schmied"
                    },
                    {
                        "name": "J√∂rg Bornschein"
                    },
                    {
                        "name": "Jordi Grau-Moya"
                    },
                    {
                        "name": "Markus Wulfmeier"
                    },
                    {
                        "name": "Razvan Pascanu"
                    }
                ],
                "author_detail": {
                    "name": "Razvan Pascanu"
                },
                "author": "Razvan Pascanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09572v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09572v3",
                "updated": "2025-04-22T17:56:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    56,
                    22,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-12T17:40:52Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    40,
                    52,
                    2,
                    71,
                    0
                ],
                "title": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks"
                },
                "summary": "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a\nstate-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as\na text-only state-of-the-art 81.36% success rate on WebVoyager.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a\nstate-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as\na text-only state-of-the-art 81.36% success rate on WebVoyager."
                },
                "authors": [
                    {
                        "name": "Lutfi Eren Erdogan"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09572v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09572v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16074v1",
                "updated": "2025-04-22T17:53:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    53,
                    29,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:53:29Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    53,
                    29,
                    1,
                    112,
                    0
                ],
                "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in\n  Large Language Models"
                },
                "summary": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/."
                },
                "authors": [
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Shaoyang Guo"
                    },
                    {
                        "name": "Zhuo-Yang Song"
                    },
                    {
                        "name": "Yunbo Sun"
                    },
                    {
                        "name": "Zeyu Cai"
                    },
                    {
                        "name": "Jiashen Wei"
                    },
                    {
                        "name": "Tianyu Luo"
                    },
                    {
                        "name": "Yixuan Yin"
                    },
                    {
                        "name": "Haoxu Zhang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Chencheng Tang"
                    },
                    {
                        "name": "Haoling Chang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Ziheng Zhou"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Jingtian Zhang"
                    },
                    {
                        "name": "Zhangyi Liu"
                    },
                    {
                        "name": "Minghao Li"
                    },
                    {
                        "name": "Yuku Zhang"
                    },
                    {
                        "name": "Boxuan Jing"
                    },
                    {
                        "name": "Xianqi Yin"
                    },
                    {
                        "name": "Yutong Ren"
                    },
                    {
                        "name": "Zizhuo Fu"
                    },
                    {
                        "name": "Weike Wang"
                    },
                    {
                        "name": "Xudong Tian"
                    },
                    {
                        "name": "Anqi Lv"
                    },
                    {
                        "name": "Laifu Man"
                    },
                    {
                        "name": "Jianxiang Li"
                    },
                    {
                        "name": "Feiyu Tao"
                    },
                    {
                        "name": "Qihua Sun"
                    },
                    {
                        "name": "Zhou Liang"
                    },
                    {
                        "name": "Yushu Mu"
                    },
                    {
                        "name": "Zhongxuan Li"
                    },
                    {
                        "name": "Jing-Jun Zhang"
                    },
                    {
                        "name": "Shutao Zhang"
                    },
                    {
                        "name": "Xiaotian Li"
                    },
                    {
                        "name": "Xingqi Xia"
                    },
                    {
                        "name": "Jiawei Lin"
                    },
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Jiahang Chen"
                    },
                    {
                        "name": "Qiuhao Xiong"
                    },
                    {
                        "name": "Binran Wang"
                    },
                    {
                        "name": "Fengyuan Wang"
                    },
                    {
                        "name": "Ziyang Ni"
                    },
                    {
                        "name": "Bohan Zhang"
                    },
                    {
                        "name": "Fan Cui"
                    },
                    {
                        "name": "Changkun Shao"
                    },
                    {
                        "name": "Qing-Hong Cao"
                    },
                    {
                        "name": "Ming-xing Luo"
                    },
                    {
                        "name": "Muhan Zhang"
                    },
                    {
                        "name": "Hua Xing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xing Zhu"
                },
                "author": "Hua Xing Zhu",
                "arxiv_comment": "21 pages ,8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16057v1",
                "updated": "2025-04-22T17:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    33,
                    53,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:33:53Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    33,
                    53,
                    1,
                    112,
                    0
                ],
                "title": "Automated Static Vulnerability Detection via a Holistic Neuro-symbolic\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Static Vulnerability Detection via a Holistic Neuro-symbolic\n  Approach"
                },
                "summary": "Static vulnerability detection is still a challenging problem and demands\nexcessive human efforts, e.g., manual curation of good vulnerability patterns.\nNone of prior works, including classic program analysis or Large Language Model\n(LLM)-based approaches, have fully automated such vulnerability pattern\ngenerations with reasonable detection accuracy. In this paper, we design and\nimplement, MoCQ, a novel holistic neuro-symbolic framework that combines the\ncomplementary strengths of LLMs and classical static analysis to enable\nscalable vulnerability detection. The key insight is that MoCQ leverages an LLM\nto automatically extract vulnerability patterns and translate them into\ndetection queries, and then on static analysis to refine such queries in a\nfeedback loop and eventually execute them for analyzing large codebases and\nmining vulnerabilities. We evaluate MoCQ on seven types of vulnerabilities\nspanning two programming languages. We found MoCQ-generated queries uncovered\nat least 12 patterns that were missed by experts. On a ground truth dataset,\nMoCQ achieved comparable precision and recall compared to expert-crafted\nqueries. Moreover, MoCQ has identified seven previously unknown vulnerabilities\nin real-world applications, demonstrating its practical effectiveness. We have\nresponsibly disclosed them to the corresponding developers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static vulnerability detection is still a challenging problem and demands\nexcessive human efforts, e.g., manual curation of good vulnerability patterns.\nNone of prior works, including classic program analysis or Large Language Model\n(LLM)-based approaches, have fully automated such vulnerability pattern\ngenerations with reasonable detection accuracy. In this paper, we design and\nimplement, MoCQ, a novel holistic neuro-symbolic framework that combines the\ncomplementary strengths of LLMs and classical static analysis to enable\nscalable vulnerability detection. The key insight is that MoCQ leverages an LLM\nto automatically extract vulnerability patterns and translate them into\ndetection queries, and then on static analysis to refine such queries in a\nfeedback loop and eventually execute them for analyzing large codebases and\nmining vulnerabilities. We evaluate MoCQ on seven types of vulnerabilities\nspanning two programming languages. We found MoCQ-generated queries uncovered\nat least 12 patterns that were missed by experts. On a ground truth dataset,\nMoCQ achieved comparable precision and recall compared to expert-crafted\nqueries. Moreover, MoCQ has identified seven previously unknown vulnerabilities\nin real-world applications, demonstrating its practical effectiveness. We have\nresponsibly disclosed them to the corresponding developers."
                },
                "authors": [
                    {
                        "name": "Penghui Li"
                    },
                    {
                        "name": "Songchen Yao"
                    },
                    {
                        "name": "Josef Sarfati Korich"
                    },
                    {
                        "name": "Changhua Luo"
                    },
                    {
                        "name": "Jianjia Yu"
                    },
                    {
                        "name": "Yinzhi Cao"
                    },
                    {
                        "name": "Junfeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Yang"
                },
                "author": "Junfeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16056v1",
                "updated": "2025-04-22T17:32:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    32,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:32:48Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    32,
                    48,
                    1,
                    112,
                    0
                ],
                "title": "Honey, I Shrunk the Language Model: Impact of Knowledge Distillation\n  Methods on Performance and Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honey, I Shrunk the Language Model: Impact of Knowledge Distillation\n  Methods on Performance and Explainability"
                },
                "summary": "Artificial Intelligence (AI) has increasingly influenced modern society,\nrecently in particular through significant advancements in Large Language\nModels (LLMs). However, high computational and storage demands of LLMs still\nlimit their deployment in resource-constrained environments. Knowledge\ndistillation addresses this challenge by training a small student model from a\nlarger teacher model. Previous research has introduced several distillation\nmethods for both generating training data and for training the student model.\nDespite their relevance, the effects of state-of-the-art distillation methods\non model performance and explainability have not been thoroughly investigated\nand compared. In this work, we enlarge the set of available methods by applying\ncritique-revision prompting to distillation for data generation and by\nsynthesizing existing methods for training. For these methods, we provide a\nsystematic comparison based on the widely used Commonsense Question-Answering\n(CQA) dataset. While we measure performance via student model accuracy, we\nemploy a human-grounded study to evaluate explainability. We contribute new\ndistillation methods and their comparison in terms of both performance and\nexplainability. This should further advance the distillation of small language\nmodels and, thus, contribute to broader applicability and faster diffusion of\nLLM technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has increasingly influenced modern society,\nrecently in particular through significant advancements in Large Language\nModels (LLMs). However, high computational and storage demands of LLMs still\nlimit their deployment in resource-constrained environments. Knowledge\ndistillation addresses this challenge by training a small student model from a\nlarger teacher model. Previous research has introduced several distillation\nmethods for both generating training data and for training the student model.\nDespite their relevance, the effects of state-of-the-art distillation methods\non model performance and explainability have not been thoroughly investigated\nand compared. In this work, we enlarge the set of available methods by applying\ncritique-revision prompting to distillation for data generation and by\nsynthesizing existing methods for training. For these methods, we provide a\nsystematic comparison based on the widely used Commonsense Question-Answering\n(CQA) dataset. While we measure performance via student model accuracy, we\nemploy a human-grounded study to evaluate explainability. We contribute new\ndistillation methods and their comparison in terms of both performance and\nexplainability. This should further advance the distillation of small language\nmodels and, thus, contribute to broader applicability and faster diffusion of\nLLM technology."
                },
                "authors": [
                    {
                        "name": "Daniel Hendriks"
                    },
                    {
                        "name": "Philipp Spitzer"
                    },
                    {
                        "name": "Niklas K√ºhl"
                    },
                    {
                        "name": "Gerhard Satzger"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Satzger"
                },
                "author": "Gerhard Satzger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16046v1",
                "updated": "2025-04-22T17:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    16,
                    53,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    16,
                    53,
                    1,
                    112,
                    0
                ],
                "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certified Mitigation of Worst-Case LLM Copyright Infringement"
                },
                "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Jiacan Yu"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16039v1",
                "updated": "2025-04-22T17:04:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    4,
                    2,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T17:04:02Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    4,
                    2,
                    1,
                    112,
                    0
                ],
                "title": "A Comparative and Measurement-Based Study on Real-Time Network KPI\n  Extraction Methods for 5G and Beyond Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative and Measurement-Based Study on Real-Time Network KPI\n  Extraction Methods for 5G and Beyond Applications"
                },
                "summary": "Key performance indicators (KPIs), which can be extracted from the\nstandardized interfaces of network equipment defined by current standards,\nconstitute a primary data source that can be leveraged in the development of\nnon-standardized new equipment, architectures, and computational tools. In\nnext-generation technologies, the demand for data has evolved beyond the\nconventional log generation or export capabilities provided by existing\nlicensed network monitoring tools. There is now a growing need to collect such\ndata at specific time intervals and with defined granularities. At this stage,\nthe development of real-time KPI extraction methods and enabling their exchange\nbetween both standardized/commercialized and non-standardized components or\ntools has become increasingly critical. This study presents a comprehensive\nevaluation of three distinct KPI extraction methodologies applied to two\ncommercially available devices. The analysis aims to uncover the strengths,\nweaknesses, and overall efficacy of these approaches under varying conditions,\nand highlights the critical insights into the practical capabilities and\nlimitations. The findings serve as a foundational guide for the seamless\nintegration and robust testing of novel technologies and approaches within\ncommercial telecommunication networks. This work aspires to bridge the gap\nbetween technological innovation and real-world applicability, fostering\nenhanced decision-making in network deployment and optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key performance indicators (KPIs), which can be extracted from the\nstandardized interfaces of network equipment defined by current standards,\nconstitute a primary data source that can be leveraged in the development of\nnon-standardized new equipment, architectures, and computational tools. In\nnext-generation technologies, the demand for data has evolved beyond the\nconventional log generation or export capabilities provided by existing\nlicensed network monitoring tools. There is now a growing need to collect such\ndata at specific time intervals and with defined granularities. At this stage,\nthe development of real-time KPI extraction methods and enabling their exchange\nbetween both standardized/commercialized and non-standardized components or\ntools has become increasingly critical. This study presents a comprehensive\nevaluation of three distinct KPI extraction methodologies applied to two\ncommercially available devices. The analysis aims to uncover the strengths,\nweaknesses, and overall efficacy of these approaches under varying conditions,\nand highlights the critical insights into the practical capabilities and\nlimitations. The findings serve as a foundational guide for the seamless\nintegration and robust testing of novel technologies and approaches within\ncommercial telecommunication networks. This work aspires to bridge the gap\nbetween technological innovation and real-world applicability, fostering\nenhanced decision-making in network deployment and optimization."
                },
                "authors": [
                    {
                        "name": "Batuhan Kaplan"
                    },
                    {
                        "name": "Samed Ke≈üir"
                    },
                    {
                        "name": "Ahmet Faruk Co≈ükun"
                    }
                ],
                "author_detail": {
                    "name": "Ahmet Faruk Co≈ükun"
                },
                "author": "Ahmet Faruk Co≈ükun",
                "arxiv_comment": "6 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16032v1",
                "updated": "2025-04-22T16:56:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    56,
                    59,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T16:56:59Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    56,
                    59,
                    1,
                    112,
                    0
                ],
                "title": "LLMs meet Federated Learning for Scalable and Secure IoT Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs meet Federated Learning for Scalable and Secure IoT Management"
                },
                "summary": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions."
                },
                "authors": [
                    {
                        "name": "Yazan Otoum"
                    },
                    {
                        "name": "Arghavan Asad"
                    },
                    {
                        "name": "Amiya Nayak"
                    }
                ],
                "author_detail": {
                    "name": "Amiya Nayak"
                },
                "author": "Amiya Nayak",
                "arxiv_comment": "This work has been submitted to the IEEE Global Communications\n  Conference (GLOBECOM) 2025 for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16030v1",
                "updated": "2025-04-22T16:52:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    52,
                    9,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T16:52:09Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    52,
                    9,
                    1,
                    112,
                    0
                ],
                "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale"
                },
                "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc."
                },
                "authors": [
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Ziyun Zeng"
                    },
                    {
                        "name": "Yiqi Lin"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "CVPR 2025. If any references are missing, please contact\n  joyachen@u.nus.edu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16027v1",
                "updated": "2025-04-22T16:44:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    44,
                    39,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T16:44:39Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    44,
                    39,
                    1,
                    112,
                    0
                ],
                "title": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs\n  DeepSeek-V3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs\n  DeepSeek-V3"
                },
                "summary": "Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection"
                },
                "authors": [
                    {
                        "name": "Ahmed R. Sadik"
                    },
                    {
                        "name": "Siddhata Govind"
                    }
                ],
                "author_detail": {
                    "name": "Siddhata Govind"
                },
                "author": "Siddhata Govind",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02355v4",
                "updated": "2025-04-22T16:15:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    15,
                    47,
                    1,
                    112,
                    0
                ],
                "published": "2024-10-03T10:06:27Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    6,
                    27,
                    3,
                    277,
                    0
                ],
                "title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models"
                },
                "summary": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.7%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.7%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit."
                },
                "authors": [
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yunshan Ma"
                    },
                    {
                        "name": "Shi Jie"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Tat-seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-seng Chua"
                },
                "author": "Tat-seng Chua",
                "arxiv_journal_ref": "13th International Conference on Learning Representations (ICLR\n  2025 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16005v2",
                "updated": "2025-04-23T09:59:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    59,
                    31,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T16:14:31Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    14,
                    31,
                    1,
                    112,
                    0
                ],
                "title": "CAPO: Cost-Aware Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Cost-Aware Prompt Optimization"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."
                },
                "authors": [
                    {
                        "name": "Tom Zehle"
                    },
                    {
                        "name": "Moritz Schlager"
                    },
                    {
                        "name": "Timo Hei√ü"
                    },
                    {
                        "name": "Matthias Feurer"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Feurer"
                },
                "author": "Matthias Feurer",
                "arxiv_comment": "Submitted to AutoML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15995v1",
                "updated": "2025-04-22T16:00:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    0,
                    11,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T16:00:11Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    0,
                    11,
                    1,
                    112,
                    0
                ],
                "title": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical\n  Federated Learning"
                },
                "summary": "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL."
                },
                "authors": [
                    {
                        "name": "Sindhuja Madabushi"
                    },
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Haider Ali"
                    },
                    {
                        "name": "Jin-Hee Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jin-Hee Cho"
                },
                "author": "Jin-Hee Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15991v1",
                "updated": "2025-04-22T15:53:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    53,
                    59,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T15:53:59Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    53,
                    59,
                    1,
                    112,
                    0
                ],
                "title": "Efficient Adaptation of Deep Neural Networks for Semantic Segmentation\n  in Space Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Adaptation of Deep Neural Networks for Semantic Segmentation\n  in Space Applications"
                },
                "summary": "In recent years, the application of Deep Learning techniques has shown\nremarkable success in various computer vision tasks, paving the way for their\ndeployment in extraterrestrial exploration. Transfer learning has emerged as a\npowerful strategy for addressing the scarcity of labeled data in these novel\nenvironments. This paper represents one of the first efforts in evaluating the\nfeasibility of employing adapters toward efficient transfer learning for rock\nsegmentation in extraterrestrial landscapes, mainly focusing on lunar and\nmartian terrains. Our work suggests that the use of adapters, strategically\nintegrated into a pre-trained backbone model, can be successful in reducing\nboth bandwidth and memory requirements for the target extraterrestrial device.\nIn this study, we considered two memory-saving strategies: layer fusion (to\nreduce to zero the inference overhead) and an ``adapter ranking'' (to also\nreduce the transmission cost). Finally, we evaluate these results in terms of\ntask performance, memory, and computation on embedded devices, evidencing\ntrade-offs that open the road to more research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the application of Deep Learning techniques has shown\nremarkable success in various computer vision tasks, paving the way for their\ndeployment in extraterrestrial exploration. Transfer learning has emerged as a\npowerful strategy for addressing the scarcity of labeled data in these novel\nenvironments. This paper represents one of the first efforts in evaluating the\nfeasibility of employing adapters toward efficient transfer learning for rock\nsegmentation in extraterrestrial landscapes, mainly focusing on lunar and\nmartian terrains. Our work suggests that the use of adapters, strategically\nintegrated into a pre-trained backbone model, can be successful in reducing\nboth bandwidth and memory requirements for the target extraterrestrial device.\nIn this study, we considered two memory-saving strategies: layer fusion (to\nreduce to zero the inference overhead) and an ``adapter ranking'' (to also\nreduce the transmission cost). Finally, we evaluate these results in terms of\ntask performance, memory, and computation on embedded devices, evidencing\ntrade-offs that open the road to more research in the field."
                },
                "authors": [
                    {
                        "name": "Leonardo Olivi"
                    },
                    {
                        "name": "Edoardo Santero Mormile"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Tartaglione"
                },
                "author": "Enzo Tartaglione",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15989v1",
                "updated": "2025-04-22T15:51:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    51,
                    0,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T15:51:00Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    51,
                    0,
                    1,
                    112,
                    0
                ],
                "title": "Token-Aware Coding Flow: A Study with Nano Surge in Reasoning Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Aware Coding Flow: A Study with Nano Surge in Reasoning Model"
                },
                "summary": "With the widespread application of large-scale language models (LLMs) in\nsoftware engineering, the Chain of Thought (CoT) approach has emerged as a\ncrucial tool for driving automated code generation and optimization. However,\ndespite the significant success of CoT methods in generating high-quality code,\nthe issue of token inflation during the reasoning process remains a formidable\nchallenge to model performance and efficiency, particularly when dealing with\ncomplex code smells. Code smells not only affect the maintainability and\nscalability of code but also significantly increase the computational burden\nduring LLM inference, leading to excessive token consumption and, consequently,\nreduced reasoning efficiency. This paper introduces an innovative Token-Aware\nCoding Flow method, aimed at addressing the token inflation problem caused by\nsmelly code in the CoT process. Through experimentation, we validate the\nsynergistic effect of code refactoring and prompt engineering strategies,\ndemonstrating that after eliminating code smells, token consumption during\nmodel inference is significantly reduced. The experimental results show that\nrefactored code, while maintaining functional consistency, can reduce token\nconsumption by up to 50\\%. Additionally, by explicitly prompting the type of\ncode smells in the prompt and incorporating strategies such as context\nawareness and role constraints, we further optimize the reasoning process,\nachieving a 24.5\\% to 30\\% reduction in token consumption. These optimizations\nnot only significantly enhance the model's reasoning efficiency and improve\ncode generation quality but also provide new insights for addressing\nperformance bottlenecks in complex code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of large-scale language models (LLMs) in\nsoftware engineering, the Chain of Thought (CoT) approach has emerged as a\ncrucial tool for driving automated code generation and optimization. However,\ndespite the significant success of CoT methods in generating high-quality code,\nthe issue of token inflation during the reasoning process remains a formidable\nchallenge to model performance and efficiency, particularly when dealing with\ncomplex code smells. Code smells not only affect the maintainability and\nscalability of code but also significantly increase the computational burden\nduring LLM inference, leading to excessive token consumption and, consequently,\nreduced reasoning efficiency. This paper introduces an innovative Token-Aware\nCoding Flow method, aimed at addressing the token inflation problem caused by\nsmelly code in the CoT process. Through experimentation, we validate the\nsynergistic effect of code refactoring and prompt engineering strategies,\ndemonstrating that after eliminating code smells, token consumption during\nmodel inference is significantly reduced. The experimental results show that\nrefactored code, while maintaining functional consistency, can reduce token\nconsumption by up to 50\\%. Additionally, by explicitly prompting the type of\ncode smells in the prompt and incorporating strategies such as context\nawareness and role constraints, we further optimize the reasoning process,\nachieving a 24.5\\% to 30\\% reduction in token consumption. These optimizations\nnot only significantly enhance the model's reasoning efficiency and improve\ncode generation quality but also provide new insights for addressing\nperformance bottlenecks in complex code generation tasks."
                },
                "authors": [
                    {
                        "name": "Junwei Hu"
                    },
                    {
                        "name": "Weicheng Zheng"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Yihan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Liu"
                },
                "author": "Yihan Liu",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03621v2",
                "updated": "2025-04-22T15:45:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    45,
                    39,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-04T15:26:10Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    26,
                    10,
                    2,
                    339,
                    0
                ],
                "title": "Network-aided Efficient LLM Services With Denoising-inspired Prompt\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network-aided Efficient LLM Services With Denoising-inspired Prompt\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, leading to their increasing adoption in diverse services\ndelivered through wireless networks. There is a growing trend toward longer\nprompts to better leverage LLMs' capabilities and address difficult tasks.\nHowever, longer prompts not only increase data transmission costs but also\nrequire more computing resources and processing time, which impacts overall\nsystem efficiency and user experience. To address this challenge, we propose\nJoint Power and Prompt Optimization (JPPO), a framework that combines Small\nLanguage Model (SLM)-based prompt compression with wireless power allocation\noptimization. By deploying SLM at edge devices for prompt compression and\nemploying Deep Reinforcement Learning (DRL) for joint optimization of\ncompression ratio and transmission power, JPPO effectively balances service\nquality with resource efficiency. Furthermore, inspired by denoising diffusion\nmodels, we design a denoising-inspired prompt compression approach that\niteratively compresses prompts by gradually removing non-critical information,\nfurther enhancing the framework's performance. Experimental results with long\nprompt tokens demonstrate that our framework achieves high service fidelity\nwhile optimizing power usage in wireless LLM services, significantly reducing\nthe total service response time. With our DRL-based JPPO, the framework\nmaintains fidelity comparable to the no-compression baseline while still\nachieving a 17% service time reduction through adaptive compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, leading to their increasing adoption in diverse services\ndelivered through wireless networks. There is a growing trend toward longer\nprompts to better leverage LLMs' capabilities and address difficult tasks.\nHowever, longer prompts not only increase data transmission costs but also\nrequire more computing resources and processing time, which impacts overall\nsystem efficiency and user experience. To address this challenge, we propose\nJoint Power and Prompt Optimization (JPPO), a framework that combines Small\nLanguage Model (SLM)-based prompt compression with wireless power allocation\noptimization. By deploying SLM at edge devices for prompt compression and\nemploying Deep Reinforcement Learning (DRL) for joint optimization of\ncompression ratio and transmission power, JPPO effectively balances service\nquality with resource efficiency. Furthermore, inspired by denoising diffusion\nmodels, we design a denoising-inspired prompt compression approach that\niteratively compresses prompts by gradually removing non-critical information,\nfurther enhancing the framework's performance. Experimental results with long\nprompt tokens demonstrate that our framework achieves high service fidelity\nwhile optimizing power usage in wireless LLM services, significantly reducing\nthe total service response time. With our DRL-based JPPO, the framework\nmaintains fidelity comparable to the no-compression baseline while still\nachieving a 17% service time reduction through adaptive compression."
                },
                "authors": [
                    {
                        "name": "Feiran You"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "arxiv_comment": "This paper has been accepted by SCIENCE CHINA Information Sciences.\n  arXiv admin note: substantial text overlap with arXiv:2411.18010",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15987v1",
                "updated": "2025-04-22T15:42:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    42,
                    33,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T15:42:33Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    42,
                    33,
                    1,
                    112,
                    0
                ],
                "title": "Few-shot Hate Speech Detection Based on the MindSpore Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Hate Speech Detection Based on the MindSpore Framework"
                },
                "summary": "The proliferation of hate speech on social media poses a significant threat\nto online communities, requiring effective detection systems. While deep\nlearning models have shown promise, their performance often deteriorates in\nfew-shot or low-resource settings due to reliance on large annotated corpora.\nTo address this, we propose MS-FSLHate, a prompt-enhanced neural framework for\nfew-shot hate speech detection implemented on the MindSpore deep learning\nplatform. The model integrates learnable prompt embeddings, a CNN-BiLSTM\nbackbone with attention pooling, and synonym-based adversarial data\naugmentation to improve generalization. Experimental results on two benchmark\ndatasets-HateXplain and HSOL-demonstrate that our approach outperforms\ncompetitive baselines in precision, recall, and F1-score. Additionally, the\nframework shows high efficiency and scalability, suggesting its suitability for\ndeployment in resource-constrained environments. These findings highlight the\npotential of combining prompt-based learning with adversarial augmentation for\nrobust and adaptable hate speech detection in few-shot scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of hate speech on social media poses a significant threat\nto online communities, requiring effective detection systems. While deep\nlearning models have shown promise, their performance often deteriorates in\nfew-shot or low-resource settings due to reliance on large annotated corpora.\nTo address this, we propose MS-FSLHate, a prompt-enhanced neural framework for\nfew-shot hate speech detection implemented on the MindSpore deep learning\nplatform. The model integrates learnable prompt embeddings, a CNN-BiLSTM\nbackbone with attention pooling, and synonym-based adversarial data\naugmentation to improve generalization. Experimental results on two benchmark\ndatasets-HateXplain and HSOL-demonstrate that our approach outperforms\ncompetitive baselines in precision, recall, and F1-score. Additionally, the\nframework shows high efficiency and scalability, suggesting its suitability for\ndeployment in resource-constrained environments. These findings highlight the\npotential of combining prompt-based learning with adversarial augmentation for\nrobust and adaptable hate speech detection in few-shot scenarios."
                },
                "authors": [
                    {
                        "name": "Zhenkai Qin"
                    },
                    {
                        "name": "Dongze Wu"
                    },
                    {
                        "name": "Yuxin Liu"
                    },
                    {
                        "name": "Guifang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guifang Yang"
                },
                "author": "Guifang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16683v2",
                "updated": "2025-04-22T15:37:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    37,
                    21,
                    1,
                    112,
                    0
                ],
                "published": "2024-08-29T16:28:43Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    28,
                    43,
                    3,
                    242,
                    0
                ],
                "title": "A Catalog of Fairness-Aware Practices in Machine Learning Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Catalog of Fairness-Aware Practices in Machine Learning Engineering"
                },
                "summary": "Machine learning's widespread adoption in decision-making processes raises\nconcerns about fairness, particularly regarding the treatment of sensitive\nfeatures and potential discrimination against minorities. The software\nengineering community has responded by developing fairness-oriented metrics,\nempirical studies, and approaches. However, there remains a gap in\nunderstanding and categorizing practices for engineering fairness throughout\nthe machine learning lifecycle. This paper presents a novel catalog of\npractices for addressing fairness in machine learning derived from a systematic\nmapping study. The study identifies and categorizes 28 practices from existing\nliterature, mapping them onto different stages of the machine learning\nlifecycle. From this catalog, the authors extract actionable items and\nimplications for both researchers and practitioners in software engineering.\nThis work aims to provide a comprehensive resource for integrating fairness\nconsiderations into the development and deployment of machine learning systems,\nenhancing their reliability, accountability, and credibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning's widespread adoption in decision-making processes raises\nconcerns about fairness, particularly regarding the treatment of sensitive\nfeatures and potential discrimination against minorities. The software\nengineering community has responded by developing fairness-oriented metrics,\nempirical studies, and approaches. However, there remains a gap in\nunderstanding and categorizing practices for engineering fairness throughout\nthe machine learning lifecycle. This paper presents a novel catalog of\npractices for addressing fairness in machine learning derived from a systematic\nmapping study. The study identifies and categorizes 28 practices from existing\nliterature, mapping them onto different stages of the machine learning\nlifecycle. From this catalog, the authors extract actionable items and\nimplications for both researchers and practitioners in software engineering.\nThis work aims to provide a comprehensive resource for integrating fairness\nconsiderations into the development and deployment of machine learning systems,\nenhancing their reliability, accountability, and credibility."
                },
                "authors": [
                    {
                        "name": "Gianmario Voria"
                    },
                    {
                        "name": "Giulia Sellitto"
                    },
                    {
                        "name": "Carmine Ferrara"
                    },
                    {
                        "name": "Francesco Abate"
                    },
                    {
                        "name": "Andrea De Lucia"
                    },
                    {
                        "name": "Filomena Ferrucci"
                    },
                    {
                        "name": "Gemma Catolino"
                    },
                    {
                        "name": "Fabio Palomba"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Palomba"
                },
                "author": "Fabio Palomba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15965v1",
                "updated": "2025-04-22T15:05:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    5,
                    4,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T15:05:04Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    5,
                    4,
                    1,
                    112,
                    0
                ],
                "title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era\n  of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era\n  of LLMs"
                },
                "summary": "Memory is the process of encoding, storing, and retrieving information,\nallowing humans to retain experiences, knowledge, skills, and facts over time,\nand serving as the foundation for growth and effective interaction with the\nworld. It plays a crucial role in shaping our identity, making decisions,\nlearning from past experiences, building relationships, and adapting to\nchanges. In the era of large language models (LLMs), memory refers to the\nability of an AI system to retain, recall, and use information from past\ninteractions to improve future responses and interactions. Although previous\nresearch and reviews have provided detailed descriptions of memory mechanisms,\nthere is still a lack of a systematic review that summarizes and analyzes the\nrelationship between the memory of LLM-driven AI systems and human memory, as\nwell as how we can be inspired by human memory to construct more powerful\nmemory systems. To achieve this, in this paper, we propose a comprehensive\nsurvey on the memory of LLM-driven AI systems. In particular, we first conduct\na detailed analysis of the categories of human memory and relate them to the\nmemory of AI systems. Second, we systematically organize existing\nmemory-related work and propose a categorization method based on three\ndimensions (object, form, and time) and eight quadrants. Finally, we illustrate\nsome open problems regarding the memory of current AI systems and outline\npossible future directions for memory in the era of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is the process of encoding, storing, and retrieving information,\nallowing humans to retain experiences, knowledge, skills, and facts over time,\nand serving as the foundation for growth and effective interaction with the\nworld. It plays a crucial role in shaping our identity, making decisions,\nlearning from past experiences, building relationships, and adapting to\nchanges. In the era of large language models (LLMs), memory refers to the\nability of an AI system to retain, recall, and use information from past\ninteractions to improve future responses and interactions. Although previous\nresearch and reviews have provided detailed descriptions of memory mechanisms,\nthere is still a lack of a systematic review that summarizes and analyzes the\nrelationship between the memory of LLM-driven AI systems and human memory, as\nwell as how we can be inspired by human memory to construct more powerful\nmemory systems. To achieve this, in this paper, we propose a comprehensive\nsurvey on the memory of LLM-driven AI systems. In particular, we first conduct\na detailed analysis of the categories of human memory and relate them to the\nmemory of AI systems. Second, we systematically organize existing\nmemory-related work and propose a categorization method based on three\ndimensions (object, form, and time) and eight quadrants. Finally, we illustrate\nsome open problems regarding the memory of current AI systems and outline\npossible future directions for memory in the era of large language models."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "26 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15958v1",
                "updated": "2025-04-22T14:55:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    55,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:55:23Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    55,
                    23,
                    1,
                    112,
                    0
                ],
                "title": "FreeGraftor: Training-Free Cross-Image Feature Grafting for\n  Subject-Driven Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeGraftor: Training-Free Cross-Image Feature Grafting for\n  Subject-Driven Text-to-Image Generation"
                },
                "summary": "Subject-driven image generation aims to synthesize novel scenes that\nfaithfully preserve subject identity from reference images while adhering to\ntextual guidance, yet existing methods struggle with a critical trade-off\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\nand resource-intensive subject-specific optimization, while zero-shot methods\nfail to maintain adequate subject consistency. In this work, we propose\nFreeGraftor, a training-free framework that addresses these limitations through\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\nmatching and position-constrained attention fusion to transfer visual details\nfrom reference subjects to the generated image. Additionally, our framework\nincorporates a novel noise initialization strategy to preserve geometry priors\nof reference subjects for robust feature matching. Extensive qualitative and\nquantitative experiments demonstrate that our method enables precise subject\nidentity transfer while maintaining text-aligned scene synthesis. Without\nrequiring model fine-tuning or additional training, FreeGraftor significantly\noutperforms existing zero-shot and training-free approaches in both subject\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\nto multi-subject generation, making it practical for real-world deployment. Our\ncode is available at https://github.com/Nihukat/FreeGraftor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subject-driven image generation aims to synthesize novel scenes that\nfaithfully preserve subject identity from reference images while adhering to\ntextual guidance, yet existing methods struggle with a critical trade-off\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\nand resource-intensive subject-specific optimization, while zero-shot methods\nfail to maintain adequate subject consistency. In this work, we propose\nFreeGraftor, a training-free framework that addresses these limitations through\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\nmatching and position-constrained attention fusion to transfer visual details\nfrom reference subjects to the generated image. Additionally, our framework\nincorporates a novel noise initialization strategy to preserve geometry priors\nof reference subjects for robust feature matching. Extensive qualitative and\nquantitative experiments demonstrate that our method enables precise subject\nidentity transfer while maintaining text-aligned scene synthesis. Without\nrequiring model fine-tuning or additional training, FreeGraftor significantly\noutperforms existing zero-shot and training-free approaches in both subject\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\nto multi-subject generation, making it practical for real-world deployment. Our\ncode is available at https://github.com/Nihukat/FreeGraftor."
                },
                "authors": [
                    {
                        "name": "Zebin Yao"
                    },
                    {
                        "name": "Lei Ren"
                    },
                    {
                        "name": "Huixing Jiang"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Xiaojie Wang"
                    },
                    {
                        "name": "Ruifan Li"
                    },
                    {
                        "name": "Fangxiang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fangxiang Feng"
                },
                "author": "Fangxiang Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19503v2",
                "updated": "2025-04-22T14:41:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    41,
                    35,
                    1,
                    112,
                    0
                ],
                "published": "2024-10-25T12:10:49Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    10,
                    49,
                    4,
                    299,
                    0
                ],
                "title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWITCH: Studying with Teacher for Knowledge Distillation of Large\n  Language Models"
                },
                "summary": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) as training data being\nparticularly notable for reducing the mismatch between training and inference.\nHowever, SGOs often produce noisy and biased sequences, which can lead to\nmisguidance from the teacher model, especially in long sequences. To mitigate\nthese challenges, we propose SWITCH (Studying WIth TeaCHer for Knowledge\nDistillation), a novel approach that strategically incorporates the teacher\nmodel during the student's sequence generation. SWITCH identifies discrepancies\nbetween the token probabilities of the teacher and student models, allowing the\nteacher to intervene selectively, particularly in long sequences that are more\nprone to teacher misguidance. Extensive experimental results across three model\nfamilies and five instruction-following datasets show that SWITCH surpasses\ntraditional KD methods, particularly excelling in the generation of long\nsequential data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) as training data being\nparticularly notable for reducing the mismatch between training and inference.\nHowever, SGOs often produce noisy and biased sequences, which can lead to\nmisguidance from the teacher model, especially in long sequences. To mitigate\nthese challenges, we propose SWITCH (Studying WIth TeaCHer for Knowledge\nDistillation), a novel approach that strategically incorporates the teacher\nmodel during the student's sequence generation. SWITCH identifies discrepancies\nbetween the token probabilities of the teacher and student models, allowing the\nteacher to intervene selectively, particularly in long sequences that are more\nprone to teacher misguidance. Extensive experimental results across three model\nfamilies and five instruction-following datasets show that SWITCH surpasses\ntraditional KD methods, particularly excelling in the generation of long\nsequential data."
                },
                "authors": [
                    {
                        "name": "Jahyun Koo"
                    },
                    {
                        "name": "Yerin Hwang"
                    },
                    {
                        "name": "Yongil Kim"
                    },
                    {
                        "name": "Taegwan Kang"
                    },
                    {
                        "name": "Hyunkyung Bae"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "arxiv_comment": "NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15941v1",
                "updated": "2025-04-22T14:35:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    35,
                    16,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:35:16Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    35,
                    16,
                    1,
                    112,
                    0
                ],
                "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in\n  Machine Translation by Overcoming Gender Binarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in\n  Machine Translation by Overcoming Gender Binarity"
                },
                "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub."
                },
                "authors": [
                    {
                        "name": "Fanny Jourdan"
                    },
                    {
                        "name": "Yannick Chevalier"
                    },
                    {
                        "name": "C√©cile Favre"
                    }
                ],
                "author_detail": {
                    "name": "C√©cile Favre"
                },
                "author": "C√©cile Favre",
                "arxiv_comment": "FAccT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15933v1",
                "updated": "2025-04-22T14:21:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    21,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:21:34Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    21,
                    34,
                    1,
                    112,
                    0
                ],
                "title": "Low-Rank Adaptation of Neural Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation of Neural Fields"
                },
                "summary": "Processing visual data often involves small adjustments or sequences of\nchanges, such as in image filtering, surface smoothing, and video storage.\nWhile established graphics techniques like normal mapping and video compression\nexploit redundancy to encode such small changes efficiently, the problem of\nencoding small changes to neural fields (NF) -- neural network\nparameterizations of visual or physical functions -- has received less\nattention.\n  We propose a parameter-efficient strategy for updating neural fields using\nlow-rank adaptations (LoRA). LoRA, a method from the parameter-efficient\nfine-tuning LLM community, encodes small updates to pre-trained models with\nminimal computational overhead. We adapt LoRA to instance-specific neural\nfields, avoiding the need for large pre-trained models yielding a pipeline\nsuitable for low-compute hardware.\n  We validate our approach with experiments in image filtering, video\ncompression, and geometry editing, demonstrating its effectiveness and\nversatility for representing neural field updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing visual data often involves small adjustments or sequences of\nchanges, such as in image filtering, surface smoothing, and video storage.\nWhile established graphics techniques like normal mapping and video compression\nexploit redundancy to encode such small changes efficiently, the problem of\nencoding small changes to neural fields (NF) -- neural network\nparameterizations of visual or physical functions -- has received less\nattention.\n  We propose a parameter-efficient strategy for updating neural fields using\nlow-rank adaptations (LoRA). LoRA, a method from the parameter-efficient\nfine-tuning LLM community, encodes small updates to pre-trained models with\nminimal computational overhead. We adapt LoRA to instance-specific neural\nfields, avoiding the need for large pre-trained models yielding a pipeline\nsuitable for low-compute hardware.\n  We validate our approach with experiments in image filtering, video\ncompression, and geometry editing, demonstrating its effectiveness and\nversatility for representing neural field updates."
                },
                "authors": [
                    {
                        "name": "Anh Truong"
                    },
                    {
                        "name": "Ahmed H. Mahmoud"
                    },
                    {
                        "name": "Mina Konakoviƒá Lukoviƒá"
                    },
                    {
                        "name": "Justin Solomon"
                    }
                ],
                "author_detail": {
                    "name": "Justin Solomon"
                },
                "author": "Justin Solomon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15930v1",
                "updated": "2025-04-22T14:19:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    19,
                    6,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:19:06Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    19,
                    6,
                    1,
                    112,
                    0
                ],
                "title": "StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with\n  Disaggregated Stream Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with\n  Disaggregated Stream Generation"
                },
                "summary": "Reinforcement learning (RL) has become the core post-training technique for\nlarge language models (LLMs). RL for LLMs involves two stages: generation and\ntraining. The LLM first generates samples online, which are then used to derive\nrewards for training. The conventional view holds that the colocated\narchitecture, where the two stages share resources via temporal multiplexing,\noutperforms the disaggregated architecture, in which dedicated resources are\nassigned to each stage. However, in real-world deployments, we observe that the\ncolocated architecture suffers from resource coupling, where the two stages are\nconstrained to use the same resources. This coupling compromises the\nscalability and cost-efficiency of colocated RL in large-scale training. In\ncontrast, the disaggregated architecture allows for flexible resource\nallocation, supports heterogeneous training setups, and facilitates\ncross-datacenter deployment.\n  StreamRL is designed with disaggregation from first principles and fully\nunlocks its potential by addressing two types of performance bottlenecks in\nexisting disaggregated RL frameworks: pipeline bubbles, caused by stage\ndependencies, and skewness bubbles, resulting from long-tail output length\ndistributions. To address pipeline bubbles, StreamRL breaks the traditional\nstage boundary in synchronous RL algorithms through stream generation and\nachieves full overlapping in asynchronous RL. To address skewness bubbles,\nStreamRL employs an output-length ranker model to identify long-tail samples\nand reduces generation time via skewness-aware dispatching and scheduling.\nExperiments show that StreamRL improves throughput by up to 2.66x compared to\nexisting state-of-the-art systems, and improves cost-effectiveness by up to\n1.33x in a heterogeneous, cross-datacenter setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become the core post-training technique for\nlarge language models (LLMs). RL for LLMs involves two stages: generation and\ntraining. The LLM first generates samples online, which are then used to derive\nrewards for training. The conventional view holds that the colocated\narchitecture, where the two stages share resources via temporal multiplexing,\noutperforms the disaggregated architecture, in which dedicated resources are\nassigned to each stage. However, in real-world deployments, we observe that the\ncolocated architecture suffers from resource coupling, where the two stages are\nconstrained to use the same resources. This coupling compromises the\nscalability and cost-efficiency of colocated RL in large-scale training. In\ncontrast, the disaggregated architecture allows for flexible resource\nallocation, supports heterogeneous training setups, and facilitates\ncross-datacenter deployment.\n  StreamRL is designed with disaggregation from first principles and fully\nunlocks its potential by addressing two types of performance bottlenecks in\nexisting disaggregated RL frameworks: pipeline bubbles, caused by stage\ndependencies, and skewness bubbles, resulting from long-tail output length\ndistributions. To address pipeline bubbles, StreamRL breaks the traditional\nstage boundary in synchronous RL algorithms through stream generation and\nachieves full overlapping in asynchronous RL. To address skewness bubbles,\nStreamRL employs an output-length ranker model to identify long-tail samples\nand reduces generation time via skewness-aware dispatching and scheduling.\nExperiments show that StreamRL improves throughput by up to 2.66x compared to\nexisting state-of-the-art systems, and improves cost-effectiveness by up to\n1.33x in a heterogeneous, cross-datacenter setting."
                },
                "authors": [
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Chao Jin"
                    },
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21131v3",
                "updated": "2025-04-22T14:15:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    15,
                    38,
                    1,
                    112,
                    0
                ],
                "published": "2024-10-28T15:33:37Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    33,
                    37,
                    0,
                    302,
                    0
                ],
                "title": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments"
                },
                "summary": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks."
                },
                "authors": [
                    {
                        "name": "Marharyta Domnich"
                    },
                    {
                        "name": "Julius V√§lja"
                    },
                    {
                        "name": "Rasmus Moorits Veski"
                    },
                    {
                        "name": "Giacomo Magnifico"
                    },
                    {
                        "name": "Kadi Tulver"
                    },
                    {
                        "name": "Eduard Barbu"
                    },
                    {
                        "name": "Raul Vicente"
                    }
                ],
                "author_detail": {
                    "name": "Raul Vicente"
                },
                "author": "Raul Vicente",
                "arxiv_doi": "10.1609/aaai.v39i15.33791",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1609/aaai.v39i15.33791",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.21131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper extends the AAAI-2025 version by including the Appendix",
                "arxiv_journal_ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  39(15), 16308-16316, 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09597v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09597v4",
                "updated": "2025-04-22T14:11:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    11,
                    33,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-13T14:31:52Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    31,
                    52,
                    6,
                    103,
                    0
                ],
                "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions."
                },
                "authors": [
                    {
                        "name": "Zhixuan Pan"
                    },
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09597v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09597v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15922v2",
                "updated": "2025-04-23T07:24:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    24,
                    40,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T14:06:02Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    6,
                    2,
                    1,
                    112,
                    0
                ],
                "title": "Language Models to Support Multi-Label Classification of Industrial Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models to Support Multi-Label Classification of Industrial Data"
                },
                "summary": "Multi-label requirements classification is a challenging task, especially\nwhen dealing with numerous classes at varying levels of abstraction. The\ndifficulties increases when a limited number of requirements is available to\ntrain a supervised classifier. Zero-shot learning (ZSL) does not require\ntraining data and can potentially address this problem. This paper investigates\nthe performance of zero-shot classifiers (ZSCs) on a multi-label industrial\ndataset. We focuse on classifying requirements according to a taxonomy designed\nto support requirements tracing. We compare multiple variants of ZSCs using\ndifferent embeddings, including 9 language models (LMs) with a reduced number\nof parameters (up to 3B), e.g., BERT, and 5 large LMs (LLMs) with a large\nnumber of parameters (up to 70B), e.g., Llama. Our ground truth includes 377\nrequirements and 1968 labels from 6 output spaces. For the evaluation, we adopt\ntraditional metrics, i.e., precision, recall, F1, and $F_\\beta$, as well as a\nnovel label distance metric Dn. This aims to better capture the\nclassification's hierarchical nature and provides a more nuanced evaluation of\nhow far the results are from the ground truth. 1) The top-performing model on 5\nout of 6 output spaces is T5-xl, with maximum $F_\\beta$ = 0.78 and Dn = 0.04,\nwhile BERT base outperformed the other models in one case, with maximum\n$F_\\beta$ = 0.83 and Dn = 0.04. 2) LMs with smaller parameter size produce the\nbest classification results compared to LLMs. Thus, addressing the problem in\npractice is feasible as limited computing power is needed. 3) The model\narchitecture (autoencoding, autoregression, and sentence-to-sentence)\nsignificantly affects the classifier's performance. We conclude that using ZSL\nfor multi-label requirements classification offers promising results. We also\npresent a novel metric that can be used to select the top-performing model for\nthis problem",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-label requirements classification is a challenging task, especially\nwhen dealing with numerous classes at varying levels of abstraction. The\ndifficulties increases when a limited number of requirements is available to\ntrain a supervised classifier. Zero-shot learning (ZSL) does not require\ntraining data and can potentially address this problem. This paper investigates\nthe performance of zero-shot classifiers (ZSCs) on a multi-label industrial\ndataset. We focuse on classifying requirements according to a taxonomy designed\nto support requirements tracing. We compare multiple variants of ZSCs using\ndifferent embeddings, including 9 language models (LMs) with a reduced number\nof parameters (up to 3B), e.g., BERT, and 5 large LMs (LLMs) with a large\nnumber of parameters (up to 70B), e.g., Llama. Our ground truth includes 377\nrequirements and 1968 labels from 6 output spaces. For the evaluation, we adopt\ntraditional metrics, i.e., precision, recall, F1, and $F_\\beta$, as well as a\nnovel label distance metric Dn. This aims to better capture the\nclassification's hierarchical nature and provides a more nuanced evaluation of\nhow far the results are from the ground truth. 1) The top-performing model on 5\nout of 6 output spaces is T5-xl, with maximum $F_\\beta$ = 0.78 and Dn = 0.04,\nwhile BERT base outperformed the other models in one case, with maximum\n$F_\\beta$ = 0.83 and Dn = 0.04. 2) LMs with smaller parameter size produce the\nbest classification results compared to LLMs. Thus, addressing the problem in\npractice is feasible as limited computing power is needed. 3) The model\narchitecture (autoencoding, autoregression, and sentence-to-sentence)\nsignificantly affects the classifier's performance. We conclude that using ZSL\nfor multi-label requirements classification offers promising results. We also\npresent a novel metric that can be used to select the top-performing model for\nthis problem"
                },
                "authors": [
                    {
                        "name": "Waleed Abdeen"
                    },
                    {
                        "name": "Michael Unterkalmsteiner"
                    },
                    {
                        "name": "Krzysztof Wnuk"
                    },
                    {
                        "name": "Alessio Ferrari"
                    },
                    {
                        "name": "Panagiota Chatzipetrou"
                    }
                ],
                "author_detail": {
                    "name": "Panagiota Chatzipetrou"
                },
                "author": "Panagiota Chatzipetrou",
                "arxiv_comment": "Accepted at SANER Conference 2025. Awaiting publication by IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15921v1",
                "updated": "2025-04-22T14:06:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    6,
                    1,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:06:01Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    6,
                    1,
                    1,
                    112,
                    0
                ],
                "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting"
                },
                "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication."
                },
                "authors": [
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Dimitrios Korkinof"
                    },
                    {
                        "name": "Shaogang Gong"
                    },
                    {
                        "name": "Mariano Beguerisse-Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Mariano Beguerisse-Diaz"
                },
                "author": "Mariano Beguerisse-Diaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15917v1",
                "updated": "2025-04-22T14:02:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    2,
                    57,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T14:02:57Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    2,
                    57,
                    1,
                    112,
                    0
                ],
                "title": "Towards Test Generation from Task Description for Mobile Testing with\n  Multi-modal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Test Generation from Task Description for Mobile Testing with\n  Multi-modal Reasoning"
                },
                "summary": "In Android GUI testing, generating an action sequence for a task that can be\nreplayed as a test script is common. Generating sequences of actions and\nrespective test scripts from task goals described in natural language can\neliminate the need for manually writing test scripts. However, existing\napproaches based on large language models (LLM) often struggle with identifying\nthe final action, and either end prematurely or continue past the final screen.\nIn this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent\nframework that iteratively determines the next action and leverages visual\nimages of screens to detect the task's completeness. The multi-modal approach\nenhances our model in two significant ways. First, this approach enables it to\navoid prematurely terminating a task when textual content alone provides\nmisleading indications of task completion. Additionally, visual input helps the\ntool avoid errors when changes in the GUI do not directly affect functionality\ntoward task completion, such as adjustments to font sizes or colors. Second,\nthe multi-modal approach also ensures the tool not progress beyond the final\nscreen, which might lack explicit textual indicators of task completion but\ncould display a visual element indicating task completion, which is common in\nGUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%,\noutperforming the best baseline relatively by 23.5%. We also demonstrate that\nour multi-modal framework with images and texts enables the LLM to better\ndetermine when a task is completed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Android GUI testing, generating an action sequence for a task that can be\nreplayed as a test script is common. Generating sequences of actions and\nrespective test scripts from task goals described in natural language can\neliminate the need for manually writing test scripts. However, existing\napproaches based on large language models (LLM) often struggle with identifying\nthe final action, and either end prematurely or continue past the final screen.\nIn this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent\nframework that iteratively determines the next action and leverages visual\nimages of screens to detect the task's completeness. The multi-modal approach\nenhances our model in two significant ways. First, this approach enables it to\navoid prematurely terminating a task when textual content alone provides\nmisleading indications of task completion. Additionally, visual input helps the\ntool avoid errors when changes in the GUI do not directly affect functionality\ntoward task completion, such as adjustments to font sizes or colors. Second,\nthe multi-modal approach also ensures the tool not progress beyond the final\nscreen, which might lack explicit textual indicators of task completion but\ncould display a visual element indicating task completion, which is common in\nGUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%,\noutperforming the best baseline relatively by 23.5%. We also demonstrate that\nour multi-modal framework with images and texts enables the LLM to better\ndetermine when a task is completed."
                },
                "authors": [
                    {
                        "name": "Hieu Huynh"
                    },
                    {
                        "name": "Hai Phung"
                    },
                    {
                        "name": "Hao Pham"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Vu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Vu Nguyen"
                },
                "author": "Vu Nguyen",
                "arxiv_comment": "Under review for a conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09385v2",
                "updated": "2025-04-22T13:56:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    56,
                    32,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-12T15:52:41Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    52,
                    41,
                    3,
                    347,
                    0
                ],
                "title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore\n  LLMs' Complex Reasoning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore\n  LLMs' Complex Reasoning Capabilities"
                },
                "summary": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Fabrizio Davide"
                    },
                    {
                        "name": "Pietro Torre"
                    },
                    {
                        "name": "Leonardo Ercolani"
                    },
                    {
                        "name": "Andrea Gaggioli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Gaggioli"
                },
                "author": "Andrea Gaggioli",
                "arxiv_comment": "47 pages, 8 figures, 17 tables, appendix with data and code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15909v1",
                "updated": "2025-04-22T13:55:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    55,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:55:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    55,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "Synergizing RAG and Reasoning: A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing RAG and Reasoning: A Systematic Review"
                },
                "summary": "Recent breakthroughs in large language models (LLMs), particularly in\nreasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to\nunprecedented levels. By synergizing retrieval mechanisms with advanced\nreasoning, LLMs can now tackle increasingly complex problems. This paper\npresents a systematic review of the collaborative interplay between RAG and\nreasoning, clearly defining \"reasoning\" within the RAG context. It construct a\ncomprehensive taxonomy encompassing multi-dimensional collaborative objectives,\nrepresentative paradigms, and technical implementations, and analyze the\nbidirectional synergy methods. Additionally, we critically evaluate current\nlimitations in RAG assessment, including the absence of intermediate\nsupervision for multi-step reasoning and practical challenges related to\ncost-risk trade-offs. To bridge theory and practice, we provide practical\nguidelines tailored to diverse real-world applications. Finally, we identify\npromising research directions, such as graph-based knowledge integration,\nhybrid model collaboration, and RL-driven optimization. Overall, this work\npresents a theoretical framework and practical foundation to advance RAG\nsystems in academia and industry, fostering the next generation of RAG\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large language models (LLMs), particularly in\nreasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to\nunprecedented levels. By synergizing retrieval mechanisms with advanced\nreasoning, LLMs can now tackle increasingly complex problems. This paper\npresents a systematic review of the collaborative interplay between RAG and\nreasoning, clearly defining \"reasoning\" within the RAG context. It construct a\ncomprehensive taxonomy encompassing multi-dimensional collaborative objectives,\nrepresentative paradigms, and technical implementations, and analyze the\nbidirectional synergy methods. Additionally, we critically evaluate current\nlimitations in RAG assessment, including the absence of intermediate\nsupervision for multi-step reasoning and practical challenges related to\ncost-risk trade-offs. To bridge theory and practice, we provide practical\nguidelines tailored to diverse real-world applications. Finally, we identify\npromising research directions, such as graph-based knowledge integration,\nhybrid model collaboration, and RL-driven optimization. Overall, this work\npresents a theoretical framework and practical foundation to advance RAG\nsystems in academia and industry, fostering the next generation of RAG\nsolutions."
                },
                "authors": [
                    {
                        "name": "Yunfan Gao"
                    },
                    {
                        "name": "Yun Xiong"
                    },
                    {
                        "name": "Yijie Zhong"
                    },
                    {
                        "name": "Yuxi Bi"
                    },
                    {
                        "name": "Ming Xue"
                    },
                    {
                        "name": "Haofen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haofen Wang"
                },
                "author": "Haofen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14411v2",
                "updated": "2025-04-22T13:52:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    52,
                    0,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-19T21:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    21,
                    58,
                    0,
                    5,
                    109,
                    0
                ],
                "title": "Planet as a Brain: Towards Internet of AgentSites based on AIOS Server",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planet as a Brain: Towards Internet of AgentSites based on AIOS Server"
                },
                "summary": "The internet is undergoing a historical transformation from the \"Internet of\nWebsites\" to the \"Internet of AgentSites.\" While traditional Websites served as\nthe foundation for information hosting and dissemination, a new frontier is\nemerging where AgentSites serve as the hubs of the internet, where each\nAgentSite hosts one or more AI agents that receive tasks, address them, and\ndeliver actionable solutions, marking a significant shift in the digital\nlandscape and representing the next generation of online ecosystems. Under this\nvision, AIOS, the AI Agent Operating System, serves as the server for the\ndevelopment, deployment and execution of AI agents, which is a fundamental\ninfrastructure for the Internet of Agentsites.\n  In this paper, we introduce AIOS Server, a runtime framework to host agents\nand enable global-scale collaboration among decentralized agents. AIOS Server\nprovides a communication protocol leveraging the Model Context Protocol (MCP)\nand JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node\noperates as a server to host and execute agents, while supporting peer-to-peer\ncoordination without reliance on centralized orchestration. Based on AIOS\nServer, we further present the world's first practically deployed Internet of\nAgentsites (AIOS-IoA), including AgentHub for agent registration and discovery\nand AgentChat for interactive communication, at https://planet.aios.foundation.\nThe agent discovery mechanism based on Distributed Hash Tables (DHT) and a\nGossip protocol serves as the search engine for the internet of agentsites.\nThis work provides a practical foundation for building the Internet of\nAgentsites-a new paradigm where autonomous agents become first-class citizens\nof the web. The implementation is available at\nhttps://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS\nmain branch at https://github.com/agiresearch/AIOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The internet is undergoing a historical transformation from the \"Internet of\nWebsites\" to the \"Internet of AgentSites.\" While traditional Websites served as\nthe foundation for information hosting and dissemination, a new frontier is\nemerging where AgentSites serve as the hubs of the internet, where each\nAgentSite hosts one or more AI agents that receive tasks, address them, and\ndeliver actionable solutions, marking a significant shift in the digital\nlandscape and representing the next generation of online ecosystems. Under this\nvision, AIOS, the AI Agent Operating System, serves as the server for the\ndevelopment, deployment and execution of AI agents, which is a fundamental\ninfrastructure for the Internet of Agentsites.\n  In this paper, we introduce AIOS Server, a runtime framework to host agents\nand enable global-scale collaboration among decentralized agents. AIOS Server\nprovides a communication protocol leveraging the Model Context Protocol (MCP)\nand JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node\noperates as a server to host and execute agents, while supporting peer-to-peer\ncoordination without reliance on centralized orchestration. Based on AIOS\nServer, we further present the world's first practically deployed Internet of\nAgentsites (AIOS-IoA), including AgentHub for agent registration and discovery\nand AgentChat for interactive communication, at https://planet.aios.foundation.\nThe agent discovery mechanism based on Distributed Hash Tables (DHT) and a\nGossip protocol serves as the search engine for the internet of agentsites.\nThis work provides a practical foundation for building the Internet of\nAgentsites-a new paradigm where autonomous agents become first-class citizens\nof the web. The implementation is available at\nhttps://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS\nmain branch at https://github.com/agiresearch/AIOS."
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15903v1",
                "updated": "2025-04-22T13:43:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    43,
                    58,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:43:58Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    43,
                    58,
                    1,
                    112,
                    0
                ],
                "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning\n  Corpus (ARC) Tasks with Model Temperature Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning\n  Corpus (ARC) Tasks with Model Temperature Considerations"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility."
                },
                "authors": [
                    {
                        "name": "Nikhil Khandalkar"
                    },
                    {
                        "name": "Pavan Yadav"
                    },
                    {
                        "name": "Krishna Shinde"
                    },
                    {
                        "name": "Lokesh B. Ramegowda"
                    },
                    {
                        "name": "Rajarshi Das"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Das"
                },
                "author": "Rajarshi Das",
                "arxiv_comment": "60 pages, 25 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15900v1",
                "updated": "2025-04-22T13:41:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    41,
                    26,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    41,
                    26,
                    1,
                    112,
                    0
                ],
                "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement\n  Learning"
                },
                "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding."
                },
                "authors": [
                    {
                        "name": "Cheng Wen"
                    },
                    {
                        "name": "Tingwei Guo"
                    },
                    {
                        "name": "Shuaijiang Zhao"
                    },
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15895v1",
                "updated": "2025-04-22T13:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    36,
                    53,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:36:53Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    36,
                    53,
                    1,
                    112,
                    0
                ],
                "title": "Dynamic Early Exit in Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Early Exit in Reasoning Models"
                },
                "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%."
                },
                "authors": [
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Yongjie Duan"
                    },
                    {
                        "name": "Zheliang Zhu"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14350v2",
                "updated": "2025-04-22T13:31:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    31,
                    25,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-19T16:32:28Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    16,
                    32,
                    28,
                    5,
                    109,
                    0
                ],
                "title": "Time's Up! An Empirical Study of LLM Reasoning Ability Under Output\n  Length Constraint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time's Up! An Empirical Study of LLM Reasoning Ability Under Output\n  Length Constraint"
                },
                "summary": "Recent work has demonstrated the remarkable potential of Large Language\nModels (LLMs) in test-time scaling. By making the models think before\nanswering, they are able to achieve much higher accuracy with extra inference\ncomputation. However, in many real-world scenarios, models are used under time\nconstraints, where an answer should be given to the user within a certain\noutput length. It is unclear whether and how the reasoning abilities of LLMs\nremain effective under such constraints. We take a first look at this problem\nby conducting an in-depth empirical study. Specifically, we test more than 25\nLLMs on common reasoning datasets under a wide range of output length budgets,\nand we analyze the correlation between the inference accuracy and various\nproperties including model type, model size, prompt style, etc. We also\nconsider the mappings between the token budgets and the actual on-device\nlatency budgets. The results have demonstrated several interesting findings\nregarding the budget-aware LLM reasoning that differ from the unconstrained\nsituation, e.g. the optimal choices of model sizes and prompts change under\ndifferent budgets. These findings offer practical guidance for users to deploy\nLLMs under real-world latency constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated the remarkable potential of Large Language\nModels (LLMs) in test-time scaling. By making the models think before\nanswering, they are able to achieve much higher accuracy with extra inference\ncomputation. However, in many real-world scenarios, models are used under time\nconstraints, where an answer should be given to the user within a certain\noutput length. It is unclear whether and how the reasoning abilities of LLMs\nremain effective under such constraints. We take a first look at this problem\nby conducting an in-depth empirical study. Specifically, we test more than 25\nLLMs on common reasoning datasets under a wide range of output length budgets,\nand we analyze the correlation between the inference accuracy and various\nproperties including model type, model size, prompt style, etc. We also\nconsider the mappings between the token budgets and the actual on-device\nlatency budgets. The results have demonstrated several interesting findings\nregarding the budget-aware LLM reasoning that differ from the unconstrained\nsituation, e.g. the optimal choices of model sizes and prompts change under\ndifferent budgets. These findings offer practical guidance for users to deploy\nLLMs under real-world latency constraints."
                },
                "authors": [
                    {
                        "name": "Yi Sun"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jiaqiang Li"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Huiwen Zheng"
                    },
                    {
                        "name": "Yan Liang"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15867v1",
                "updated": "2025-04-22T13:09:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    9,
                    20,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T13:09:20Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    9,
                    20,
                    1,
                    112,
                    0
                ],
                "title": "Inducing Vulnerable Code Generation in LLM Coding Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inducing Vulnerable Code Generation in LLM Coding Assistants"
                },
                "summary": "Due to insufficient domain knowledge, LLM coding assistants often reference\nrelated solutions from the Internet to address programming problems. However,\nincorporating external information into LLMs' code generation process\nintroduces new security risks. In this paper, we reveal a real-world threat,\nnamed HACKODE, where attackers exploit referenced external information to embed\nattack sequences, causing LLMs to produce code with vulnerabilities such as\nbuffer overflows and incomplete validations. We designed a prototype of the\nattack, which generates effective attack sequences for potential diverse inputs\nwith various user queries and prompt templates. Through the evaluation on two\ngeneral LLMs and two code LLMs, we demonstrate that the attack is effective,\nachieving an 84.29% success rate. Additionally, on a real-world application,\nHACKODE achieves 75.92% ASR, demonstrating its real-world impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to insufficient domain knowledge, LLM coding assistants often reference\nrelated solutions from the Internet to address programming problems. However,\nincorporating external information into LLMs' code generation process\nintroduces new security risks. In this paper, we reveal a real-world threat,\nnamed HACKODE, where attackers exploit referenced external information to embed\nattack sequences, causing LLMs to produce code with vulnerabilities such as\nbuffer overflows and incomplete validations. We designed a prototype of the\nattack, which generates effective attack sequences for potential diverse inputs\nwith various user queries and prompt templates. Through the evaluation on two\ngeneral LLMs and two code LLMs, we demonstrate that the attack is effective,\nachieving an 84.29% success rate. Additionally, on a real-world application,\nHACKODE achieves 75.92% ASR, demonstrating its real-world impact."
                },
                "authors": [
                    {
                        "name": "Binqi Zeng"
                    },
                    {
                        "name": "Quan Zhang"
                    },
                    {
                        "name": "Chijin Zhou"
                    },
                    {
                        "name": "Gwihwan Go"
                    },
                    {
                        "name": "Yu Jiang"
                    },
                    {
                        "name": "Heyuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Heyuan Shi"
                },
                "author": "Heyuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15848v1",
                "updated": "2025-04-22T12:43:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    43,
                    37,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T12:43:37Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    43,
                    37,
                    1,
                    112,
                    0
                ],
                "title": "Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based\n  Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based\n  Sentiment Analysis"
                },
                "summary": "Multimodal aspect-based sentiment classification (MASC) is an emerging task\ndue to an increase in user-generated multimodal content on social platforms,\naimed at predicting sentiment polarity toward specific aspect targets (i.e.,\nentities or attributes explicitly mentioned in text-image pairs). Despite\nextensive efforts and significant achievements in existing MASC, substantial\ngaps remain in understanding fine-grained visual content and the cognitive\nrationales derived from semantic content and impressions (cognitive\ninterpretations of emotions evoked by image content). In this study, we present\nChimera: a cognitive and aesthetic sentiment causality understanding framework\nto derive fine-grained holistic features of aspects and infer the fundamental\ndrivers of sentiment expression from both semantic perspectives and\naffective-cognitive resonance (the synergistic effect between emotional\nresponses and cognitive interpretations). Specifically, this framework first\nincorporates visual patch features for patch-word alignment. Meanwhile, it\nextracts coarse-grained visual features (e.g., overall image representation)\nand fine-grained visual regions (e.g., aspect-related regions) and translates\nthem into corresponding textual descriptions (e.g., facial, aesthetic).\nFinally, we leverage the sentimental causes and impressions generated by a\nlarge language model (LLM) to enhance the model's awareness of sentimental cues\nevoked by semantic content and affective-cognitive resonance. Experimental\nresults on standard MASC datasets demonstrate the effectiveness of the proposed\nmodel, which also exhibits greater flexibility to MASC compared to LLMs such as\nGPT-4o. We have publicly released the complete implementation and dataset at\nhttps://github.com/Xillv/Chimera",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal aspect-based sentiment classification (MASC) is an emerging task\ndue to an increase in user-generated multimodal content on social platforms,\naimed at predicting sentiment polarity toward specific aspect targets (i.e.,\nentities or attributes explicitly mentioned in text-image pairs). Despite\nextensive efforts and significant achievements in existing MASC, substantial\ngaps remain in understanding fine-grained visual content and the cognitive\nrationales derived from semantic content and impressions (cognitive\ninterpretations of emotions evoked by image content). In this study, we present\nChimera: a cognitive and aesthetic sentiment causality understanding framework\nto derive fine-grained holistic features of aspects and infer the fundamental\ndrivers of sentiment expression from both semantic perspectives and\naffective-cognitive resonance (the synergistic effect between emotional\nresponses and cognitive interpretations). Specifically, this framework first\nincorporates visual patch features for patch-word alignment. Meanwhile, it\nextracts coarse-grained visual features (e.g., overall image representation)\nand fine-grained visual regions (e.g., aspect-related regions) and translates\nthem into corresponding textual descriptions (e.g., facial, aesthetic).\nFinally, we leverage the sentimental causes and impressions generated by a\nlarge language model (LLM) to enhance the model's awareness of sentimental cues\nevoked by semantic content and affective-cognitive resonance. Experimental\nresults on standard MASC datasets demonstrate the effectiveness of the proposed\nmodel, which also exhibits greater flexibility to MASC compared to LLMs such as\nGPT-4o. We have publicly released the complete implementation and dataset at\nhttps://github.com/Xillv/Chimera"
                },
                "authors": [
                    {
                        "name": "Luwei Xiao"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Yanhao Jia"
                    },
                    {
                        "name": "Liang He"
                    },
                    {
                        "name": "Erik Cambria"
                    }
                ],
                "author_detail": {
                    "name": "Erik Cambria"
                },
                "author": "Erik Cambria",
                "arxiv_comment": "Accepted by TAFFC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15846v1",
                "updated": "2025-04-22T12:42:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    42,
                    38,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T12:42:38Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    42,
                    38,
                    1,
                    112,
                    0
                ],
                "title": "Adaptive PCA-Based Outlier Detection for Multi-Feature Time Series in\n  Space Missions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive PCA-Based Outlier Detection for Multi-Feature Time Series in\n  Space Missions"
                },
                "summary": "Analyzing multi-featured time series data is critical for space missions\nmaking efficient event detection, potentially onboard, essential for automatic\nanalysis. However, limited onboard computational resources and data downlink\nconstraints necessitate robust methods for identifying regions of interest in\nreal time. This work presents an adaptive outlier detection algorithm based on\nthe reconstruction error of Principal Component Analysis (PCA) for feature\nreduction, designed explicitly for space mission applications. The algorithm\nadapts dynamically to evolving data distributions by using Incremental PCA,\nenabling deployment without a predefined model for all possible conditions. A\npre-scaling process normalizes each feature's magnitude while preserving\nrelative variance within feature types. We demonstrate the algorithm's\neffectiveness in detecting space plasma events, such as distinct space\nenvironments, dayside and nightside transients phenomena, and transition layers\nthrough NASA's MMS mission observations. Additionally, we apply the method to\nNASA's THEMIS data, successfully identifying a dayside transient using\nonboard-available measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing multi-featured time series data is critical for space missions\nmaking efficient event detection, potentially onboard, essential for automatic\nanalysis. However, limited onboard computational resources and data downlink\nconstraints necessitate robust methods for identifying regions of interest in\nreal time. This work presents an adaptive outlier detection algorithm based on\nthe reconstruction error of Principal Component Analysis (PCA) for feature\nreduction, designed explicitly for space mission applications. The algorithm\nadapts dynamically to evolving data distributions by using Incremental PCA,\nenabling deployment without a predefined model for all possible conditions. A\npre-scaling process normalizes each feature's magnitude while preserving\nrelative variance within feature types. We demonstrate the algorithm's\neffectiveness in detecting space plasma events, such as distinct space\nenvironments, dayside and nightside transients phenomena, and transition layers\nthrough NASA's MMS mission observations. Additionally, we apply the method to\nNASA's THEMIS data, successfully identifying a dayside transient using\nonboard-available measurements."
                },
                "authors": [
                    {
                        "name": "Jonah Ekelund"
                    },
                    {
                        "name": "Savvas Raptis"
                    },
                    {
                        "name": "Vicki Toy-Edens"
                    },
                    {
                        "name": "Wenli Mo"
                    },
                    {
                        "name": "Drew L. Turner"
                    },
                    {
                        "name": "Ian J. Cohen"
                    },
                    {
                        "name": "Stefano Markidis"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Markidis"
                },
                "author": "Stefano Markidis",
                "arxiv_comment": "Accepted to ICCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15843v1",
                "updated": "2025-04-22T12:39:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    39,
                    30,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T12:39:30Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    39,
                    30,
                    1,
                    112,
                    0
                ],
                "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model"
                },
                "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data."
                },
                "authors": [
                    {
                        "name": "Junshu Pan"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Shulin Huang"
                    },
                    {
                        "name": "Qiji Zhou"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10852v2",
                "updated": "2025-04-22T12:31:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    31,
                    32,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-15T04:21:50Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    4,
                    21,
                    50,
                    1,
                    105,
                    0
                ],
                "title": "Enhancing Features in Long-tailed Data Using Large Vision Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Features in Long-tailed Data Using Large Vision Model"
                },
                "summary": "Language-based foundation models, such as large language models (LLMs) or\nlarge vision-language models (LVLMs), have been widely studied in long-tailed\nrecognition. However, the need for linguistic data is not applicable to all\npractical tasks. In this study, we aim to explore using large vision models\n(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features\nwithout any language information. Specifically, we extract features from the\nLVM and fuse them with features in the baseline network's map and latent space\nto obtain the augmented features. Moreover, we design several prototype-based\nlosses in the latent space to further exploit the potential of the augmented\nfeatures. In the experimental section, we validate our approach on two\nbenchmark datasets: ImageNet-LT and iNaturalist2018.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-based foundation models, such as large language models (LLMs) or\nlarge vision-language models (LVLMs), have been widely studied in long-tailed\nrecognition. However, the need for linguistic data is not applicable to all\npractical tasks. In this study, we aim to explore using large vision models\n(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features\nwithout any language information. Specifically, we extract features from the\nLVM and fuse them with features in the baseline network's map and latent space\nto obtain the augmented features. Moreover, we design several prototype-based\nlosses in the latent space to further exploit the potential of the augmented\nfeatures. In the experimental section, we validate our approach on two\nbenchmark datasets: ImageNet-LT and iNaturalist2018."
                },
                "authors": [
                    {
                        "name": "Pengxiao Han"
                    },
                    {
                        "name": "Changkun Ye"
                    },
                    {
                        "name": "Jinguang Tong"
                    },
                    {
                        "name": "Cuicui Jiang"
                    },
                    {
                        "name": "Jie Hong"
                    },
                    {
                        "name": "Li Fang"
                    },
                    {
                        "name": "Xuesong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuesong Li"
                },
                "author": "Xuesong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15827v1",
                "updated": "2025-04-22T12:18:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    18,
                    26,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T12:18:26Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    18,
                    26,
                    1,
                    112,
                    0
                ],
                "title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with\n  Dual Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with\n  Dual Optimizers"
                },
                "summary": "Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Xuyang Zhong"
                    },
                    {
                        "name": "Haochen Luo"
                    },
                    {
                        "name": "Chen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Liu"
                },
                "author": "Chen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03101v2",
                "updated": "2025-04-22T11:58:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    58,
                    54,
                    1,
                    112,
                    0
                ],
                "published": "2024-08-06T11:04:37Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    11,
                    4,
                    37,
                    1,
                    219,
                    0
                ],
                "title": "LogUpdater: Automated Detection and Repair of Specific Defects in\n  Logging Statements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogUpdater: Automated Detection and Repair of Specific Defects in\n  Logging Statements"
                },
                "summary": "Developers use logging statements to track software runtime behaviors and\nsystem status. Yet, unclear or misleading logs can hide true execution patterns\nand hinder software maintenance. Current research on logging statement issues\nis limited, often only spotting one defect type and relying on manual\ncorrections instead of automation. To bridge this gap, we conduct a study to\nidentify four logging statement defect types by analyzing log-centric changes.\nThen we introduce LogUpdater, a two-stage framework for automatically detecting\nand updating these log defects. In the offline phase, LogUpdater builds a\nclassifier using synthetic defective logs to spot defect types. During online\ntesting, this classifier assesses if and how logs in code snippets need\nimprovement. LogUpdater then uses type-aware prompts from past logging updates\nto suggest fixes via a recommendation framework based on LLMs. Results show\nstrong defect detection with an F1 score of 0.625. It also greatly improves\nstatic text and dynamic variable suggestions by 48.12% and 24.90%,\nrespectively. LogUpdater successfully recommends updates 61.49% of the time on\nnew projects. We reported 40 problematic logs and their fixes on GitHub,\nleading to 25 merged changes across 11 projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developers use logging statements to track software runtime behaviors and\nsystem status. Yet, unclear or misleading logs can hide true execution patterns\nand hinder software maintenance. Current research on logging statement issues\nis limited, often only spotting one defect type and relying on manual\ncorrections instead of automation. To bridge this gap, we conduct a study to\nidentify four logging statement defect types by analyzing log-centric changes.\nThen we introduce LogUpdater, a two-stage framework for automatically detecting\nand updating these log defects. In the offline phase, LogUpdater builds a\nclassifier using synthetic defective logs to spot defect types. During online\ntesting, this classifier assesses if and how logs in code snippets need\nimprovement. LogUpdater then uses type-aware prompts from past logging updates\nto suggest fixes via a recommendation framework based on LLMs. Results show\nstrong defect detection with an F1 score of 0.625. It also greatly improves\nstatic text and dynamic variable suggestions by 48.12% and 24.90%,\nrespectively. LogUpdater successfully recommends updates 61.49% of the time on\nnew projects. We reported 40 problematic logs and their fixes on GitHub,\nleading to 25 merged changes across 11 projects."
                },
                "authors": [
                    {
                        "name": "Renyi Zhong"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Jinxi Kuang"
                    },
                    {
                        "name": "Wenwei Gu"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_doi": "10.1145/3731754",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731754",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.03101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14325v2",
                "updated": "2025-04-22T11:56:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    56,
                    45,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-19T15:29:04Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    15,
                    29,
                    4,
                    5,
                    109,
                    0
                ],
                "title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory"
                },
                "summary": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents."
                },
                "authors": [
                    {
                        "name": "Alessio Buscemi"
                    },
                    {
                        "name": "Daniele Proverbio"
                    },
                    {
                        "name": "Alessandro Di Stefano"
                    },
                    {
                        "name": "The Anh Han"
                    },
                    {
                        "name": "German Castignani"
                    },
                    {
                        "name": "Pietro Li√≤"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Li√≤"
                },
                "author": "Pietro Li√≤",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15804v1",
                "updated": "2025-04-22T11:38:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    38,
                    14,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T11:38:14Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    38,
                    14,
                    1,
                    112,
                    0
                ],
                "title": "Insights from Verification: Training a Verilog Generation LLM with\n  Reinforcement Learning with Testbench Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights from Verification: Training a Verilog Generation LLM with\n  Reinforcement Learning with Testbench Feedback"
                },
                "summary": "Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B."
                },
                "authors": [
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Bingkun Yao"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07677v2",
                "updated": "2025-04-22T11:34:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    34,
                    10,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-10T12:07:24Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    7,
                    24,
                    3,
                    100,
                    0
                ],
                "title": "Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal\n  Localization"
                },
                "summary": "Reliable localization is critical for robot navigation in complex indoor\nenvironments. In this paper, we propose an uncertainty-aware localization\nmethod that enhances the reliability of localization outputs without modifying\nthe prediction model itself. This study introduces a percentile-based rejection\nstrategy that filters out unreliable 3-DoF pose predictions based on aleatoric\nand epistemic uncertainties the network estimates. We apply this approach to a\nmulti-modal end-to-end localization that fuses RGB images and 2D LiDAR data,\nand we evaluate it across three real-world datasets collected using a\ncommercialized serving robot. Experimental results show that applying stricter\nuncertainty thresholds consistently improves pose accuracy. Specifically, the\nmean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean\norientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70%\nthresholds, respectively. Furthermore, the rejection strategy effectively\nremoves extreme outliers, resulting in better alignment with ground truth\ntrajectories. To the best of our knowledge, this is the first study to\nquantitatively demonstrate the benefits of percentile-based uncertainty\nrejection in multi-modal end-to-end localization tasks. Our approach provides a\npractical means to enhance the reliability and accuracy of localization systems\nin real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable localization is critical for robot navigation in complex indoor\nenvironments. In this paper, we propose an uncertainty-aware localization\nmethod that enhances the reliability of localization outputs without modifying\nthe prediction model itself. This study introduces a percentile-based rejection\nstrategy that filters out unreliable 3-DoF pose predictions based on aleatoric\nand epistemic uncertainties the network estimates. We apply this approach to a\nmulti-modal end-to-end localization that fuses RGB images and 2D LiDAR data,\nand we evaluate it across three real-world datasets collected using a\ncommercialized serving robot. Experimental results show that applying stricter\nuncertainty thresholds consistently improves pose accuracy. Specifically, the\nmean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean\norientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70%\nthresholds, respectively. Furthermore, the rejection strategy effectively\nremoves extreme outliers, resulting in better alignment with ground truth\ntrajectories. To the best of our knowledge, this is the first study to\nquantitatively demonstrate the benefits of percentile-based uncertainty\nrejection in multi-modal end-to-end localization tasks. Our approach provides a\npractical means to enhance the reliability and accuracy of localization systems\nin real-world deployments."
                },
                "authors": [
                    {
                        "name": "Hye-Min Won"
                    },
                    {
                        "name": "Jieun Lee"
                    },
                    {
                        "name": "Jiyong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Jiyong Oh"
                },
                "author": "Jiyong Oh",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15801v1",
                "updated": "2025-04-22T11:31:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    31,
                    50,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T11:31:50Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    31,
                    50,
                    1,
                    112,
                    0
                ],
                "title": "A closer look at how large language models trust humans: patterns and\n  biases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A closer look at how large language models trust humans: patterns and\n  biases"
                },
                "summary": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI."
                },
                "authors": [
                    {
                        "name": "Valeria Lerman"
                    },
                    {
                        "name": "Yaniv Dover"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Dover"
                },
                "author": "Yaniv Dover",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11761v2",
                "updated": "2025-04-22T11:24:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    24,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-16T13:25:42Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    25,
                    42,
                    0,
                    351,
                    0
                ],
                "title": "Harnessing Language for Coordination: A Framework and Benchmark for\n  LLM-Driven Multi-Agent Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Language for Coordination: A Framework and Benchmark for\n  LLM-Driven Multi-Agent Control"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. Their potential to facilitate human coordination with many\nagents is a promising but largely under-explored area. Such capabilities would\nbe helpful in disaster response, urban planning, and real-time strategy\nscenarios. In this work, we introduce (1) a real-time strategy game benchmark\ndesigned to evaluate these abilities and (2) a novel framework we term HIVE.\nHIVE empowers a single human to coordinate swarms of up to 2,000 agents through\na natural language dialog with an LLM. We present promising results on this\nmulti-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. Our findings also\nhighlight critical limitations of current models, including difficulties in\nprocessing spatial visual information and challenges in formulating long-term\nstrategic plans. This work sheds light on the potential and limitations of LLMs\nin human-swarm coordination, paving the way for future research in this area.\nThe HIVE project page, hive.syrkis.com, includes videos of the system in\naction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. Their potential to facilitate human coordination with many\nagents is a promising but largely under-explored area. Such capabilities would\nbe helpful in disaster response, urban planning, and real-time strategy\nscenarios. In this work, we introduce (1) a real-time strategy game benchmark\ndesigned to evaluate these abilities and (2) a novel framework we term HIVE.\nHIVE empowers a single human to coordinate swarms of up to 2,000 agents through\na natural language dialog with an LLM. We present promising results on this\nmulti-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. Our findings also\nhighlight critical limitations of current models, including difficulties in\nprocessing spatial visual information and challenges in formulating long-term\nstrategic plans. This work sheds light on the potential and limitations of LLMs\nin human-swarm coordination, paving the way for future research in this area.\nThe HIVE project page, hive.syrkis.com, includes videos of the system in\naction."
                },
                "authors": [
                    {
                        "name": "Timoth√©e Anne"
                    },
                    {
                        "name": "Noah Syrkis"
                    },
                    {
                        "name": "Meriem Elhosni"
                    },
                    {
                        "name": "Florian Turati"
                    },
                    {
                        "name": "Franck Legendre"
                    },
                    {
                        "name": "Alain Jaquier"
                    },
                    {
                        "name": "Sebastian Risi"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Risi"
                },
                "author": "Sebastian Risi",
                "arxiv_doi": "10.1109/TG.2025.3564042",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TG.2025.3564042",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.11761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06276v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06276v4",
                "updated": "2025-04-22T11:18:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    18,
                    16,
                    1,
                    112,
                    0
                ],
                "published": "2024-08-12T16:39:03Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    3,
                    0,
                    225,
                    0
                ],
                "title": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Buru Chang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_doi": "10.1145/3726302.3730055",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730055",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06276v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06276v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12915v2",
                "updated": "2025-04-22T11:11:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    11,
                    11,
                    50,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-17T13:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    5,
                    14,
                    3,
                    107,
                    0
                ],
                "title": "ConExion: Concept Extraction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConExion: Concept Extraction with Large Language Models"
                },
                "summary": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction."
                },
                "authors": [
                    {
                        "name": "Ebrahim Norouzi"
                    },
                    {
                        "name": "Sven Hertling"
                    },
                    {
                        "name": "Harald Sack"
                    }
                ],
                "author_detail": {
                    "name": "Harald Sack"
                },
                "author": "Harald Sack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15785v1",
                "updated": "2025-04-22T10:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    58,
                    27,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T10:58:27Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    58,
                    27,
                    1,
                    112,
                    0
                ],
                "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents"
                },
                "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations."
                },
                "authors": [
                    {
                        "name": "Siyu Zhou"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Guodong Long"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Jing Jiang"
                    },
                    {
                        "name": "Chengqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chengqi Zhang"
                },
                "author": "Chengqi Zhang",
                "arxiv_comment": "Code is available at https://github.com/elated-sawyer/WALL-E",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15784v1",
                "updated": "2025-04-22T10:52:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    52,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T10:52:23Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    52,
                    23,
                    1,
                    112,
                    0
                ],
                "title": "Automated Creativity Evaluation for Large Language Models: A\n  Reference-Based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Creativity Evaluation for Large Language Models: A\n  Reference-Based Approach"
                },
                "summary": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%)."
                },
                "authors": [
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Chiwei Zhu"
                    },
                    {
                        "name": "Benfeng Xu"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15780v1",
                "updated": "2025-04-22T10:45:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    45,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T10:45:23Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    45,
                    23,
                    1,
                    112,
                    0
                ],
                "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving"
                },
                "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen"
                },
                "authors": [
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Zijun Chen"
                    },
                    {
                        "name": "Renqiu Xia"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Hongbin Zhou"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Shiyang Feng"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15771v1",
                "updated": "2025-04-22T10:28:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    28,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T10:28:23Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    28,
                    23,
                    1,
                    112,
                    0
                ],
                "title": "Grounded in Context: Retrieval-Based Method for Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded in Context: Retrieval-Based Method for Hallucination Detection"
                },
                "summary": "Despite advancements in grounded content generation, production Large\nLanguage Models (LLMs) based applications still suffer from hallucinated\nanswers. We present \"Grounded in Context\" - Deepchecks' hallucination detection\nframework, designed for production-scale long-context data and tailored to\ndiverse use cases, including summarization, data extraction, and RAG. Inspired\nby RAG architecture, our method integrates retrieval and Natural Language\nInference (NLI) models to predict factual consistency between premises and\nhypotheses using an encoder-based model with only a 512-token context window.\nOur framework identifies unsupported claims with an F1 score of 0.83 in\nRAGTruth's response-level classification task, matching methods that trained on\nthe dataset, and outperforming all comparable frameworks using similar-sized\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in grounded content generation, production Large\nLanguage Models (LLMs) based applications still suffer from hallucinated\nanswers. We present \"Grounded in Context\" - Deepchecks' hallucination detection\nframework, designed for production-scale long-context data and tailored to\ndiverse use cases, including summarization, data extraction, and RAG. Inspired\nby RAG architecture, our method integrates retrieval and Natural Language\nInference (NLI) models to predict factual consistency between premises and\nhypotheses using an encoder-based model with only a 512-token context window.\nOur framework identifies unsupported claims with an F1 score of 0.83 in\nRAGTruth's response-level classification task, matching methods that trained on\nthe dataset, and outperforming all comparable frameworks using similar-sized\nmodels."
                },
                "authors": [
                    {
                        "name": "Assaf Gerner"
                    },
                    {
                        "name": "Netta Madvil"
                    },
                    {
                        "name": "Nadav Barak"
                    },
                    {
                        "name": "Alex Zaikman"
                    },
                    {
                        "name": "Jonatan Liberman"
                    },
                    {
                        "name": "Liron Hamra"
                    },
                    {
                        "name": "Rotem Brazilay"
                    },
                    {
                        "name": "Shay Tsadok"
                    },
                    {
                        "name": "Yaron Friedman"
                    },
                    {
                        "name": "Neal Harow"
                    },
                    {
                        "name": "Noam Bresler"
                    },
                    {
                        "name": "Shir Chorev"
                    },
                    {
                        "name": "Philip Tannor"
                    }
                ],
                "author_detail": {
                    "name": "Philip Tannor"
                },
                "author": "Philip Tannor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15993v2",
                "updated": "2025-04-22T10:20:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    20,
                    16,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-20T15:41:47Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    41,
                    47,
                    4,
                    355,
                    0
                ],
                "title": "Fearful Falcons and Angry Llamas: Emotion Category Annotations of\n  Arguments by Humans and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fearful Falcons and Angry Llamas: Emotion Category Annotations of\n  Arguments by Humans and LLMs"
                },
                "summary": "Arguments evoke emotions, influencing the effect of the argument itself. Not\nonly the emotional intensity but also the category influence the argument's\neffects, for instance, the willingness to adapt stances. While binary\nemotionality has been studied in arguments, there is no work on discrete\nemotion categories (e.g., \"Anger\") in such data. To fill this gap, we\ncrowdsource subjective annotations of emotion categories in a German argument\ncorpus and evaluate automatic LLM-based labeling methods. Specifically, we\ncompare three prompting strategies (zero-shot, one-shot, chain-of-thought) on\nthree large instruction-tuned language models (Falcon-7b-instruct,\nLlama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the\noutput space to be binary (is there emotionality in the argument?),\nclosed-domain (which emotion from a given label set is in the argument?), or\nopen-domain (which emotion is in the argument?). We find that emotion\ncategories enhance the prediction of emotionality in arguments, emphasizing the\nneed for discrete emotion annotations in arguments. Across all prompt settings\nand models, automatic predictions show a high recall but low precision for\npredicting anger and fear, indicating a strong bias toward negative emotions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arguments evoke emotions, influencing the effect of the argument itself. Not\nonly the emotional intensity but also the category influence the argument's\neffects, for instance, the willingness to adapt stances. While binary\nemotionality has been studied in arguments, there is no work on discrete\nemotion categories (e.g., \"Anger\") in such data. To fill this gap, we\ncrowdsource subjective annotations of emotion categories in a German argument\ncorpus and evaluate automatic LLM-based labeling methods. Specifically, we\ncompare three prompting strategies (zero-shot, one-shot, chain-of-thought) on\nthree large instruction-tuned language models (Falcon-7b-instruct,\nLlama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the\noutput space to be binary (is there emotionality in the argument?),\nclosed-domain (which emotion from a given label set is in the argument?), or\nopen-domain (which emotion is in the argument?). We find that emotion\ncategories enhance the prediction of emotionality in arguments, emphasizing the\nneed for discrete emotion annotations in arguments. Across all prompt settings\nand models, automatic predictions show a high recall but low precision for\npredicting anger and fear, indicating a strong bias toward negative emotions."
                },
                "authors": [
                    {
                        "name": "Lynn Greschner"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "arxiv_comment": "accepted to NLP4DH 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14286v2",
                "updated": "2025-04-22T10:07:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    7,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-19T13:06:03Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    13,
                    6,
                    3,
                    5,
                    109,
                    0
                ],
                "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement\n  Learning on LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement\n  Learning on LLM"
                },
                "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and\nDeepSeek's R1, highlight the significant potential of Reinforcement Learning\n(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).\nHowever, replicating these advancements across diverse domains remains\nchallenging due to limited methodological transparency. In this work, we\npresent two-Staged history-Resampling Policy Optimization (SRPO), which\nsurpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and\nLiveCodeBench benchmarks. SRPO achieves this using the same base model as\nDeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps\nrequired by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building\nupon Group Relative Policy Optimization (GRPO), we introduce two key\nmethodological innovations: (1) a two-stage cross-domain training paradigm\ndesigned to balance the development of mathematical reasoning and coding\nproficiency, and (2) History Resampling (HR), a technique to address\nineffective samples. Our comprehensive experiments validate the effectiveness\nof our approach, offering valuable insights into scaling LLM reasoning\ncapabilities across diverse tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances of reasoning models, exemplified by OpenAI's o1 and\nDeepSeek's R1, highlight the significant potential of Reinforcement Learning\n(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).\nHowever, replicating these advancements across diverse domains remains\nchallenging due to limited methodological transparency. In this work, we\npresent two-Staged history-Resampling Policy Optimization (SRPO), which\nsurpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and\nLiveCodeBench benchmarks. SRPO achieves this using the same base model as\nDeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps\nrequired by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building\nupon Group Relative Policy Optimization (GRPO), we introduce two key\nmethodological innovations: (1) a two-stage cross-domain training paradigm\ndesigned to balance the development of mathematical reasoning and coding\nproficiency, and (2) History Resampling (HR), a technique to address\nineffective samples. Our comprehensive experiments validate the effectiveness\nof our approach, offering valuable insights into scaling LLM reasoning\ncapabilities across diverse tasks."
                },
                "authors": [
                    {
                        "name": "Xiaojiang Zhang"
                    },
                    {
                        "name": "Jinghui Wang"
                    },
                    {
                        "name": "Zifei Cheng"
                    },
                    {
                        "name": "Wenhao Zhuang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Minglei Zhang"
                    },
                    {
                        "name": "Shaojie Wang"
                    },
                    {
                        "name": "Yinghan Cui"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Junyi Peng"
                    },
                    {
                        "name": "Shimiao Jiang"
                    },
                    {
                        "name": "Shiqi Kuang"
                    },
                    {
                        "name": "Shouyu Yin"
                    },
                    {
                        "name": "Chaohang Wen"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Bing Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Yu"
                },
                "author": "Bing Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15751v1",
                "updated": "2025-04-22T09:53:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    53,
                    25,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:53:25Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    53,
                    25,
                    1,
                    112,
                    0
                ],
                "title": "GADS: A Super Lightweight Model for Head Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GADS: A Super Lightweight Model for Head Pose Estimation"
                },
                "summary": "In human-computer interaction, head pose estimation profoundly influences\napplication functionality. Although utilizing facial landmarks is valuable for\nthis purpose, existing landmark-based methods prioritize precision over\nsimplicity and model size, limiting their deployment on edge devices and in\ncompute-poor environments. To bridge this gap, we propose \\textbf{Grouped\nAttention Deep Sets (GADS)}, a novel architecture based on the Deep Set\nframework. By grouping landmarks into regions and employing small Deep Set\nlayers, we reduce computational complexity. Our multihead attention mechanism\nextracts and combines inter-group information, resulting in a model that is\n$7.5\\times$ smaller and executes $25\\times$ faster than the current lightest\nstate-of-the-art model. Notably, our method achieves an impressive reduction,\nbeing $4321\\times$ smaller than the best-performing model. We introduce vanilla\nGADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three\nbenchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture\nas a robust baseline for resource-constrained head pose estimation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In human-computer interaction, head pose estimation profoundly influences\napplication functionality. Although utilizing facial landmarks is valuable for\nthis purpose, existing landmark-based methods prioritize precision over\nsimplicity and model size, limiting their deployment on edge devices and in\ncompute-poor environments. To bridge this gap, we propose \\textbf{Grouped\nAttention Deep Sets (GADS)}, a novel architecture based on the Deep Set\nframework. By grouping landmarks into regions and employing small Deep Set\nlayers, we reduce computational complexity. Our multihead attention mechanism\nextracts and combines inter-group information, resulting in a model that is\n$7.5\\times$ smaller and executes $25\\times$ faster than the current lightest\nstate-of-the-art model. Notably, our method achieves an impressive reduction,\nbeing $4321\\times$ smaller than the best-performing model. We introduce vanilla\nGADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three\nbenchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture\nas a robust baseline for resource-constrained head pose estimation methods."
                },
                "authors": [
                    {
                        "name": "Menan Velayuthan"
                    },
                    {
                        "name": "Asiri Gawesha"
                    },
                    {
                        "name": "Purushoth Velayuthan"
                    },
                    {
                        "name": "Nuwan Kodagoda"
                    },
                    {
                        "name": "Dharshana Kasthurirathna"
                    },
                    {
                        "name": "Pradeepa Samarasinghe"
                    }
                ],
                "author_detail": {
                    "name": "Pradeepa Samarasinghe"
                },
                "author": "Pradeepa Samarasinghe",
                "arxiv_comment": "16 pages, 5 tables, 10 figures, not submitted to any conference or\n  journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15721v1",
                "updated": "2025-04-22T09:11:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    11,
                    21,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:11:21Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    11,
                    21,
                    1,
                    112,
                    0
                ],
                "title": "BBAL: A Bidirectional Block Floating Point-Based Quantisation\n  Accelerator for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BBAL: A Bidirectional Block Floating Point-Based Quantisation\n  Accelerator for Large Language Models"
                },
                "summary": "Large language models (LLMs), with their billions of parameters, pose\nsubstantial challenges for deployment on edge devices, straining both memory\ncapacity and computational resources. Block Floating Point (BFP) quantisation\nreduces memory and computational overhead by converting high-overhead floating\npoint operations into low-bit fixed point operations. However, BFP requires\naligning all data to the maximum exponent, which causes loss of small and\nmoderate values, resulting in quantisation error and degradation in the\naccuracy of LLMs. To address this issue, we propose a Bidirectional Block\nFloating Point (BBFP) data format, which reduces the probability of selecting\nthe maximum as shared exponent, thereby reducing quantisation error. By\nutilizing the features in BBFP, we present a full-stack Bidirectional Block\nFloating Point-Based Quantisation Accelerator for LLMs (BBAL), primarily\ncomprising a processing element array based on BBFP, paired with proposed\ncost-effective nonlinear computation unit. Experimental results show BBAL\nachieves a 22% improvement in accuracy compared to an outlier-aware accelerator\nat similar efficiency, and a 40% efficiency improvement over a BFP-based\naccelerator at similar accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with their billions of parameters, pose\nsubstantial challenges for deployment on edge devices, straining both memory\ncapacity and computational resources. Block Floating Point (BFP) quantisation\nreduces memory and computational overhead by converting high-overhead floating\npoint operations into low-bit fixed point operations. However, BFP requires\naligning all data to the maximum exponent, which causes loss of small and\nmoderate values, resulting in quantisation error and degradation in the\naccuracy of LLMs. To address this issue, we propose a Bidirectional Block\nFloating Point (BBFP) data format, which reduces the probability of selecting\nthe maximum as shared exponent, thereby reducing quantisation error. By\nutilizing the features in BBFP, we present a full-stack Bidirectional Block\nFloating Point-Based Quantisation Accelerator for LLMs (BBAL), primarily\ncomprising a processing element array based on BBFP, paired with proposed\ncost-effective nonlinear computation unit. Experimental results show BBAL\nachieves a 22% improvement in accuracy compared to an outlier-aware accelerator\nat similar efficiency, and a 40% efficiency improvement over a BFP-based\naccelerator at similar accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Junyang Lu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "X. x. Zhang"
                    },
                    {
                        "name": "Ning Xu"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15719v1",
                "updated": "2025-04-22T09:08:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    21,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:21Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    21,
                    1,
                    112,
                    0
                ],
                "title": "Implementing Rational Choice Functions with LLMs and Measuring their\n  Alignment with User Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing Rational Choice Functions with LLMs and Measuring their\n  Alignment with User Preferences"
                },
                "summary": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain."
                },
                "authors": [
                    {
                        "name": "Anna Karnysheva"
                    },
                    {
                        "name": "Christian Drescher"
                    },
                    {
                        "name": "Dietrich Klakow"
                    }
                ],
                "author_detail": {
                    "name": "Dietrich Klakow"
                },
                "author": "Dietrich Klakow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15716v1",
                "updated": "2025-04-22T09:01:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    1,
                    4,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:01:04Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    1,
                    4,
                    1,
                    112,
                    0
                ],
                "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models"
                },
                "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Jie Zhu"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Huaixia Dou"
                    },
                    {
                        "name": "Junhui Li"
                    },
                    {
                        "name": "Lifan Guo"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09093v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09093v3",
                "updated": "2025-04-22T08:46:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    46,
                    20,
                    1,
                    112,
                    0
                ],
                "published": "2024-08-17T04:43:26Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    4,
                    43,
                    26,
                    5,
                    230,
                    0
                ],
                "title": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language\n  Models by Treating Harmful Instruction as Backdoor Trigger",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language\n  Models by Treating Harmful Instruction as Backdoor Trigger"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have showcased impressive\nperformance in a variety of multimodal tasks. On the other hand, the\nintegration of additional image modality may allow the malicious users to\ninject harmful content inside the images for jailbreaking. Unlike text-based\nLLMs, where adversaries need to select discrete tokens to conceal their\nmalicious intent using specific algorithms, the continuous nature of image\nsignals provides a direct opportunity for adversaries to inject harmful\nintentions. In this work, we propose $\\textbf{BaThe}$ ($\\textbf{Ba}$ckdoor\n$\\textbf{T}$rigger S$\\textbf{h}$i$\\textbf{e}$ld), a simple yet effective\njailbreak defense mechanism. Our work is motivated by recent research on\njailbreak backdoor attack and virtual prompt backdoor attack in generative\nlanguage models. Jailbreak backdoor attack uses harmful instructions combined\nwith manually crafted strings as triggers to make the backdoored model generate\nprohibited responses. We assume that harmful instructions can function as\ntriggers, and if we alternatively set rejection responses as the triggered\nresponse, the backdoored model then can defend against jailbreak attacks. We\nachieve this by utilizing virtual rejection prompt, similar to the virtual\nprompt backdoor attack. We embed the virtual rejection prompt into the soft\ntext embeddings, which we call ``wedge''. Our comprehensive experiments\ndemonstrate that BaThe effectively mitigates various types of jailbreak attacks\nand is adaptable to defend against unseen attacks, with minimal impact on\nMLLMs' performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have showcased impressive\nperformance in a variety of multimodal tasks. On the other hand, the\nintegration of additional image modality may allow the malicious users to\ninject harmful content inside the images for jailbreaking. Unlike text-based\nLLMs, where adversaries need to select discrete tokens to conceal their\nmalicious intent using specific algorithms, the continuous nature of image\nsignals provides a direct opportunity for adversaries to inject harmful\nintentions. In this work, we propose $\\textbf{BaThe}$ ($\\textbf{Ba}$ckdoor\n$\\textbf{T}$rigger S$\\textbf{h}$i$\\textbf{e}$ld), a simple yet effective\njailbreak defense mechanism. Our work is motivated by recent research on\njailbreak backdoor attack and virtual prompt backdoor attack in generative\nlanguage models. Jailbreak backdoor attack uses harmful instructions combined\nwith manually crafted strings as triggers to make the backdoored model generate\nprohibited responses. We assume that harmful instructions can function as\ntriggers, and if we alternatively set rejection responses as the triggered\nresponse, the backdoored model then can defend against jailbreak attacks. We\nachieve this by utilizing virtual rejection prompt, similar to the virtual\nprompt backdoor attack. We embed the virtual rejection prompt into the soft\ntext embeddings, which we call ``wedge''. Our comprehensive experiments\ndemonstrate that BaThe effectively mitigates various types of jailbreak attacks\nand is adaptable to defend against unseen attacks, with minimal impact on\nMLLMs' performance."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yirui Zhang"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09093v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09093v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15699v1",
                "updated": "2025-04-22T08:34:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    34,
                    35,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T08:34:35Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    34,
                    35,
                    1,
                    112,
                    0
                ],
                "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input\n  Moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Embodied Agent Security: From Safety Benchmarks to Input\n  Moderation"
                },
                "summary": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance."
                },
                "authors": [
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Zihan Yan"
                    },
                    {
                        "name": "Weiyang Li"
                    },
                    {
                        "name": "Chuan Ma"
                    },
                    {
                        "name": "He Chen"
                    },
                    {
                        "name": "Tao Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xiang"
                },
                "author": "Tao Xiang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15689v1",
                "updated": "2025-04-22T08:13:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    13,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T08:13:34Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    13,
                    34,
                    1,
                    112,
                    0
                ],
                "title": "The Viability of Crowdsourcing for RAG Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Viability of Crowdsourcing for RAG Evaluation"
                },
                "summary": "How good are humans at writing and judging responses in retrieval-augmented\ngeneration (RAG) scenarios? To answer this question, we investigate the\nefficacy of crowdsourcing for RAG through two complementary studies: response\nwriting and response utility judgment. We present the Crowd RAG Corpus 2025\n(CrowdRAG-25), which consists of 903 human-written and 903 LLM-generated\nresponses for the 301 topics of the TREC RAG'24 track, across the three\ndiscourse styles 'bulleted list', 'essay', and 'news'. For a selection of 65\ntopics, the corpus further contains 47,320 pairwise human judgments and 10,556\npairwise LLM judgments across seven utility dimensions (e.g., coverage and\ncoherence). Our analyses give insights into human writing behavior for RAG and\nthe viability of crowdsourcing for RAG evaluation. Human pairwise judgments\nprovide reliable and cost-effective results compared to LLM-based pairwise or\nhuman/LLM-based pointwise judgments, as well as automated comparisons with\nhuman-written reference responses. All our data and tools are freely available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How good are humans at writing and judging responses in retrieval-augmented\ngeneration (RAG) scenarios? To answer this question, we investigate the\nefficacy of crowdsourcing for RAG through two complementary studies: response\nwriting and response utility judgment. We present the Crowd RAG Corpus 2025\n(CrowdRAG-25), which consists of 903 human-written and 903 LLM-generated\nresponses for the 301 topics of the TREC RAG'24 track, across the three\ndiscourse styles 'bulleted list', 'essay', and 'news'. For a selection of 65\ntopics, the corpus further contains 47,320 pairwise human judgments and 10,556\npairwise LLM judgments across seven utility dimensions (e.g., coverage and\ncoherence). Our analyses give insights into human writing behavior for RAG and\nthe viability of crowdsourcing for RAG evaluation. Human pairwise judgments\nprovide reliable and cost-effective results compared to LLM-based pairwise or\nhuman/LLM-based pointwise judgments, as well as automated comparisons with\nhuman-written reference responses. All our data and tools are freely available."
                },
                "authors": [
                    {
                        "name": "Lukas Gienapp"
                    },
                    {
                        "name": "Tim Hagen"
                    },
                    {
                        "name": "Maik Fr√∂be"
                    },
                    {
                        "name": "Matthias Hagen"
                    },
                    {
                        "name": "Benno Stein"
                    },
                    {
                        "name": "Martin Potthast"
                    },
                    {
                        "name": "Harrisen Scells"
                    }
                ],
                "author_detail": {
                    "name": "Harrisen Scells"
                },
                "author": "Harrisen Scells",
                "arxiv_doi": "10.1145/3726302.3730093",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730093",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 9 tables, 5 figures. Accepted at SIGIR'25",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06223v2",
                "updated": "2025-04-22T08:07:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    7,
                    23,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-08T13:51:40Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    13,
                    51,
                    40,
                    5,
                    67,
                    0
                ],
                "title": "Red Team Diffuser: Exposing Toxic Continuation Vulnerabilities in\n  Vision-Language Models via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red Team Diffuser: Exposing Toxic Continuation Vulnerabilities in\n  Vision-Language Models via Reinforcement Learning"
                },
                "summary": "The growing deployment of large Vision-Language Models (VLMs) exposes\ncritical safety gaps in their alignment mechanisms. While existing jailbreak\nstudies primarily focus on VLMs' susceptibility to harmful instructions, we\nreveal a fundamental yet overlooked vulnerability: toxic text continuation,\nwhere VLMs produce highly toxic completions when prompted with harmful text\nprefixes paired with semantically adversarial images. To systematically study\nthis threat, we propose Red Team Diffuser (RTD), the first red teaming\ndiffusion model that coordinates adversarial image generation and toxic\ncontinuation through reinforcement learning. Our key innovations include\ndynamic cross-modal attack and stealth-aware optimization. For toxic text\nprefixes from an LLM safety benchmark, we conduct greedy search to identify\noptimal image prompts that maximally induce toxic completions. The discovered\nimage prompts then drive RL-based diffusion model fine-tuning, producing\nsemantically aligned adversarial images that boost toxicity rates.\nStealth-aware optimization introduces joint adversarial rewards that balance\ntoxicity maximization (via Detoxify classifier) and stealthiness (via\nBERTScore), circumventing traditional noise-based adversarial patterns.\nExperimental results demonstrate the effectiveness of RTD, increasing the\ntoxicity rate of LLaVA outputs by 10.69% over text-only baselines on the\noriginal attack set and 8.91% on an unseen set, proving generalization\ncapability. Moreover, RTD exhibits strong cross-model transferability, raising\nthe toxicity rate by 5.1% on Gemini and 26.83% on LLaMA. Our findings expose\ntwo critical flaws in current VLM alignment: (1) failure to prevent toxic\ncontinuation from harmful prefixes, and (2) overlooking cross-modal attack\nvectors. These results necessitate a paradigm shift toward multimodal red\nteaming in safety evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing deployment of large Vision-Language Models (VLMs) exposes\ncritical safety gaps in their alignment mechanisms. While existing jailbreak\nstudies primarily focus on VLMs' susceptibility to harmful instructions, we\nreveal a fundamental yet overlooked vulnerability: toxic text continuation,\nwhere VLMs produce highly toxic completions when prompted with harmful text\nprefixes paired with semantically adversarial images. To systematically study\nthis threat, we propose Red Team Diffuser (RTD), the first red teaming\ndiffusion model that coordinates adversarial image generation and toxic\ncontinuation through reinforcement learning. Our key innovations include\ndynamic cross-modal attack and stealth-aware optimization. For toxic text\nprefixes from an LLM safety benchmark, we conduct greedy search to identify\noptimal image prompts that maximally induce toxic completions. The discovered\nimage prompts then drive RL-based diffusion model fine-tuning, producing\nsemantically aligned adversarial images that boost toxicity rates.\nStealth-aware optimization introduces joint adversarial rewards that balance\ntoxicity maximization (via Detoxify classifier) and stealthiness (via\nBERTScore), circumventing traditional noise-based adversarial patterns.\nExperimental results demonstrate the effectiveness of RTD, increasing the\ntoxicity rate of LLaVA outputs by 10.69% over text-only baselines on the\noriginal attack set and 8.91% on an unseen set, proving generalization\ncapability. Moreover, RTD exhibits strong cross-model transferability, raising\nthe toxicity rate by 5.1% on Gemini and 26.83% on LLaMA. Our findings expose\ntwo critical flaws in current VLM alignment: (1) failure to prevent toxic\ncontinuation from harmful prefixes, and (2) overlooking cross-modal attack\nvectors. These results necessitate a paradigm shift toward multimodal red\nteaming in safety evaluations."
                },
                "authors": [
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Xiaosen Wang"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xingjun Ma"
                },
                "author": "Xingjun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04060v2",
                "updated": "2025-04-22T07:59:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    59,
                    31,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-05T04:57:12Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    4,
                    57,
                    12,
                    5,
                    95,
                    0
                ],
                "title": "VocalNet: Speech LLM with Multi-Token Prediction for Faster and\n  High-Quality Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VocalNet: Speech LLM with Multi-Token Prediction for Faster and\n  High-Quality Generation"
                },
                "summary": "Speech large language models (LLMs) have emerged as a prominent research\nfocus in speech processing. We introduce VocalNet-1B and VocalNet-8B, a series\nof high-performance, low-latency speech LLMs enabled by a scalable and\nmodel-agnostic training framework designed for real-time voice interaction.\nCentral to our contribution is the first application of multi-token prediction\n(MTP) to speech LLMs. This approach represents a paradigm shift from standard\nnext-token prediction (NTP), offering simultaneous improvements in generation\nspeed and quality. Informed by analysis of MTP's effect on speech generation\nand experimental comparisons, we designed a straightforward and highly\neffective MTP implementation. Experiments demonstrate that VocalNet performs on\npar with mainstream Omni LLMs even with limited training data, and\nsignificantly surpasses existing open-source speech LLMs. To foster\nreproducibility and community advancement, all model weights, inference code,\ntraining data, and framework implementations have been made publicly available\nat https://github.com/SJTU-OmniAgent/VocalNet",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech large language models (LLMs) have emerged as a prominent research\nfocus in speech processing. We introduce VocalNet-1B and VocalNet-8B, a series\nof high-performance, low-latency speech LLMs enabled by a scalable and\nmodel-agnostic training framework designed for real-time voice interaction.\nCentral to our contribution is the first application of multi-token prediction\n(MTP) to speech LLMs. This approach represents a paradigm shift from standard\nnext-token prediction (NTP), offering simultaneous improvements in generation\nspeed and quality. Informed by analysis of MTP's effect on speech generation\nand experimental comparisons, we designed a straightforward and highly\neffective MTP implementation. Experiments demonstrate that VocalNet performs on\npar with mainstream Omni LLMs even with limited training data, and\nsignificantly surpasses existing open-source speech LLMs. To foster\nreproducibility and community advancement, all model weights, inference code,\ntraining data, and framework implementations have been made publicly available\nat https://github.com/SJTU-OmniAgent/VocalNet"
                },
                "authors": [
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Heyang Liu"
                    },
                    {
                        "name": "Ziyang Cheng"
                    },
                    {
                        "name": "Ronghua Wu"
                    },
                    {
                        "name": "Qunshan Gu"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14941v2",
                "updated": "2025-04-22T07:43:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    43,
                    1,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T08:02:25Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    2,
                    25,
                    0,
                    111,
                    0
                ],
                "title": "WindVE: Collaborative CPU-NPU Vector Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindVE: Collaborative CPU-NPU Vector Embedding"
                },
                "summary": "Retrieval-Augmented Generation is a technology that enhances large language\nmodels by integrating information retrieval. In the industry, inference\nservices based on LLMs are highly sensitive to cost-performance ratio,\nprompting the need for improving hardware resource utilization in the inference\nservice. Specifically, vector embedding and retrieval processes take up to 20%\nof the total latency. Therefore, optimizing the utilization of computational\nresources in vector embeddings is crucial for enhancing the cost-performance\nratio of inference processes, which in turn boosts their product\ncompetitiveness.In this paper, we analyze the deployment costs of vector\nembedding technology in inference services, propose a theoretical formula, and\ndetermine through the mathematical expression that increasing the capacity to\nprocess concurrent queries is the key to reducing the deployment costs of\nvector embeddings. Therefore, in this paper, we focus on improving the\nproduct's capability to process concurrent queries. To optimize concurrency\nwithout sacrificing performance, we have designed a queue manager that adeptly\noffloads CPU peak queries. This manager utilizes a linear regression model to\nascertain the optimal queue depths, a critical parameter that significantly\ninfluences the efficacy of the system. We further develop a system named WindVE\nthat uses a CPU-NPU heterogeneous architecture to offload peak concurrent\nqueries, which leverages the performance differences between the two processors\nto effectively manage traffic surges. Through experiments, we compare WindVE to\nthe state-of-the-art vector embedding framework FlagEmbedding, and achieve a\nconcurrency level up to 22.3% higher than the scheme without offloading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation is a technology that enhances large language\nmodels by integrating information retrieval. In the industry, inference\nservices based on LLMs are highly sensitive to cost-performance ratio,\nprompting the need for improving hardware resource utilization in the inference\nservice. Specifically, vector embedding and retrieval processes take up to 20%\nof the total latency. Therefore, optimizing the utilization of computational\nresources in vector embeddings is crucial for enhancing the cost-performance\nratio of inference processes, which in turn boosts their product\ncompetitiveness.In this paper, we analyze the deployment costs of vector\nembedding technology in inference services, propose a theoretical formula, and\ndetermine through the mathematical expression that increasing the capacity to\nprocess concurrent queries is the key to reducing the deployment costs of\nvector embeddings. Therefore, in this paper, we focus on improving the\nproduct's capability to process concurrent queries. To optimize concurrency\nwithout sacrificing performance, we have designed a queue manager that adeptly\noffloads CPU peak queries. This manager utilizes a linear regression model to\nascertain the optimal queue depths, a critical parameter that significantly\ninfluences the efficacy of the system. We further develop a system named WindVE\nthat uses a CPU-NPU heterogeneous architecture to offload peak concurrent\nqueries, which leverages the performance differences between the two processors\nto effectively manage traffic surges. Through experiments, we compare WindVE to\nthe state-of-the-art vector embedding framework FlagEmbedding, and achieve a\nconcurrency level up to 22.3% higher than the scheme without offloading."
                },
                "authors": [
                    {
                        "name": "Jinqi Huang"
                    },
                    {
                        "name": "Xuebing Yu"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Entong Li"
                    },
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Xin chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin chen"
                },
                "author": "Xin chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15659v1",
                "updated": "2025-04-22T07:32:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    32,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T07:32:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    32,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional\n  Correctness Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional\n  Correctness Validation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have sparked growing interest\nin applying them to Electronic Design Automation (EDA) tasks, particularly\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\nbeen introduced, most focus on syntactic validity rather than functional\nvalidation with tests, leading to training examples that compile but may not\nimplement the intended behavior. We present VERICODER, a model for RTL code\ngeneration fine-tuned on a dataset validated for functional correctness. This\nfine-tuning dataset is constructed using a novel methodology that combines unit\ntest generation with feedback-directed refinement. Given a natural language\nspecification and an initial RTL design, we prompt a teacher model\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\nbased on its simulation results using the generated tests. If necessary, the\nteacher model also updates the tests to ensure they comply with the natural\nlanguage specification. As a result of this process, every example in our\ndataset is functionally validated, consisting of a natural language\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\nto 71.7% and 27.4% respectively. An ablation study further shows that models\ntrained on our functionally validated dataset outperform those trained on\nfunctionally non-validated datasets, underscoring the importance of\nhigh-quality datasets in RTL code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have sparked growing interest\nin applying them to Electronic Design Automation (EDA) tasks, particularly\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\nbeen introduced, most focus on syntactic validity rather than functional\nvalidation with tests, leading to training examples that compile but may not\nimplement the intended behavior. We present VERICODER, a model for RTL code\ngeneration fine-tuned on a dataset validated for functional correctness. This\nfine-tuning dataset is constructed using a novel methodology that combines unit\ntest generation with feedback-directed refinement. Given a natural language\nspecification and an initial RTL design, we prompt a teacher model\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\nbased on its simulation results using the generated tests. If necessary, the\nteacher model also updates the tests to ensure they comply with the natural\nlanguage specification. As a result of this process, every example in our\ndataset is functionally validated, consisting of a natural language\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\nto 71.7% and 27.4% respectively. An ablation study further shows that models\ntrained on our functionally validated dataset outperform those trained on\nfunctionally non-validated datasets, underscoring the importance of\nhigh-quality datasets in RTL code generation."
                },
                "authors": [
                    {
                        "name": "Anjiang Wei"
                    },
                    {
                        "name": "Huanmi Tan"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Daniel Mendoza"
                    },
                    {
                        "name": "Thiago S. F. X. Teixeira"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Caroline Trippel"
                    },
                    {
                        "name": "Alex Aiken"
                    }
                ],
                "author_detail": {
                    "name": "Alex Aiken"
                },
                "author": "Alex Aiken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12639v3",
                "updated": "2025-04-22T07:32:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    32,
                    21,
                    1,
                    112,
                    0
                ],
                "published": "2024-12-17T08:02:08Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    2,
                    8,
                    1,
                    352,
                    0
                ],
                "title": "Falcon: Faster and Parallel Inference of Large Language Models through\n  Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Falcon: Faster and Parallel Inference of Large Language Models through\n  Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree"
                },
                "summary": "Striking an optimal balance between minimal drafting latency and high\nspeculation accuracy to enhance the inference speed of Large Language Models\nremains a significant challenge in speculative decoding. In this paper, we\nintroduce Falcon, an innovative semi-autoregressive speculative decoding\nframework fashioned to augment both the drafter's parallelism and output\nquality. Falcon incorporates the Coupled Sequential Glancing Distillation\ntechnique, which fortifies inter-token dependencies within the same block,\nleading to increased speculation accuracy. We offer a comprehensive theoretical\nanalysis to illuminate the underlying mechanisms. Additionally, we introduce a\nCustom-Designed Decoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple forward passes as\nneeded, thereby boosting the number of drafted tokens and significantly\nimproving the overall acceptance rate. Comprehensive evaluations on benchmark\ndatasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior\nacceleration capabilities. The framework achieves a lossless speedup ratio\nranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model\nseries. These results outstrip existing speculative decoding methods for LLMs,\nincluding Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact\ndrafter architecture equivalent to merely two Transformer layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Striking an optimal balance between minimal drafting latency and high\nspeculation accuracy to enhance the inference speed of Large Language Models\nremains a significant challenge in speculative decoding. In this paper, we\nintroduce Falcon, an innovative semi-autoregressive speculative decoding\nframework fashioned to augment both the drafter's parallelism and output\nquality. Falcon incorporates the Coupled Sequential Glancing Distillation\ntechnique, which fortifies inter-token dependencies within the same block,\nleading to increased speculation accuracy. We offer a comprehensive theoretical\nanalysis to illuminate the underlying mechanisms. Additionally, we introduce a\nCustom-Designed Decoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple forward passes as\nneeded, thereby boosting the number of drafted tokens and significantly\nimproving the overall acceptance rate. Comprehensive evaluations on benchmark\ndatasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior\nacceleration capabilities. The framework achieves a lossless speedup ratio\nranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model\nseries. These results outstrip existing speculative decoding methods for LLMs,\nincluding Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact\ndrafter architecture equivalent to merely two Transformer layers."
                },
                "authors": [
                    {
                        "name": "Xiangxiang Gao"
                    },
                    {
                        "name": "Weisheng Xie"
                    },
                    {
                        "name": "Yiwei Xiang"
                    },
                    {
                        "name": "Feng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Feng Ji"
                },
                "author": "Feng Ji",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01532v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01532v2",
                "updated": "2025-04-22T07:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    22,
                    45,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-03T13:44:03Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    13,
                    44,
                    3,
                    0,
                    62,
                    0
                ],
                "title": "Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in\n  Power-Disparate Social Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in\n  Power-Disparate Social Scenarios"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsimulating human behaviour and social intelligence. However, they risk\nperpetuating societal biases, especially when demographic information is\ninvolved. We introduce a novel framework using cosine distance to measure\nsemantic shifts in responses and an LLM-judged Preference Win Rate (WR) to\nassess how demographic prompts affect response quality across power-disparate\nsocial scenarios. Evaluating five LLMs over 100 diverse social scenarios and\nnine demographic axes, our findings suggest a \"default persona\" bias toward\nmiddle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist\nviews. Moreover, interactions involving specific demographics are associated\nwith lower-quality responses. Lastly, the presence of power disparities\nincreases variability in response semantics and quality across demographic\ngroups, suggesting that implicit biases may be heightened under\npower-imbalanced conditions. These insights expose the demographic biases\ninherent in LLMs and offer potential paths toward future bias mitigation\nefforts in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsimulating human behaviour and social intelligence. However, they risk\nperpetuating societal biases, especially when demographic information is\ninvolved. We introduce a novel framework using cosine distance to measure\nsemantic shifts in responses and an LLM-judged Preference Win Rate (WR) to\nassess how demographic prompts affect response quality across power-disparate\nsocial scenarios. Evaluating five LLMs over 100 diverse social scenarios and\nnine demographic axes, our findings suggest a \"default persona\" bias toward\nmiddle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist\nviews. Moreover, interactions involving specific demographics are associated\nwith lower-quality responses. Lastly, the presence of power disparities\nincreases variability in response semantics and quality across demographic\ngroups, suggesting that implicit biases may be heightened under\npower-imbalanced conditions. These insights expose the demographic biases\ninherent in LLMs and offer potential paths toward future bias mitigation\nefforts in LLMs."
                },
                "authors": [
                    {
                        "name": "Bryan Chen Zhengyu Tan"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "NAACL 2025; 10 pages of main text. For interactive visualisation, see\n  https://inc0mple.github.io/Implicit_Bias_Interactive_Data_Viz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01532v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01532v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15649v1",
                "updated": "2025-04-22T07:15:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    15,
                    7,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T07:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    15,
                    7,
                    1,
                    112,
                    0
                ],
                "title": "RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video\n  Super-Resolution"
                },
                "summary": "As a fundamental challenge in visual computing, video super-resolution (VSR)\nfocuses on reconstructing highdefinition video sequences from their degraded\nlowresolution counterparts. While deep convolutional neural networks have\ndemonstrated state-of-the-art performance in spatial-temporal super-resolution\ntasks, their computationally intensive nature poses significant deployment\nchallenges for resource-constrained edge devices, particularly in real-time\nmobile video processing scenarios where power efficiency and latency\nconstraints coexist. In this work, we propose a Reparameterizable Architecture\nfor High Fidelity Video Super Resolution method, named RepNet-VSR, for\nreal-time 4x video super-resolution. On the REDS validation set, the proposed\nmodel achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per\n10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an\nexcellent balance between restoration quality and deployment efficiency. The\nproposed method scores higher than the previous champion algorithm of MAI video\nsuper-resolution challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental challenge in visual computing, video super-resolution (VSR)\nfocuses on reconstructing highdefinition video sequences from their degraded\nlowresolution counterparts. While deep convolutional neural networks have\ndemonstrated state-of-the-art performance in spatial-temporal super-resolution\ntasks, their computationally intensive nature poses significant deployment\nchallenges for resource-constrained edge devices, particularly in real-time\nmobile video processing scenarios where power efficiency and latency\nconstraints coexist. In this work, we propose a Reparameterizable Architecture\nfor High Fidelity Video Super Resolution method, named RepNet-VSR, for\nreal-time 4x video super-resolution. On the REDS validation set, the proposed\nmodel achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per\n10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an\nexcellent balance between restoration quality and deployment efficiency. The\nproposed method scores higher than the previous champion algorithm of MAI video\nsuper-resolution challenge."
                },
                "authors": [
                    {
                        "name": "Biao Wu"
                    },
                    {
                        "name": "Diankai Zhang"
                    },
                    {
                        "name": "Shaoli Liu"
                    },
                    {
                        "name": "Si Gao"
                    },
                    {
                        "name": "Chengjian Zheng"
                    },
                    {
                        "name": "Ning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ning Wang"
                },
                "author": "Ning Wang",
                "arxiv_comment": "Champion Solution for CVPR 2025 MAI VSR Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15640v1",
                "updated": "2025-04-22T06:57:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    57,
                    49,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T06:57:49Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    57,
                    49,
                    1,
                    112,
                    0
                ],
                "title": "Cost-Effective Text Clustering with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Effective Text Clustering with Large Language Models"
                },
                "summary": "Text clustering aims to automatically partition a collection of text\ndocuments into distinct clusters based on linguistic features. In the\nliterature, this task is usually framed as metric clustering based on text\nembeddings from pre-trained encoders or a graph clustering problem upon\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\nlanguage models (LLMs) bring significant advancement in this field by offering\ncontextualized text embeddings and highly accurate similarity scores, but\nmeanwhile, present grand challenges to cope with substantial computational\nand/or financial overhead caused by numerous API-based queries or inference\ncalls to the models.\n  In response, this paper proposes TECL, a cost-effective framework that taps\ninto the feedback from LLMs for accurate text clustering within a limited\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\nfurther leverages such constraints as supervision signals input to our weighted\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\nextraction of pairwise constraints through carefully-crafted prompting\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\nconsistently and considerably outperforms existing solutions in unsupervised\ntext clustering under the same query cost for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text clustering aims to automatically partition a collection of text\ndocuments into distinct clusters based on linguistic features. In the\nliterature, this task is usually framed as metric clustering based on text\nembeddings from pre-trained encoders or a graph clustering problem upon\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\nlanguage models (LLMs) bring significant advancement in this field by offering\ncontextualized text embeddings and highly accurate similarity scores, but\nmeanwhile, present grand challenges to cope with substantial computational\nand/or financial overhead caused by numerous API-based queries or inference\ncalls to the models.\n  In response, this paper proposes TECL, a cost-effective framework that taps\ninto the feedback from LLMs for accurate text clustering within a limited\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\nfurther leverages such constraints as supervision signals input to our weighted\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\nextraction of pairwise constraints through carefully-crafted prompting\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\nconsistently and considerably outperforms existing solutions in unsupervised\ntext clustering under the same query cost for LLMs."
                },
                "authors": [
                    {
                        "name": "Hongtao Wang"
                    },
                    {
                        "name": "Taiyan Zhang"
                    },
                    {
                        "name": "Renchi Yang"
                    },
                    {
                        "name": "Jianliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianliang Xu"
                },
                "author": "Jianliang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15637v1",
                "updated": "2025-04-22T06:56:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    56,
                    15,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T06:56:15Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    56,
                    15,
                    1,
                    112,
                    0
                ],
                "title": "DR.FIX: Automatically Fixing Data Races at Industry Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DR.FIX: Automatically Fixing Data Races at Industry Scale"
                },
                "summary": "Data races are a prevalent class of concurrency bugs in shared-memory\nparallel programs, posing significant challenges to software reliability and\nreproducibility. While there is an extensive body of research on detecting data\nraces and a wealth of practical detection tools across various programming\nlanguages, considerably less effort has been directed toward automatically\nfixing data races at an industrial scale. In large codebases, data races are\ncontinuously introduced and exhibit myriad patterns, making automated fixing\nparticularly challenging.\n  In this paper, we tackle the problem of automatically fixing data races at an\nindustrial scale. We present Dr.Fix, a tool that combines large language models\n(LLMs) with program analysis to generate fixes for data races in real-world\nsettings, effectively addressing a broad spectrum of racy patterns in complex\ncode contexts. Implemented for Go--the programming language widely used in\nmodern microservice architectures where concurrency is pervasive and data races\nare common--Dr.Fix seamlessly integrates into existing development workflows.\nWe detail the design of Dr.Fix and examine how individual design choices\ninfluence the quality of the fixes produced. Over the past 18 months, Dr.Fix\nhas been integrated into developer workflows at Uber demonstrating its\npractical utility. During this period, Dr.Fix produced patches for 224 (55%)\nfrom a corpus of 404 data races spanning various categories; 193 of these\npatches (86%) were accepted by more than a hundred developers via code reviews\nand integrated into the codebase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data races are a prevalent class of concurrency bugs in shared-memory\nparallel programs, posing significant challenges to software reliability and\nreproducibility. While there is an extensive body of research on detecting data\nraces and a wealth of practical detection tools across various programming\nlanguages, considerably less effort has been directed toward automatically\nfixing data races at an industrial scale. In large codebases, data races are\ncontinuously introduced and exhibit myriad patterns, making automated fixing\nparticularly challenging.\n  In this paper, we tackle the problem of automatically fixing data races at an\nindustrial scale. We present Dr.Fix, a tool that combines large language models\n(LLMs) with program analysis to generate fixes for data races in real-world\nsettings, effectively addressing a broad spectrum of racy patterns in complex\ncode contexts. Implemented for Go--the programming language widely used in\nmodern microservice architectures where concurrency is pervasive and data races\nare common--Dr.Fix seamlessly integrates into existing development workflows.\nWe detail the design of Dr.Fix and examine how individual design choices\ninfluence the quality of the fixes produced. Over the past 18 months, Dr.Fix\nhas been integrated into developer workflows at Uber demonstrating its\npractical utility. During this period, Dr.Fix produced patches for 224 (55%)\nfrom a corpus of 404 data races spanning various categories; 193 of these\npatches (86%) were accepted by more than a hundred developers via code reviews\nand integrated into the codebase."
                },
                "authors": [
                    {
                        "name": "Farnaz Behrang"
                    },
                    {
                        "name": "Zhizhou Zhang"
                    },
                    {
                        "name": "Georgian-Vlad Saioc"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Milind Chabbi"
                    }
                ],
                "author_detail": {
                    "name": "Milind Chabbi"
                },
                "author": "Milind Chabbi",
                "arxiv_comment": "To appear in PLDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15630v1",
                "updated": "2025-04-22T06:42:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    42,
                    22,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T06:42:22Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    42,
                    22,
                    1,
                    112,
                    0
                ],
                "title": "Exploiting Contextual Knowledge in LLMs through V-usable Information\n  based Layer Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Contextual Knowledge in LLMs through V-usable Information\n  based Layer Enhancement"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet they often struggle with context-faithfulness generations\nthat properly reflect contextual knowledge. While existing approaches focus on\nenhancing the decoding strategies, they ignore the fundamental mechanism of how\ncontextual information is processed within LLMs' internal states. As a result,\nLLMs remain limited in their ability to fully leverage contextual knowledge. In\nthis paper, we propose Context-aware Layer Enhancement (CaLE), a novel\nintervention method that enhances the utilization of contextual knowledge\nwithin LLMs' internal representations. By employing V-usable information\nanalysis, CaLE strategically amplifies the growth of contextual information at\nan optimal layer, thereby enriching representations in the final layer. Our\nexperiments demonstrate that CaLE effectively improves context-faithful\ngeneration in Question-Answering tasks, particularly in scenarios involving\nunknown or conflicting contextual knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet they often struggle with context-faithfulness generations\nthat properly reflect contextual knowledge. While existing approaches focus on\nenhancing the decoding strategies, they ignore the fundamental mechanism of how\ncontextual information is processed within LLMs' internal states. As a result,\nLLMs remain limited in their ability to fully leverage contextual knowledge. In\nthis paper, we propose Context-aware Layer Enhancement (CaLE), a novel\nintervention method that enhances the utilization of contextual knowledge\nwithin LLMs' internal representations. By employing V-usable information\nanalysis, CaLE strategically amplifies the growth of contextual information at\nan optimal layer, thereby enriching representations in the final layer. Our\nexperiments demonstrate that CaLE effectively improves context-faithful\ngeneration in Question-Answering tasks, particularly in scenarios involving\nunknown or conflicting contextual knowledge."
                },
                "authors": [
                    {
                        "name": "Xiaowei Yuan"
                    },
                    {
                        "name": "Zhao Yang"
                    },
                    {
                        "name": "Ziyang Huang"
                    },
                    {
                        "name": "Yequan Wang"
                    },
                    {
                        "name": "Siqi Fan"
                    },
                    {
                        "name": "Yiming Ju"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15629v1",
                "updated": "2025-04-22T06:41:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    41,
                    25,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T06:41:25Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    41,
                    25,
                    1,
                    112,
                    0
                ],
                "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation\n  Correction"
                },
                "summary": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of\nLarge Language Models (LLMs), revolutionizing information search and\nconsumption. RAG systems combine traditional search capabilities with LLMs to\ngenerate comprehensive answers to user queries, ideally with accurate\ncitations. However, in our experience of developing a RAG product, LLMs often\nstruggle with source attribution, aligning with other industry studies\nreporting citation accuracy rates of only about 74% for popular generative\nsearch engines. To address this, we present efficient post-processing\nalgorithms to improve citation accuracy in LLM-generated responses, with\nminimal impact on latency and cost. Our approaches cross-check generated\ncitations against retrieved articles using methods including keyword + semantic\nmatching, fine tuned model with BERTScore, and a lightweight LLM-based\ntechnique. Our experimental results demonstrate a relative improvement of\n15.46% in the overall accuracy metrics of our RAG system. This significant\nenhancement potentially enables a shift from our current larger language model\nto a relatively smaller model that is approximately 12x more cost-effective and\n3x faster in inference time, while maintaining comparable performance. This\nresearch contributes to enhancing the reliability and trustworthiness of\nAI-generated content in information retrieval and summarization tasks which is\ncritical to gain customer trust especially in commercial products.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of\nLarge Language Models (LLMs), revolutionizing information search and\nconsumption. RAG systems combine traditional search capabilities with LLMs to\ngenerate comprehensive answers to user queries, ideally with accurate\ncitations. However, in our experience of developing a RAG product, LLMs often\nstruggle with source attribution, aligning with other industry studies\nreporting citation accuracy rates of only about 74% for popular generative\nsearch engines. To address this, we present efficient post-processing\nalgorithms to improve citation accuracy in LLM-generated responses, with\nminimal impact on latency and cost. Our approaches cross-check generated\ncitations against retrieved articles using methods including keyword + semantic\nmatching, fine tuned model with BERTScore, and a lightweight LLM-based\ntechnique. Our experimental results demonstrate a relative improvement of\n15.46% in the overall accuracy metrics of our RAG system. This significant\nenhancement potentially enables a shift from our current larger language model\nto a relatively smaller model that is approximately 12x more cost-effective and\n3x faster in inference time, while maintaining comparable performance. This\nresearch contributes to enhancing the reliability and trustworthiness of\nAI-generated content in information retrieval and summarization tasks which is\ncritical to gain customer trust especially in commercial products."
                },
                "authors": [
                    {
                        "name": "Harsh Maheshwari"
                    },
                    {
                        "name": "Srikanth Tenneti"
                    },
                    {
                        "name": "Alwarappan Nakkiran"
                    }
                ],
                "author_detail": {
                    "name": "Alwarappan Nakkiran"
                },
                "author": "Alwarappan Nakkiran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15622v1",
                "updated": "2025-04-22T06:28:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    28,
                    8,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T06:28:08Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    28,
                    8,
                    1,
                    112,
                    0
                ],
                "title": "Exploring the Role of Large Language Models in Cybersecurity: A\n  Systematic Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Large Language Models in Cybersecurity: A\n  Systematic Survey"
                },
                "summary": "With the rapid development of technology and the acceleration of\ndigitalisation, the frequency and complexity of cyber security threats are\nincreasing. Traditional cybersecurity approaches, often based on static rules\nand predefined scenarios, are struggling to adapt to the rapidly evolving\nnature of modern cyberattacks. There is an urgent need for more adaptive and\nintelligent defence strategies. The emergence of Large Language Model (LLM)\nprovides an innovative solution to cope with the increasingly severe cyber\nthreats, and its potential in analysing complex attack patterns, predicting\nthreats and assisting real-time response has attracted a lot of attention in\nthe field of cybersecurity, and exploring how to effectively use LLM to defend\nagainst cyberattacks has become a hot topic in the current research field. This\nsurvey examines the applications of LLM from the perspective of the cyber\nattack lifecycle, focusing on the three phases of defense reconnaissance,\nfoothold establishment, and lateral movement, and it analyzes the potential of\nLLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how\nLLM-based security solutions are deployed and applied in different network\nscenarios. It also summarizes the internal and external risk issues faced by\nLLM during its application. Finally, this survey also points out the facing\nrisk issues and possible future research directions in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of technology and the acceleration of\ndigitalisation, the frequency and complexity of cyber security threats are\nincreasing. Traditional cybersecurity approaches, often based on static rules\nand predefined scenarios, are struggling to adapt to the rapidly evolving\nnature of modern cyberattacks. There is an urgent need for more adaptive and\nintelligent defence strategies. The emergence of Large Language Model (LLM)\nprovides an innovative solution to cope with the increasingly severe cyber\nthreats, and its potential in analysing complex attack patterns, predicting\nthreats and assisting real-time response has attracted a lot of attention in\nthe field of cybersecurity, and exploring how to effectively use LLM to defend\nagainst cyberattacks has become a hot topic in the current research field. This\nsurvey examines the applications of LLM from the perspective of the cyber\nattack lifecycle, focusing on the three phases of defense reconnaissance,\nfoothold establishment, and lateral movement, and it analyzes the potential of\nLLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how\nLLM-based security solutions are deployed and applied in different network\nscenarios. It also summarizes the internal and external risk issues faced by\nLLM during its application. Finally, this survey also points out the facing\nrisk issues and possible future research directions in this domain."
                },
                "authors": [
                    {
                        "name": "Shuang Tian"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Jiqiang Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Xuangou Wu"
                    },
                    {
                        "name": "Xiaoqiang Zhu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Weiting Zhang"
                    },
                    {
                        "name": "Zhenhui Yuan"
                    },
                    {
                        "name": "Shiwen Mao"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15619v1",
                "updated": "2025-04-22T06:19:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    19,
                    38,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T06:19:38Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    19,
                    38,
                    1,
                    112,
                    0
                ],
                "title": "AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced\n  Preference Optimization"
                },
                "summary": "Preference alignment through Direct Preference Optimization (DPO) has\ndemonstrated significant effectiveness in aligning multimodal large language\nmodels (MLLMs) with human preferences. However, existing methods focus\nprimarily on language preferences while neglecting the critical visual context.\nIn this paper, we propose an Adaptive Vision-enhanced Preference optimization\n(AdaViP) that addresses these limitations through two key innovations: (1)\nvision-based preference pair construction, which integrates multiple visual\nfoundation models to strategically remove key visual elements from the image,\nenhancing MLLMs' sensitivity to visual details; and (2) adaptive preference\noptimization that dynamically balances vision- and language-based preferences\nfor more accurate alignment. Extensive evaluations across different benchmarks\ndemonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%\nreductions in response-level and mentioned-level hallucination respectively on\nthe Object HalBench, significantly outperforming current state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference alignment through Direct Preference Optimization (DPO) has\ndemonstrated significant effectiveness in aligning multimodal large language\nmodels (MLLMs) with human preferences. However, existing methods focus\nprimarily on language preferences while neglecting the critical visual context.\nIn this paper, we propose an Adaptive Vision-enhanced Preference optimization\n(AdaViP) that addresses these limitations through two key innovations: (1)\nvision-based preference pair construction, which integrates multiple visual\nfoundation models to strategically remove key visual elements from the image,\nenhancing MLLMs' sensitivity to visual details; and (2) adaptive preference\noptimization that dynamically balances vision- and language-based preferences\nfor more accurate alignment. Extensive evaluations across different benchmarks\ndemonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%\nreductions in response-level and mentioned-level hallucination respectively on\nthe Object HalBench, significantly outperforming current state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Jinghan Li"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01801v2",
                "updated": "2025-04-22T06:19:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    19,
                    26,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-02T15:09:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    9,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Investigating and Scaling up Code-Switching for Multilingual Language\n  Model Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating and Scaling up Code-Switching for Multilingual Language\n  Model Pre-Training"
                },
                "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite the extreme language imbalance in the pre-training data. In this paper,\nwe closely examine the reasons behind this phenomenon, focusing on the\npre-training corpus. We find that the existence of code-switching, alternating\nbetween different languages within a context, is key to multilingual\ncapabilities. We conduct an analysis to investigate code-switching in the\npre-training corpus, examining its presence and categorizing it into four types\nwithin two quadrants. We then assess its impact on multilingual performance.\nThese types of code-switching data are unbalanced in proportions and\ndemonstrate different effects on facilitating language transfer. To better\nexplore the power of code-switching for language alignment during pre-training,\nwe investigate the strategy of synthetic code-switching. We continuously scale\nup the synthetic code-switching data and observe remarkable improvements in\nboth benchmarks and representation space. Extensive experiments indicate that\nincorporating synthetic code-switching data enables better language alignment\nand generalizes well to high, medium, and low-resource languages with\npre-training corpora of varying qualities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite the extreme language imbalance in the pre-training data. In this paper,\nwe closely examine the reasons behind this phenomenon, focusing on the\npre-training corpus. We find that the existence of code-switching, alternating\nbetween different languages within a context, is key to multilingual\ncapabilities. We conduct an analysis to investigate code-switching in the\npre-training corpus, examining its presence and categorizing it into four types\nwithin two quadrants. We then assess its impact on multilingual performance.\nThese types of code-switching data are unbalanced in proportions and\ndemonstrate different effects on facilitating language transfer. To better\nexplore the power of code-switching for language alignment during pre-training,\nwe investigate the strategy of synthetic code-switching. We continuously scale\nup the synthetic code-switching data and observe remarkable improvements in\nboth benchmarks and representation space. Extensive experiments indicate that\nincorporating synthetic code-switching data enables better language alignment\nand generalizes well to high, medium, and low-resource languages with\npre-training corpora of varying qualities."
                },
                "authors": [
                    {
                        "name": "Zhijun Wang"
                    },
                    {
                        "name": "Jiahuan Li"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Xue Han"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Shujian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shujian Huang"
                },
                "author": "Shujian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15610v2",
                "updated": "2025-04-23T04:59:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    59,
                    47,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T06:08:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    8,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in\n  Resource-Constrained Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in\n  Resource-Constrained Settings"
                },
                "summary": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy."
                },
                "authors": [
                    {
                        "name": "Md Millat Hosen"
                    }
                ],
                "author_detail": {
                    "name": "Md Millat Hosen"
                },
                "author": "Md Millat Hosen",
                "arxiv_comment": "18 pages, 6 figures (3 graphs + 3 flowchart/architecture diagrams),\n  submitted as a preprint for review consideration in AI for Education or\n  Machine Learning applications in low-resource settings. Includes detailed\n  experiments with LoRA and quantization methods for efficient LLM fine-tuning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05 (Learning and adaptive systems), 68T07 (Artificial\n  intelligence and education)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04223v2",
                "updated": "2025-04-22T05:49:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    5,
                    49,
                    32,
                    1,
                    112,
                    0
                ],
                "published": "2024-11-06T19:39:48Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    19,
                    39,
                    48,
                    2,
                    311,
                    0
                ],
                "title": "Diversity Helps Jailbreak Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity Helps Jailbreak Large Language Models"
                },
                "summary": "We have uncovered a powerful jailbreak technique that leverages large\nlanguage models' ability to diverge from prior context, enabling them to bypass\nsafety constraints and generate harmful outputs. By simply instructing the LLM\nto deviate and obfuscate previous attacks, our method dramatically outperforms\nexisting approaches, achieving up to a 62.83% higher success rate in\ncompromising ten leading chatbots, including GPT-4, Gemini, and Llama, while\nusing only 12.9% of the queries. This revelation exposes a critical flaw in\ncurrent LLM safety training, suggesting that existing methods may merely mask\nvulnerabilities rather than eliminate them. Our findings sound an urgent alarm\nfor the need to revolutionize testing methodologies to ensure robust and\nreliable LLM security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have uncovered a powerful jailbreak technique that leverages large\nlanguage models' ability to diverge from prior context, enabling them to bypass\nsafety constraints and generate harmful outputs. By simply instructing the LLM\nto deviate and obfuscate previous attacks, our method dramatically outperforms\nexisting approaches, achieving up to a 62.83% higher success rate in\ncompromising ten leading chatbots, including GPT-4, Gemini, and Llama, while\nusing only 12.9% of the queries. This revelation exposes a critical flaw in\ncurrent LLM safety training, suggesting that existing methods may merely mask\nvulnerabilities rather than eliminate them. Our findings sound an urgent alarm\nfor the need to revolutionize testing methodologies to ensure robust and\nreliable LLM security."
                },
                "authors": [
                    {
                        "name": "Weiliang Zhao"
                    },
                    {
                        "name": "Daniel Ben-Levi"
                    },
                    {
                        "name": "Wei Hao"
                    },
                    {
                        "name": "Junfeng Yang"
                    },
                    {
                        "name": "Chengzhi Mao"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhi Mao"
                },
                "author": "Chengzhi Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15600v1",
                "updated": "2025-04-22T05:40:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    5,
                    40,
                    59,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T05:40:59Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    5,
                    40,
                    59,
                    1,
                    112,
                    0
                ],
                "title": "Research on Navigation Methods Based on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on Navigation Methods Based on LLMs"
                },
                "summary": "In recent years, the field of indoor navigation has witnessed groundbreaking\nadvancements through the integration of Large Language Models (LLMs).\nTraditional navigation approaches relying on pre-built maps or reinforcement\nlearning exhibit limitations such as poor generalization and limited\nadaptability to dynamic environments. In contrast, LLMs offer a novel paradigm\nfor complex indoor navigation tasks by leveraging their exceptional semantic\ncomprehension, reasoning capabilities, and zero-shot generalization properties.\nWe propose an LLM-based navigation framework that leverages function calling\ncapabilities, positioning the LLM as the central controller. Our methodology\ninvolves modular decomposition of conventional navigation functions into\nreusable LLM tools with expandable configurations. This is complemented by a\nsystematically designed, transferable system prompt template and interaction\nworkflow that can be easily adapted across different implementations.\nExperimental validation in PyBullet simulation environments across diverse\nscenarios demonstrates the substantial potential and effectiveness of our\napproach, particularly in achieving context-aware navigation through dynamic\ntool composition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the field of indoor navigation has witnessed groundbreaking\nadvancements through the integration of Large Language Models (LLMs).\nTraditional navigation approaches relying on pre-built maps or reinforcement\nlearning exhibit limitations such as poor generalization and limited\nadaptability to dynamic environments. In contrast, LLMs offer a novel paradigm\nfor complex indoor navigation tasks by leveraging their exceptional semantic\ncomprehension, reasoning capabilities, and zero-shot generalization properties.\nWe propose an LLM-based navigation framework that leverages function calling\ncapabilities, positioning the LLM as the central controller. Our methodology\ninvolves modular decomposition of conventional navigation functions into\nreusable LLM tools with expandable configurations. This is complemented by a\nsystematically designed, transferable system prompt template and interaction\nworkflow that can be easily adapted across different implementations.\nExperimental validation in PyBullet simulation environments across diverse\nscenarios demonstrates the substantial potential and effectiveness of our\napproach, particularly in achieving context-aware navigation through dynamic\ntool composition."
                },
                "authors": [
                    {
                        "name": "Anlong Zhang"
                    },
                    {
                        "name": "Jianmin Ji"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Ji"
                },
                "author": "Jianmin Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14064v2",
                "updated": "2025-04-22T05:28:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    5,
                    28,
                    27,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-18T20:36:10Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    20,
                    36,
                    10,
                    4,
                    108,
                    0
                ],
                "title": "DoomArena: A framework for Testing AI Agents Against Evolving Security\n  Threats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoomArena: A framework for Testing AI Agents Against Evolving Security\n  Threats"
                },
                "summary": "We present DoomArena, a security evaluation framework for AI agents.\nDoomArena is designed on three principles: 1) It is a plug-in framework and\nintegrates easily into realistic agentic frameworks like BrowserGym (for web\nagents) and $\\tau$-bench (for tool calling agents); 2) It is configurable and\nallows for detailed threat modeling, allowing configuration of specific\ncomponents of the agentic framework being attackable, and specifying targets\nfor the attacker; and 3) It is modular and decouples the development of attacks\nfrom details of the environment in which the agent is deployed, allowing for\nthe same attacks to be applied across multiple environments. We illustrate\nseveral advantages of our framework, including the ability to adapt to new\nthreat models and environments easily, the ability to easily combine several\npreviously published attacks to enable comprehensive and fine-grained security\ntesting, and the ability to analyze trade-offs between various vulnerabilities\nand performance. We apply DoomArena to state-of-the-art (SOTA) web and\ntool-calling agents and find a number of surprising results: 1) SOTA agents\nhave varying levels of vulnerability to different threat models (malicious user\nvs malicious environment), and there is no Pareto dominant agent across all\nthreat models; 2) When multiple attacks are applied to an agent, they often\ncombine constructively; 3) Guardrail model-based defenses seem to fail, while\ndefenses based on powerful SOTA LLMs work better. DoomArena is available at\nhttps://github.com/ServiceNow/DoomArena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DoomArena, a security evaluation framework for AI agents.\nDoomArena is designed on three principles: 1) It is a plug-in framework and\nintegrates easily into realistic agentic frameworks like BrowserGym (for web\nagents) and $\\tau$-bench (for tool calling agents); 2) It is configurable and\nallows for detailed threat modeling, allowing configuration of specific\ncomponents of the agentic framework being attackable, and specifying targets\nfor the attacker; and 3) It is modular and decouples the development of attacks\nfrom details of the environment in which the agent is deployed, allowing for\nthe same attacks to be applied across multiple environments. We illustrate\nseveral advantages of our framework, including the ability to adapt to new\nthreat models and environments easily, the ability to easily combine several\npreviously published attacks to enable comprehensive and fine-grained security\ntesting, and the ability to analyze trade-offs between various vulnerabilities\nand performance. We apply DoomArena to state-of-the-art (SOTA) web and\ntool-calling agents and find a number of surprising results: 1) SOTA agents\nhave varying levels of vulnerability to different threat models (malicious user\nvs malicious environment), and there is no Pareto dominant agent across all\nthreat models; 2) When multiple attacks are applied to an agent, they often\ncombine constructively; 3) Guardrail model-based defenses seem to fail, while\ndefenses based on powerful SOTA LLMs work better. DoomArena is available at\nhttps://github.com/ServiceNow/DoomArena."
                },
                "authors": [
                    {
                        "name": "Leo Boisvert"
                    },
                    {
                        "name": "Mihir Bansal"
                    },
                    {
                        "name": "Chandra Kiran Reddy Evuru"
                    },
                    {
                        "name": "Gabriel Huang"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Avinandan Bose"
                    },
                    {
                        "name": "Maryam Fazel"
                    },
                    {
                        "name": "Quentin Cappart"
                    },
                    {
                        "name": "Jason Stanley"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Krishnamurthy Dvijotham"
                    }
                ],
                "author_detail": {
                    "name": "Krishnamurthy Dvijotham"
                },
                "author": "Krishnamurthy Dvijotham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05295v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05295v4",
                "updated": "2025-04-22T05:20:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    5,
                    20,
                    39,
                    1,
                    112,
                    0
                ],
                "published": "2024-10-03T17:59:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    59,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to\n  Jailbreak LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to\n  Jailbreak LLMs"
                },
                "summary": "In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that\ncan automatically discover as many jailbreak strategies as possible from\nscratch, without any human intervention or predefined scopes (e.g., specified\ncandidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo\ncan significantly outperform baseline methods, achieving a 74.3% higher average\nattack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an\n88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a\nunified framework that can incorporate existing human-designed jailbreak\nstrategies in a plug-and-play manner. By integrating human-designed strategies,\nAutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on\nGPT-4-1106-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that\ncan automatically discover as many jailbreak strategies as possible from\nscratch, without any human intervention or predefined scopes (e.g., specified\ncandidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo\ncan significantly outperform baseline methods, achieving a 74.3% higher average\nattack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an\n88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a\nunified framework that can incorporate existing human-designed jailbreak\nstrategies in a plug-and-play manner. By integrating human-designed strategies,\nAutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on\nGPT-4-1106-turbo."
                },
                "authors": [
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Edward Suh"
                    },
                    {
                        "name": "Yevgeniy Vorobeychik"
                    },
                    {
                        "name": "Zhuoqing Mao"
                    },
                    {
                        "name": "Somesh Jha"
                    },
                    {
                        "name": "Patrick McDaniel"
                    },
                    {
                        "name": "Huan Sun"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "arxiv_comment": "ICLR 2025 Spotlight. Project Page:\n  https://autodans.github.io/AutoDAN-Turbo Code:\n  https://github.com/SaFoLab-WISC/AutoDAN-Turbo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05295v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05295v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07989v2",
                "updated": "2025-04-22T05:18:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    5,
                    18,
                    24,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-07T10:33:14Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    33,
                    14,
                    0,
                    97,
                    0
                ],
                "title": "Regional Tiny Stories: Using Small Models to Compare Language Learning\n  and Tokenizer Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regional Tiny Stories: Using Small Models to Compare Language Learning\n  and Tokenizer Performance"
                },
                "summary": "Small Language Models (SLMs) offer efficient alternatives to LLMs for\nspecific domains. The 2023 TinyStories study developed an English dataset that\nallows SLMs with 1 to 10 million parameters to produce coherent outputs. Our\nresearch expands this framework by translating the original dataset into Indian\nlanguages and creating synthetic data using LLMs. We focus on Hindi, Marathi,\nand Bengali, evaluating SLMs for regional language processing and understanding\nlinguistic complexity. We show that SLMs efficiently process regional languages\nwith significantly fewer parameters than LLMs, providing a complementary\nframework for ``inference based evaluation\" of tokenization strategies and\nlinguistic complexity. Our analysis shows that language-specific tokenizers\noutperform general-purpose ones for Indian languages. Empirical validations,\nsupported by information-theoretic and morphological analyses, provides\nfundamental understanding behind the better performance of Hindi models over\nMarathi and Bengali. Additionally, we show that synthetic datasets outperform\ntranslated content for training SLMs. Correlation analyses reveal\ncross-linguistic patterns and language-specific relationships between\ncreativity, grammatical precision, and narrative completeness. These findings\nadvance both the practical application of SLMs to underserved languages and our\ntheoretical understanding of neural language development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models (SLMs) offer efficient alternatives to LLMs for\nspecific domains. The 2023 TinyStories study developed an English dataset that\nallows SLMs with 1 to 10 million parameters to produce coherent outputs. Our\nresearch expands this framework by translating the original dataset into Indian\nlanguages and creating synthetic data using LLMs. We focus on Hindi, Marathi,\nand Bengali, evaluating SLMs for regional language processing and understanding\nlinguistic complexity. We show that SLMs efficiently process regional languages\nwith significantly fewer parameters than LLMs, providing a complementary\nframework for ``inference based evaluation\" of tokenization strategies and\nlinguistic complexity. Our analysis shows that language-specific tokenizers\noutperform general-purpose ones for Indian languages. Empirical validations,\nsupported by information-theoretic and morphological analyses, provides\nfundamental understanding behind the better performance of Hindi models over\nMarathi and Bengali. Additionally, we show that synthetic datasets outperform\ntranslated content for training SLMs. Correlation analyses reveal\ncross-linguistic patterns and language-specific relationships between\ncreativity, grammatical precision, and narrative completeness. These findings\nadvance both the practical application of SLMs to underserved languages and our\ntheoretical understanding of neural language development."
                },
                "authors": [
                    {
                        "name": "Nirvan Patil"
                    },
                    {
                        "name": "Malhar Abhay Inamdar"
                    },
                    {
                        "name": "Agnivo Gosai"
                    },
                    {
                        "name": "Guruprasad Pathak"
                    },
                    {
                        "name": "Anish Joshi"
                    },
                    {
                        "name": "Aryan Sagavekar"
                    },
                    {
                        "name": "Anish Joshirao"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "34 pages, 24 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11972v2",
                "updated": "2025-04-22T05:04:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    5,
                    4,
                    40,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-16T11:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    8,
                    46,
                    2,
                    106,
                    0
                ],
                "title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA"
                },
                "summary": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.22 (EM) and 0.40 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.22 (EM) and 0.40 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks."
                },
                "authors": [
                    {
                        "name": "Xanh Ho"
                    },
                    {
                        "name": "Jiahao Huang"
                    },
                    {
                        "name": "Florian Boudin"
                    },
                    {
                        "name": "Akiko Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Akiko Aizawa"
                },
                "author": "Akiko Aizawa",
                "arxiv_comment": "17 pages; code and data are available at\n  https://github.com/Alab-NII/llm-judge-extract-qa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15585v1",
                "updated": "2025-04-22T05:02:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    5,
                    2,
                    49,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T05:02:49Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    5,
                    2,
                    49,
                    1,
                    112,
                    0
                ],
                "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment"
                },
                "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field."
                },
                "authors": [
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Jiahao Wu"
                    },
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Shiqian Zhao"
                    },
                    {
                        "name": "Chenlong Yin"
                    },
                    {
                        "name": "Jinhu Fu"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Zhihao Xu"
                    },
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Xinyun Zhou"
                    },
                    {
                        "name": "Weifei Jin"
                    },
                    {
                        "name": "Fanci Meng"
                    },
                    {
                        "name": "Junyuan Mao"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Minghe Wang"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Chengwei Liu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Qiankun Li"
                    },
                    {
                        "name": "Chongye Guo"
                    },
                    {
                        "name": "Yalan Qin"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Donghai Hong"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Yifan Jiang"
                    },
                    {
                        "name": "Dongxia Wang"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Yufei Guo"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Yanwei Yue"
                    },
                    {
                        "name": "Wenke Huang"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Qing Guo"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Cong Wu"
                    },
                    {
                        "name": "Jing Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    },
                    {
                        "name": "Guowen Xu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Hongwei Li"
                    },
                    {
                        "name": "Wenyuan Xu"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Ke Tang"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15188v2",
                "updated": "2025-04-22T04:22:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    4,
                    22,
                    9,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T15:57:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    57,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Weak-Strong Collaboration by Aligning Preferences"
                },
                "summary": "Current Large Language Models (LLMs) excel in general reasoning yet struggle\nwith specialized tasks requiring proprietary or domain-specific knowledge.\nFine-tuning large models for every niche application is often infeasible due to\nblack-box constraints and high computational overhead. To address this, we\npropose a collaborative framework that pairs a specialized weak model with a\ngeneral strong model. The weak model, tailored to specific domains, produces\ninitial drafts and background information, while the strong model leverages its\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\ncollaborative feedback to fine-tunes the weak model, which quantifies the\ninfluence of the weak model's contributions in the collaboration procedure and\nestablishes preference pairs to guide preference tuning of the weak model. We\nvalidate our framework through experiments on three domains. We find that the\ncollaboration significantly outperforms each model alone by leveraging\ncomplementary strengths. Moreover, aligning the weak model with the\ncollaborative preference further enhances overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) excel in general reasoning yet struggle\nwith specialized tasks requiring proprietary or domain-specific knowledge.\nFine-tuning large models for every niche application is often infeasible due to\nblack-box constraints and high computational overhead. To address this, we\npropose a collaborative framework that pairs a specialized weak model with a\ngeneral strong model. The weak model, tailored to specific domains, produces\ninitial drafts and background information, while the strong model leverages its\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\ncollaborative feedback to fine-tunes the weak model, which quantifies the\ninfluence of the weak model's contributions in the collaboration procedure and\nestablishes preference pairs to guide preference tuning of the weak model. We\nvalidate our framework through experiments on three domains. We find that the\ncollaboration significantly outperforms each model alone by leveraging\ncomplementary strengths. Moreover, aligning the weak model with the\ncollaborative preference further enhances overall performance."
                },
                "authors": [
                    {
                        "name": "Yizhu Jiao"
                    },
                    {
                        "name": "Xuchao Zhang"
                    },
                    {
                        "name": "Zhaoyang Wang"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Zhun Deng"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15573v1",
                "updated": "2025-04-22T04:07:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    4,
                    7,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T04:07:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    4,
                    7,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction"
                },
                "summary": "The improvement of LLMs' instruction-following capabilities depends\ncritically on the availability of high-quality instruction-response pairs.\nWhile existing automatic data synthetic methods alleviate the burden of manual\ncuration, they often rely heavily on either the quality of seed data or strong\nassumptions about the structure and content of web documents. To tackle these\nchallenges, we propose Web Reconstruction (WebR), a fully automated framework\nfor synthesizing high-quality instruction-tuning (IT) data directly from raw\nweb documents with minimal assumptions. Leveraging the inherent diversity of\nraw web content, we conceptualize web reconstruction as an instruction-tuning\ndata synthesis task via a novel dual-perspective paradigm--Web as Instruction\nand Web as Response--where each web document is designated as either an\ninstruction or a response to trigger the reconstruction process. Comprehensive\nexperiments show that datasets generated by WebR outperform state-of-the-art\nbaselines by up to 16.65% across four instruction-following benchmarks.\nNotably, WebR demonstrates superior compatibility, data efficiency, and\nscalability, enabling enhanced domain adaptation with minimal effort. The data\nand code are publicly available at https://github.com/YJiangcm/WebR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The improvement of LLMs' instruction-following capabilities depends\ncritically on the availability of high-quality instruction-response pairs.\nWhile existing automatic data synthetic methods alleviate the burden of manual\ncuration, they often rely heavily on either the quality of seed data or strong\nassumptions about the structure and content of web documents. To tackle these\nchallenges, we propose Web Reconstruction (WebR), a fully automated framework\nfor synthesizing high-quality instruction-tuning (IT) data directly from raw\nweb documents with minimal assumptions. Leveraging the inherent diversity of\nraw web content, we conceptualize web reconstruction as an instruction-tuning\ndata synthesis task via a novel dual-perspective paradigm--Web as Instruction\nand Web as Response--where each web document is designated as either an\ninstruction or a response to trigger the reconstruction process. Comprehensive\nexperiments show that datasets generated by WebR outperform state-of-the-art\nbaselines by up to 16.65% across four instruction-following benchmarks.\nNotably, WebR demonstrates superior compatibility, data efficiency, and\nscalability, enabling enhanced domain adaptation with minimal effort. The data\nand code are publicly available at https://github.com/YJiangcm/WebR."
                },
                "authors": [
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Yan Xu"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "15 pages, 11 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10834v3",
                "updated": "2025-04-22T03:55:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    3,
                    55,
                    14,
                    1,
                    112,
                    0
                ],
                "published": "2024-07-15T15:45:07Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    15,
                    45,
                    7,
                    0,
                    197,
                    0
                ],
                "title": "MetaLLM: A High-performant and Cost-efficient Dynamic Framework for\n  Wrapping LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaLLM: A High-performant and Cost-efficient Dynamic Framework for\n  Wrapping LLMs"
                },
                "summary": "The rapid progress in machine learning (ML) has brought forth many large\nlanguage models (LLMs) that excel in various tasks and areas. These LLMs come\nwith different abilities and costs in terms of computation or pricing. Since\nthe demand for each query can vary, e.g., because of the queried domain or its\ncomplexity, defaulting to one LLM in an application is not usually the best\nchoice, whether it is the biggest, priciest, or even the one with the best\naverage test performance. Consequently, picking the right LLM that is both\naccurate and cost-effective for an application is necessary yet remains a\nchallenge. In this paper, we introduce MetaLLM, a framework that dynamically\nand intelligently routes each query to the optimal LLM (among several available\nLLMs) for classification and multi-choice question-answering tasks, achieving\nsignificantly improved accuracy and cost-effectiveness. By framing the\nselection problem as a multi-armed bandit, MetaLLM balances prediction accuracy\nand cost efficiency under uncertainty. Our experiments, conducted on popular\nLLM platforms such as OpenAI and Together AI, as well as open-source LLM,\nshowcase MetaLLM's efficacy in real-world scenarios, laying the groundwork for\nfuture extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in machine learning (ML) has brought forth many large\nlanguage models (LLMs) that excel in various tasks and areas. These LLMs come\nwith different abilities and costs in terms of computation or pricing. Since\nthe demand for each query can vary, e.g., because of the queried domain or its\ncomplexity, defaulting to one LLM in an application is not usually the best\nchoice, whether it is the biggest, priciest, or even the one with the best\naverage test performance. Consequently, picking the right LLM that is both\naccurate and cost-effective for an application is necessary yet remains a\nchallenge. In this paper, we introduce MetaLLM, a framework that dynamically\nand intelligently routes each query to the optimal LLM (among several available\nLLMs) for classification and multi-choice question-answering tasks, achieving\nsignificantly improved accuracy and cost-effectiveness. By framing the\nselection problem as a multi-armed bandit, MetaLLM balances prediction accuracy\nand cost efficiency under uncertainty. Our experiments, conducted on popular\nLLM platforms such as OpenAI and Together AI, as well as open-source LLM,\nshowcase MetaLLM's efficacy in real-world scenarios, laying the groundwork for\nfuture extensions."
                },
                "authors": [
                    {
                        "name": "Quang H. Nguyen"
                    },
                    {
                        "name": "Thinh Dao"
                    },
                    {
                        "name": "Duy C. Hoang"
                    },
                    {
                        "name": "Juliette Decugis"
                    },
                    {
                        "name": "Saurav Manchanda"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    },
                    {
                        "name": "Khoa D. Doan"
                    }
                ],
                "author_detail": {
                    "name": "Khoa D. Doan"
                },
                "author": "Khoa D. Doan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15564v1",
                "updated": "2025-04-22T03:33:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    3,
                    33,
                    57,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T03:33:57Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    3,
                    33,
                    57,
                    1,
                    112,
                    0
                ],
                "title": "A Large-scale Class-level Benchmark Dataset for Code Generation with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large-scale Class-level Benchmark Dataset for Code Generation with\n  LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\npromising capabilities in code generation tasks. However, most existing\nbenchmarks focus on isolated functions and fail to capture the complexity of\nreal-world, class-level software structures. To address this gap, we introduce\na large-scale, Python class-level dataset curated from $13{,}174$ real-world\nopen-source projects. The dataset contains over 842,000 class skeletons, each\nincluding class and method signatures, along with associated docstrings when\navailable. We preserve structural and contextual dependencies critical to\nrealistic software development scenarios and enrich the dataset with static\ncode metrics to support downstream analysis. To evaluate the usefulness of this\ndataset, we use extracted class skeletons as prompts for GPT-4 to generate full\nclass implementations. Results show that the LLM-generated classes exhibit\nstrong lexical and structural similarity to human-written counterparts, with\naverage ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.\nThese findings confirm that well-structured prompts derived from real-world\nclass skeletons significantly enhance LLM performance in class-level code\ngeneration. This dataset offers a valuable resource for benchmarking, training,\nand improving LLMs in realistic software engineering contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\npromising capabilities in code generation tasks. However, most existing\nbenchmarks focus on isolated functions and fail to capture the complexity of\nreal-world, class-level software structures. To address this gap, we introduce\na large-scale, Python class-level dataset curated from $13{,}174$ real-world\nopen-source projects. The dataset contains over 842,000 class skeletons, each\nincluding class and method signatures, along with associated docstrings when\navailable. We preserve structural and contextual dependencies critical to\nrealistic software development scenarios and enrich the dataset with static\ncode metrics to support downstream analysis. To evaluate the usefulness of this\ndataset, we use extracted class skeletons as prompts for GPT-4 to generate full\nclass implementations. Results show that the LLM-generated classes exhibit\nstrong lexical and structural similarity to human-written counterparts, with\naverage ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.\nThese findings confirm that well-structured prompts derived from real-world\nclass skeletons significantly enhance LLM performance in class-level code\ngeneration. This dataset offers a valuable resource for benchmarking, training,\nand improving LLMs in realistic software engineering contexts."
                },
                "authors": [
                    {
                        "name": "Musfiqur Rahman"
                    },
                    {
                        "name": "SayedHassan Khatoonabadi"
                    },
                    {
                        "name": "Emad Shihab"
                    }
                ],
                "author_detail": {
                    "name": "Emad Shihab"
                },
                "author": "Emad Shihab",
                "arxiv_comment": "This paper was submitted to the 29th International Conference on\n  Evaluation and Assessment in Software Engineering (EASE 2025) AI models/data\n  track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15552v1",
                "updated": "2025-04-22T03:14:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    3,
                    14,
                    29,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T03:14:29Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    3,
                    14,
                    29,
                    1,
                    112,
                    0
                ],
                "title": "A Multi-Agent Framework for Automated Qinqiang Opera Script Generation\n  Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Framework for Automated Qinqiang Opera Script Generation\n  Using Large Language Models"
                },
                "summary": "This paper introduces a novel multi-Agent framework that automates the end to\nend production of Qinqiang opera by integrating Large Language Models , visual\ngeneration, and Text to Speech synthesis. Three specialized agents collaborate\nin sequence: Agent1 uses an LLM to craft coherent, culturally grounded\nscripts;Agent2 employs visual generation models to render contextually accurate\nstage scenes; and Agent3 leverages TTS to produce synchronized, emotionally\nexpressive vocal performances. In a case study on Dou E Yuan, the system\nachieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,\nand 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point\nimprovement over a Single Agent baseline. Ablation experiments demonstrate that\nremoving Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,\nunderscoring the value of modular collaboration. This work showcases how AI\ndriven pipelines can streamline and scale the preservation of traditional\nperforming arts, and points toward future enhancements in cross modal\nalignment, richer emotional nuance, and support for additional opera genres.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel multi-Agent framework that automates the end to\nend production of Qinqiang opera by integrating Large Language Models , visual\ngeneration, and Text to Speech synthesis. Three specialized agents collaborate\nin sequence: Agent1 uses an LLM to craft coherent, culturally grounded\nscripts;Agent2 employs visual generation models to render contextually accurate\nstage scenes; and Agent3 leverages TTS to produce synchronized, emotionally\nexpressive vocal performances. In a case study on Dou E Yuan, the system\nachieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,\nand 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point\nimprovement over a Single Agent baseline. Ablation experiments demonstrate that\nremoving Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,\nunderscoring the value of modular collaboration. This work showcases how AI\ndriven pipelines can streamline and scale the preservation of traditional\nperforming arts, and points toward future enhancements in cross modal\nalignment, richer emotional nuance, and support for additional opera genres."
                },
                "authors": [
                    {
                        "name": "Gengxian Cao"
                    },
                    {
                        "name": "Fengyuan Li"
                    },
                    {
                        "name": "Hong Duan"
                    },
                    {
                        "name": "Ye Yang"
                    },
                    {
                        "name": "Bofeng Wang"
                    },
                    {
                        "name": "Donghe Li"
                    }
                ],
                "author_detail": {
                    "name": "Donghe Li"
                },
                "author": "Donghe Li",
                "arxiv_comment": "17 pages,7 figures,1 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15549v1",
                "updated": "2025-04-22T03:11:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    3,
                    11,
                    10,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T03:11:10Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    3,
                    11,
                    10,
                    1,
                    112,
                    0
                ],
                "title": "Do It For Me vs. Do It With Me: Investigating User Perceptions of\n  Different Paradigms of Automation in Copilots for Feature-Rich Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do It For Me vs. Do It With Me: Investigating User Perceptions of\n  Different Paradigms of Automation in Copilots for Feature-Rich Software"
                },
                "summary": "Large Language Model (LLM)-based in-application assistants, or copilots, can\nautomate software tasks, but users often prefer learning by doing, raising\nquestions about the optimal level of automation for an effective user\nexperience. We investigated two automation paradigms by designing and\nimplementing a fully automated copilot (AutoCopilot) and a semi-automated\ncopilot (GuidedCopilot) that automates trivial steps while offering\nstep-by-step visual guidance. In a user study (N=20) across data analysis and\nvisual design tasks, GuidedCopilot outperformed AutoCopilot in user control,\nsoftware utility, and learnability, especially for exploratory and creative\ntasks, while AutoCopilot saved time for simpler visual tasks. A follow-up\ndesign exploration (N=10) enhanced GuidedCopilot with task-and state-aware\nfeatures, including in-context preview clips and adaptive instructions. Our\nfindings highlight the critical role of user control and tailored guidance in\ndesigning the next generation of copilots that enhance productivity, support\ndiverse skill levels, and foster deeper software engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based in-application assistants, or copilots, can\nautomate software tasks, but users often prefer learning by doing, raising\nquestions about the optimal level of automation for an effective user\nexperience. We investigated two automation paradigms by designing and\nimplementing a fully automated copilot (AutoCopilot) and a semi-automated\ncopilot (GuidedCopilot) that automates trivial steps while offering\nstep-by-step visual guidance. In a user study (N=20) across data analysis and\nvisual design tasks, GuidedCopilot outperformed AutoCopilot in user control,\nsoftware utility, and learnability, especially for exploratory and creative\ntasks, while AutoCopilot saved time for simpler visual tasks. A follow-up\ndesign exploration (N=10) enhanced GuidedCopilot with task-and state-aware\nfeatures, including in-context preview clips and adaptive instructions. Our\nfindings highlight the critical role of user control and tailored guidance in\ndesigning the next generation of copilots that enhance productivity, support\ndiverse skill levels, and foster deeper software engagement."
                },
                "authors": [
                    {
                        "name": "Anjali Khurana"
                    },
                    {
                        "name": "Xiaotian Su"
                    },
                    {
                        "name": "April Yi Wang"
                    },
                    {
                        "name": "Parmit K Chilana"
                    }
                ],
                "author_detail": {
                    "name": "Parmit K Chilana"
                },
                "author": "Parmit K Chilana",
                "arxiv_doi": "10.1145/3706598.3713431",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713431",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in the CHI Conference on Human Factors in\n  Computing Systems (CHI 2025), April 26 - May 1, 2025, Yokohama, Japan",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10284v2",
                "updated": "2025-04-22T03:09:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    3,
                    9,
                    56,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-14T14:52:28Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    52,
                    28,
                    0,
                    104,
                    0
                ],
                "title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the\n  Evaluation Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the\n  Evaluation Protocol"
                },
                "summary": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table."
                },
                "authors": [
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Jiefu Ou"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15548v1",
                "updated": "2025-04-22T02:59:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    2,
                    59,
                    3,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T02:59:03Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    2,
                    59,
                    3,
                    1,
                    112,
                    0
                ],
                "title": "LLM-based Semantic Augmentation for Harmful Content Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Semantic Augmentation for Harmful Content Detection"
                },
                "summary": "Recent advances in large language models (LLMs) have demonstrated strong\nperformance on simple text classification tasks, frequently under zero-shot\nsettings. However, their efficacy declines when tackling complex social media\nchallenges such as propaganda detection, hateful meme classification, and\ntoxicity identification. Much of the existing work has focused on using LLMs to\ngenerate synthetic training data, overlooking the potential of LLM-based text\npreprocessing and semantic augmentation. In this paper, we introduce an\napproach that prompts LLMs to clean noisy text and provide context-rich\nexplanations, thereby enhancing training sets without substantial increases in\ndata volume. We systematically evaluate on the SemEval 2024 multi-label\nPersuasive Meme dataset and further validate on the Google Jigsaw toxic\ncomments and Facebook hateful memes datasets to assess generalizability. Our\nresults reveal that zero-shot LLM classification underperforms on these\nhigh-context tasks compared to supervised models. In contrast, integrating\nLLM-based semantic augmentation yields performance on par with approaches that\nrely on human-annotated data, at a fraction of the cost. These findings\nunderscore the importance of strategically incorporating LLMs into machine\nlearning (ML) pipeline for social media classification tasks, offering broad\nimplications for combating harmful content online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have demonstrated strong\nperformance on simple text classification tasks, frequently under zero-shot\nsettings. However, their efficacy declines when tackling complex social media\nchallenges such as propaganda detection, hateful meme classification, and\ntoxicity identification. Much of the existing work has focused on using LLMs to\ngenerate synthetic training data, overlooking the potential of LLM-based text\npreprocessing and semantic augmentation. In this paper, we introduce an\napproach that prompts LLMs to clean noisy text and provide context-rich\nexplanations, thereby enhancing training sets without substantial increases in\ndata volume. We systematically evaluate on the SemEval 2024 multi-label\nPersuasive Meme dataset and further validate on the Google Jigsaw toxic\ncomments and Facebook hateful memes datasets to assess generalizability. Our\nresults reveal that zero-shot LLM classification underperforms on these\nhigh-context tasks compared to supervised models. In contrast, integrating\nLLM-based semantic augmentation yields performance on par with approaches that\nrely on human-annotated data, at a fraction of the cost. These findings\nunderscore the importance of strategically incorporating LLMs into machine\nlearning (ML) pipeline for social media classification tasks, offering broad\nimplications for combating harmful content online."
                },
                "authors": [
                    {
                        "name": "Elyas Meguellati"
                    },
                    {
                        "name": "Assaad Zeghina"
                    },
                    {
                        "name": "Shazia Sadiq"
                    },
                    {
                        "name": "Gianluca Demartini"
                    }
                ],
                "author_detail": {
                    "name": "Gianluca Demartini"
                },
                "author": "Gianluca Demartini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15546v1",
                "updated": "2025-04-22T02:52:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    2,
                    52,
                    8,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T02:52:08Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    2,
                    52,
                    8,
                    1,
                    112,
                    0
                ],
                "title": "A Framework for Testing and Adapting REST APIs as LLM Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Testing and Adapting REST APIs as LLM Tools"
                },
                "summary": "Large Language Models (LLMs) are enabling autonomous agents to perform\ncomplex workflows using external tools or functions, often provided via REST\nAPIs in enterprise systems. However, directly utilizing these APIs as tools\nposes challenges due to their complex input schemas, elaborate responses, and\noften ambiguous documentation. Current benchmarks for tool testing do not\nadequately address these complexities, leading to a critical gap in evaluating\nAPI readiness for agent-driven automation. In this work, we present a novel\ntesting framework aimed at evaluating and enhancing the readiness of REST APIs\nto function as tools for LLM-based agents. Our framework transforms apis as\ntools, generates comprehensive test cases for the APIs, translates tests cases\ninto natural language instructions suitable for agents, enriches tool\ndefinitions and evaluates the agent's ability t correctly invoke the API and\nprocess its inputs and responses. To provide actionable insights, we analyze\nthe outcomes of 750 test cases, presenting a detailed taxonomy of errors,\nincluding input misinterpretation, output handling inconsistencies, and schema\nmismatches. Additionally, we classify these test cases to streamline debugging\nand refinement of tool integrations. This work offers a foundational step\ntoward enabling enterprise APIs as tools, improving their usability in\nagent-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are enabling autonomous agents to perform\ncomplex workflows using external tools or functions, often provided via REST\nAPIs in enterprise systems. However, directly utilizing these APIs as tools\nposes challenges due to their complex input schemas, elaborate responses, and\noften ambiguous documentation. Current benchmarks for tool testing do not\nadequately address these complexities, leading to a critical gap in evaluating\nAPI readiness for agent-driven automation. In this work, we present a novel\ntesting framework aimed at evaluating and enhancing the readiness of REST APIs\nto function as tools for LLM-based agents. Our framework transforms apis as\ntools, generates comprehensive test cases for the APIs, translates tests cases\ninto natural language instructions suitable for agents, enriches tool\ndefinitions and evaluates the agent's ability t correctly invoke the API and\nprocess its inputs and responses. To provide actionable insights, we analyze\nthe outcomes of 750 test cases, presenting a detailed taxonomy of errors,\nincluding input misinterpretation, output handling inconsistencies, and schema\nmismatches. Additionally, we classify these test cases to streamline debugging\nand refinement of tool integrations. This work offers a foundational step\ntoward enabling enterprise APIs as tools, improving their usability in\nagent-based applications."
                },
                "authors": [
                    {
                        "name": "Jayachandu Bandlamudi"
                    },
                    {
                        "name": "Ritwik Chaudhuri"
                    },
                    {
                        "name": "Neelamadhav Gantayat"
                    },
                    {
                        "name": "Kushal Mukherjee"
                    },
                    {
                        "name": "Prerna Agarwal"
                    },
                    {
                        "name": "Renuka Sindhgatta"
                    },
                    {
                        "name": "Sameep Mehta"
                    }
                ],
                "author_detail": {
                    "name": "Sameep Mehta"
                },
                "author": "Sameep Mehta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]