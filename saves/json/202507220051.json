[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.13961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13961v1",
                "updated": "2025-07-18T14:24:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:24:29Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "title": "Secretive Hotplug Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secretive Hotplug Coded Caching"
                },
                "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.06433",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v2",
                "updated": "2025-07-18T13:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    29,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "arxiv_journal_ref": "Proceedings of the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25), September 22--26, 2025, Prague, Czech Republic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v1",
                "updated": "2025-07-18T06:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v3",
                "updated": "2025-07-18T01:49:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    49,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v2",
                "updated": "2025-07-18T01:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    36,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v1",
                "updated": "2025-07-17T23:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Fernando Berm√∫dez-Medina"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Lezhi Li"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Victoria M√∂nchJuan Haladjian"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Zhao Meng"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Raunak Sinha"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Mehrdad Farajtbar"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Emily Zhang"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "David G√ºera"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shang-Chen Wu"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Shang-Chen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11273v1",
                "updated": "2025-07-15T12:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T12:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding"
                },
                "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v3",
                "updated": "2025-07-15T11:31:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    31,
                    14,
                    1,
                    196,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag) accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11121v1",
                "updated": "2025-07-15T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "title": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging"
                },
                "summary": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging."
                },
                "authors": [
                    {
                        "name": "Tatsunori Shibuya"
                    },
                    {
                        "name": "Eichi Terasawa"
                    },
                    {
                        "name": "Hiromi Kimura"
                    },
                    {
                        "name": "Takeshi Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Fujiwara"
                },
                "author": "Takeshi Fujiwara",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11067v1",
                "updated": "2025-07-15T08:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T08:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit"
                },
                "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Lin Gan"
                    },
                    {
                        "name": "Wubing Wan"
                    },
                    {
                        "name": "Zeyu Song"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Lanke He"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zekun Yin"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "arxiv_comment": "Yinuo Wang and Tianqi Mao contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v1",
                "updated": "2025-07-14T20:38:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v2",
                "updated": "2025-07-14T19:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    51,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10757v1",
                "updated": "2025-07-14T19:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:31:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block"
                },
                "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo."
                },
                "authors": [
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v4",
                "updated": "2025-07-14T18:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    18,
                    22,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v1",
                "updated": "2025-07-14T17:49:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonz√°lez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Mart√≠n"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v4",
                "updated": "2025-07-11T22:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    22,
                    14,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v3",
                "updated": "2025-07-11T19:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    19,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing"
                },
                "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Crist√≥bal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "Jos√© Mar√≠a Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Mu√±oz"
                    },
                    {
                        "name": "Manuel Gil P√©rez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10579v1",
                "updated": "2025-07-11T10:57:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:57:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors"
                },
                "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain."
                },
                "authors": [
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ana√Øs Tack"
                    },
                    {
                        "name": "Justin Vasselli"
                    }
                ],
                "author_detail": {
                    "name": "Justin Vasselli"
                },
                "author": "Justin Vasselli",
                "arxiv_comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "Jos√© Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "Jos√© Emilio Labra Gayo"
                },
                "author": "Jos√© Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08278v1",
                "updated": "2025-07-11T02:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "title": "Observation of the electric Breit-Rabi Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of the electric Breit-Rabi Effect"
                },
                "summary": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions."
                },
                "authors": [
                    {
                        "name": "S. -Z. Wang"
                    },
                    {
                        "name": "S. -B. Wang"
                    },
                    {
                        "name": "Z. -J. Tao"
                    },
                    {
                        "name": "T. Xia"
                    },
                    {
                        "name": "Z. -T. Lu"
                    }
                ],
                "author_detail": {
                    "name": "Z. -T. Lu"
                },
                "author": "Z. -T. Lu",
                "arxiv_doi": "10.1073/pnas.2423902122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2423902122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_journal_ref": "122 (26)e2423902122 June 27 2025",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08232v1",
                "updated": "2025-07-11T00:36:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T00:36:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08143v1",
                "updated": "2025-07-10T20:03:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores"
                },
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07990v1",
                "updated": "2025-07-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs"
                },
                "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm."
                },
                "authors": [
                    {
                        "name": "Jeongseok Hyun"
                    },
                    {
                        "name": "Sukjun Hwang"
                    },
                    {
                        "name": "Su Ho Han"
                    },
                    {
                        "name": "Taeoh Kim"
                    },
                    {
                        "name": "Inwoong Lee"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Joon-Young Lee"
                    },
                    {
                        "name": "Seon Joo Kim"
                    },
                    {
                        "name": "Minho Shim"
                    }
                ],
                "author_detail": {
                    "name": "Minho Shim"
                },
                "author": "Minho Shim",
                "arxiv_comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v1",
                "updated": "2025-07-10T17:47:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code and models are available at https://github.com/NVlabs/Long-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v3",
                "updated": "2025-07-10T17:10:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    10,
                    49,
                    3,
                    191,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07400v1",
                "updated": "2025-07-10T03:39:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T03:39:23Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows"
                },
                "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows."
                },
                "authors": [
                    {
                        "name": "Zaifeng Pan"
                    },
                    {
                        "name": "Ajjkumar Patel"
                    },
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Yipeng Shen"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Wan-Lu Li"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Yufei Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ding"
                },
                "author": "Yufei Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v1",
                "updated": "2025-07-10T01:51:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07290v1",
                "updated": "2025-07-09T21:18:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T21:18:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics"
                },
                "summary": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults."
                },
                "authors": [
                    {
                        "name": "M. Karakaya"
                    },
                    {
                        "name": "I. Gurbuz"
                    },
                    {
                        "name": "L. Fulanovic"
                    },
                    {
                        "name": "U. Adem"
                    }
                ],
                "author_detail": {
                    "name": "U. Adem"
                },
                "author": "U. Adem",
                "arxiv_doi": "10.1039/D4TC01735H",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1039/D4TC01735H",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted version of the article published in J. Mater. Chem. C. 10\n  Pages, 7 Figures. Plus SI file as a single pdf",
                "arxiv_journal_ref": "J. Mater. Chem. C, 2024,12, 19612-19619",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06739v1",
                "updated": "2025-07-09T10:53:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:53:05Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold"
                },
                "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."
                },
                "authors": [
                    {
                        "name": "Zishen Huang"
                    },
                    {
                        "name": "Chunyu Yang"
                    },
                    {
                        "name": "Mengyuan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Ren"
                },
                "author": "Mengyuan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v2",
                "updated": "2025-07-09T07:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Safety Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Safety Inference Scaling"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for\n  LLM Safety Assurance\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06567v1",
                "updated": "2025-07-09T05:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v3",
                "updated": "2025-07-09T04:43:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    4,
                    43,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06517v1",
                "updated": "2025-07-09T03:33:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T03:33:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance."
                },
                "authors": [
                    {
                        "name": "Zicong Tang"
                    },
                    {
                        "name": "Shi Luohe"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "arxiv_comment": "Accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v2",
                "updated": "2025-07-09T02:35:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    35,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted By ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v2",
                "updated": "2025-07-08T21:23:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    21,
                    23,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "21 pages, 10 figures. Supplement 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06349v1",
                "updated": "2025-07-08T19:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T19:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design"
                },
                "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization."
                },
                "authors": [
                    {
                        "name": "Erin Ransom"
                    },
                    {
                        "name": "Andrew Lim"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v3",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07061v1",
                "updated": "2025-07-08T09:20:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:20:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems"
                },
                "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems."
                },
                "authors": [
                    {
                        "name": "Shervin Ghaffari"
                    },
                    {
                        "name": "Zohre Bahranifard"
                    },
                    {
                        "name": "Mohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Akbari"
                },
                "author": "Mohammad Akbari",
                "arxiv_comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v4",
                "updated": "2025-07-08T07:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03622v3",
                "updated": "2025-07-08T02:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    15,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2023-06-06T12:19:05Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    12,
                    19,
                    5,
                    1,
                    157,
                    0
                ],
                "title": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference"
                },
                "summary": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Xiaonan Luo"
                    },
                    {
                        "name": "Zhuohao Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Yu Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yu Ding"
                },
                "author": "Yu Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v2",
                "updated": "2025-07-08T00:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    51,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07120v1",
                "updated": "2025-07-07T19:47:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T19:47:24Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding"
                },
                "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical."
                },
                "authors": [
                    {
                        "name": "Nidhi Bhatia"
                    },
                    {
                        "name": "Ankit More"
                    },
                    {
                        "name": "Ritika Borkar"
                    },
                    {
                        "name": "Tiyasa Mitra"
                    },
                    {
                        "name": "Ramon Matas"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Maximilian Golub"
                    },
                    {
                        "name": "Dheevatsa Mudigere"
                    },
                    {
                        "name": "Brian Pharris"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    }
                ],
                "author_detail": {
                    "name": "Bita Darvish Rouhani"
                },
                "author": "Bita Darvish Rouhani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v2",
                "updated": "2025-07-07T09:25:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    25,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max L√ºbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v1",
                "updated": "2025-07-06T15:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v2",
                "updated": "2025-07-05T15:51:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    51,
                    57,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas K√∂hler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v2",
                "updated": "2025-07-05T15:40:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    40,
                    51,
                    5,
                    186,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v2",
                "updated": "2025-07-05T13:37:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    37,
                    48,
                    5,
                    186,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Gabriel Franco"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03980v1",
                "updated": "2025-07-05T10:11:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T10:11:37Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "title": "Combination generators with optimal cache utilization and communication\n  free parallel execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combination generators with optimal cache utilization and communication\n  free parallel execution"
                },
                "summary": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators."
                },
                "authors": [
                    {
                        "name": "Xi He"
                    },
                    {
                        "name": "Max. A. Little"
                    }
                ],
                "author_detail": {
                    "name": "Max. A. Little"
                },
                "author": "Max. A. Little",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03919v1",
                "updated": "2025-07-05T06:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T06:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "title": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery"
                },
                "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design"
                },
                "authors": [
                    {
                        "name": "Duy Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy Le"
                },
                "author": "Duy Le",
                "arxiv_comment": "6 pages, 3 figures, 3 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v3",
                "updated": "2025-07-05T01:08:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    1,
                    8,
                    40,
                    5,
                    186,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Sean Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "36 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v1",
                "updated": "2025-07-04T21:09:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. K√ºhn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. K√ºhn"
                },
                "author": "Martin J. K√ºhn",
                "arxiv_comment": "29 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03445v1",
                "updated": "2025-07-04T10:01:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "summary": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results."
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13360v1",
                "updated": "2025-07-04T09:35:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    35,
                    0,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:35:00Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    35,
                    0,
                    4,
                    185,
                    0
                ],
                "title": "Low-Light Enhancement via Encoder-Decoder Network with Illumination\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Light Enhancement via Encoder-Decoder Network with Illumination\n  Guidance"
                },
                "summary": "This paper introduces a novel deep learning framework for low-light image\nenhancement, named the Encoder-Decoder Network with Illumination Guidance\n(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination\nmap, derived from Bright Channel Prior (BCP), as a guidance input. This\nillumination guidance helps the network focus on underexposed regions,\neffectively steering the enhancement process. To further improve the model's\nrepresentational power, a Spatial Pyramid Pooling (SPP) module is incorporated\nto extract multi-scale contextual features, enabling better handling of diverse\nlighting conditions. Additionally, the Swish activation function is employed to\nensure smoother gradient propagation during training. EDNIG is optimized within\na Generative Adversarial Network (GAN) framework using a composite loss\nfunction that combines adversarial loss, pixel-wise mean squared error (MSE),\nand perceptual loss. Experimental results show that EDNIG achieves competitive\nperformance compared to state-of-the-art methods in quantitative metrics and\nvisual quality, while maintaining lower model complexity, demonstrating its\nsuitability for real-world applications. The source code for this work is\navailable at https://github.com/tranleanh/ednig.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel deep learning framework for low-light image\nenhancement, named the Encoder-Decoder Network with Illumination Guidance\n(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination\nmap, derived from Bright Channel Prior (BCP), as a guidance input. This\nillumination guidance helps the network focus on underexposed regions,\neffectively steering the enhancement process. To further improve the model's\nrepresentational power, a Spatial Pyramid Pooling (SPP) module is incorporated\nto extract multi-scale contextual features, enabling better handling of diverse\nlighting conditions. Additionally, the Swish activation function is employed to\nensure smoother gradient propagation during training. EDNIG is optimized within\na Generative Adversarial Network (GAN) framework using a composite loss\nfunction that combines adversarial loss, pixel-wise mean squared error (MSE),\nand perceptual loss. Experimental results show that EDNIG achieves competitive\nperformance compared to state-of-the-art methods in quantitative metrics and\nvisual quality, while maintaining lower model complexity, demonstrating its\nsuitability for real-world applications. The source code for this work is\navailable at https://github.com/tranleanh/ednig."
                },
                "authors": [
                    {
                        "name": "Le-Anh Tran"
                    },
                    {
                        "name": "Chung Nguyen Tran"
                    },
                    {
                        "name": "Ngoc-Luu Nguyen"
                    },
                    {
                        "name": "Nhan Cach Dang"
                    },
                    {
                        "name": "Jordi Carrabina"
                    },
                    {
                        "name": "David Castells-Rufas"
                    },
                    {
                        "name": "Minh Son Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Minh Son Nguyen"
                },
                "author": "Minh Son Nguyen",
                "arxiv_comment": "6 pages, 3 figures, ICCCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03396v1",
                "updated": "2025-07-04T09:03:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:03:18Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "title": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge"
                },
                "summary": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains."
                },
                "authors": [
                    {
                        "name": "Fariborz Momtazzadeh"
                    },
                    {
                        "name": "Farshad Sohbatzadeh"
                    },
                    {
                        "name": "Hamed Soltani Ahmadi"
                    },
                    {
                        "name": "Ramin Mehrabifard"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Mehrabifard"
                },
                "author": "Ramin Mehrabifard",
                "arxiv_doi": "10.1007/S40042-025-01392-9.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/S40042-025-01392-9.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.03396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v3",
                "updated": "2025-07-04T06:49:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    31,
                    4,
                    185,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v3",
                "updated": "2025-07-04T06:36:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    36,
                    38,
                    4,
                    185,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03231v1",
                "updated": "2025-07-04T00:16:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T00:16:15Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "title": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching"
                },
                "summary": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Brian Plancher"
                    }
                ],
                "author_detail": {
                    "name": "Brian Plancher"
                },
                "author": "Brian Plancher",
                "arxiv_comment": "Accepted to IROS 2025, 7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03153v1",
                "updated": "2025-07-03T20:20:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T20:20:33Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "title": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference"
                },
                "summary": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware."
                },
                "authors": [
                    {
                        "name": "Weishu Deng"
                    },
                    {
                        "name": "Yujie Yang"
                    },
                    {
                        "name": "Peiran Du"
                    },
                    {
                        "name": "Lingfeng Xiang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Jia Rao"
                    }
                ],
                "author_detail": {
                    "name": "Jia Rao"
                },
                "author": "Jia Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04789v2",
                "updated": "2025-07-03T17:11:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    11,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2023-12-08T02:03:55Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    2,
                    3,
                    55,
                    4,
                    342,
                    0
                ],
                "title": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System"
                },
                "summary": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jishen Zhao"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_doi": "10.1145/3676642.3736119",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736119",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appears in the Proceedings of the 30th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems,\n  Volume 3 (ASPLOS 25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v3",
                "updated": "2025-07-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    22,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02397v1",
                "updated": "2025-07-03T07:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics"
                },
                "summary": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li",
                "arxiv_comment": "16 pages, 5 figures, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02227v1",
                "updated": "2025-07-03T01:22:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations"
                },
                "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications."
                },
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02006v1",
                "updated": "2025-07-02T00:35:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T00:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design"
                },
                "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Shakya Jayakody"
                    },
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v1",
                "updated": "2025-07-01T22:27:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01154v1",
                "updated": "2025-07-01T19:28:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T19:28:37Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDP: Private Training Large Language Models with Efficient DP-SGD"
                },
                "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."
                },
                "authors": [
                    {
                        "name": "Liangyu Wang"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zihang Xiang"
                    },
                    {
                        "name": "David E. Keyes"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00797v1",
                "updated": "2025-07-01T14:30:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T14:30:31Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator"
                },
                "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."
                },
                "authors": [
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Haroon Waris"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Jianfei Jiang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Guanghui He"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui He"
                },
                "author": "Guanghui He",
                "arxiv_comment": "DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.14136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14136v1",
                "updated": "2025-07-18T17:59:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    59,
                    24,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:59:24Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    59,
                    24,
                    4,
                    199,
                    0
                ],
                "title": "Missing baryons recovered: a measurement of the gas fraction in galaxies\n  and groups with the kinematic Sunyaev-Zel'dovich effect and CMB lensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing baryons recovered: a measurement of the gas fraction in galaxies\n  and groups with the kinematic Sunyaev-Zel'dovich effect and CMB lensing"
                },
                "summary": "We present new constraints on the halo masses and matter density profiles of\nDESI galaxy groups by cross-correlating samples of Luminous Red Galaxies (LRGs)\nand Bright Galaxy Survey (BGS) galaxies with the publicly available CMB lensing\nconvergence map from ACT DR6. This provides an independent, lensing-based\ncalibration of halo masses, complementary to methods relying on clustering or\ndynamics. We derive constraints on the mean halo mass for three DESI-selected\nsamples, finding $\\log(M_{\\rm halo}/(M_\\odot/h)) \\approx 13.18$, 13.03 and\n13.02 for the Main LRG, Extended LRG, and BGS samples, respectively. Using a\nhalo model approach, we also compare the projected galaxy-matter density\nprofiles with previously reported gas profiles inferred from measurements of\nthe kinematic Sunyaev-Zel'dovich (kSZ) effect. This work addresses one of the\nkey uncertainties in interpreting kSZ signals -- the unknown host halo mass\ndistribution -- by providing an independent and consistent mass calibration.\nThe agreement between the gas and total mass profiles at large aperture\nsuggests that sufficiently far from the group center (2--3 virial radii), we\nrecover all the baryons, offering a resolution to the 'missing baryon' problem.\nWe further study the cumulative gas fractions for all galaxies as well as for\nthe most massive galaxy groups in the sample ($\\log(M_{\\rm halo}/(M_\\odot/h))\n\\approx 13.5$), finding values that are physically sensible and in agreement\nwith previous findings using kSZ and X-ray data: compared to the TNG300\nsimulation, the observed gas fractions are systematically lower at fixed radius\nby $\\gtrsim$4$\\sigma$, providing compelling, independent evidence for stronger\nbaryonic feedback in the real Universe. These findings highlight the power of\ncombining CMB lensing with galaxy surveys to probe the interplay between\nbaryons and dark matter in group-sized halos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present new constraints on the halo masses and matter density profiles of\nDESI galaxy groups by cross-correlating samples of Luminous Red Galaxies (LRGs)\nand Bright Galaxy Survey (BGS) galaxies with the publicly available CMB lensing\nconvergence map from ACT DR6. This provides an independent, lensing-based\ncalibration of halo masses, complementary to methods relying on clustering or\ndynamics. We derive constraints on the mean halo mass for three DESI-selected\nsamples, finding $\\log(M_{\\rm halo}/(M_\\odot/h)) \\approx 13.18$, 13.03 and\n13.02 for the Main LRG, Extended LRG, and BGS samples, respectively. Using a\nhalo model approach, we also compare the projected galaxy-matter density\nprofiles with previously reported gas profiles inferred from measurements of\nthe kinematic Sunyaev-Zel'dovich (kSZ) effect. This work addresses one of the\nkey uncertainties in interpreting kSZ signals -- the unknown host halo mass\ndistribution -- by providing an independent and consistent mass calibration.\nThe agreement between the gas and total mass profiles at large aperture\nsuggests that sufficiently far from the group center (2--3 virial radii), we\nrecover all the baryons, offering a resolution to the 'missing baryon' problem.\nWe further study the cumulative gas fractions for all galaxies as well as for\nthe most massive galaxy groups in the sample ($\\log(M_{\\rm halo}/(M_\\odot/h))\n\\approx 13.5$), finding values that are physically sensible and in agreement\nwith previous findings using kSZ and X-ray data: compared to the TNG300\nsimulation, the observed gas fractions are systematically lower at fixed radius\nby $\\gtrsim$4$\\sigma$, providing compelling, independent evidence for stronger\nbaryonic feedback in the real Universe. These findings highlight the power of\ncombining CMB lensing with galaxy surveys to probe the interplay between\nbaryons and dark matter in group-sized halos."
                },
                "authors": [
                    {
                        "name": "Boryana Hadzhiyska"
                    },
                    {
                        "name": "Simone Ferraro"
                    },
                    {
                        "name": "Gerrit S. Farren"
                    },
                    {
                        "name": "Noah Sailer"
                    },
                    {
                        "name": "Rongpu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Rongpu Zhou"
                },
                "author": "Rongpu Zhou",
                "arxiv_comment": "22 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09942v3",
                "updated": "2025-07-18T17:52:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    52,
                    57,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-15T03:56:17Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    56,
                    17,
                    3,
                    135,
                    0
                ],
                "title": "Better Understanding Triple Differences Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Understanding Triple Differences Estimators"
                },
                "summary": "Triple Differences (DDD) designs are widely used in empirical work to relax\nparallel trends assumptions in Difference-in-Differences (DiD) settings. This\npaper highlights that common DDD implementations -- such as taking the\ndifference between two DiDs or applying three-way fixed effects regressions --\nare generally invalid when identification requires conditioning on covariates.\nIn staggered adoption settings, the common DiD practice of pooling all\nnot-yet-treated units as a comparison group can introduce additional bias, even\nwhen covariates are not required for identification. These insights challenge\nconventional empirical strategies and underscore the need for estimators\ntailored specifically to DDD structures. We develop regression adjustment,\ninverse probability weighting, and doubly robust estimators that remain valid\nunder covariate-adjusted DDD parallel trends. For staggered designs, we\ndemonstrate how to effectively utilize multiple comparison groups to obtain\nmore informative inferences. Simulations and three empirical applications\nhighlight bias reductions and precision gains relative to standard approaches.\nA companion R package is available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triple Differences (DDD) designs are widely used in empirical work to relax\nparallel trends assumptions in Difference-in-Differences (DiD) settings. This\npaper highlights that common DDD implementations -- such as taking the\ndifference between two DiDs or applying three-way fixed effects regressions --\nare generally invalid when identification requires conditioning on covariates.\nIn staggered adoption settings, the common DiD practice of pooling all\nnot-yet-treated units as a comparison group can introduce additional bias, even\nwhen covariates are not required for identification. These insights challenge\nconventional empirical strategies and underscore the need for estimators\ntailored specifically to DDD structures. We develop regression adjustment,\ninverse probability weighting, and doubly robust estimators that remain valid\nunder covariate-adjusted DDD parallel trends. For staggered designs, we\ndemonstrate how to effectively utilize multiple comparison groups to obtain\nmore informative inferences. Simulations and three empirical applications\nhighlight bias reductions and precision gains relative to standard approaches.\nA companion R package is available."
                },
                "authors": [
                    {
                        "name": "Marcelo Ortiz-Villavicencio"
                    },
                    {
                        "name": "Pedro H. C. Sant'Anna"
                    }
                ],
                "author_detail": {
                    "name": "Pedro H. C. Sant'Anna"
                },
                "author": "Pedro H. C. Sant'Anna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06848v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06848v5",
                "updated": "2025-07-18T17:52:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    52,
                    45,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-12T15:34:24Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    15,
                    34,
                    24,
                    6,
                    12,
                    0
                ],
                "title": "A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models"
                },
                "summary": "Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we present Feynman-Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models - even\nwith off-the-shelf rewards - can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we present Feynman-Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models - even\nwith off-the-shelf rewards - can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering ."
                },
                "authors": [
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Zachary Horvitz"
                    },
                    {
                        "name": "Ryan Teehan"
                    },
                    {
                        "name": "Mengye Ren"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Kathleen McKeown"
                    },
                    {
                        "name": "Rajesh Ranganath"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Ranganath"
                },
                "author": "Rajesh Ranganath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06848v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06848v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03304v2",
                "updated": "2025-07-18T17:50:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    50,
                    4,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-05T16:03:17Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    3,
                    17,
                    2,
                    36,
                    0
                ],
                "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning"
                },
                "summary": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose Divergence-driven\nZeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer\nadaptation by incorporating projections to ZO updates, generating\ndiverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning. Our code is released at\nhttps://anonymous.4open.science/r/DiZO-E86D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose Divergence-driven\nZeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer\nadaptation by incorporating projections to ZO updates, generating\ndiverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning. Our code is released at\nhttps://anonymous.4open.science/r/DiZO-E86D."
                },
                "authors": [
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Caiwei Ding"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Jaewoo Lee"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14111v1",
                "updated": "2025-07-18T17:43:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    43,
                    56,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:43:56Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    43,
                    56,
                    4,
                    199,
                    0
                ],
                "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning"
                },
                "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources."
                },
                "authors": [
                    {
                        "name": "Xiaoya Li"
                    },
                    {
                        "name": "Xiaofei Sun"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Chris Shum"
                    }
                ],
                "author_detail": {
                    "name": "Chris Shum"
                },
                "author": "Chris Shum",
                "arxiv_comment": "Preprint Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14107v1",
                "updated": "2025-07-18T17:39:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    39,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:39:03Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    39,
                    3,
                    4,
                    199,
                    0
                ],
                "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps\n  Using Large Language Models for Bridge Condition Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Interpretation of Non-Destructive Evaluation Contour Maps\n  Using Large Language Models for Bridge Condition Assessment"
                },
                "summary": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments."
                },
                "authors": [
                    {
                        "name": "Viraj Nishesh Darji"
                    },
                    {
                        "name": "Callie C. Liao"
                    },
                    {
                        "name": "Duoduo Liao"
                    }
                ],
                "author_detail": {
                    "name": "Duoduo Liao"
                },
                "author": "Duoduo Liao",
                "arxiv_doi": "10.1109/BigData62323.2024.10825532",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/BigData62323.2024.10825532",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.14107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE BigData, Year: 2024; Page: 3258-3263",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14096v1",
                "updated": "2025-07-18T17:23:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    23,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:23:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    23,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts\n  (PLABA) track",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts\n  (PLABA) track"
                },
                "summary": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools."
                },
                "authors": [
                    {
                        "name": "Brian Ondov"
                    },
                    {
                        "name": "William Xia"
                    },
                    {
                        "name": "Kush Attal"
                    },
                    {
                        "name": "Ishita Unde"
                    },
                    {
                        "name": "Jerry He"
                    },
                    {
                        "name": "Hoa Dang"
                    },
                    {
                        "name": "Ian Soboroff"
                    },
                    {
                        "name": "Dina Demner-Fushman"
                    }
                ],
                "author_detail": {
                    "name": "Dina Demner-Fushman"
                },
                "author": "Dina Demner-Fushman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02768v2",
                "updated": "2025-07-18T17:21:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    21,
                    42,
                    4,
                    199,
                    0
                ],
                "published": "2025-04-03T17:05:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    5,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs"
                },
                "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages"
                },
                "authors": [
                    {
                        "name": "Jaap Jumelet"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Joakim Nivre"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12272v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12272v5",
                "updated": "2025-07-18T17:21:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    21,
                    27,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-17T19:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    19,
                    16,
                    37,
                    0,
                    48,
                    0
                ],
                "title": "Learning to Reason at the Frontier of Learnability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason at the Frontier of Learnability"
                },
                "summary": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs."
                },
                "authors": [
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Anya Sims"
                    },
                    {
                        "name": "Johannes Forkel"
                    },
                    {
                        "name": "Mattie Fellows"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12272v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12272v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14088v1",
                "updated": "2025-07-18T17:13:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    13,
                    21,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:13:21Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    13,
                    21,
                    4,
                    199,
                    0
                ],
                "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time\n  Human-AI Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time\n  Human-AI Collaboration"
                },
                "summary": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system."
                },
                "authors": [
                    {
                        "name": "Xiyun Li"
                    },
                    {
                        "name": "Yining Ding"
                    },
                    {
                        "name": "Yuhua Jiang"
                    },
                    {
                        "name": "Yunlong Zhao"
                    },
                    {
                        "name": "Runpeng Xie"
                    },
                    {
                        "name": "Shuang Xu"
                    },
                    {
                        "name": "Yuanhua Ni"
                    },
                    {
                        "name": "Yiqin Yang"
                    },
                    {
                        "name": "Bo Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Xu"
                },
                "author": "Bo Xu",
                "arxiv_journal_ref": "cogsci-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04617v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04617v3",
                "updated": "2025-07-18T17:06:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    6,
                    0,
                    4,
                    199,
                    0
                ],
                "published": "2024-09-06T21:00:57Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    21,
                    0,
                    57,
                    4,
                    250,
                    0
                ],
                "title": "Sparse Rewards Can Self-Train Dialogue Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Rewards Can Self-Train Dialogue Agents"
                },
                "summary": "Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)\nagents, especially in multi-turn dialogue tasks, have been primarily driven by\nsupervised fine-tuning and high-quality human feedback. However, as base LLM\nmodels continue to improve, acquiring meaningful human feedback has become\nincreasingly challenging and costly. In certain domains, base LLM agents may\neventually exceed human capabilities, making traditional feedback-driven\nmethods impractical. In this paper, we introduce a novel self-improvement\nparadigm that empowers LLM agents to autonomously enhance their performance\nwithout external human feedback. Our method, Juxtaposed Outcomes for Simulation\nHarvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward\nsimulation environment to extract ideal behaviors and further train the LLM on\nits own outputs. We present ToolWOZ, a sparse reward tool-calling simulation\nenvironment derived from MultiWOZ. We demonstrate that models trained with\nJOSH, both small and frontier, significantly improve tool-based interactions\nwhile preserving general model capabilities across diverse benchmarks. Our code\nand data are publicly available on GitHub at\nhttps://github.com/asappresearch/josh-llm-simulation-training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)\nagents, especially in multi-turn dialogue tasks, have been primarily driven by\nsupervised fine-tuning and high-quality human feedback. However, as base LLM\nmodels continue to improve, acquiring meaningful human feedback has become\nincreasingly challenging and costly. In certain domains, base LLM agents may\neventually exceed human capabilities, making traditional feedback-driven\nmethods impractical. In this paper, we introduce a novel self-improvement\nparadigm that empowers LLM agents to autonomously enhance their performance\nwithout external human feedback. Our method, Juxtaposed Outcomes for Simulation\nHarvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward\nsimulation environment to extract ideal behaviors and further train the LLM on\nits own outputs. We present ToolWOZ, a sparse reward tool-calling simulation\nenvironment derived from MultiWOZ. We demonstrate that models trained with\nJOSH, both small and frontier, significantly improve tool-based interactions\nwhile preserving general model capabilities across diverse benchmarks. Our code\nand data are publicly available on GitHub at\nhttps://github.com/asappresearch/josh-llm-simulation-training"
                },
                "authors": [
                    {
                        "name": "Barrett Martin Lattimer"
                    },
                    {
                        "name": "Varun Gangal"
                    },
                    {
                        "name": "Ryan McDonald"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "Accepted to ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04617v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04617v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14079v1",
                "updated": "2025-07-18T17:00:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    0,
                    27,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:00:27Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    0,
                    27,
                    4,
                    199,
                    0
                ],
                "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of\n  Heterogeneous Clinical Notes Across Hospital Visits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of\n  Heterogeneous Clinical Notes Across Hospital Visits"
                },
                "summary": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings."
                },
                "authors": [
                    {
                        "name": "Garapati Keerthana"
                    },
                    {
                        "name": "Manik Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Manik Gupta"
                },
                "author": "Manik Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14069v1",
                "updated": "2025-07-18T16:47:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    47,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:47:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    47,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "Edge Intelligence with Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Intelligence with Spiking Neural Networks"
                },
                "summary": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence."
                },
                "authors": [
                    {
                        "name": "Shuiguang Deng"
                    },
                    {
                        "name": "Di Yu"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Xin Du"
                    },
                    {
                        "name": "Linshan Jiang"
                    },
                    {
                        "name": "Xiaofan Zhao"
                    },
                    {
                        "name": "Wentao Tong"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Weijia Fang"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Gang Pan"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Albert Y. Zomaya"
                    }
                ],
                "author_detail": {
                    "name": "Albert Y. Zomaya"
                },
                "author": "Albert Y. Zomaya",
                "arxiv_comment": "This work has been submitted to Proceeding of IEEE for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12723v2",
                "updated": "2025-07-18T16:43:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    43,
                    12,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-19T05:25:29Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    25,
                    29,
                    0,
                    139,
                    0
                ],
                "title": "On-Policy Optimization with Group Equivalent Preference for\n  Multi-Programming Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Policy Optimization with Group Equivalent Preference for\n  Multi-Programming Language Understanding"
                },
                "summary": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages."
                },
                "authors": [
                    {
                        "name": "Haoyuan Wu"
                    },
                    {
                        "name": "Rui Ming"
                    },
                    {
                        "name": "Jilong Gao"
                    },
                    {
                        "name": "Hangyu Zhao"
                    },
                    {
                        "name": "Xueyi Chen"
                    },
                    {
                        "name": "Yikai Yang"
                    },
                    {
                        "name": "Haisheng Zheng"
                    },
                    {
                        "name": "Zhuolun He"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14049v1",
                "updated": "2025-07-18T16:15:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    15,
                    9,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:15:09Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    15,
                    9,
                    4,
                    199,
                    0
                ],
                "title": "EdgeVLA: Efficient Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeVLA: Efficient Vision-Language-Action Models"
                },
                "summary": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch."
                },
                "authors": [
                    {
                        "name": "Pawe≈Ç Budzianowski"
                    },
                    {
                        "name": "Wesley Maa"
                    },
                    {
                        "name": "Matthew Freed"
                    },
                    {
                        "name": "Jingxiang Mo"
                    },
                    {
                        "name": "Winston Hsiao"
                    },
                    {
                        "name": "Aaron Xie"
                    },
                    {
                        "name": "Tomasz M≈Çoduchowski"
                    },
                    {
                        "name": "Viraj Tipnis"
                    },
                    {
                        "name": "Benjamin Bolte"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Bolte"
                },
                "author": "Benjamin Bolte",
                "arxiv_journal_ref": "IROS-MoMA3 Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13394v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13394v2",
                "updated": "2025-07-18T16:14:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    14,
                    51,
                    4,
                    199,
                    0
                ],
                "published": "2024-10-17T09:45:32Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    9,
                    45,
                    32,
                    3,
                    291,
                    0
                ],
                "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs"
                },
                "summary": "Evaluating machine-generated text remains a significant challenge in NLP,\nespecially for non-English languages. Current methodologies, including\nautomated metrics, human assessments, and LLM-based evaluations, predominantly\nfocus on English, revealing a significant gap in multilingual evaluation\nframeworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an\nextensible framework that includes evaluator LLMs (Hercule) and a novel test\nset (Recon) specifically designed for multilingual evaluation. Our test set\nfeatures 500 human-annotated instructions spanning various task capabilities\nalong with human judgment scores across six languages. This would enable\nbenchmarking of general-purpose multilingual LLMs and facilitate\nmeta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a\ncross-lingual evaluation model that addresses the scarcity of reference answers\nin the target language by learning to assign scores to responses based on\neasily available reference answers in English. Our experiments demonstrate that\nHercule aligns more closely with human judgments compared to proprietary\nmodels, demonstrating the effectiveness of such cross-lingual evaluation in low\nresource scenarios. Further, it is also effective in zero-shot evaluation on\nunseen languages. This study is the first comprehensive examination of\ncross-lingual evaluation using LLMs, presenting a scalable and effective\napproach for multilingual assessment. All code, datasets, and models will be\npublicly available to enable further research in this important area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating machine-generated text remains a significant challenge in NLP,\nespecially for non-English languages. Current methodologies, including\nautomated metrics, human assessments, and LLM-based evaluations, predominantly\nfocus on English, revealing a significant gap in multilingual evaluation\nframeworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an\nextensible framework that includes evaluator LLMs (Hercule) and a novel test\nset (Recon) specifically designed for multilingual evaluation. Our test set\nfeatures 500 human-annotated instructions spanning various task capabilities\nalong with human judgment scores across six languages. This would enable\nbenchmarking of general-purpose multilingual LLMs and facilitate\nmeta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a\ncross-lingual evaluation model that addresses the scarcity of reference answers\nin the target language by learning to assign scores to responses based on\neasily available reference answers in English. Our experiments demonstrate that\nHercule aligns more closely with human judgments compared to proprietary\nmodels, demonstrating the effectiveness of such cross-lingual evaluation in low\nresource scenarios. Further, it is also effective in zero-shot evaluation on\nunseen languages. This study is the first comprehensive examination of\ncross-lingual evaluation using LLMs, presenting a scalable and effective\napproach for multilingual assessment. All code, datasets, and models will be\npublicly available to enable further research in this important area."
                },
                "authors": [
                    {
                        "name": "Sumanth Doddapaneni"
                    },
                    {
                        "name": "Mohammed Safi Ur Rahman Khan"
                    },
                    {
                        "name": "Dilip Venkatesh"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Anoop Kunchukuttan"
                    },
                    {
                        "name": "Mitesh M. Khapra"
                    }
                ],
                "author_detail": {
                    "name": "Mitesh M. Khapra"
                },
                "author": "Mitesh M. Khapra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13394v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13394v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14045v1",
                "updated": "2025-07-18T16:13:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    13,
                    35,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:13:35Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    13,
                    35,
                    4,
                    199,
                    0
                ],
                "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in\n  Benchmark Biomedical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in\n  Benchmark Biomedical Tasks"
                },
                "summary": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications."
                },
                "authors": [
                    {
                        "name": "Israt Jahan"
                    },
                    {
                        "name": "Md Tahmid Rahman Laskar"
                    },
                    {
                        "name": "Chun Peng"
                    },
                    {
                        "name": "Jimmy Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Huang"
                },
                "author": "Jimmy Huang",
                "arxiv_comment": "Accepted at Canadian AI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14041v1",
                "updated": "2025-07-18T16:11:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    11,
                    12,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:11:12Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    11,
                    12,
                    4,
                    199,
                    0
                ],
                "title": "Extreme value distribution for GRB prompt data -- How unexpected was the\n  BOAT event?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme value distribution for GRB prompt data -- How unexpected was the\n  BOAT event?"
                },
                "summary": "Gamma-Ray Bursts (GRBs) are known to be unpredictable in time and position. A\nfew (observationally) exceptional events have been observed, as GRB221009A that\nstands out for its fluence and peak flux, being orders of magnitude higher than\nwhat measured so far. Analyzing the observed fluence, peak flux or duration\ndistributions typically requires one to assume some scenarios, and the\nconsistency of the observed data with the predictions turns out to be an\nimportant model diagnostic. However, it is also of interest to model these\ndistributions using general statistical properties that do not rely on specific\nmodel assumptions, allowing one to derive inferences only based on the\nconsistency of the observed distributions with the hypothesis of one single\npopulation of events that generate them. We obtained fluences, peak fluxes and\ndurations from the catalogues of GRBs observed by the CGRO-BATSE and Fermi-GBM\ninstruments. We selected the extreme values in slots of equal duration and\nmodelled their distributions by the generalized extreme value (GEV) formalism.\nThe GEV distribution is a limit distribution naturally arising when the number\nof observations is large and is essentially independent of the phenomena\nproducing the observed data. The distributions of extreme values for fluences,\npeak fluxes and durations are consistent with being extracted from a single\npopulation of events but the fluence and peak flux recorded for GRB221009A\nconstitutes a striking exception. The probability to observe such an event,\nassuming it is a cosmological GRB, is low, with a median value of about one\nevent per millennium for the fluence and about one event per century for the\npeak flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma-Ray Bursts (GRBs) are known to be unpredictable in time and position. A\nfew (observationally) exceptional events have been observed, as GRB221009A that\nstands out for its fluence and peak flux, being orders of magnitude higher than\nwhat measured so far. Analyzing the observed fluence, peak flux or duration\ndistributions typically requires one to assume some scenarios, and the\nconsistency of the observed data with the predictions turns out to be an\nimportant model diagnostic. However, it is also of interest to model these\ndistributions using general statistical properties that do not rely on specific\nmodel assumptions, allowing one to derive inferences only based on the\nconsistency of the observed distributions with the hypothesis of one single\npopulation of events that generate them. We obtained fluences, peak fluxes and\ndurations from the catalogues of GRBs observed by the CGRO-BATSE and Fermi-GBM\ninstruments. We selected the extreme values in slots of equal duration and\nmodelled their distributions by the generalized extreme value (GEV) formalism.\nThe GEV distribution is a limit distribution naturally arising when the number\nof observations is large and is essentially independent of the phenomena\nproducing the observed data. The distributions of extreme values for fluences,\npeak fluxes and durations are consistent with being extracted from a single\npopulation of events but the fluence and peak flux recorded for GRB221009A\nconstitutes a striking exception. The probability to observe such an event,\nassuming it is a cosmological GRB, is low, with a median value of about one\nevent per millennium for the fluence and about one event per century for the\npeak flux."
                },
                "authors": [
                    {
                        "name": "Stefano Covino"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Covino"
                },
                "author": "Stefano Covino",
                "arxiv_comment": "A&A, in press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14035v1",
                "updated": "2025-07-18T16:07:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    7,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:07:36Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    7,
                    36,
                    4,
                    199,
                    0
                ],
                "title": "Toward Practical Fluid Antenna Systems: Co-Optimizing Hardware and\n  Software for Port Selection and Beamforming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Practical Fluid Antenna Systems: Co-Optimizing Hardware and\n  Software for Port Selection and Beamforming"
                },
                "summary": "This paper proposes a hardware-software co-design approach to efficiently\noptimize beamforming and port selection in fluid antenna systems (FASs). To\nbegin with, a fluid-antenna (FA)-enabled downlink multi-cell multiple-input\nmultiple-output (MIMO) network is modeled, and a weighted sum-rate (WSR)\nmaximization problem is formulated. Second, a method that integrates graph\nneural networks (GNNs) with random port selection (RPS) is proposed to jointly\noptimize beamforming and port selection, while also assessing the benefits and\nlimitations of random selection. Third, an instruction-driven deep learning\naccelerator based on a field-programmable gate array (FPGA) is developed to\nminimize inference latency. To further enhance efficiency, a scheduling\nalgorithm is introduced to reduce redundant computations and minimize the idle\ntime of computing cores. Simulation results demonstrate that the proposed\nGNN-RPS approach achieves competitive communication performance. Furthermore,\nexperimental evaluations indicate that the FPGA-based accelerator maintains low\nlatency while simultaneously executing beamforming inference for multiple port\nselections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a hardware-software co-design approach to efficiently\noptimize beamforming and port selection in fluid antenna systems (FASs). To\nbegin with, a fluid-antenna (FA)-enabled downlink multi-cell multiple-input\nmultiple-output (MIMO) network is modeled, and a weighted sum-rate (WSR)\nmaximization problem is formulated. Second, a method that integrates graph\nneural networks (GNNs) with random port selection (RPS) is proposed to jointly\noptimize beamforming and port selection, while also assessing the benefits and\nlimitations of random selection. Third, an instruction-driven deep learning\naccelerator based on a field-programmable gate array (FPGA) is developed to\nminimize inference latency. To further enhance efficiency, a scheduling\nalgorithm is introduced to reduce redundant computations and minimize the idle\ntime of computing cores. Simulation results demonstrate that the proposed\nGNN-RPS approach achieves competitive communication performance. Furthermore,\nexperimental evaluations indicate that the FPGA-based accelerator maintains low\nlatency while simultaneously executing beamforming inference for multiple port\nselections."
                },
                "authors": [
                    {
                        "name": "Sai Xu"
                    },
                    {
                        "name": "Kai-Kit Wong"
                    },
                    {
                        "name": "Yanan Du"
                    },
                    {
                        "name": "Hanjiang Hong"
                    },
                    {
                        "name": "Chan-Byoung Chae"
                    },
                    {
                        "name": "Baiyang Liu"
                    },
                    {
                        "name": "Kin-Fai Tong"
                    }
                ],
                "author_detail": {
                    "name": "Kin-Fai Tong"
                },
                "author": "Kin-Fai Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14034v1",
                "updated": "2025-07-18T16:06:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    6,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:06:03Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    6,
                    3,
                    4,
                    199,
                    0
                ],
                "title": "Architecting Human-AI Cocreation for Technical Services -- Interaction\n  Modes and Contingency Factors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Human-AI Cocreation for Technical Services -- Interaction\n  Modes and Contingency Factors"
                },
                "summary": "Agentic AI systems, powered by Large Language Models (LLMs), offer\ntransformative potential for value co-creation in technical services. However,\npersistent challenges like hallucinations and operational brittleness limit\ntheir autonomous use, creating a critical need for robust frameworks to guide\nhuman-AI collaboration. Drawing on established Human-AI teaming research and\nanalogies from fields like autonomous driving, this paper develops a structured\ntaxonomy of human-agent interaction. Based on case study research within\ntechnical support platforms, we propose a six-mode taxonomy that organizes\ncollaboration across a spectrum of AI autonomy. This spectrum is anchored by\nthe Human-Out-of-the-Loop (HOOTL) model for full automation and the\nHuman-Augmented Model (HAM) for passive AI assistance. Between these poles, the\nframework specifies four distinct intermediate structures. These include the\nHuman-in-Command (HIC) model, where AI proposals re-quire mandatory human\napproval, and the Human-in-the-Process (HITP) model for structured work-flows\nwith deterministic human tasks. The taxonomy further delineates the\nHuman-in-the-Loop (HITL) model, which facilitates agent-initiated escalation\nupon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables\ndiscretionary human oversight of an autonomous AI. The primary contribution of\nthis work is a comprehensive framework that connects this taxonomy to key\ncontingency factors -- such as task complexity, operational risk, and system\nreliability -- and their corresponding conceptual architectures. By providing a\nsystematic method for selecting and designing an appropriate level of human\noversight, our framework offers practitioners a crucial tool to navigate the\ntrade-offs between automation and control, thereby fostering the development of\nsafer, more effective, and context-aware technical service systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems, powered by Large Language Models (LLMs), offer\ntransformative potential for value co-creation in technical services. However,\npersistent challenges like hallucinations and operational brittleness limit\ntheir autonomous use, creating a critical need for robust frameworks to guide\nhuman-AI collaboration. Drawing on established Human-AI teaming research and\nanalogies from fields like autonomous driving, this paper develops a structured\ntaxonomy of human-agent interaction. Based on case study research within\ntechnical support platforms, we propose a six-mode taxonomy that organizes\ncollaboration across a spectrum of AI autonomy. This spectrum is anchored by\nthe Human-Out-of-the-Loop (HOOTL) model for full automation and the\nHuman-Augmented Model (HAM) for passive AI assistance. Between these poles, the\nframework specifies four distinct intermediate structures. These include the\nHuman-in-Command (HIC) model, where AI proposals re-quire mandatory human\napproval, and the Human-in-the-Process (HITP) model for structured work-flows\nwith deterministic human tasks. The taxonomy further delineates the\nHuman-in-the-Loop (HITL) model, which facilitates agent-initiated escalation\nupon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables\ndiscretionary human oversight of an autonomous AI. The primary contribution of\nthis work is a comprehensive framework that connects this taxonomy to key\ncontingency factors -- such as task complexity, operational risk, and system\nreliability -- and their corresponding conceptual architectures. By providing a\nsystematic method for selecting and designing an appropriate level of human\noversight, our framework offers practitioners a crucial tool to navigate the\ntrade-offs between automation and control, thereby fostering the development of\nsafer, more effective, and context-aware technical service systems."
                },
                "authors": [
                    {
                        "name": "Jochen Wulf"
                    },
                    {
                        "name": "Jurg Meierhofer"
                    },
                    {
                        "name": "Frank Hannich"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hannich"
                },
                "author": "Frank Hannich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14032v1",
                "updated": "2025-07-18T16:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    0,
                    11,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    0,
                    11,
                    4,
                    199,
                    0
                ],
                "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language\n  Models"
                },
                "summary": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale."
                },
                "authors": [
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Erika Barcelos"
                    },
                    {
                        "name": "Roger French"
                    },
                    {
                        "name": "Yinghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yinghui Wu"
                },
                "author": "Yinghui Wu",
                "arxiv_comment": "Accepted to the 24th International Semantic Web Conference Research\n  Track (ISWC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09567v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09567v5",
                "updated": "2025-07-18T15:57:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    57,
                    54,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-12T17:35:03Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    35,
                    3,
                    2,
                    71,
                    0
                ],
                "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning\n  Large Language Models"
                },
                "summary": "Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"inference-time scaling.\" This survey seeks\nto fill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and inference-time scaling,\noffering insights into how these processes manifest in practice. (4) Finally,\nwe identify significant research gaps and highlight promising future\ndirections, including the integration of multi-modal reasoning, efficiency\nimprovements, and enhanced knowledge frameworks. By providing a structured\noverview, this survey aims to inspire future research and further the\ndevelopment of logical reasoning in artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"inference-time scaling.\" This survey seeks\nto fill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and inference-time scaling,\noffering insights into how these processes manifest in practice. (4) Finally,\nwe identify significant research gaps and highlight promising future\ndirections, including the integration of multi-modal reasoning, efficiency\nimprovements, and enhanced knowledge frameworks. By providing a structured\noverview, this survey aims to inspire future research and further the\ndevelopment of logical reasoning in artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Jiannan Guan"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Te Gao"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Paper list and Github tutorial are available at\n  https://github.com/LightChen233/Awesome-Long-Chain-of-Thought-Reasoning.\n  Update 250+ New Reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09567v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09567v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14031v1",
                "updated": "2025-07-18T15:57:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    57,
                    53,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T15:57:53Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    57,
                    53,
                    4,
                    199,
                    0
                ],
                "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest\n  Electrical Impedance Tomography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest\n  Electrical Impedance Tomography"
                },
                "summary": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise."
                },
                "authors": [
                    {
                        "name": "Hao Fang"
                    },
                    {
                        "name": "Sihao Teng"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Siyi Yuan"
                    },
                    {
                        "name": "Huaiwu He"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Yunjie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yunjie Yang"
                },
                "author": "Yunjie Yang",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14019v1",
                "updated": "2025-07-18T15:35:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    35,
                    17,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T15:35:17Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    35,
                    17,
                    4,
                    199,
                    0
                ],
                "title": "On the importance of tail assumptions in climate extreme event\n  attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the importance of tail assumptions in climate extreme event\n  attribution"
                },
                "summary": "Extreme weather events are becoming more frequent and intense, posing serious\nthreats to human life, biodiversity, and ecosystems. A key objective of extreme\nevent attribution (EEA) is to assess whether and to what extent anthropogenic\nclimate change influences such events. Central to EEA is the accurate\nstatistical characterization of atmospheric extremes, which are inherently\nmultivariate or spatial due to their measurement over high-dimensional grids.\nWithin the counterfactual causal inference framework of Pearl, we evaluate how\ntail assumptions affect attribution conclusions by comparing three multivariate\nmodeling approaches for estimating causation metrics. These include: (i) the\nmultivariate generalized Pareto distribution, which imposes an invariant tail\ndependence structure; (ii) the factor copula model of Castro-Camilo and Huser\n(2020), which offers flexible subasymptotic behavior; and (iii) the model of\nHuser and Wadsworth (2019), which smoothly transitions between different forms\nof extremal dependence. We assess the implications of these modeling choices in\nboth simulated scenarios (under varying forms of model misspecification) and\nreal data applications, using weekly winter maxima over Europe from the\nM\\'et\\'eo-France CNRM model and daily precipitation from the ACCESS-CM2 model\nover the U.S. Our findings highlight that tail assumptions critically shape\ncausality metrics in EEA. Misspecification of the extremal dependence structure\ncan lead to substantially different and potentially misleading attribution\nconclusions, underscoring the need for careful model selection and evaluation\nwhen quantifying the influence of climate change on extreme events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme weather events are becoming more frequent and intense, posing serious\nthreats to human life, biodiversity, and ecosystems. A key objective of extreme\nevent attribution (EEA) is to assess whether and to what extent anthropogenic\nclimate change influences such events. Central to EEA is the accurate\nstatistical characterization of atmospheric extremes, which are inherently\nmultivariate or spatial due to their measurement over high-dimensional grids.\nWithin the counterfactual causal inference framework of Pearl, we evaluate how\ntail assumptions affect attribution conclusions by comparing three multivariate\nmodeling approaches for estimating causation metrics. These include: (i) the\nmultivariate generalized Pareto distribution, which imposes an invariant tail\ndependence structure; (ii) the factor copula model of Castro-Camilo and Huser\n(2020), which offers flexible subasymptotic behavior; and (iii) the model of\nHuser and Wadsworth (2019), which smoothly transitions between different forms\nof extremal dependence. We assess the implications of these modeling choices in\nboth simulated scenarios (under varying forms of model misspecification) and\nreal data applications, using weekly winter maxima over Europe from the\nM\\'et\\'eo-France CNRM model and daily precipitation from the ACCESS-CM2 model\nover the U.S. Our findings highlight that tail assumptions critically shape\ncausality metrics in EEA. Misspecification of the extremal dependence structure\ncan lead to substantially different and potentially misleading attribution\nconclusions, underscoring the need for careful model selection and evaluation\nwhen quantifying the influence of climate change on extreme events."
                },
                "authors": [
                    {
                        "name": "Mengran Li"
                    },
                    {
                        "name": "Daniela Castro-Camilo"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Castro-Camilo"
                },
                "author": "Daniela Castro-Camilo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G32, 62H10, 62P12, 62H20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14017v1",
                "updated": "2025-07-18T15:31:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    31,
                    16,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T15:31:16Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    31,
                    16,
                    4,
                    199,
                    0
                ],
                "title": "Efficient Temporal Tokenization for Mobility Prediction with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Temporal Tokenization for Mobility Prediction with Large\n  Language Models"
                },
                "summary": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haoyu He"
                    },
                    {
                        "name": "Haozheng Luo"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Qi R. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qi R. Wang"
                },
                "author": "Qi R. Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14000v1",
                "updated": "2025-07-18T15:14:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    14,
                    56,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T15:14:56Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    14,
                    56,
                    4,
                    199,
                    0
                ],
                "title": "Photonic Fabric Platform for AI Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic Fabric Platform for AI Accelerators"
                },
                "summary": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute."
                },
                "authors": [
                    {
                        "name": "Jing Ding"
                    },
                    {
                        "name": "Trung Diep"
                    }
                ],
                "author_detail": {
                    "name": "Trung Diep"
                },
                "author": "Trung Diep",
                "arxiv_comment": "12 pages, 14 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20383v2",
                "updated": "2025-07-18T14:58:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    58,
                    15,
                    4,
                    199,
                    0
                ],
                "published": "2024-12-29T07:11:44Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    7,
                    11,
                    44,
                    6,
                    364,
                    0
                ],
                "title": "Progressively Exploring and Exploiting Cost-Free Data to Break\n  Fine-Grained Classification Barriers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressively Exploring and Exploiting Cost-Free Data to Break\n  Fine-Grained Classification Barriers"
                },
                "summary": "Current fine-grained classification research primarily focuses on\nfine-grained feature learning. However, in real-world scenarios, fine-grained\ndata annotation is challenging, and the features and semantics are highly\ndiverse and frequently changing. These issues create inherent barriers between\ntraditional experimental settings and real-world applications, limiting the\neffectiveness of conventional fine-grained classification methods. Although\nsome recent studies have provided potential solutions to these issues, most of\nthem still rely on limited supervised information and thus fail to offer\neffective solutions. In this paper, based on theoretical analysis, we propose a\nnovel learning paradigm to break the barriers in fine-grained classification.\nThis paradigm enables the model to progressively learn during inference,\nthereby leveraging cost-free data to more accurately represent fine-grained\ncategories and adapt to dynamic semantic changes. On this basis, an efficient\nEXPloring and EXPloiting strategy and method (EXP2) is designed. Thereinto,\nuseful inference data samples are explored according to class representations\nand exploited to optimize classifiers. Experimental results demonstrate the\ngeneral effectiveness of our method, providing guidance for future in-depth\nunderstanding and exploration of real-world fine-grained classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current fine-grained classification research primarily focuses on\nfine-grained feature learning. However, in real-world scenarios, fine-grained\ndata annotation is challenging, and the features and semantics are highly\ndiverse and frequently changing. These issues create inherent barriers between\ntraditional experimental settings and real-world applications, limiting the\neffectiveness of conventional fine-grained classification methods. Although\nsome recent studies have provided potential solutions to these issues, most of\nthem still rely on limited supervised information and thus fail to offer\neffective solutions. In this paper, based on theoretical analysis, we propose a\nnovel learning paradigm to break the barriers in fine-grained classification.\nThis paradigm enables the model to progressively learn during inference,\nthereby leveraging cost-free data to more accurately represent fine-grained\ncategories and adapt to dynamic semantic changes. On this basis, an efficient\nEXPloring and EXPloiting strategy and method (EXP2) is designed. Thereinto,\nuseful inference data samples are explored according to class representations\nand exploited to optimize classifiers. Experimental results demonstrate the\ngeneral effectiveness of our method, providing guidance for future in-depth\nunderstanding and exploration of real-world fine-grained classification."
                },
                "authors": [
                    {
                        "name": "Li-Jun Zhao"
                    },
                    {
                        "name": "Zhen-Duo Chen"
                    },
                    {
                        "name": "Zhi-Yuan Xue"
                    },
                    {
                        "name": "Xin Luo"
                    },
                    {
                        "name": "Xin-Shun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xin-Shun Xu"
                },
                "author": "Xin-Shun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13991v1",
                "updated": "2025-07-18T14:58:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    58,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:58:03Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    58,
                    3,
                    4,
                    199,
                    0
                ],
                "title": "Inflated hot Jupiters: inferring average atmospheric velocity via Ohmic\n  models coupled with internal dynamo evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inflated hot Jupiters: inferring average atmospheric velocity via Ohmic\n  models coupled with internal dynamo evolution"
                },
                "summary": "The inflated radii observed in hundreds of hot Jupiters (HJ) represent a\nlong-standing open issue. In this study, we quantitatively investigate this\nphenomenon within the framework of Ohmic dissipation arising from magnetic\ninduction in the atmosphere, one of the most promising mechanisms for\nexplaining the radius anomaly. We simulate the evolution of irradiated giant\nplanets with {\\tt MESA}, spanning the observed range of masses and equilibrium\ntemperatures, incorporating an internal source of Ohmic dissipation that\nextends to deep layers of the envelope. We infer average atmospheric wind\nintensities, averaged in the region $p\\lesssim 10$ bar, in the range 0.01-1\nkm/s in order to reproduce the range of observed radii, decreasing roughly\nlinearly with planetary mass, and much more steeply with equilibrium\ntemperature. This is consistent with the expected effects of magnetic drag from\nthe induced field, which is higher for more intense irradiation, via\nconductivity, and for larger masses, which have higher dynamo fields. Due to\nthe evolution of the dynamo field and the proportionality of the induced\ncurrents on it, the Ohmic efficiency typically decreases by at least one order\nof magnitude from 0.1 to 10 Gyr, at contrast with the common assumption of a\nconstant-in-time value. Notably, the extent of the main convective region, and\nthe associated heat flux supporting the dynamo, is reduced in the presence of\nstrong Ohmic dissipation, which in turn depends on the dynamo field strength,\ngenerating a non-trivial coupling of the latter with the atmospheric induction,\npotentially leading to an oscillatory behaviour of the field strength. These\nfindings remain generally valid even when accounting for a long-term increase\nin the main-sequence host star luminosity, although this case can more readily\nlead to HJ re-inflation, consistent with previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inflated radii observed in hundreds of hot Jupiters (HJ) represent a\nlong-standing open issue. In this study, we quantitatively investigate this\nphenomenon within the framework of Ohmic dissipation arising from magnetic\ninduction in the atmosphere, one of the most promising mechanisms for\nexplaining the radius anomaly. We simulate the evolution of irradiated giant\nplanets with {\\tt MESA}, spanning the observed range of masses and equilibrium\ntemperatures, incorporating an internal source of Ohmic dissipation that\nextends to deep layers of the envelope. We infer average atmospheric wind\nintensities, averaged in the region $p\\lesssim 10$ bar, in the range 0.01-1\nkm/s in order to reproduce the range of observed radii, decreasing roughly\nlinearly with planetary mass, and much more steeply with equilibrium\ntemperature. This is consistent with the expected effects of magnetic drag from\nthe induced field, which is higher for more intense irradiation, via\nconductivity, and for larger masses, which have higher dynamo fields. Due to\nthe evolution of the dynamo field and the proportionality of the induced\ncurrents on it, the Ohmic efficiency typically decreases by at least one order\nof magnitude from 0.1 to 10 Gyr, at contrast with the common assumption of a\nconstant-in-time value. Notably, the extent of the main convective region, and\nthe associated heat flux supporting the dynamo, is reduced in the presence of\nstrong Ohmic dissipation, which in turn depends on the dynamo field strength,\ngenerating a non-trivial coupling of the latter with the atmospheric induction,\npotentially leading to an oscillatory behaviour of the field strength. These\nfindings remain generally valid even when accounting for a long-term increase\nin the main-sequence host star luminosity, although this case can more readily\nlead to HJ re-inflation, consistent with previous studies."
                },
                "authors": [
                    {
                        "name": "Daniele Vigan√≤"
                    },
                    {
                        "name": "Soumya Sengupta"
                    },
                    {
                        "name": "Cl√†udia Soriano-Guerrero"
                    },
                    {
                        "name": "Rosalba Perna"
                    },
                    {
                        "name": "Albert Elias-L√≥pez"
                    },
                    {
                        "name": "Sandeep Kumar"
                    },
                    {
                        "name": "Taner Akg√ºn"
                    }
                ],
                "author_detail": {
                    "name": "Taner Akg√ºn"
                },
                "author": "Taner Akg√ºn",
                "arxiv_comment": "Accepted for publication in A&A main journal. 22 pages, 14 figures, 1\n  table. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09046v2",
                "updated": "2025-07-18T14:52:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    52,
                    19,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-10T17:59:21Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    21,
                    1,
                    161,
                    0
                ],
                "title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual\n  Backpropagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual\n  Backpropagation"
                },
                "summary": "Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework."
                },
                "authors": [
                    {
                        "name": "Xiaowen Ma"
                    },
                    {
                        "name": "Chenyang Lin"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13985v1",
                "updated": "2025-07-18T14:45:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    45,
                    54,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:45:54Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    45,
                    54,
                    4,
                    199,
                    0
                ],
                "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation"
                },
                "summary": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io."
                },
                "authors": [
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yuli Tian"
                    },
                    {
                        "name": "Kun Lan"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Lin Wang"
                    },
                    {
                        "name": "Pan Hui"
                    },
                    {
                        "name": "Peng Yuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peng Yuan Zhou"
                },
                "author": "Peng Yuan Zhou",
                "arxiv_comment": "Extended version of ECCV 2024 paper \"DreamScene\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10310v2",
                "updated": "2025-07-18T14:44:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    44,
                    19,
                    4,
                    199,
                    0
                ],
                "published": "2024-02-15T20:21:40Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    20,
                    21,
                    40,
                    3,
                    46,
                    0
                ],
                "title": "Interpretable Imitation Learning via Generative Adversarial STL\n  Inference and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Imitation Learning via Generative Adversarial STL\n  Inference and Control"
                },
                "summary": "Imitation learning methods have demonstrated considerable success in teaching\nautonomous systems complex tasks through expert demonstrations. However, a\nlimitation of these methods is their lack of interpretability, particularly in\nunderstanding the specific task the learning agent aims to accomplish. In this\npaper, we propose a novel imitation learning method that combines Signal\nTemporal Logic (STL) inference and control synthesis, enabling the explicit\nrepresentation of the task as an STL formula. This approach not only provides a\nclear understanding of the task but also supports the integration of human\nknowledge and allows for adaptation to out-of-distribution scenarios by\nmanually adjusting the STL formulas and fine-tuning the policy. We employ a\nGenerative Adversarial Network (GAN)-inspired approach to train both the\ninference and policy networks, effectively narrowing the gap between expert and\nlearned policies. The efficiency of our algorithm is demonstrated through\nsimulations, showcasing its practical applicability and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning methods have demonstrated considerable success in teaching\nautonomous systems complex tasks through expert demonstrations. However, a\nlimitation of these methods is their lack of interpretability, particularly in\nunderstanding the specific task the learning agent aims to accomplish. In this\npaper, we propose a novel imitation learning method that combines Signal\nTemporal Logic (STL) inference and control synthesis, enabling the explicit\nrepresentation of the task as an STL formula. This approach not only provides a\nclear understanding of the task but also supports the integration of human\nknowledge and allows for adaptation to out-of-distribution scenarios by\nmanually adjusting the STL formulas and fine-tuning the policy. We employ a\nGenerative Adversarial Network (GAN)-inspired approach to train both the\ninference and policy networks, effectively narrowing the gap between expert and\nlearned policies. The efficiency of our algorithm is demonstrated through\nsimulations, showcasing its practical applicability and adaptability."
                },
                "authors": [
                    {
                        "name": "Wenliang Liu"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Erfan Aasi"
                    },
                    {
                        "name": "Daniela Rus"
                    },
                    {
                        "name": "Roberto Tron"
                    },
                    {
                        "name": "Calin Belta"
                    }
                ],
                "author_detail": {
                    "name": "Calin Belta"
                },
                "author": "Calin Belta",
                "arxiv_comment": "Published at NeuS 2025 (International Conference on Neuro-symbolic\n  Systems)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13970v1",
                "updated": "2025-07-18T14:32:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    32,
                    45,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:32:45Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    32,
                    45,
                    4,
                    199,
                    0
                ],
                "title": "A segmented robot grasping perception neural network for edge AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A segmented robot grasping perception neural network for edge AI"
                },
                "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation."
                },
                "authors": [
                    {
                        "name": "Casper Br√∂cheler"
                    },
                    {
                        "name": "Thomas Vroom"
                    },
                    {
                        "name": "Derrick Timmermans"
                    },
                    {
                        "name": "Alan van den Akker"
                    },
                    {
                        "name": "Guangzhi Tang"
                    },
                    {
                        "name": "Charalampos S. Kouzinopoulos"
                    },
                    {
                        "name": "Rico M√∂ckel"
                    }
                ],
                "author_detail": {
                    "name": "Rico M√∂ckel"
                },
                "author": "Rico M√∂ckel",
                "arxiv_comment": "Accepted by SMC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.9; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13965v1",
                "updated": "2025-07-18T14:28:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    28,
                    11,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:28:11Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    28,
                    11,
                    4,
                    199,
                    0
                ],
                "title": "A regression-based approach for bidirectional proximal causal inference\n  in the presence of unmeasured confounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A regression-based approach for bidirectional proximal causal inference\n  in the presence of unmeasured confounding"
                },
                "summary": "Proxy variables are commonly used in causal inference when unmeasured\nconfounding exists. While most existing proximal methods assume a\nunidirectional causal relationship between two primary variables, many social\nand biological systems exhibit complex feedback mechanisms that imply\nbidirectional causality. In this paper, using regression-based models, we\nextend the proximal framework to identify bidirectional causal effects in the\npresence of unmeasured confounding. We establish the identification of\nbidirectional causal effects and develop a sensitivity analysis method for\nviolations of the proxy structural conditions. Building on this identification\nresult, we derive bidirectional two-stage least squares estimators that are\nconsistent and asymptotically normal under standard regularity conditions.\nSimulation studies demonstrate that our approach delivers unbiased causal\neffect estimates and outperforms some standard methods. The simulation results\nalso confirm the reliability of the sensitivity analysis procedure. Applying\nour methodology to a state-level panel dataset from 1985 to 2014 in the United\nStates, we examine the bidirectional causal effects between abortion rates and\nmurder rates. The analysis reveals a consistent negative effect of abortion\nrates on murder rates, while also detecting a potential reciprocal effect from\nmurder rates to abortion rates that conventional unidirectional analyses have\nnot considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proxy variables are commonly used in causal inference when unmeasured\nconfounding exists. While most existing proximal methods assume a\nunidirectional causal relationship between two primary variables, many social\nand biological systems exhibit complex feedback mechanisms that imply\nbidirectional causality. In this paper, using regression-based models, we\nextend the proximal framework to identify bidirectional causal effects in the\npresence of unmeasured confounding. We establish the identification of\nbidirectional causal effects and develop a sensitivity analysis method for\nviolations of the proxy structural conditions. Building on this identification\nresult, we derive bidirectional two-stage least squares estimators that are\nconsistent and asymptotically normal under standard regularity conditions.\nSimulation studies demonstrate that our approach delivers unbiased causal\neffect estimates and outperforms some standard methods. The simulation results\nalso confirm the reliability of the sensitivity analysis procedure. Applying\nour methodology to a state-level panel dataset from 1985 to 2014 in the United\nStates, we examine the bidirectional causal effects between abortion rates and\nmurder rates. The analysis reveals a consistent negative effect of abortion\nrates on murder rates, while also detecting a potential reciprocal effect from\nmurder rates to abortion rates that conventional unidirectional analyses have\nnot considered."
                },
                "authors": [
                    {
                        "name": "Jiaqi Min"
                    },
                    {
                        "name": "Xueyue Zhang"
                    },
                    {
                        "name": "Shanshan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Shanshan Luo"
                },
                "author": "Shanshan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13957v1",
                "updated": "2025-07-18T14:22:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    22,
                    5,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:22:05Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    22,
                    5,
                    4,
                    199,
                    0
                ],
                "title": "DUALRec: A Hybrid Sequential and Language Model Framework for\n  Context-Aware Movie Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DUALRec: A Hybrid Sequential and Language Model Framework for\n  Context-Aware Movie Recommendation"
                },
                "summary": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders."
                },
                "authors": [
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Raoul Grasman"
                    }
                ],
                "author_detail": {
                    "name": "Raoul Grasman"
                },
                "author": "Raoul Grasman",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68T50, 62M45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.6; H.3.4; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13956v1",
                "updated": "2025-07-18T14:21:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    21,
                    24,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:21:24Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    21,
                    24,
                    4,
                    199,
                    0
                ],
                "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction"
                },
                "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis."
                },
                "authors": [
                    {
                        "name": "Yutao Jin"
                    },
                    {
                        "name": "Haowen Xiao"
                    },
                    {
                        "name": "Jielei Chu"
                    },
                    {
                        "name": "Fengmao Lv"
                    },
                    {
                        "name": "Yuxiao Li"
                    },
                    {
                        "name": "Tianrui Li"
                    }
                ],
                "author_detail": {
                    "name": "Tianrui Li"
                },
                "author": "Tianrui Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13952v1",
                "updated": "2025-07-18T14:20:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    20,
                    54,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:20:54Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    20,
                    54,
                    4,
                    199,
                    0
                ],
                "title": "Estimating Cognitive Effort from Functional Near-Infrared Spectroscopy\n  (fNIRS) Signals using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Cognitive Effort from Functional Near-Infrared Spectroscopy\n  (fNIRS) Signals using Machine Learning"
                },
                "summary": "The estimation of cognitive effort could potentially help educators to modify\nmaterial to enhance learning effectiveness and student engagement. Where\ncognitive load refers how much work the brain is doing while someone is\nlearning or doing a task cognitive effort consider both load and behavioral\nperformance. Cognitive effort can be captured by measuring oxygen flow and\nbehavioral performance during a task. This study infers cognitive effort\nmetrics using machine learning models based on oxygenated hemoglobin collected\nby using functional near-infrared spectroscopy from the prefrontal cortex\nduring an educational gameplay. In our study, sixteen participants responded to\nsixteen questions in an in-house Unity-based educational game. The quiz was\ndivided into two sessions, each session consisting of two task segments. We\nextracted temporal statistical and functional connectivity features from\ncollected oxygenated hemoglobin and analyzed their correlation with quiz\nperformance. We trained multiple machine learning models to predict quiz\nperformance from oxygenated hemoglobin features and achieved accuracies ranging\nfrom 58\\% to 67\\% accuracy. These predictions were used to calculate cognitive\neffort via relative neural involvement and efficiency, which consider both\nbrain activation and behavioral performance. Although quiz score predictions\nachieved moderate accuracy, the derived relative neural efficiency and\ninvolvement values remained robust. Since both metrics are based on the\nrelative positions of standardized brain activation and performance scores,\neven small misclassifications in predicted scores preserved the overall\ncognitive effort trends observed during gameplay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The estimation of cognitive effort could potentially help educators to modify\nmaterial to enhance learning effectiveness and student engagement. Where\ncognitive load refers how much work the brain is doing while someone is\nlearning or doing a task cognitive effort consider both load and behavioral\nperformance. Cognitive effort can be captured by measuring oxygen flow and\nbehavioral performance during a task. This study infers cognitive effort\nmetrics using machine learning models based on oxygenated hemoglobin collected\nby using functional near-infrared spectroscopy from the prefrontal cortex\nduring an educational gameplay. In our study, sixteen participants responded to\nsixteen questions in an in-house Unity-based educational game. The quiz was\ndivided into two sessions, each session consisting of two task segments. We\nextracted temporal statistical and functional connectivity features from\ncollected oxygenated hemoglobin and analyzed their correlation with quiz\nperformance. We trained multiple machine learning models to predict quiz\nperformance from oxygenated hemoglobin features and achieved accuracies ranging\nfrom 58\\% to 67\\% accuracy. These predictions were used to calculate cognitive\neffort via relative neural involvement and efficiency, which consider both\nbrain activation and behavioral performance. Although quiz score predictions\nachieved moderate accuracy, the derived relative neural efficiency and\ninvolvement values remained robust. Since both metrics are based on the\nrelative positions of standardized brain activation and performance scores,\neven small misclassifications in predicted scores preserved the overall\ncognitive effort trends observed during gameplay."
                },
                "authors": [
                    {
                        "name": "Shayla Sharmin"
                    },
                    {
                        "name": "Roghayeh Leila Barmaki"
                    }
                ],
                "author_detail": {
                    "name": "Roghayeh Leila Barmaki"
                },
                "author": "Roghayeh Leila Barmaki",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.13883",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13949v1",
                "updated": "2025-07-18T14:18:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    18,
                    18,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:18:18Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    18,
                    18,
                    4,
                    199,
                    0
                ],
                "title": "Exploiting Primacy Effect To Improve Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Primacy Effect To Improve Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications."
                },
                "authors": [
                    {
                        "name": "Bianca Raimondi"
                    },
                    {
                        "name": "Maurizio Gabbrielli"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Gabbrielli"
                },
                "author": "Maurizio Gabbrielli",
                "arxiv_comment": "Accepted by RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13933v1",
                "updated": "2025-07-18T14:09:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    9,
                    4,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:09:04Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    9,
                    4,
                    4,
                    199,
                    0
                ],
                "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preprint: Did I Just Browse A Website Written by LLMs?"
                },
                "summary": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem."
                },
                "authors": [
                    {
                        "name": "Sichang \"Steven\" He"
                    },
                    {
                        "name": "Ramesh Govindan"
                    },
                    {
                        "name": "Harsha V. Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Harsha V. Madhyastha"
                },
                "author": "Harsha V. Madhyastha",
                "arxiv_comment": "In submission. 2 pages. 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13920v1",
                "updated": "2025-07-18T13:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    50,
                    57,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T13:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    50,
                    57,
                    4,
                    199,
                    0
                ],
                "title": "Reframing attention as a reinforcement learning problem for causal\n  discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reframing attention as a reinforcement learning problem for causal\n  discovery"
                },
                "summary": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes."
                },
                "authors": [
                    {
                        "name": "Turan Orujlu"
                    },
                    {
                        "name": "Christian Gumbsch"
                    },
                    {
                        "name": "Martin V. Butz"
                    },
                    {
                        "name": "Charley M Wu"
                    }
                ],
                "author_detail": {
                    "name": "Charley M Wu"
                },
                "author": "Charley M Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13919v1",
                "updated": "2025-07-18T13:50:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    50,
                    9,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T13:50:09Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    50,
                    9,
                    4,
                    199,
                    0
                ],
                "title": "The Levers of Political Persuasion with Conversational AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Levers of Political Persuasion with Conversational AI"
                },
                "summary": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy."
                },
                "authors": [
                    {
                        "name": "Kobi Hackenburg"
                    },
                    {
                        "name": "Ben M. Tappin"
                    },
                    {
                        "name": "Luke Hewitt"
                    },
                    {
                        "name": "Ed Saunders"
                    },
                    {
                        "name": "Sid Black"
                    },
                    {
                        "name": "Hause Lin"
                    },
                    {
                        "name": "Catherine Fist"
                    },
                    {
                        "name": "Helen Margetts"
                    },
                    {
                        "name": "David G. Rand"
                    },
                    {
                        "name": "Christopher Summerfield"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Summerfield"
                },
                "author": "Christopher Summerfield",
                "arxiv_comment": "19 pages, 4 figures. Our supplementary materials file can be found at\n  https://github.com/kobihackenburg/scaling-conversational-AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13918v1",
                "updated": "2025-07-18T13:49:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    49,
                    50,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T13:49:50Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    49,
                    50,
                    4,
                    199,
                    0
                ],
                "title": "Confidence Intervals for Random Forest Permutation Importance with\n  Missing Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Intervals for Random Forest Permutation Importance with\n  Missing Data"
                },
                "summary": "Random Forests are renowned for their predictive accuracy, but valid\ninference, particularly about permutation-based feature importances, remains\nchallenging. Existing methods, such as the confidence intervals (CIs) from\nIshwaran et al. (2019), are promising but assume complete feature observation.\nHowever, real-world data often contains missing values. In this paper, we\ninvestigate how common imputation techniques affect the validity of Random\nForest permutation-importance CIs when data are incomplete. Through an\nextensive simulation and real-world benchmark study, we compare\nstate-of-the-art imputation methods across various missing-data mechanisms and\nmissing rates. Our results show that single-imputation strategies lead to low\nCI coverage. As a remedy, we adapt Rubin's rule to aggregate feature-importance\nestimates and their variances over several imputed datasets and account for\nimputation uncertainty. Our numerical results indicate that the adjusted CIs\nachieve better nominal coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Forests are renowned for their predictive accuracy, but valid\ninference, particularly about permutation-based feature importances, remains\nchallenging. Existing methods, such as the confidence intervals (CIs) from\nIshwaran et al. (2019), are promising but assume complete feature observation.\nHowever, real-world data often contains missing values. In this paper, we\ninvestigate how common imputation techniques affect the validity of Random\nForest permutation-importance CIs when data are incomplete. Through an\nextensive simulation and real-world benchmark study, we compare\nstate-of-the-art imputation methods across various missing-data mechanisms and\nmissing rates. Our results show that single-imputation strategies lead to low\nCI coverage. As a remedy, we adapt Rubin's rule to aggregate feature-importance\nestimates and their variances over several imputed datasets and account for\nimputation uncertainty. Our numerical results indicate that the adjusted CIs\nachieve better nominal coverage."
                },
                "authors": [
                    {
                        "name": "Nico F√∂ge"
                    },
                    {
                        "name": "Markus Pauly"
                    }
                ],
                "author_detail": {
                    "name": "Markus Pauly"
                },
                "author": "Markus Pauly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13893v1",
                "updated": "2025-07-18T13:16:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    16,
                    31,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T13:16:31Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    16,
                    31,
                    4,
                    199,
                    0
                ],
                "title": "The spin-orbit alignment hypothesis in millisecond pulsars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spin-orbit alignment hypothesis in millisecond pulsars"
                },
                "summary": "Millisecond pulsars (MSPs) are spun up during their accretion phase in a\nbinary system. The exchange of angular momentum between the accretion disk and\nthe star tends to align the spin and orbital angular momenta on a very short\ntime scale compared to the accretion stage. In this work, we study a subset of\n$\\gamma$-ray MSPs in binaries for which the orbital inclination $i$ has been\naccurately constrained thanks to the Shapiro delay measurements. Our goal is to\nconstrain the observer viewing angle $\\zeta$ and to check whether it agrees\nwith the orbital inclination $i$, thus $\\zeta \\approx i$. We use a Bayesian\ninference technique to fit the MSP $\\gamma$-ray light-curves based on the third\n$\\gamma$-ray pulsar catalogue (3PC). The emission model relies on the striped\nwind model deduced from force-free neutron star magnetosphere simulations. We\nfound good agreement between those two angles ($i$ and $\\zeta$) for a\nsignificant fraction of our sample, between 1/2 and 2/3, confirming the\nspin-orbit alignment scenario during the accretion stage. However about one\nthird of our sample deviates significantly from this alignment. The reasons are\nmanifold: either the $\\gamma$-ray fit is not reliable or some precession and\nexternal torque avoid an almost perfect alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millisecond pulsars (MSPs) are spun up during their accretion phase in a\nbinary system. The exchange of angular momentum between the accretion disk and\nthe star tends to align the spin and orbital angular momenta on a very short\ntime scale compared to the accretion stage. In this work, we study a subset of\n$\\gamma$-ray MSPs in binaries for which the orbital inclination $i$ has been\naccurately constrained thanks to the Shapiro delay measurements. Our goal is to\nconstrain the observer viewing angle $\\zeta$ and to check whether it agrees\nwith the orbital inclination $i$, thus $\\zeta \\approx i$. We use a Bayesian\ninference technique to fit the MSP $\\gamma$-ray light-curves based on the third\n$\\gamma$-ray pulsar catalogue (3PC). The emission model relies on the striped\nwind model deduced from force-free neutron star magnetosphere simulations. We\nfound good agreement between those two angles ($i$ and $\\zeta$) for a\nsignificant fraction of our sample, between 1/2 and 2/3, confirming the\nspin-orbit alignment scenario during the accretion stage. However about one\nthird of our sample deviates significantly from this alignment. The reasons are\nmanifold: either the $\\gamma$-ray fit is not reliable or some precession and\nexternal torque avoid an almost perfect alignment."
                },
                "authors": [
                    {
                        "name": "Alexandra Lorange"
                    },
                    {
                        "name": "J√©r√¥me P√©tri"
                    },
                    {
                        "name": "Matt√©o Sautron"
                    }
                ],
                "author_detail": {
                    "name": "Matt√©o Sautron"
                },
                "author": "Matt√©o Sautron",
                "arxiv_comment": "This work has been submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13881v1",
                "updated": "2025-07-18T12:59:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    59,
                    17,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T12:59:17Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    59,
                    17,
                    4,
                    199,
                    0
                ],
                "title": "Using LLMs to identify features of personal and professional skills in\n  an open-response situational judgment test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs to identify features of personal and professional skills in\n  an open-response situational judgment test"
                },
                "summary": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills."
                },
                "authors": [
                    {
                        "name": "Cole Walsh"
                    },
                    {
                        "name": "Rodica Ivan"
                    },
                    {
                        "name": "Muhammad Zafar Iqbal"
                    },
                    {
                        "name": "Colleen Robb"
                    }
                ],
                "author_detail": {
                    "name": "Colleen Robb"
                },
                "author": "Colleen Robb",
                "arxiv_comment": "10 pages, 2 figures, 4 tables; this work was accepted for\n  presentation at the 2025 Artificial Intelligence in Measurement and Education\n  Conference in Pittsburgh, Pennsylvania, United States",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13874v1",
                "updated": "2025-07-18T12:54:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    54,
                    28,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T12:54:28Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    54,
                    28,
                    4,
                    199,
                    0
                ],
                "title": "Large Language Models as Innovators: A Framework to Leverage Latent\n  Space Exploration for Novelty Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Innovators: A Framework to Leverage Latent\n  Space Exploration for Novelty Discovery"
                },
                "summary": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration."
                },
                "authors": [
                    {
                        "name": "Mateusz Bystro≈Ñski"
                    },
                    {
                        "name": "Miko≈Çaj Ho≈Çysz"
                    },
                    {
                        "name": "Grzegorz Piotrowski"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    },
                    {
                        "name": "Tomasz Kajdanowicz"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kajdanowicz"
                },
                "author": "Tomasz Kajdanowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11291v2",
                "updated": "2025-07-18T12:46:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    46,
                    45,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-15T13:18:04Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    13,
                    18,
                    4,
                    1,
                    196,
                    0
                ],
                "title": "Permutation patterns in streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Permutation patterns in streams"
                },
                "summary": "Permutation patterns and pattern avoidance are central, well-studied concepts\nin combinatorics and computer science. Given two permutations $\\tau$ and $\\pi$,\nthe pattern matching problem (PPM) asks whether $\\tau$ contains $\\pi$. This\nproblem arises in various contexts in computer science and statistics and has\nbeen studied extensively in exact-, parameterized-, approximate-,\nproperty-testing- and other formulations.\n  In this paper, we study pattern matching in a streaming setting, when the\ninput $\\tau$ is revealed sequentially, one element at a time. There is\nextensive work on the space complexity of various statistics in streams of\nintegers. The novelty of our setting is that the input stream is a permutation,\nwhich allows inferring some information about future inputs. Our algorithms\ncrucially take advantage of this fact, while existing lower bound techniques\nbecome difficult to apply.\n  We show that the complexity of the problem changes dramatically depending on\nthe pattern $\\pi$. The space requirement is: $\\Theta(k\\log{n})$ for the\nmonotone patterns $\\pi = 12\\dots k$, or $\\pi = k\\dots21$, $O(\\sqrt{n\\log{n}})$\nfor $\\pi \\in \\{312,132\\}$, $O(\\sqrt{n} \\log n)$ for $\\pi \\in \\{231,213\\}$, and\n$\\widetilde{\\Theta}_{\\pi}(n)$ for all other $\\pi$. If $\\tau$ is an arbitrary\nsequence of integers (not necessary a permutation), we show that the complexity\nis $\\widetilde{\\Theta}_{\\pi}(n)$ in all except the first (monotone) cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Permutation patterns and pattern avoidance are central, well-studied concepts\nin combinatorics and computer science. Given two permutations $\\tau$ and $\\pi$,\nthe pattern matching problem (PPM) asks whether $\\tau$ contains $\\pi$. This\nproblem arises in various contexts in computer science and statistics and has\nbeen studied extensively in exact-, parameterized-, approximate-,\nproperty-testing- and other formulations.\n  In this paper, we study pattern matching in a streaming setting, when the\ninput $\\tau$ is revealed sequentially, one element at a time. There is\nextensive work on the space complexity of various statistics in streams of\nintegers. The novelty of our setting is that the input stream is a permutation,\nwhich allows inferring some information about future inputs. Our algorithms\ncrucially take advantage of this fact, while existing lower bound techniques\nbecome difficult to apply.\n  We show that the complexity of the problem changes dramatically depending on\nthe pattern $\\pi$. The space requirement is: $\\Theta(k\\log{n})$ for the\nmonotone patterns $\\pi = 12\\dots k$, or $\\pi = k\\dots21$, $O(\\sqrt{n\\log{n}})$\nfor $\\pi \\in \\{312,132\\}$, $O(\\sqrt{n} \\log n)$ for $\\pi \\in \\{231,213\\}$, and\n$\\widetilde{\\Theta}_{\\pi}(n)$ for all other $\\pi$. If $\\tau$ is an arbitrary\nsequence of integers (not necessary a permutation), we show that the complexity\nis $\\widetilde{\\Theta}_{\\pi}(n)$ in all except the first (monotone) cases."
                },
                "authors": [
                    {
                        "name": "Benjamin Aram Berendsohn"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Aram Berendsohn"
                },
                "author": "Benjamin Aram Berendsohn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16247v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16247v3",
                "updated": "2025-07-18T12:37:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    37,
                    43,
                    4,
                    199,
                    0
                ],
                "published": "2024-12-20T00:01:16Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    0,
                    1,
                    16,
                    4,
                    355,
                    0
                ],
                "title": "Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models"
                },
                "summary": "Sparse dictionary learning (DL) has emerged as a powerful approach to extract\nsemantically meaningful concepts from the internals of large language models\n(LLMs) trained mainly in the text domain. In this work, we explore whether DL\ncan extract meaningful concepts from less human-interpretable scientific data,\nsuch as vision foundation models trained on cell microscopy images, where\nlimited prior knowledge exists about which high-level concepts should arise. We\npropose a novel combination of a sparse DL algorithm, Iterative Codebook\nFeature Learning (ICFL), with a PCA whitening pre-processing step derived from\ncontrol data. Using this combined approach, we successfully retrieve\nbiologically meaningful concepts, such as cell types and genetic perturbations.\nMoreover, we demonstrate how our method reveals subtle morphological changes\narising from human-interpretable interventions, offering a promising new\ndirection for scientific discovery via mechanistic interpretability in\nbioimaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse dictionary learning (DL) has emerged as a powerful approach to extract\nsemantically meaningful concepts from the internals of large language models\n(LLMs) trained mainly in the text domain. In this work, we explore whether DL\ncan extract meaningful concepts from less human-interpretable scientific data,\nsuch as vision foundation models trained on cell microscopy images, where\nlimited prior knowledge exists about which high-level concepts should arise. We\npropose a novel combination of a sparse DL algorithm, Iterative Codebook\nFeature Learning (ICFL), with a PCA whitening pre-processing step derived from\ncontrol data. Using this combined approach, we successfully retrieve\nbiologically meaningful concepts, such as cell types and genetic perturbations.\nMoreover, we demonstrate how our method reveals subtle morphological changes\narising from human-interpretable interventions, offering a promising new\ndirection for scientific discovery via mechanistic interpretability in\nbioimaging."
                },
                "authors": [
                    {
                        "name": "Konstantin Donhauser"
                    },
                    {
                        "name": "Kristina Ulicna"
                    },
                    {
                        "name": "Gemma Elyse Moran"
                    },
                    {
                        "name": "Aditya Ravuri"
                    },
                    {
                        "name": "Kian Kenyon-Dean"
                    },
                    {
                        "name": "Cian Eastwood"
                    },
                    {
                        "name": "Jason Hartford"
                    }
                ],
                "author_detail": {
                    "name": "Jason Hartford"
                },
                "author": "Jason Hartford",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16247v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16247v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04800v3",
                "updated": "2025-07-18T12:34:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    34,
                    45,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-03T06:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    6,
                    54,
                    5,
                    0,
                    62,
                    0
                ],
                "title": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated\n  Information on Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated\n  Information on Retrieval-Augmented Generation"
                },
                "summary": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH."
                },
                "authors": [
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Tingyue Pan"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Ruiran Yan"
                    },
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Jiaying Lin"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13859v1",
                "updated": "2025-07-18T12:28:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    28,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T12:28:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    28,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data\n  Memorization and Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data\n  Memorization and Knowledge Injection"
                },
                "summary": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible."
                },
                "authors": [
                    {
                        "name": "Aleksandr Gashkov"
                    },
                    {
                        "name": "Aleksandr Perevalov"
                    },
                    {
                        "name": "Maria Eltsova"
                    },
                    {
                        "name": "Andreas Both"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Both"
                },
                "author": "Andreas Both",
                "arxiv_comment": "Winner of Best Paper Award at the 25th International Conference on\n  Web Engineering (ICWE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13857v1",
                "updated": "2025-07-18T12:23:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    23,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T12:23:47Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    23,
                    47,
                    4,
                    199,
                    0
                ],
                "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised\n  Monocular Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised\n  Monocular Depth Estimation"
                },
                "summary": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods."
                },
                "authors": [
                    {
                        "name": "Max van den Hoven"
                    },
                    {
                        "name": "Kishaan Jeeveswaran"
                    },
                    {
                        "name": "Pieter Piscaer"
                    },
                    {
                        "name": "Thijs Wensveen"
                    },
                    {
                        "name": "Elahe Arani"
                    },
                    {
                        "name": "Bahram Zonooz"
                    }
                ],
                "author_detail": {
                    "name": "Bahram Zonooz"
                },
                "author": "Bahram Zonooz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13858v1",
                "updated": "2025-07-18T12:23:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    23,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T12:23:47Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    23,
                    47,
                    4,
                    199,
                    0
                ],
                "title": "InTraVisTo: Inside Transformer Visualisation Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InTraVisTo: Inside Transformer Visualisation Tool"
                },
                "summary": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs."
                },
                "authors": [
                    {
                        "name": "Nicol√≤ Brunello"
                    },
                    {
                        "name": "Davide Rigamonti"
                    },
                    {
                        "name": "Andrea Sassella"
                    },
                    {
                        "name": "Vincenzo Scotti"
                    },
                    {
                        "name": "Mark James Carman"
                    }
                ],
                "author_detail": {
                    "name": "Mark James Carman"
                },
                "author": "Mark James Carman",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10484v2",
                "updated": "2025-07-18T12:09:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    9,
                    59,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-17T05:20:38Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    20,
                    38,
                    4,
                    17,
                    0
                ],
                "title": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study\n  of ChatGPT and Claude",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study\n  of ChatGPT and Claude"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled human-like\nresponses across various tasks, raising questions about their ethical\ndecision-making capabilities and potential biases. This study investigates\nprotected attributes in LLMs through systematic evaluation of their responses\nto ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5\nSonnet - we analyzed their decision-making patterns across multiple protected\nattributes including age, gender, race, appearance, and disability status.\nThrough 11,200 experimental trials involving both single-factor and two-factor\nprotected attribute combinations, we evaluated the models' ethical preferences,\nsensitivity, stability, and clustering of preferences. Our findings reveal\nsignificant protected attributeses in both models, with consistent preferences\nfor certain features (e.g., \"good-looking\") and systematic neglect of others.\nNotably, while GPT-3.5 Turbo showed stronger preferences aligned with\ntraditional power structures, Claude 3.5 Sonnet demonstrated more diverse\nprotected attribute choices. We also found that ethical sensitivity\nsignificantly decreases in more complex scenarios involving multiple protected\nattributes. Additionally, linguistic referents heavily influence the models'\nethical evaluations, as demonstrated by differing responses to racial\ndescriptors (e.g., \"Yellow\" versus \"Asian\"). These findings highlight critical\nconcerns about the potential impact of LLM biases in autonomous decision-making\nsystems and emphasize the need for careful consideration of protected\nattributes in AI development. Our study contributes to the growing body of\nresearch on AI ethics by providing a systematic framework for evaluating\nprotected attributes in LLMs' ethical decision-making capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled human-like\nresponses across various tasks, raising questions about their ethical\ndecision-making capabilities and potential biases. This study investigates\nprotected attributes in LLMs through systematic evaluation of their responses\nto ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5\nSonnet - we analyzed their decision-making patterns across multiple protected\nattributes including age, gender, race, appearance, and disability status.\nThrough 11,200 experimental trials involving both single-factor and two-factor\nprotected attribute combinations, we evaluated the models' ethical preferences,\nsensitivity, stability, and clustering of preferences. Our findings reveal\nsignificant protected attributeses in both models, with consistent preferences\nfor certain features (e.g., \"good-looking\") and systematic neglect of others.\nNotably, while GPT-3.5 Turbo showed stronger preferences aligned with\ntraditional power structures, Claude 3.5 Sonnet demonstrated more diverse\nprotected attribute choices. We also found that ethical sensitivity\nsignificantly decreases in more complex scenarios involving multiple protected\nattributes. Additionally, linguistic referents heavily influence the models'\nethical evaluations, as demonstrated by differing responses to racial\ndescriptors (e.g., \"Yellow\" versus \"Asian\"). These findings highlight critical\nconcerns about the potential impact of LLM biases in autonomous decision-making\nsystems and emphasize the need for careful consideration of protected\nattributes in AI development. Our study contributes to the growing body of\nresearch on AI ethics by providing a systematic framework for evaluating\nprotected attributes in LLMs' ethical decision-making capabilities."
                },
                "authors": [
                    {
                        "name": "Yile Yan"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Wentao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Xu"
                },
                "author": "Wentao Xu",
                "arxiv_comment": "This paper has been accepted by International AAAI Conference on Web\n  and Social Media 2026, sunny Los Angeles, California",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13841v1",
                "updated": "2025-07-18T11:55:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    55,
                    18,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T11:55:18Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    55,
                    18,
                    4,
                    199,
                    0
                ],
                "title": "Modeling Fair Play in Detective Stories with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Fair Play in Detective Stories with Language Models"
                },
                "summary": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality."
                },
                "authors": [
                    {
                        "name": "Eitan Wagner"
                    },
                    {
                        "name": "Renana Keydar"
                    },
                    {
                        "name": "Omri Abend"
                    }
                ],
                "author_detail": {
                    "name": "Omri Abend"
                },
                "author": "Omri Abend",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13833v1",
                "updated": "2025-07-18T11:41:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    41,
                    49,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T11:41:49Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    41,
                    49,
                    4,
                    199,
                    0
                ],
                "title": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient\n  LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient\n  LLM Post-Training"
                },
                "summary": "Reinforcement learning (RL) has become the pivotal post-training technique\nfor large language model. Effectively scaling reinforcement learning is now the\nkey to unlocking advanced reasoning capabilities and ensuring safe,\ngoal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually\nemploy a hybrid-controller architecture where a single-controller dispatches\nthe overall execution logic and manages overall data transfer and the\nmulti-controller executes distributed computation. For large-scale\nreinforcement learning, minor load imbalances can introduce significant\nbottlenecks, ultimately constraining the scalability of the system. To address\nthis limitation, we introduce DistFlow, a novel, fully distributed RL framework\ndesigned to break scaling barrier. We adopt a multi-controller paradigm that\ndispatches data transfer and execution tasks to all workers, which eliminates\nthe centralized node. This allows each worker to operate independently, leading\nto near-linear scalability up to thousands of GPUs and dramatic efficiency\ngains. Furthermore, our architecture decouples resource configuration from\nexecution logic, allowing each worker to have a unique execution flow, offering\nsignificant flexibility for rapid and cost-effective algorithmic\nexperimentation. Extensive experiments show that DistFlow achieves excellent\nlinear scalability and up to a 7x end-to-end throughput improvement over\nstate-of-the-art (SOTA) frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become the pivotal post-training technique\nfor large language model. Effectively scaling reinforcement learning is now the\nkey to unlocking advanced reasoning capabilities and ensuring safe,\ngoal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually\nemploy a hybrid-controller architecture where a single-controller dispatches\nthe overall execution logic and manages overall data transfer and the\nmulti-controller executes distributed computation. For large-scale\nreinforcement learning, minor load imbalances can introduce significant\nbottlenecks, ultimately constraining the scalability of the system. To address\nthis limitation, we introduce DistFlow, a novel, fully distributed RL framework\ndesigned to break scaling barrier. We adopt a multi-controller paradigm that\ndispatches data transfer and execution tasks to all workers, which eliminates\nthe centralized node. This allows each worker to operate independently, leading\nto near-linear scalability up to thousands of GPUs and dramatic efficiency\ngains. Furthermore, our architecture decouples resource configuration from\nexecution logic, allowing each worker to have a unique execution flow, offering\nsignificant flexibility for rapid and cost-effective algorithmic\nexperimentation. Extensive experiments show that DistFlow achieves excellent\nlinear scalability and up to a 7x end-to-end throughput improvement over\nstate-of-the-art (SOTA) frameworks."
                },
                "authors": [
                    {
                        "name": "Zhixin Wang"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Jiarui Hu"
                    },
                    {
                        "name": "Dian Yang"
                    },
                    {
                        "name": "Jinlong Hou"
                    },
                    {
                        "name": "Siyuan Feng"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Yuan Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Qi"
                },
                "author": "Yuan Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04295v3",
                "updated": "2025-07-18T11:37:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    37,
                    12,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-06T08:39:26Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    8,
                    39,
                    26,
                    6,
                    187,
                    0
                ],
                "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with\n  Educators in the Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with\n  Educators in the Loop"
                },
                "summary": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students."
                },
                "authors": [
                    {
                        "name": "Runcong Zhao"
                    },
                    {
                        "name": "Artem Bobrov"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01558v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01558v3",
                "updated": "2025-07-18T11:35:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    35,
                    1,
                    4,
                    199,
                    0
                ],
                "published": "2024-05-05T19:10:19Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    19,
                    10,
                    19,
                    6,
                    126,
                    0
                ],
                "title": "Visual Grounding Methods for Efficient Interaction with Desktop\n  Graphical User Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Grounding Methods for Efficient Interaction with Desktop\n  Graphical User Interfaces"
                },
                "summary": "Most visual grounding solutions primarily focus on realistic images. However,\napplications involving synthetic images, such as Graphical User Interfaces\n(GUIs), remain limited. This restricts the development of autonomous computer\nvision-powered artificial intelligence (AI) agents for automatic application\ninteraction. Enabling AI to effectively understand and interact with GUIs is\ncrucial to advancing automation in software testing, accessibility, and\nhuman-computer interaction. In this work, we explore Instruction Visual\nGrounding (IVG), a multi-modal approach to object identification within a GUI.\nMore precisely, given a natural language instruction and a GUI screen, IVG\nlocates the coordinates of the element on the screen where the instruction\nshould be executed. We propose two main methods: (1) IVGocr, which combines a\nLarge Language Model (LLM), an object detection model, and an Optical Character\nRecognition (OCR) module; and (2) IVGdirect, which uses a multimodal\narchitecture for end-to-end grounding. For each method, we introduce a\ndedicated dataset. In addition, we propose the Central Point Validation (CPV)\nmetric, a relaxed variant of the classical Central Proximity Score (CPS)\nmetric. Our final test dataset is publicly released to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most visual grounding solutions primarily focus on realistic images. However,\napplications involving synthetic images, such as Graphical User Interfaces\n(GUIs), remain limited. This restricts the development of autonomous computer\nvision-powered artificial intelligence (AI) agents for automatic application\ninteraction. Enabling AI to effectively understand and interact with GUIs is\ncrucial to advancing automation in software testing, accessibility, and\nhuman-computer interaction. In this work, we explore Instruction Visual\nGrounding (IVG), a multi-modal approach to object identification within a GUI.\nMore precisely, given a natural language instruction and a GUI screen, IVG\nlocates the coordinates of the element on the screen where the instruction\nshould be executed. We propose two main methods: (1) IVGocr, which combines a\nLarge Language Model (LLM), an object detection model, and an Optical Character\nRecognition (OCR) module; and (2) IVGdirect, which uses a multimodal\narchitecture for end-to-end grounding. For each method, we introduce a\ndedicated dataset. In addition, we propose the Central Point Validation (CPV)\nmetric, a relaxed variant of the classical Central Proximity Score (CPS)\nmetric. Our final test dataset is publicly released to support future research."
                },
                "authors": [
                    {
                        "name": "El Hassane Ettifouri"
                    },
                    {
                        "name": "Jessica L√≥pez Espejel"
                    },
                    {
                        "name": "Laura Minkova"
                    },
                    {
                        "name": "Tassnim Dardouri"
                    },
                    {
                        "name": "Walid Dahhane"
                    }
                ],
                "author_detail": {
                    "name": "Walid Dahhane"
                },
                "author": "Walid Dahhane",
                "arxiv_comment": "Preprint submitted to Engineering Applications of Artificial\n  Intelligence journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01558v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01558v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13827v1",
                "updated": "2025-07-18T11:31:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    31,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T11:31:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    31,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "Question-Answer Extraction from Scientific Articles Using Knowledge\n  Graphs and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-Answer Extraction from Scientific Articles Using Knowledge\n  Graphs and Large Language Models"
                },
                "summary": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents."
                },
                "authors": [
                    {
                        "name": "Hosein Azarbonyad"
                    },
                    {
                        "name": "Zi Long Zhu"
                    },
                    {
                        "name": "Georgios Cheirmpos"
                    },
                    {
                        "name": "Zubair Afzal"
                    },
                    {
                        "name": "Vikrant Yadav"
                    },
                    {
                        "name": "Georgios Tsatsaronis"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Tsatsaronis"
                },
                "author": "Georgios Tsatsaronis",
                "arxiv_comment": "SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13822v1",
                "updated": "2025-07-18T11:20:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    20,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T11:20:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    20,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs"
                },
                "summary": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications."
                },
                "authors": [
                    {
                        "name": "Shad Nygren"
                    },
                    {
                        "name": "Pinar Avci"
                    },
                    {
                        "name": "Andre Daniels"
                    },
                    {
                        "name": "Reza Rassol"
                    },
                    {
                        "name": "Afshin Beheshti"
                    },
                    {
                        "name": "Diego Galeano"
                    }
                ],
                "author_detail": {
                    "name": "Diego Galeano"
                },
                "author": "Diego Galeano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13820v1",
                "updated": "2025-07-18T11:12:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    12,
                    44,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T11:12:44Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    12,
                    44,
                    4,
                    199,
                    0
                ],
                "title": "Team of One: Cracking Complex Video QA with Model Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Team of One: Cracking Complex Video QA with Model Synergy"
                },
                "summary": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development."
                },
                "authors": [
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Zhaoran Zhao"
                    },
                    {
                        "name": "Xiongjun Guan"
                    },
                    {
                        "name": "Yingjian Zhu"
                    },
                    {
                        "name": "Hongzhu Yi"
                    },
                    {
                        "name": "Xinming Wang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Zhepeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhepeng Wang"
                },
                "author": "Zhepeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15761v2",
                "updated": "2025-07-18T11:10:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    10,
                    7,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-13T20:55:48Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    20,
                    55,
                    48,
                    3,
                    44,
                    0
                ],
                "title": "AIvaluateXR: An Evaluation Framework for on-Device AI in XR with\n  Benchmarking Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIvaluateXR: An Evaluation Framework for on-Device AI in XR with\n  Benchmarking Results"
                },
                "summary": "The deployment of large language models (LLMs) on extended reality (XR)\ndevices has great potential to advance the field of human-AI interaction. In\nthe case of direct, on-device model inference, selecting the appropriate model\nand device for specific tasks remains challenging. In this paper, we present\nAIvaluateXR, a comprehensive evaluation framework for benchmarking LLMs running\non XR devices. To demonstrate the framework, we deploy 17 selected LLMs across\nfour XR platforms: Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision\nPro, and conduct an extensive evaluation. Our experimental setup measures four\nkey metrics: performance consistency, processing speed, memory usage, and\nbattery consumption. For each of the 68 model-device pairs, we assess\nperformance under varying string lengths, batch sizes, and thread counts,\nanalyzing the trade-offs for real-time XR applications. We propose a unified\nevaluation method based on the 3D Pareto Optimality theory to select the\noptimal device-model pairs from quality and speed objectives. Additionally, we\ncompare the efficiency of on-device LLMs with client-server and cloud-based\nsetups, and evaluate their accuracy on two interactive tasks. We believe our\nfindings offer valuable insight to guide future optimization efforts for LLM\ndeployment on XR devices. Our evaluation method can be used as standard\ngroundwork for further research and development in this emerging field. The\nsource code and supplementary materials are available at:\nwww.nanovis.org/AIvaluateXR.html",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) on extended reality (XR)\ndevices has great potential to advance the field of human-AI interaction. In\nthe case of direct, on-device model inference, selecting the appropriate model\nand device for specific tasks remains challenging. In this paper, we present\nAIvaluateXR, a comprehensive evaluation framework for benchmarking LLMs running\non XR devices. To demonstrate the framework, we deploy 17 selected LLMs across\nfour XR platforms: Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision\nPro, and conduct an extensive evaluation. Our experimental setup measures four\nkey metrics: performance consistency, processing speed, memory usage, and\nbattery consumption. For each of the 68 model-device pairs, we assess\nperformance under varying string lengths, batch sizes, and thread counts,\nanalyzing the trade-offs for real-time XR applications. We propose a unified\nevaluation method based on the 3D Pareto Optimality theory to select the\noptimal device-model pairs from quality and speed objectives. Additionally, we\ncompare the efficiency of on-device LLMs with client-server and cloud-based\nsetups, and evaluate their accuracy on two interactive tasks. We believe our\nfindings offer valuable insight to guide future optimization efforts for LLM\ndeployment on XR devices. Our evaluation method can be used as standard\ngroundwork for further research and development in this emerging field. The\nsource code and supplementary materials are available at:\nwww.nanovis.org/AIvaluateXR.html"
                },
                "authors": [
                    {
                        "name": "Dawar Khan"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Omar Mena"
                    },
                    {
                        "name": "Donggang Jia"
                    },
                    {
                        "name": "Alexandre Kouyoumdjian"
                    },
                    {
                        "name": "Ivan Viola"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Viola"
                },
                "author": "Ivan Viola",
                "arxiv_comment": "AIvaluateXR is updated version of LoXR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13774v2",
                "updated": "2025-07-18T10:56:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    56,
                    13,
                    4,
                    199,
                    0
                ],
                "published": "2025-04-18T16:22:20Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    20,
                    4,
                    108,
                    0
                ],
                "title": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs"
                },
                "summary": "Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information."
                },
                "authors": [
                    {
                        "name": "Tamim Al Mahmud"
                    },
                    {
                        "name": "Najeeb Jebreel"
                    },
                    {
                        "name": "Josep Domingo-Ferrer"
                    },
                    {
                        "name": "David Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "David Sanchez"
                },
                "author": "David Sanchez",
                "arxiv_doi": "10.1016/j.neunet.2025.107879",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.neunet.2025.107879",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the updated version of the preprint, revised following\n  acceptance for publication in Elsevier Neural Networks Journal. The paper is\n  now published (18 July 2025) with DOI:\n  https://doi.org/10.1016/j.neunet.2025.107879",
                "arxiv_journal_ref": "Neural Networks, 2025, Article 107879",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08102v3",
                "updated": "2025-07-18T10:53:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    53,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-14T13:19:47Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    19,
                    47,
                    1,
                    14,
                    0
                ],
                "title": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design."
                },
                "authors": [
                    {
                        "name": "Wenlu Fan"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Wentao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Xu"
                },
                "author": "Wentao Xu",
                "arxiv_comment": "This paper has been accepted by the International AAAI Conference on\n  Web and Social Media (ICWSM) 2026(Los Angeles, California, U.S.)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13814v1",
                "updated": "2025-07-18T10:52:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    52,
                    22,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T10:52:22Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    52,
                    22,
                    4,
                    199,
                    0
                ],
                "title": "CodeEdu: A Multi-Agent Collaborative Platform for Personalized Coding\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeEdu: A Multi-Agent Collaborative Platform for Personalized Coding\n  Education"
                },
                "summary": "Large Language Models (LLMs) have demonstrated considerable potential in\nimproving coding education by providing support for code writing, explanation,\nand debugging. However, existing LLM-based approaches generally fail to assess\nstudents' abilities, design learning plans, provide personalized material\naligned with individual learning goals, and enable interactive learning.\nCurrent work mostly uses single LLM agents, which limits their ability to\nunderstand complex code repositories and schedule step-by-step tutoring. Recent\nresearch has shown that multi-agent LLMs can collaborate to solve complicated\nproblems in various domains like software engineering, but their potential in\nthe field of education remains unexplored. In this work, we introduce CodeEdu,\nan innovative multi-agent collaborative platform that combines LLMs with tool\nuse to provide proactive and personalized education in coding. Unlike static\npipelines, CodeEdu dynamically allocates agents and tasks to meet student\nneeds. Various agents in CodeEdu undertake certain functions specifically,\nincluding task planning, personalized material generation, real-time QA,\nstep-by-step tutoring, code execution, debugging, and learning report\ngeneration, facilitated with extensive external tools to improve task\nefficiency. Automated evaluations reveal that CodeEdu substantially enhances\nstudents' coding performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated considerable potential in\nimproving coding education by providing support for code writing, explanation,\nand debugging. However, existing LLM-based approaches generally fail to assess\nstudents' abilities, design learning plans, provide personalized material\naligned with individual learning goals, and enable interactive learning.\nCurrent work mostly uses single LLM agents, which limits their ability to\nunderstand complex code repositories and schedule step-by-step tutoring. Recent\nresearch has shown that multi-agent LLMs can collaborate to solve complicated\nproblems in various domains like software engineering, but their potential in\nthe field of education remains unexplored. In this work, we introduce CodeEdu,\nan innovative multi-agent collaborative platform that combines LLMs with tool\nuse to provide proactive and personalized education in coding. Unlike static\npipelines, CodeEdu dynamically allocates agents and tasks to meet student\nneeds. Various agents in CodeEdu undertake certain functions specifically,\nincluding task planning, personalized material generation, real-time QA,\nstep-by-step tutoring, code execution, debugging, and learning report\ngeneration, facilitated with extensive external tools to improve task\nefficiency. Automated evaluations reveal that CodeEdu substantially enhances\nstudents' coding performance."
                },
                "authors": [
                    {
                        "name": "Jianing Zhao"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Zhiyuan Wen"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Jianing Yin"
                    },
                    {
                        "name": "Ruosong Yang"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "4 pages, 4 figures. Demo video available at:\n  https://youtu.be/9iIVmTT4CVk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09763v2",
                "updated": "2025-07-18T10:39:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    39,
                    58,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-13T19:45:16Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    19,
                    45,
                    16,
                    6,
                    194,
                    0
                ],
                "title": "Updated Earth Tomography Using Atmospheric Neutrinos at IceCube",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated Earth Tomography Using Atmospheric Neutrinos at IceCube"
                },
                "summary": "The IceCube Neutrino Observatory has observed a sample of high purity,\nprimarily atmospheric, muon neutrino events over 11 years from all directions\nbelow the horizon, spanning the energy range 500 GeV to 100 TeV. While this\nsample was initially used for an eV-scale sterile neutrino search, its purity\nand spanned parameter space can also be used to perform an earth tomography.\nThis flux of neutrinos traverses the earth and is attenuated in varying amounts\ndepending on the energy and traversed column density of the event. By\nparameterizing the earth as multiple constant--density shells, IceCube can\nmeasure the upgoing neutrino flux as a function of the declination, yielding an\ninference of the density of each shell. In this talk, the latest sensitivities\nof this analysis and comparisons with the previous measurement are presented.\nIn addition, the analysis procedure, details about the data sample, and\nsystematic effects are also explained. This analysis is one of the latest,\nweak-force driven, non-gravitational, measurements of the earth's density and\nmass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The IceCube Neutrino Observatory has observed a sample of high purity,\nprimarily atmospheric, muon neutrino events over 11 years from all directions\nbelow the horizon, spanning the energy range 500 GeV to 100 TeV. While this\nsample was initially used for an eV-scale sterile neutrino search, its purity\nand spanned parameter space can also be used to perform an earth tomography.\nThis flux of neutrinos traverses the earth and is attenuated in varying amounts\ndepending on the energy and traversed column density of the event. By\nparameterizing the earth as multiple constant--density shells, IceCube can\nmeasure the upgoing neutrino flux as a function of the declination, yielding an\ninference of the density of each shell. In this talk, the latest sensitivities\nof this analysis and comparisons with the previous measurement are presented.\nIn addition, the analysis procedure, details about the data sample, and\nsystematic effects are also explained. This analysis is one of the latest,\nweak-force driven, non-gravitational, measurements of the earth's density and\nmass."
                },
                "authors": [
                    {
                        "name": "Alex Wen"
                    }
                ],
                "author_detail": {
                    "name": "Alex Wen"
                },
                "arxiv_affiliation": "for the IceCube Collaboration",
                "author": "Alex Wen",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.13745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.13745v3",
                "updated": "2025-07-18T10:34:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    34,
                    56,
                    4,
                    199,
                    0
                ],
                "published": "2023-05-23T07:04:05Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    7,
                    4,
                    5,
                    1,
                    143,
                    0
                ],
                "title": "LightCurve MoE: A Dynamic Sparse Routing Mixture-of-Experts Architecture\n  for Efficient Stellar Light Curve Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightCurve MoE: A Dynamic Sparse Routing Mixture-of-Experts Architecture\n  for Efficient Stellar Light Curve Classification"
                },
                "summary": "The classification of stellar light curves has become a key task in modern\ntime-domain astronomy, fueled by the rapid growth of data from large-scale\nsurveys such as Kepler and TESS. Although deep learning models have achieved\nhigh accuracy in this area, their computational costs can limit scalability. To\ntackle this issue, we propose LightCurve MoE, a Mixture-of-Experts (MoE)\narchitecture that combines dynamic sparse routing with a dual-gating mechanism\nto balance accuracy, efficiency, and robustness. Our model includes five\nspecialized experts, each using a different feature extraction method-such as\nwavelet transforms, Gramian angular fields, and recurrence plots-to capture\nunique patterns in the light curves. A dual-gating mechanism evaluates these\nexpert outputs by analyzing both frequency and time-domain features, allowing\nthe model to adaptively weigh each expert's contribution. During inference,\nonly the top 3 out of 5 experts are activated per sample using a Top-k routing\nstrategy, reducing computational cost by 40% compared to dense models while\npreserving strong accuracy (96%). The model also includes entropy\nregularization and a technique to retain inactive experts during training,\nensuring stable and effective learning. By combining sparse computation with\nmulti-modal feature fusion, LightCurve MoE offers a scalable solution for\nfuture large-scale photometric surveys like LSST and Global Open Transient\nTelescope Array (GOTTA), where processing efficiency is crucial due to the\nmassive volume of daily data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classification of stellar light curves has become a key task in modern\ntime-domain astronomy, fueled by the rapid growth of data from large-scale\nsurveys such as Kepler and TESS. Although deep learning models have achieved\nhigh accuracy in this area, their computational costs can limit scalability. To\ntackle this issue, we propose LightCurve MoE, a Mixture-of-Experts (MoE)\narchitecture that combines dynamic sparse routing with a dual-gating mechanism\nto balance accuracy, efficiency, and robustness. Our model includes five\nspecialized experts, each using a different feature extraction method-such as\nwavelet transforms, Gramian angular fields, and recurrence plots-to capture\nunique patterns in the light curves. A dual-gating mechanism evaluates these\nexpert outputs by analyzing both frequency and time-domain features, allowing\nthe model to adaptively weigh each expert's contribution. During inference,\nonly the top 3 out of 5 experts are activated per sample using a Top-k routing\nstrategy, reducing computational cost by 40% compared to dense models while\npreserving strong accuracy (96%). The model also includes entropy\nregularization and a technique to retain inactive experts during training,\nensuring stable and effective learning. By combining sparse computation with\nmulti-modal feature fusion, LightCurve MoE offers a scalable solution for\nfuture large-scale photometric surveys like LSST and Global Open Transient\nTelescope Array (GOTTA), where processing efficiency is crucial due to the\nmassive volume of daily data."
                },
                "authors": [
                    {
                        "name": "Cunshi Wang"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xinrui Song"
                    },
                    {
                        "name": "Jiacheng Xu"
                    },
                    {
                        "name": "Henggeng Han"
                    },
                    {
                        "name": "Yuyang Li"
                    },
                    {
                        "name": "Xinjie Hu"
                    },
                    {
                        "name": "Huiqin Yang"
                    },
                    {
                        "name": "Jifeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Liu"
                },
                "author": "Jifeng Liu",
                "arxiv_comment": "40 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.13745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.13745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13807v1",
                "updated": "2025-07-18T10:34:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    34,
                    49,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T10:34:49Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    34,
                    49,
                    4,
                    199,
                    0
                ],
                "title": "Efficient Bayesian analysis of kilonovae and GRB afterglows with fiesta",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Bayesian analysis of kilonovae and GRB afterglows with fiesta"
                },
                "summary": "Gamma-ray burst (GRB) afterglows and kilonovae (KNe) are electromagnetic\ntransients that can accompany binary neutron star (BNS) mergers. Therefore,\nstudying their emission processes is of general interest for constraining\ncosmological parameters or the behavior of ultra-dense matter. One common\nmethod to analyze electromagnetic data from BNS mergers is to sample a Bayesian\nposterior over the parameters of a physical model for the transient. However,\nsampling the posterior is computationally costly, and because of the many\nlikelihood evaluations needed in this process, detailed models are too\nexpensive to be used directly in inference. In the present article, we address\nthis problem by introducing fiesta, a python package to train machine learning\n(ML) surrogates for GRB afterglow and kilonova models that can accelerate\nlikelihood evaluations. Specifically, we introduce extensive ML surrogates for\nthe state-of-the-art GRB afterglow models afterglowpy and pyblastafterglow, as\nwell as a new surrogate for the KN emission as modeled by the possis code. Our\nsurrogates enable evaluation of the lightcurve posterior within minutes. We\nalso provide built-in posterior sampling capabilities in fiesta that rely on\nthe flowMC package which scale efficiently to higher dimensions when adding up\nto tens of nuisance sampling parameters. Because of its use of the JAX\nframework, fiesta also allows for GPU acceleration during both surrogate\ntraining and posterior sampling. We apply our framework to reanalyze\nAT2017gfo/GRB170817A and GRB211211A using our surrogates, thus employing the\nnew pyblastafterglow model for the first time in Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma-ray burst (GRB) afterglows and kilonovae (KNe) are electromagnetic\ntransients that can accompany binary neutron star (BNS) mergers. Therefore,\nstudying their emission processes is of general interest for constraining\ncosmological parameters or the behavior of ultra-dense matter. One common\nmethod to analyze electromagnetic data from BNS mergers is to sample a Bayesian\nposterior over the parameters of a physical model for the transient. However,\nsampling the posterior is computationally costly, and because of the many\nlikelihood evaluations needed in this process, detailed models are too\nexpensive to be used directly in inference. In the present article, we address\nthis problem by introducing fiesta, a python package to train machine learning\n(ML) surrogates for GRB afterglow and kilonova models that can accelerate\nlikelihood evaluations. Specifically, we introduce extensive ML surrogates for\nthe state-of-the-art GRB afterglow models afterglowpy and pyblastafterglow, as\nwell as a new surrogate for the KN emission as modeled by the possis code. Our\nsurrogates enable evaluation of the lightcurve posterior within minutes. We\nalso provide built-in posterior sampling capabilities in fiesta that rely on\nthe flowMC package which scale efficiently to higher dimensions when adding up\nto tens of nuisance sampling parameters. Because of its use of the JAX\nframework, fiesta also allows for GPU acceleration during both surrogate\ntraining and posterior sampling. We apply our framework to reanalyze\nAT2017gfo/GRB170817A and GRB211211A using our surrogates, thus employing the\nnew pyblastafterglow model for the first time in Bayesian inference."
                },
                "authors": [
                    {
                        "name": "Hauke Koehn"
                    },
                    {
                        "name": "Thibeau Wouters"
                    },
                    {
                        "name": "Peter T. H. Pang"
                    },
                    {
                        "name": "Mattia Bulla"
                    },
                    {
                        "name": "Henrik Rose"
                    },
                    {
                        "name": "Hannah Wichern"
                    },
                    {
                        "name": "Tim Dietrich"
                    }
                ],
                "author_detail": {
                    "name": "Tim Dietrich"
                },
                "author": "Tim Dietrich",
                "arxiv_comment": "18 pages, 15 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13801v1",
                "updated": "2025-07-18T10:24:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    24,
                    58,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T10:24:58Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    24,
                    58,
                    4,
                    199,
                    0
                ],
                "title": "One Step Closer: Creating the Future to Boost Monocular Semantic Scene\n  Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Step Closer: Creating the Future to Boost Monocular Semantic Scene\n  Completion"
                },
                "summary": "In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a\ncritical perception task for autonomous driving due to its ability to infer\ncomplete 3D scene layouts and semantics from single 2D images. However, in\nreal-world traffic scenarios, a significant portion of the scene remains\noccluded or outside the camera's field of view -- a fundamental challenge that\nexisting monocular SSC methods fail to address adequately. To overcome these\nlimitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC\nframework that leverages pseudo-future frame prediction to expand the model's\neffective perceptual range. Our approach combines poses and depths to establish\naccurate 3D correspondences, enabling geometrically-consistent fusion of past,\npresent, and predicted future frames in 3D space. Unlike conventional methods\nthat rely on simple feature stacking, our 3D-aware architecture achieves more\nrobust scene completion by explicitly modeling spatial-temporal relationships.\nComprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks\ndemonstrate state-of-the-art performance, validating the effectiveness of our\napproach, highlighting our method's ability to improve occlusion reasoning and\n3D scene completion accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a\ncritical perception task for autonomous driving due to its ability to infer\ncomplete 3D scene layouts and semantics from single 2D images. However, in\nreal-world traffic scenarios, a significant portion of the scene remains\noccluded or outside the camera's field of view -- a fundamental challenge that\nexisting monocular SSC methods fail to address adequately. To overcome these\nlimitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC\nframework that leverages pseudo-future frame prediction to expand the model's\neffective perceptual range. Our approach combines poses and depths to establish\naccurate 3D correspondences, enabling geometrically-consistent fusion of past,\npresent, and predicted future frames in 3D space. Unlike conventional methods\nthat rely on simple feature stacking, our 3D-aware architecture achieves more\nrobust scene completion by explicitly modeling spatial-temporal relationships.\nComprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks\ndemonstrate state-of-the-art performance, validating the effectiveness of our\napproach, highlighting our method's ability to improve occlusion reasoning and\n3D scene completion accuracy."
                },
                "authors": [
                    {
                        "name": "Haoang Lu"
                    },
                    {
                        "name": "Yuanqi Su"
                    },
                    {
                        "name": "Xiaoning Zhang"
                    },
                    {
                        "name": "Hao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Hu"
                },
                "author": "Hao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11264v3",
                "updated": "2025-07-18T10:09:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    9,
                    5,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-20T04:11:21Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    4,
                    11,
                    21,
                    0,
                    20,
                    0
                ],
                "title": "Code Readability in the Age of Large Language Models: An Industrial Case\n  Study from Atlassian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Readability in the Age of Large Language Models: An Industrial Case\n  Study from Atlassian"
                },
                "summary": "Software engineers spend a significant amount of time reading code during the\nsoftware development process, especially in the age of large language models\n(LLMs) that can automatically generate code. However, little is known about the\nreadability of the LLM-generated code and whether it is still important from\npractitioners' perspectives in this new era. In this paper, we conduct a survey\nto explore the practitioners' perspectives on code readability in the age of\nLLMs and investigate the readability of our LLM-based software development\nagents framework, HULA, by comparing its generated code with human-written code\nin real-world scenarios. Overall, the findings underscore that (1) readability\nremains a critical aspect of software development; (2) the readability of our\nLLM-generated code is comparable to human-written code, fostering the\nestablishment of appropriate trust and driving the broad adoption of our\nLLM-powered software development platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software engineers spend a significant amount of time reading code during the\nsoftware development process, especially in the age of large language models\n(LLMs) that can automatically generate code. However, little is known about the\nreadability of the LLM-generated code and whether it is still important from\npractitioners' perspectives in this new era. In this paper, we conduct a survey\nto explore the practitioners' perspectives on code readability in the age of\nLLMs and investigate the readability of our LLM-based software development\nagents framework, HULA, by comparing its generated code with human-written code\nin real-world scenarios. Overall, the findings underscore that (1) readability\nremains a critical aspect of software development; (2) the readability of our\nLLM-generated code is comparable to human-written code, fostering the\nestablishment of appropriate trust and driving the broad adoption of our\nLLM-powered software development platform."
                },
                "authors": [
                    {
                        "name": "Wannita Takerngsaksiri"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    },
                    {
                        "name": "Micheal Fu"
                    },
                    {
                        "name": "Jirat Pasuksmit"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Wu"
                },
                "author": "Ming Wu",
                "arxiv_comment": "11 pages, 7 figures, 8 tables, Accepted at ICSME",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13788v1",
                "updated": "2025-07-18T10:00:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    0,
                    24,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T10:00:24Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    0,
                    24,
                    4,
                    199,
                    0
                ],
                "title": "Debiased Machine Learning for Unobserved Heterogeneity: High-Dimensional\n  Panels and Measurement Error Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiased Machine Learning for Unobserved Heterogeneity: High-Dimensional\n  Panels and Measurement Error Models"
                },
                "summary": "Developing robust inference for models with nonparametric Unobserved\nHeterogeneity (UH) is both important and challenging. We propose novel Debiased\nMachine Learning (DML) procedures for valid inference on functionals of UH,\nallowing for partial identification of multivariate target and high-dimensional\nnuisance parameters. Our main contribution is a full characterization of all\nrelevant Neyman-orthogonal moments in models with nonparametric UH, where\nrelevance means informativeness about the parameter of interest. Under\nadditional support conditions, orthogonal moments are globally robust to the\ndistribution of the UH. They may still involve other high-dimensional nuisance\nparameters, but their local robustness reduces regularization bias and enables\nvalid DML inference. We apply these results to: (i) common parameters, average\nmarginal effects, and variances of UH in panel data models with\nhigh-dimensional controls; (ii) moments of the common factor in the Kotlarski\nmodel with a factor loading; and (iii) smooth functionals of teacher\nvalue-added. Monte Carlo simulations show substantial efficiency gains from\nusing efficient orthogonal moments relative to ad-hoc choices. We illustrate\nthe practical value of our approach by showing that existing estimates of the\naverage and variance effects of maternal smoking on child birth weight are\nrobust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing robust inference for models with nonparametric Unobserved\nHeterogeneity (UH) is both important and challenging. We propose novel Debiased\nMachine Learning (DML) procedures for valid inference on functionals of UH,\nallowing for partial identification of multivariate target and high-dimensional\nnuisance parameters. Our main contribution is a full characterization of all\nrelevant Neyman-orthogonal moments in models with nonparametric UH, where\nrelevance means informativeness about the parameter of interest. Under\nadditional support conditions, orthogonal moments are globally robust to the\ndistribution of the UH. They may still involve other high-dimensional nuisance\nparameters, but their local robustness reduces regularization bias and enables\nvalid DML inference. We apply these results to: (i) common parameters, average\nmarginal effects, and variances of UH in panel data models with\nhigh-dimensional controls; (ii) moments of the common factor in the Kotlarski\nmodel with a factor loading; and (iii) smooth functionals of teacher\nvalue-added. Monte Carlo simulations show substantial efficiency gains from\nusing efficient orthogonal moments relative to ad-hoc choices. We illustrate\nthe practical value of our approach by showing that existing estimates of the\naverage and variance effects of maternal smoking on child birth weight are\nrobust."
                },
                "authors": [
                    {
                        "name": "Facundo Arga√±araz"
                    },
                    {
                        "name": "Juan Carlos Escanciano"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Escanciano"
                },
                "author": "Juan Carlos Escanciano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13783v1",
                "updated": "2025-07-18T09:55:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    55,
                    51,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T09:55:51Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    55,
                    51,
                    4,
                    199,
                    0
                ],
                "title": "Spectroscopic Characterization of LOFAR Radio-emitting M dwarfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectroscopic Characterization of LOFAR Radio-emitting M dwarfs"
                },
                "summary": "Recent observations with the LOw Frequency ARray (LOFAR) have revealed 19\nnearby M dwarfs showing bright circularly polarised radio emission. One of the\npossible sources of such emission is through magnetic star-planet interactions\n(MSPI) with unseen close-in planets. We present initial results from a\nspectroscopic survey with the Habitable-zone Planet Finder (HPF) and NEID\nspectrographs designed to characterize this sample and further investigate the\norigin of the radio emission. We provide four new insights into the sample. I)\nWe uniformly characterize the stellar properties, constraining their effective\ntemperatures, surface gravities, metallicities, projected rotational\nvelocities, rotation periods, stellar radii, and stellar inclinations where\npossible. Further, from a homogenous analysis of the HPF spectra, we infer\ntheir chromospheric activity and spectroscopic multiplicity states. From this,\nwe identify GJ 625, GJ 1151, and LHS 2395 as single, quiescent stars amenable\nto precise RV follow-up, making them strong MSPI candidates. II) We show that\nthe distribution of stellar inclinations are compatible with an isotropic\ndistribution, providing no evidence for a preference to pole-on configurations.\nIII) We refine the radial velocity solution for GJ 625 b, the only currently\nknown close-in planet in the sample, reducing the uncertainty in its orbital\nperiod by a factor of three, to facilitate future phase-dependent radio\nanalysis. IV) Finally, we identify GJ 3861 as a spectroscopic binary with an\norbital period of $P=14.841181_{-0.00010}^{+0.00011}$ d, making it the only\nconfirmed binary with a relatively short orbit in the sample, where we surmise\nthe radio emission is likely related to magnetospheric interactions between the\ntwo stars. These results advance our understanding of radio-emitting M dwarfs\nand establish an observational foundation for identifying MSPI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent observations with the LOw Frequency ARray (LOFAR) have revealed 19\nnearby M dwarfs showing bright circularly polarised radio emission. One of the\npossible sources of such emission is through magnetic star-planet interactions\n(MSPI) with unseen close-in planets. We present initial results from a\nspectroscopic survey with the Habitable-zone Planet Finder (HPF) and NEID\nspectrographs designed to characterize this sample and further investigate the\norigin of the radio emission. We provide four new insights into the sample. I)\nWe uniformly characterize the stellar properties, constraining their effective\ntemperatures, surface gravities, metallicities, projected rotational\nvelocities, rotation periods, stellar radii, and stellar inclinations where\npossible. Further, from a homogenous analysis of the HPF spectra, we infer\ntheir chromospheric activity and spectroscopic multiplicity states. From this,\nwe identify GJ 625, GJ 1151, and LHS 2395 as single, quiescent stars amenable\nto precise RV follow-up, making them strong MSPI candidates. II) We show that\nthe distribution of stellar inclinations are compatible with an isotropic\ndistribution, providing no evidence for a preference to pole-on configurations.\nIII) We refine the radial velocity solution for GJ 625 b, the only currently\nknown close-in planet in the sample, reducing the uncertainty in its orbital\nperiod by a factor of three, to facilitate future phase-dependent radio\nanalysis. IV) Finally, we identify GJ 3861 as a spectroscopic binary with an\norbital period of $P=14.841181_{-0.00010}^{+0.00011}$ d, making it the only\nconfirmed binary with a relatively short orbit in the sample, where we surmise\nthe radio emission is likely related to magnetospheric interactions between the\ntwo stars. These results advance our understanding of radio-emitting M dwarfs\nand establish an observational foundation for identifying MSPI."
                },
                "authors": [
                    {
                        "name": "E. Koo"
                    },
                    {
                        "name": "G. Stefansson"
                    },
                    {
                        "name": "R. D. Kavanagh"
                    },
                    {
                        "name": "M. Delamer"
                    },
                    {
                        "name": "S. Mahadevan"
                    },
                    {
                        "name": "J. R. Callingham"
                    },
                    {
                        "name": "H. Vedantham"
                    },
                    {
                        "name": "P. Robertson"
                    },
                    {
                        "name": "D. Bruijne"
                    },
                    {
                        "name": "C. F. Bender"
                    },
                    {
                        "name": "C. I. Ca√±as"
                    },
                    {
                        "name": "S. Diddams"
                    },
                    {
                        "name": "J. I. Espinoza-Retamal"
                    },
                    {
                        "name": "R. B. Fernandes"
                    },
                    {
                        "name": "S. Halverson"
                    },
                    {
                        "name": "S. Kanodia"
                    },
                    {
                        "name": "D. Krolikowski"
                    },
                    {
                        "name": "A. S. J. Lin"
                    },
                    {
                        "name": "B. J. S. Pope"
                    },
                    {
                        "name": "A. Roy"
                    },
                    {
                        "name": "C. Schwab"
                    },
                    {
                        "name": "R. Terrien"
                    },
                    {
                        "name": "J. T. Wright"
                    }
                ],
                "author_detail": {
                    "name": "J. T. Wright"
                },
                "author": "J. T. Wright",
                "arxiv_comment": "19 pages, 13 figures, submitted to Astronomy & Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02183v2",
                "updated": "2025-07-18T09:37:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    37,
                    22,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-04T04:25:51Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    4,
                    25,
                    51,
                    5,
                    4,
                    0
                ],
                "title": "Data-Driven Reduced-Order Models for Port-Hamiltonian Systems with\n  Operator Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Reduced-Order Models for Port-Hamiltonian Systems with\n  Operator Inference"
                },
                "summary": "Hamiltonian operator inference has been developed in [Sharma, H., Wang, Z.,\nKramer, B., Physica D: Nonlinear Phenomena, 431, p.133122, 2022] to learn\nstructure-preserving reduced-order models (ROMs) for Hamiltonian systems. The\nmethod constructs a low-dimensional model using only data and knowledge of the\nfunctional form of the Hamiltonian. The resulting ROMs preserve the intrinsic\nstructure of the system, ensuring that the mechanical and physical properties\nof the system are maintained. In this work, we extend this approach to\nport-Hamiltonian systems, which generalize Hamiltonian systems by including\nenergy dissipation, external input, and output. Based on snapshots of the\nsystem's state and output, together with the information about the functional\nform of the Hamiltonian, reduced operators are inferred through optimization\nand are then used to construct data-driven ROMs. To further alleviate the\ncomplexity of evaluating nonlinear terms in the ROMs, a hyper-reduction method\nvia discrete empirical interpolation is applied. Accordingly, we derive error\nestimates for the ROM approximations of the state and output. Finally, we\ndemonstrate the structure preservation, as well as the accuracy of the proposed\nport-Hamiltonian operator inference framework, through numerical experiments on\na linear mass-spring-damper problem and a nonlinear Toda lattice problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hamiltonian operator inference has been developed in [Sharma, H., Wang, Z.,\nKramer, B., Physica D: Nonlinear Phenomena, 431, p.133122, 2022] to learn\nstructure-preserving reduced-order models (ROMs) for Hamiltonian systems. The\nmethod constructs a low-dimensional model using only data and knowledge of the\nfunctional form of the Hamiltonian. The resulting ROMs preserve the intrinsic\nstructure of the system, ensuring that the mechanical and physical properties\nof the system are maintained. In this work, we extend this approach to\nport-Hamiltonian systems, which generalize Hamiltonian systems by including\nenergy dissipation, external input, and output. Based on snapshots of the\nsystem's state and output, together with the information about the functional\nform of the Hamiltonian, reduced operators are inferred through optimization\nand are then used to construct data-driven ROMs. To further alleviate the\ncomplexity of evaluating nonlinear terms in the ROMs, a hyper-reduction method\nvia discrete empirical interpolation is applied. Accordingly, we derive error\nestimates for the ROM approximations of the state and output. Finally, we\ndemonstrate the structure preservation, as well as the accuracy of the proposed\nport-Hamiltonian operator inference framework, through numerical experiments on\na linear mass-spring-damper problem and a nonlinear Toda lattice problem."
                },
                "authors": [
                    {
                        "name": "Yuwei Geng"
                    },
                    {
                        "name": "Lili Ju"
                    },
                    {
                        "name": "Boris Kramer"
                    },
                    {
                        "name": "Zhu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Wang"
                },
                "author": "Zhu Wang",
                "arxiv_comment": "28 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65P99, 65L70",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03221v3",
                "updated": "2025-07-18T09:35:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    35,
                    18,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-03T09:12:52Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    12,
                    52,
                    1,
                    154,
                    0
                ],
                "title": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into\n  Structured Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into\n  Structured Knowledge"
                },
                "summary": "As the volume of scientific literature grows, efficient knowledge\norganization becomes increasingly challenging. Traditional approaches to\nstructuring scientific content are time-consuming and require significant\ndomain expertise, highlighting the need for tool support. We present\nExtracTable, a Human-in-the-Loop (HITL) workflow and framework that assists\nresearchers in transforming unstructured publications into structured\nrepresentations. The workflow combines large language models (LLMs) with\nuser-defined schemas and is designed for downstream integration into knowledge\ngraphs (KGs). Developed and evaluated in the context of the Open Research\nKnowledge Graph (ORKG), ExtracTable automates key steps such as document\npreprocessing and data extraction while ensuring user oversight through\nvalidation. In an evaluation with ORKG community participants following the\nQuality Improvement Paradigm (QIP), ExtracTable demonstrated high usability and\npractical value. Participants gave it an average System Usability Scale (SUS)\nscore of 84.17 (A+, the highest rating). The time to progress from a research\ninterest to literature-based insights was reduced from between 4 hours and 2\nweeks to an average of 24:40 minutes. By streamlining corpus creation and\nstructured data extraction for knowledge graph integration, ExtracTable\nleverages LLMs and user models to accelerate literature reviews. However, human\nvalidation remains essential to ensure quality, and future work will address\nimproving extraction accuracy and entity linking to existing knowledge\nresources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the volume of scientific literature grows, efficient knowledge\norganization becomes increasingly challenging. Traditional approaches to\nstructuring scientific content are time-consuming and require significant\ndomain expertise, highlighting the need for tool support. We present\nExtracTable, a Human-in-the-Loop (HITL) workflow and framework that assists\nresearchers in transforming unstructured publications into structured\nrepresentations. The workflow combines large language models (LLMs) with\nuser-defined schemas and is designed for downstream integration into knowledge\ngraphs (KGs). Developed and evaluated in the context of the Open Research\nKnowledge Graph (ORKG), ExtracTable automates key steps such as document\npreprocessing and data extraction while ensuring user oversight through\nvalidation. In an evaluation with ORKG community participants following the\nQuality Improvement Paradigm (QIP), ExtracTable demonstrated high usability and\npractical value. Participants gave it an average System Usability Scale (SUS)\nscore of 84.17 (A+, the highest rating). The time to progress from a research\ninterest to literature-based insights was reduced from between 4 hours and 2\nweeks to an average of 24:40 minutes. By streamlining corpus creation and\nstructured data extraction for knowledge graph integration, ExtracTable\nleverages LLMs and user models to accelerate literature reviews. However, human\nvalidation remains essential to ensure quality, and future work will address\nimproving extraction accuracy and entity linking to existing knowledge\nresources."
                },
                "authors": [
                    {
                        "name": "Lena John"
                    },
                    {
                        "name": "Ahmed Malek Ghanmi"
                    },
                    {
                        "name": "Tim Wittenborg"
                    },
                    {
                        "name": "S√∂ren Auer"
                    },
                    {
                        "name": "Oliver Karras"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Karras"
                },
                "author": "Oliver Karras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08924v2",
                "updated": "2025-07-18T09:31:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    31,
                    19,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-11T17:56:32Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    56,
                    32,
                    4,
                    192,
                    0
                ],
                "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for\n  LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for\n  LLM Evaluation"
                },
                "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available."
                },
                "authors": [
                    {
                        "name": "Seokhee Hong"
                    },
                    {
                        "name": "Sunkyoung Kim"
                    },
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Soyeon Kim"
                    },
                    {
                        "name": "Yeonjung Hong"
                    },
                    {
                        "name": "Jinsik Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinsik Lee"
                },
                "author": "Jinsik Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13764v1",
                "updated": "2025-07-18T09:16:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    16,
                    55,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T09:16:55Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    16,
                    55,
                    4,
                    199,
                    0
                ],
                "title": "On consistency of the MLE under finite mixtures of location-scale\n  distributions with a structural parameter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On consistency of the MLE under finite mixtures of location-scale\n  distributions with a structural parameter"
                },
                "summary": "We provide a general and rigorous proof for the strong consistency of maximum\nlikelihood estimators of the cumulative distribution function of the mixing\ndistribution and structural parameter under finite mixtures of location-scale\ndistributions with a structural parameter. The consistency results do not\nrequire the parameter space of location and scale to be compact. We illustrate\nthe results by applying them to finite mixtures of location-scale distributions\nwith the component density function being one of the commonly used density\nfunctions: normal, logistic, extreme-value, or $t$. An extension of the strong\nconsistency results to finite mixtures of multivariate elliptical distributions\nis also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a general and rigorous proof for the strong consistency of maximum\nlikelihood estimators of the cumulative distribution function of the mixing\ndistribution and structural parameter under finite mixtures of location-scale\ndistributions with a structural parameter. The consistency results do not\nrequire the parameter space of location and scale to be compact. We illustrate\nthe results by applying them to finite mixtures of location-scale distributions\nwith the component density function being one of the commonly used density\nfunctions: normal, logistic, extreme-value, or $t$. An extension of the strong\nconsistency results to finite mixtures of multivariate elliptical distributions\nis also discussed."
                },
                "authors": [
                    {
                        "name": "Guanfu Liu"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yukun Liu"
                    },
                    {
                        "name": "Xiaolong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Pu"
                },
                "author": "Xiaolong Pu",
                "arxiv_doi": "10.1016/j.jspi.2018.05.003",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jspi.2018.05.003",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.13764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Statistical Planning and Inference, 199, 29-44 (2019)",
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19838v2",
                "updated": "2025-07-18T09:15:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    15,
                    39,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-24T17:57:26Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    57,
                    26,
                    1,
                    175,
                    0
                ],
                "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution"
                },
                "summary": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems."
                },
                "authors": [
                    {
                        "name": "Liangbin Xie"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Shian Du"
                    },
                    {
                        "name": "Menghan Xia"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Fanghua Yu"
                    },
                    {
                        "name": "Ziyan Chen"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Jiantao Zhou"
                    },
                    {
                        "name": "Chao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Chao Dong"
                },
                "author": "Chao Dong",
                "arxiv_comment": "Project webpage available at https://simplegvr.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13762v1",
                "updated": "2025-07-18T09:15:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    15,
                    35,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T09:15:35Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    15,
                    35,
                    4,
                    199,
                    0
                ],
                "title": "MolPIF: A Parameter Interpolation Flow Model for Molecule Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MolPIF: A Parameter Interpolation Flow Model for Molecule Generation"
                },
                "summary": "Advances in deep learning for molecular generation show promise in\naccelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown\nimpressive performance across diverse chemical tasks, with their success often\nascribed to the paradigm of modeling in a low-variance parameter space.\nHowever, the Bayesian inference-based strategy imposes limitations on designing\nmore flexible distribution transformation pathways, making it challenging to\nadapt to diverse data distributions and varied task requirements. Furthermore,\nthe potential for simpler, more efficient parameter-space-based models is\nunexplored. To address this, we propose a novel Parameter Interpolation Flow\nmodel (named PIF) with detailed theoretical foundation, training, and inference\nprocedures. We then develop MolPIF for structure-based drug design,\ndemonstrating its superior performance across diverse metrics compared to\nbaselines. This work validates the effectiveness of parameter-space-based\ngenerative modeling paradigm for molecules and offers new perspectives for\nmodel design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in deep learning for molecular generation show promise in\naccelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown\nimpressive performance across diverse chemical tasks, with their success often\nascribed to the paradigm of modeling in a low-variance parameter space.\nHowever, the Bayesian inference-based strategy imposes limitations on designing\nmore flexible distribution transformation pathways, making it challenging to\nadapt to diverse data distributions and varied task requirements. Furthermore,\nthe potential for simpler, more efficient parameter-space-based models is\nunexplored. To address this, we propose a novel Parameter Interpolation Flow\nmodel (named PIF) with detailed theoretical foundation, training, and inference\nprocedures. We then develop MolPIF for structure-based drug design,\ndemonstrating its superior performance across diverse metrics compared to\nbaselines. This work validates the effectiveness of parameter-space-based\ngenerative modeling paradigm for molecules and offers new perspectives for\nmodel design."
                },
                "authors": [
                    {
                        "name": "Yaowei Jin"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Wenkai Xiang"
                    },
                    {
                        "name": "Duanhua Cao"
                    },
                    {
                        "name": "Dan Teng"
                    },
                    {
                        "name": "Zhehuan Fan"
                    },
                    {
                        "name": "Jiacheng Xiong"
                    },
                    {
                        "name": "Xia Sheng"
                    },
                    {
                        "name": "Chuanlong Zeng"
                    },
                    {
                        "name": "Mingyue Zheng"
                    },
                    {
                        "name": "Qian Shi"
                    }
                ],
                "author_detail": {
                    "name": "Qian Shi"
                },
                "author": "Qian Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13759v1",
                "updated": "2025-07-18T09:06:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    6,
                    49,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T09:06:49Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    6,
                    49,
                    4,
                    199,
                    0
                ],
                "title": "OntView: What you See is What you Meant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OntView: What you See is What you Meant"
                },
                "summary": "In the field of knowledge management and computer science, ontologies provide\na structured framework for modeling domain-specific knowledge by defining\nconcepts and their relationships. However, the lack of tools that provide\neffective visualization is still a significant challenge. While numerous\nontology editors and viewers exist, most of them fail to graphically represent\nontology structures in a meaningful and non-overwhelming way, limiting users'\nability to comprehend dependencies and properties within large ontological\nframeworks.\n  In this paper, we present OntView, an ontology viewer that is designed to\nprovide users with an intuitive visual representation of ontology concepts and\ntheir formal definitions through a user-friendly interface. Building on the use\nof a DL reasoner, OntView follows a \"What you see is what you meant\" paradigm,\nshowing the actual inferred knowledge. One key aspect for this is its ability\nto visualize General Concept Inclusions (GCI), a feature absent in existing\nvisualization tools. Moreover, to avoid a possible information overload,\nOntView also offers different ways to show a simplified view of the ontology\nby: 1) creating ontology summaries by assessing the importance of the concepts\n(according to different available algorithms), 2) focusing the visualization on\nthe existing TBox elements between two given classes and 3) allowing to\nhide/show different branches in a dynamic way without losing the semantics.\nOntView has been released with an open-source license for the whole community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of knowledge management and computer science, ontologies provide\na structured framework for modeling domain-specific knowledge by defining\nconcepts and their relationships. However, the lack of tools that provide\neffective visualization is still a significant challenge. While numerous\nontology editors and viewers exist, most of them fail to graphically represent\nontology structures in a meaningful and non-overwhelming way, limiting users'\nability to comprehend dependencies and properties within large ontological\nframeworks.\n  In this paper, we present OntView, an ontology viewer that is designed to\nprovide users with an intuitive visual representation of ontology concepts and\ntheir formal definitions through a user-friendly interface. Building on the use\nof a DL reasoner, OntView follows a \"What you see is what you meant\" paradigm,\nshowing the actual inferred knowledge. One key aspect for this is its ability\nto visualize General Concept Inclusions (GCI), a feature absent in existing\nvisualization tools. Moreover, to avoid a possible information overload,\nOntView also offers different ways to show a simplified view of the ontology\nby: 1) creating ontology summaries by assessing the importance of the concepts\n(according to different available algorithms), 2) focusing the visualization on\nthe existing TBox elements between two given classes and 3) allowing to\nhide/show different branches in a dynamic way without losing the semantics.\nOntView has been released with an open-source license for the whole community."
                },
                "authors": [
                    {
                        "name": "Carlos Bobed"
                    },
                    {
                        "name": "Carlota Quintana"
                    },
                    {
                        "name": "Eduardo Mena"
                    },
                    {
                        "name": "Jorge Bobed"
                    },
                    {
                        "name": "Fernando Bobillo"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Bobillo"
                },
                "author": "Fernando Bobillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13758v1",
                "updated": "2025-07-18T09:06:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    6,
                    10,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T09:06:10Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    6,
                    10,
                    4,
                    199,
                    0
                ],
                "title": "The Emperor's New Chain-of-Thought: Probing Reasoning Theater Bias in\n  Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emperor's New Chain-of-Thought: Probing Reasoning Theater Bias in\n  Large Reasoning Models"
                },
                "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and o1 are increasingly used\nas automated evaluators, raising critical questions about their vulnerability\nto the aesthetics of reasoning in LLM-as-a-judge settings. We introduce\nTHEATER, a comprehensive benchmark to systematically evaluate this\nvulnerability-termed Reasoning Theater Bias (RTB)-by comparing LLMs and LRMs\nacross subjective preference and objective factual datasets. Through\ninvestigation of six bias types including Simple Cues and Fake\nChain-of-Thought, we uncover three key findings: (1) in a critical paradox,\nreasoning-specialized LRMs are consistently more susceptible to RTB than\ngeneral-purpose LLMs, particularly in subjective tasks; (2) this creates a\ntask-dependent trade-off, where LRMs show more robustness on factual tasks but\nless on subjective ones; and (3) we identify 'shallow reasoning'-plausible but\nflawed arguments-as the most potent form of RTB. To address this, we design and\nevaluate two prompting strategies: a targeted system prompt that improves\naccuracy by up to 12% on factual tasks but only 1-3% on subjective tasks, and a\nself-reflection mechanism that shows similarly limited effectiveness in the\nmore vulnerable subjective domains. Our work reveals that RTB is a deep-seated\nchallenge for LRM-based evaluation and provides a systematic framework for\ndeveloping more genuinely robust and trustworthy LRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) like DeepSeek-R1 and o1 are increasingly used\nas automated evaluators, raising critical questions about their vulnerability\nto the aesthetics of reasoning in LLM-as-a-judge settings. We introduce\nTHEATER, a comprehensive benchmark to systematically evaluate this\nvulnerability-termed Reasoning Theater Bias (RTB)-by comparing LLMs and LRMs\nacross subjective preference and objective factual datasets. Through\ninvestigation of six bias types including Simple Cues and Fake\nChain-of-Thought, we uncover three key findings: (1) in a critical paradox,\nreasoning-specialized LRMs are consistently more susceptible to RTB than\ngeneral-purpose LLMs, particularly in subjective tasks; (2) this creates a\ntask-dependent trade-off, where LRMs show more robustness on factual tasks but\nless on subjective ones; and (3) we identify 'shallow reasoning'-plausible but\nflawed arguments-as the most potent form of RTB. To address this, we design and\nevaluate two prompting strategies: a targeted system prompt that improves\naccuracy by up to 12% on factual tasks but only 1-3% on subjective tasks, and a\nself-reflection mechanism that shows similarly limited effectiveness in the\nmore vulnerable subjective domains. Our work reveals that RTB is a deep-seated\nchallenge for LRM-based evaluation and provides a systematic framework for\ndeveloping more genuinely robust and trustworthy LRMs."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yubo Fan"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13756v1",
                "updated": "2025-07-18T09:00:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    0,
                    5,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T09:00:05Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    0,
                    5,
                    4,
                    199,
                    0
                ],
                "title": "Modeling HMI observables for the study of solar oscillations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling HMI observables for the study of solar oscillations"
                },
                "summary": "Context: Helioseismology aims to infer the properties of the solar interior\nby analyzing observations of acoustic oscillations. The interpretation of the\nhelioseismic data is however complicated by the non-trivial relationship\nbetween helioseismic observables and the physical perturbations associated with\nacoustic modes, as well as by various instrumental effects. Aims: We aim to\nimprove our understanding of the signature of acoustic modes measured in the\nHelioseismic and Magnetic Imager (HMI) continuum intensity and Doppler velocity\nobservables by accounting for radiative transfer, solar background rotation,\nand spacecraft velocity. Methods: We start with a background model atmosphere\nthat accurately reproduces solar limb darkening and the Fe I 6173{\\AA} spectral\nline profile. We employ first-order perturbation theory to model the effect of\nacoustic oscillations on inferred intensity and velocity. By solving the\nradiative transfer equation in the atmosphere, we synthesize the spectral line,\nconvolve it with the six HMI spectral windows, and deduce continuum intensity\n(hmi.Ic_45s) and Doppler velocity (hmi.V_45s) according to the HMI algorithm.\nResults: We analytically derive the relationship between mode displacement in\nthe atmosphere and the HMI observables, and show that both intensity and\nvelocity deviate significantly from simple approximations. Specifically, the\ncontinuum intensity does not simply reflect the true continuum value, while the\nline-of-sight velocity does not correspond to a straightforward projection of\nthe velocity at a fixed height in the atmosphere. Our results indicate that\nthese deviations are substantial, with amplitudes of approximately 10% and\nphase shifts of around 10 degrees across the detector for both observables.\nMoreover, these effects are highly dependent on the acoustic mode under\nconsideration and the position on the solar disk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Helioseismology aims to infer the properties of the solar interior\nby analyzing observations of acoustic oscillations. The interpretation of the\nhelioseismic data is however complicated by the non-trivial relationship\nbetween helioseismic observables and the physical perturbations associated with\nacoustic modes, as well as by various instrumental effects. Aims: We aim to\nimprove our understanding of the signature of acoustic modes measured in the\nHelioseismic and Magnetic Imager (HMI) continuum intensity and Doppler velocity\nobservables by accounting for radiative transfer, solar background rotation,\nand spacecraft velocity. Methods: We start with a background model atmosphere\nthat accurately reproduces solar limb darkening and the Fe I 6173{\\AA} spectral\nline profile. We employ first-order perturbation theory to model the effect of\nacoustic oscillations on inferred intensity and velocity. By solving the\nradiative transfer equation in the atmosphere, we synthesize the spectral line,\nconvolve it with the six HMI spectral windows, and deduce continuum intensity\n(hmi.Ic_45s) and Doppler velocity (hmi.V_45s) according to the HMI algorithm.\nResults: We analytically derive the relationship between mode displacement in\nthe atmosphere and the HMI observables, and show that both intensity and\nvelocity deviate significantly from simple approximations. Specifically, the\ncontinuum intensity does not simply reflect the true continuum value, while the\nline-of-sight velocity does not correspond to a straightforward projection of\nthe velocity at a fixed height in the atmosphere. Our results indicate that\nthese deviations are substantial, with amplitudes of approximately 10% and\nphase shifts of around 10 degrees across the detector for both observables.\nMoreover, these effects are highly dependent on the acoustic mode under\nconsideration and the position on the solar disk."
                },
                "authors": [
                    {
                        "name": "D. Fournier"
                    },
                    {
                        "name": "N. M. Kostogryz"
                    },
                    {
                        "name": "L. Gizon"
                    },
                    {
                        "name": "J. Schou"
                    },
                    {
                        "name": "V. Witzke"
                    },
                    {
                        "name": "A. I. Shapiro"
                    },
                    {
                        "name": "I. Milic"
                    }
                ],
                "author_detail": {
                    "name": "I. Milic"
                },
                "author": "I. Milic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13753v1",
                "updated": "2025-07-18T08:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    59,
                    2,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    59,
                    2,
                    4,
                    199,
                    0
                ],
                "title": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for\n  High-Quality Video Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for\n  High-Quality Video Synthesis"
                },
                "summary": "In recent years, large text-to-video (T2V) synthesis models have garnered\nconsiderable attention for their abilities to generate videos from textual\ndescriptions. However, achieving both high imaging quality and effective motion\nrepresentation remains a significant challenge for these T2V models. Existing\napproaches often adapt pre-trained text-to-image (T2I) models to refine video\nframes, leading to issues such as flickering and artifacts due to\ninconsistencies across frames. In this paper, we introduce EVS, a training-free\nEncapsulated Video Synthesizer that composes T2I and T2V models to enhance both\nvisual fidelity and motion smoothness of generated videos. Our approach\nutilizes a well-trained diffusion-based T2I model to refine low-quality video\nframes by treating them as out-of-distribution samples, effectively optimizing\nthem with noising and denoising steps. Meanwhile, we employ T2V backbones to\nensure consistent motion dynamics. By encapsulating the T2V temporal-only prior\ninto the T2I generation process, EVS successfully leverages the strengths of\nboth types of models, resulting in videos of improved imaging and motion\nquality. Experimental results validate the effectiveness of our approach\ncompared to previous approaches. Our composition process also leads to a\nsignificant improvement of 1.6x-4.5x speedup in inference time. Source codes:\nhttps://github.com/Tonniia/EVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large text-to-video (T2V) synthesis models have garnered\nconsiderable attention for their abilities to generate videos from textual\ndescriptions. However, achieving both high imaging quality and effective motion\nrepresentation remains a significant challenge for these T2V models. Existing\napproaches often adapt pre-trained text-to-image (T2I) models to refine video\nframes, leading to issues such as flickering and artifacts due to\ninconsistencies across frames. In this paper, we introduce EVS, a training-free\nEncapsulated Video Synthesizer that composes T2I and T2V models to enhance both\nvisual fidelity and motion smoothness of generated videos. Our approach\nutilizes a well-trained diffusion-based T2I model to refine low-quality video\nframes by treating them as out-of-distribution samples, effectively optimizing\nthem with noising and denoising steps. Meanwhile, we employ T2V backbones to\nensure consistent motion dynamics. By encapsulating the T2V temporal-only prior\ninto the T2I generation process, EVS successfully leverages the strengths of\nboth types of models, resulting in videos of improved imaging and motion\nquality. Experimental results validate the effectiveness of our approach\ncompared to previous approaches. Our composition process also leads to a\nsignificant improvement of 1.6x-4.5x speedup in inference time. Source codes:\nhttps://github.com/Tonniia/EVS."
                },
                "authors": [
                    {
                        "name": "Tongtong Su"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Bingyan Liu"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Dongming Lu"
                    }
                ],
                "author_detail": {
                    "name": "Dongming Lu"
                },
                "author": "Dongming Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13743v1",
                "updated": "2025-07-18T08:44:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    44,
                    27,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:44:27Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    44,
                    27,
                    4,
                    199,
                    0
                ],
                "title": "PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for\n  Equality in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for\n  Equality in LLMs"
                },
                "summary": "Large Language Models (LLMs) frequently reproduce the gender- and\nsexual-identity prejudices embedded in their training corpora, leading to\noutputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of\ngreat importance. To achieve this, we evaluate two parameter-efficient\nfine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt\ntuning - as lightweight alternatives to full-model fine-tuning for mitigating\nsuch biases. Using the WinoQueer benchmark, we quantify bias in three\nopen-source LLMs and observe baseline bias scores reaching up to 98 (out of\n100) across a range of queer identities defined by gender and/or sexual\norientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%\nadditional parameters) on a curated QueerNews corpus reduces those scores by up\nto 50 points and raises neutrality from virtually 0% to as much as 36%.\nSoft-prompt tuning (10 virtual tokens) delivers only marginal improvements.\nThese findings show that LoRA can deliver meaningful fairness gains with\nminimal computation. We advocate broader adoption of community-informed PEFT,\nthe creation of larger queer-authored corpora, and richer evaluation suites\nbeyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) frequently reproduce the gender- and\nsexual-identity prejudices embedded in their training corpora, leading to\noutputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of\ngreat importance. To achieve this, we evaluate two parameter-efficient\nfine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt\ntuning - as lightweight alternatives to full-model fine-tuning for mitigating\nsuch biases. Using the WinoQueer benchmark, we quantify bias in three\nopen-source LLMs and observe baseline bias scores reaching up to 98 (out of\n100) across a range of queer identities defined by gender and/or sexual\norientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%\nadditional parameters) on a curated QueerNews corpus reduces those scores by up\nto 50 points and raises neutrality from virtually 0% to as much as 36%.\nSoft-prompt tuning (10 virtual tokens) delivers only marginal improvements.\nThese findings show that LoRA can deliver meaningful fairness gains with\nminimal computation. We advocate broader adoption of community-informed PEFT,\nthe creation of larger queer-authored corpora, and richer evaluation suites\nbeyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive."
                },
                "authors": [
                    {
                        "name": "Maluna Menke"
                    },
                    {
                        "name": "Thilo Hagendorff"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Hagendorff"
                },
                "author": "Thilo Hagendorff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13742v1",
                "updated": "2025-07-18T08:42:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    42,
                    20,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:42:20Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    42,
                    20,
                    4,
                    199,
                    0
                ],
                "title": "Search-Optimized Quantization in Biomedical Ontology Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-Optimized Quantization in Biomedical Ontology Alignment"
                },
                "summary": "In the fast-moving world of AI, as organizations and researchers develop more\nadvanced models, they face challenges due to their sheer size and computational\ndemands. Deploying such models on edge devices or in resource-constrained\nenvironments adds further challenges related to energy consumption, memory\nusage and latency. To address these challenges, emerging trends are shaping the\nfuture of efficient model optimization techniques. From this premise, by\nemploying supervised state-of-the-art transformer-based models, this research\nintroduces a systematic method for ontology alignment, grounded in cosine-based\nsemantic similarity between a biomedical layman vocabulary and the Unified\nMedical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to\nsearch for target optimizations among different Execution Providers (EPs) using\nthe ONNX Runtime backend, followed by an assembled process of dynamic\nquantization employing Intel Neural Compressor and IPEX (Intel Extension for\nPyTorch). Through our optimization process, we conduct extensive assessments on\nthe two tasks from the DEFT 2020 Evaluation Campaign, achieving a new\nstate-of-the-art in both. We retain performance metrics intact, while attaining\nan average inference speed-up of 20x and reducing memory usage by approximately\n70%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the fast-moving world of AI, as organizations and researchers develop more\nadvanced models, they face challenges due to their sheer size and computational\ndemands. Deploying such models on edge devices or in resource-constrained\nenvironments adds further challenges related to energy consumption, memory\nusage and latency. To address these challenges, emerging trends are shaping the\nfuture of efficient model optimization techniques. From this premise, by\nemploying supervised state-of-the-art transformer-based models, this research\nintroduces a systematic method for ontology alignment, grounded in cosine-based\nsemantic similarity between a biomedical layman vocabulary and the Unified\nMedical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to\nsearch for target optimizations among different Execution Providers (EPs) using\nthe ONNX Runtime backend, followed by an assembled process of dynamic\nquantization employing Intel Neural Compressor and IPEX (Intel Extension for\nPyTorch). Through our optimization process, we conduct extensive assessments on\nthe two tasks from the DEFT 2020 Evaluation Campaign, achieving a new\nstate-of-the-art in both. We retain performance metrics intact, while attaining\nan average inference speed-up of 20x and reducing memory usage by approximately\n70%."
                },
                "authors": [
                    {
                        "name": "Oussama Bouaggad"
                    },
                    {
                        "name": "Natalia Grabar"
                    }
                ],
                "author_detail": {
                    "name": "Natalia Grabar"
                },
                "author": "Natalia Grabar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02145v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02145v4",
                "updated": "2025-07-18T08:39:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    39,
                    33,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-04T09:19:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    19,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "From Words to Collisions: LLM-Guided Evaluation and Adversarial\n  Generation of Safety-Critical Driving Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Words to Collisions: LLM-Guided Evaluation and Adversarial\n  Generation of Safety-Critical Driving Scenarios"
                },
                "summary": "Ensuring the safety of autonomous vehicles requires virtual scenario-based\ntesting, which depends on the robust evaluation and generation of\nsafety-critical scenarios. So far, researchers have used scenario-based testing\nframeworks that rely heavily on handcrafted scenarios as safety metrics. To\nreduce the effort of human interpretation and overcome the limited scalability\nof these approaches, we combine Large Language Models (LLMs) with structured\nscenario parsing and prompt engineering to automatically evaluate and generate\nsafety-critical driving scenarios. We introduce Cartesian and Ego-centric\nprompt strategies for scenario evaluation, and an adversarial generation module\nthat modifies trajectories of risk-inducing vehicles (ego-attackers) to create\ncritical scenarios. We validate our approach using a 2D simulation framework\nand multiple pre-trained LLMs. The results show that the evaluation module\neffectively detects collision scenarios and infers scenario safety. Meanwhile,\nthe new generation module identifies high-risk agents and synthesizes\nrealistic, safety-critical scenarios. We conclude that an LLM equipped with\ndomain-informed prompting techniques can effectively evaluate and generate\nsafety-critical driving scenarios, reducing dependence on handcrafted metrics.\nWe release our open-source code and scenarios at:\nhttps://github.com/TUM-AVS/From-Words-to-Collisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety of autonomous vehicles requires virtual scenario-based\ntesting, which depends on the robust evaluation and generation of\nsafety-critical scenarios. So far, researchers have used scenario-based testing\nframeworks that rely heavily on handcrafted scenarios as safety metrics. To\nreduce the effort of human interpretation and overcome the limited scalability\nof these approaches, we combine Large Language Models (LLMs) with structured\nscenario parsing and prompt engineering to automatically evaluate and generate\nsafety-critical driving scenarios. We introduce Cartesian and Ego-centric\nprompt strategies for scenario evaluation, and an adversarial generation module\nthat modifies trajectories of risk-inducing vehicles (ego-attackers) to create\ncritical scenarios. We validate our approach using a 2D simulation framework\nand multiple pre-trained LLMs. The results show that the evaluation module\neffectively detects collision scenarios and infers scenario safety. Meanwhile,\nthe new generation module identifies high-risk agents and synthesizes\nrealistic, safety-critical scenarios. We conclude that an LLM equipped with\ndomain-informed prompting techniques can effectively evaluate and generate\nsafety-critical driving scenarios, reducing dependence on handcrafted metrics.\nWe release our open-source code and scenarios at:\nhttps://github.com/TUM-AVS/From-Words-to-Collisions."
                },
                "authors": [
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Mattia Piccinini"
                    },
                    {
                        "name": "Korbinian Moller"
                    },
                    {
                        "name": "Amr Alanwar"
                    },
                    {
                        "name": "Johannes Betz"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Betz"
                },
                "author": "Johannes Betz",
                "arxiv_comment": "Final Version and Paper Accepted at IEEE ITSC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02145v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02145v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13737v1",
                "updated": "2025-07-18T08:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    33,
                    30,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    33,
                    30,
                    4,
                    199,
                    0
                ],
                "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal\n  Sensors and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal\n  Sensors and LLMs"
                },
                "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed."
                },
                "authors": [
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Xiaoyuan Ren"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Onat Gungor"
                    },
                    {
                        "name": "Xiaofan Yu"
                    },
                    {
                        "name": "Tajana Rosing"
                    }
                ],
                "author_detail": {
                    "name": "Tajana Rosing"
                },
                "author": "Tajana Rosing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13736v1",
                "updated": "2025-07-18T08:32:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    32,
                    34,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:32:34Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    32,
                    34,
                    4,
                    199,
                    0
                ],
                "title": "An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic\n  MPSoC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic\n  MPSoC"
                },
                "summary": "This work presents a multi-layer DNN scheduling framework as an extension of\nOctopuScheduler, providing an end-to-end flow from PyTorch models to inference\non a single SpiNNaker2 chip. Together with a front-end comprised of\nquantization and lowering steps, the proposed framework enables the edge-based\nexecution of large and complex DNNs up to transformer scale using the\nneuromorphic platform SpiNNaker2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a multi-layer DNN scheduling framework as an extension of\nOctopuScheduler, providing an end-to-end flow from PyTorch models to inference\non a single SpiNNaker2 chip. Together with a front-end comprised of\nquantization and lowering steps, the proposed framework enables the edge-based\nexecution of large and complex DNNs up to transformer scale using the\nneuromorphic platform SpiNNaker2."
                },
                "authors": [
                    {
                        "name": "Matthias Jobst"
                    },
                    {
                        "name": "Tim Langer"
                    },
                    {
                        "name": "Chen Liu"
                    },
                    {
                        "name": "Mehmet Alici"
                    },
                    {
                        "name": "Hector A. Gonzalez"
                    },
                    {
                        "name": "Christian Mayr"
                    }
                ],
                "author_detail": {
                    "name": "Christian Mayr"
                },
                "author": "Christian Mayr",
                "arxiv_comment": "Poster at ACM ICONS 2025 - International Conference on Neuromorphic\n  Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06358v3",
                "updated": "2025-07-18T08:29:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    29,
                    35,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-10T11:20:10Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    20,
                    10,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Tuning Bandits: Enabling Few-Shot Generalization for Efficient\n  Multi-Task Offline RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Tuning Bandits: Enabling Few-Shot Generalization for Efficient\n  Multi-Task Offline RL"
                },
                "summary": "Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nReinforcement Learning (RL) pre-training by leveraging stochastic trajectory\nprompts to identify the target task. However, these prompts are sampled\nuniformly from expert demonstrations, overlooking a critical limitation: not\nall prompts are equally informative for differentiating between tasks. This\nlimits generalization and adaptation, especially in low-data or open-world\nsettings where sample efficiency is crucial. To address this issue, we propose\na lightweight, inference-time, bandit-based prompt-tuning framework. The bandit\nexplores and optimizes trajectory prompt selection to enhance task performance,\nwhile avoiding costly fine-tuning of the transformer backbone. Our experiments\nindicate not only clear performance gains due to bandit-based prompt-tuning,\nbut also better sample complexity, scalability, and prompt space exploration\ncompared to prompt-tuning baselines. These results highlights the importance of\nadaptive prompt selection mechanisms for efficient generalization in offline\nmulti-task RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nReinforcement Learning (RL) pre-training by leveraging stochastic trajectory\nprompts to identify the target task. However, these prompts are sampled\nuniformly from expert demonstrations, overlooking a critical limitation: not\nall prompts are equally informative for differentiating between tasks. This\nlimits generalization and adaptation, especially in low-data or open-world\nsettings where sample efficiency is crucial. To address this issue, we propose\na lightweight, inference-time, bandit-based prompt-tuning framework. The bandit\nexplores and optimizes trajectory prompt selection to enhance task performance,\nwhile avoiding costly fine-tuning of the transformer backbone. Our experiments\nindicate not only clear performance gains due to bandit-based prompt-tuning,\nbut also better sample complexity, scalability, and prompt space exploration\ncompared to prompt-tuning baselines. These results highlights the importance of\nadaptive prompt selection mechanisms for efficient generalization in offline\nmulti-task RL."
                },
                "authors": [
                    {
                        "name": "Finn Rietz"
                    },
                    {
                        "name": "Oleg Smirnov"
                    },
                    {
                        "name": "Sara Karimi"
                    },
                    {
                        "name": "Lele Cao"
                    }
                ],
                "author_detail": {
                    "name": "Lele Cao"
                },
                "author": "Lele Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13732v1",
                "updated": "2025-07-18T08:28:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    28,
                    53,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:28:53Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    28,
                    53,
                    4,
                    199,
                    0
                ],
                "title": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction"
                },
                "summary": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Guillaume Zambrano"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Zambrano"
                },
                "author": "Guillaume Zambrano",
                "arxiv_comment": "23 pages, 24 figures shorter version submitted to JURIX 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11571v2",
                "updated": "2025-07-18T08:23:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    23,
                    14,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-13T08:27:45Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    27,
                    45,
                    4,
                    164,
                    0
                ],
                "title": "VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather\n  than Previous Memories?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather\n  than Previous Memories?"
                },
                "summary": "Recent extensive works have demonstrated that by introducing long CoT, the\ncapabilities of MLLMs to solve complex problems can be effectively enhanced.\nHowever, the reasons for the effectiveness of such paradigms remain unclear. It\nis challenging to analysis with quantitative results how much the model's\nspecific extraction of visual cues and its subsequent so-called reasoning\nduring inference process contribute to the performance improvements. Therefore,\nevaluating the faithfulness of MLLMs' reasoning to visual information is\ncrucial. To address this issue, we first present a cue-driven automatic and\ncontrollable editing pipeline with the help of GPT-Image-1. It enables the\nautomatic and precise editing of specific visual cues based on the instruction.\nFurthermore, we introduce VFaith-Bench, the first benchmark to evaluate MLLMs'\nvisual reasoning capabilities and analyze the source of such capabilities with\nan emphasis on the visual faithfulness. Using the designed pipeline, we\nconstructed comparative question-answer pairs by altering the visual cues in\nimages that are crucial for solving the original reasoning problem, thereby\nchanging the question's answer. By testing similar questions with images that\nhave different details, the average accuracy reflects the model's visual\nreasoning ability, while the difference in accuracy before and after editing\nthe test set images effectively reveals the relationship between the model's\nreasoning ability and visual perception. We further designed specific metrics\nto expose this relationship. VFaith-Bench includes 755 entries divided into\nfive distinct subsets, along with an additional human-labeled perception task.\nWe conducted in-depth testing and analysis of existing mainstream flagship\nmodels and prominent open-source model series/reasoning models on VFaith-Bench,\nfurther investigating the underlying factors of their reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent extensive works have demonstrated that by introducing long CoT, the\ncapabilities of MLLMs to solve complex problems can be effectively enhanced.\nHowever, the reasons for the effectiveness of such paradigms remain unclear. It\nis challenging to analysis with quantitative results how much the model's\nspecific extraction of visual cues and its subsequent so-called reasoning\nduring inference process contribute to the performance improvements. Therefore,\nevaluating the faithfulness of MLLMs' reasoning to visual information is\ncrucial. To address this issue, we first present a cue-driven automatic and\ncontrollable editing pipeline with the help of GPT-Image-1. It enables the\nautomatic and precise editing of specific visual cues based on the instruction.\nFurthermore, we introduce VFaith-Bench, the first benchmark to evaluate MLLMs'\nvisual reasoning capabilities and analyze the source of such capabilities with\nan emphasis on the visual faithfulness. Using the designed pipeline, we\nconstructed comparative question-answer pairs by altering the visual cues in\nimages that are crucial for solving the original reasoning problem, thereby\nchanging the question's answer. By testing similar questions with images that\nhave different details, the average accuracy reflects the model's visual\nreasoning ability, while the difference in accuracy before and after editing\nthe test set images effectively reveals the relationship between the model's\nreasoning ability and visual perception. We further designed specific metrics\nto expose this relationship. VFaith-Bench includes 755 entries divided into\nfive distinct subsets, along with an additional human-labeled perception task.\nWe conducted in-depth testing and analysis of existing mainstream flagship\nmodels and prominent open-source model series/reasoning models on VFaith-Bench,\nfurther investigating the underlying factors of their reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Jiachen Yu"
                    },
                    {
                        "name": "Yufei Zhan"
                    },
                    {
                        "name": "Ziheng Wu"
                    },
                    {
                        "name": "Yousong Zhu"
                    },
                    {
                        "name": "Jinqiao Wang"
                    },
                    {
                        "name": "Minghui Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Minghui Qiu"
                },
                "author": "Minghui Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13729v1",
                "updated": "2025-07-18T08:20:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    20,
                    16,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:20:16Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    20,
                    16,
                    4,
                    199,
                    0
                ],
                "title": "AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios\n  with an Agentic LLM Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios\n  with an Agentic LLM Framework"
                },
                "summary": "Rare, yet critical, scenarios pose a significant challenge in testing and\nevaluating autonomous driving planners. Relying solely on real-world driving\nscenes requires collecting massive datasets to capture these scenarios. While\nautomatic generation of traffic scenarios appears promising, data-driven models\nrequire extensive training data and often lack fine-grained control over the\noutput. Moreover, generating novel scenarios from scratch can introduce a\ndistributional shift from the original training scenes which undermines the\nvalidity of evaluations especially for learning-based planners. To sidestep\nthis, recent work proposes to generate challenging scenarios by augmenting\noriginal scenarios from the test set. However, this involves the manual\naugmentation of scenarios by domain experts. An approach that is unable to meet\nthe demands for scale in the evaluation of self-driving systems. Therefore,\nthis paper introduces a novel LLM-agent based framework for augmenting\nreal-world traffic scenarios using natural language descriptions, addressing\nthe limitations of existing methods. A key innovation is the use of an agentic\ndesign, enabling fine-grained control over the output and maintaining high\nperformance even with smaller, cost-effective LLMs. Extensive human expert\nevaluation demonstrates our framework's ability to accurately adhere to user\nintent, generating high quality augmented scenarios comparable to those created\nmanually.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare, yet critical, scenarios pose a significant challenge in testing and\nevaluating autonomous driving planners. Relying solely on real-world driving\nscenes requires collecting massive datasets to capture these scenarios. While\nautomatic generation of traffic scenarios appears promising, data-driven models\nrequire extensive training data and often lack fine-grained control over the\noutput. Moreover, generating novel scenarios from scratch can introduce a\ndistributional shift from the original training scenes which undermines the\nvalidity of evaluations especially for learning-based planners. To sidestep\nthis, recent work proposes to generate challenging scenarios by augmenting\noriginal scenarios from the test set. However, this involves the manual\naugmentation of scenarios by domain experts. An approach that is unable to meet\nthe demands for scale in the evaluation of self-driving systems. Therefore,\nthis paper introduces a novel LLM-agent based framework for augmenting\nreal-world traffic scenarios using natural language descriptions, addressing\nthe limitations of existing methods. A key innovation is the use of an agentic\ndesign, enabling fine-grained control over the output and maintaining high\nperformance even with smaller, cost-effective LLMs. Extensive human expert\nevaluation demonstrates our framework's ability to accurately adhere to user\nintent, generating high quality augmented scenarios comparable to those created\nmanually."
                },
                "authors": [
                    {
                        "name": "Yu Yao"
                    },
                    {
                        "name": "Salil Bhatnagar"
                    },
                    {
                        "name": "Markus Mazzola"
                    },
                    {
                        "name": "Vasileios Belagiannis"
                    },
                    {
                        "name": "Igor Gilitschenski"
                    },
                    {
                        "name": "Luigi Palmieri"
                    },
                    {
                        "name": "Simon Razniewski"
                    },
                    {
                        "name": "Marcel Hallgarten"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Hallgarten"
                },
                "author": "Marcel Hallgarten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20349v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20349v4",
                "updated": "2025-07-18T08:13:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    13,
                    7,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-26T09:20:42Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    20,
                    42,
                    2,
                    85,
                    0
                ],
                "title": "Consistency Trajectory Matching for One-Step Generative Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Trajectory Matching for One-Step Generative Super-Resolution"
                },
                "summary": "Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency."
                },
                "authors": [
                    {
                        "name": "Weiyi You"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Leheng Zhang"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Kexuan Shi"
                    },
                    {
                        "name": "Shuhang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhang Gu"
                },
                "author": "Shuhang Gu",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20349v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20349v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12144v2",
                "updated": "2025-07-18T08:05:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    5,
                    51,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-16T11:22:18Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    22,
                    18,
                    2,
                    197,
                    0
                ],
                "title": "FourCastNet 3: A geometric approach to probabilistic machine-learning\n  weather forecasting at scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FourCastNet 3: A geometric approach to probabilistic machine-learning\n  weather forecasting at scale"
                },
                "summary": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 60-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 4 minutes. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 60-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 4 minutes. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions."
                },
                "authors": [
                    {
                        "name": "Boris Bonev"
                    },
                    {
                        "name": "Thorsten Kurth"
                    },
                    {
                        "name": "Ankur Mahesh"
                    },
                    {
                        "name": "Mauro Bisson"
                    },
                    {
                        "name": "Jean Kossaifi"
                    },
                    {
                        "name": "Karthik Kashinath"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "William D. Collins"
                    },
                    {
                        "name": "Michael S. Pritchard"
                    },
                    {
                        "name": "Alexander Keller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Keller"
                },
                "author": "Alexander Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "86-10, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.6.5; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13712v1",
                "updated": "2025-07-18T07:52:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    52,
                    19,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T07:52:19Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    52,
                    19,
                    4,
                    199,
                    0
                ],
                "title": "LLaPipe: LLM-Guided Reinforcement Learning for Automated Data\n  Preparation Pipeline Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaPipe: LLM-Guided Reinforcement Learning for Automated Data\n  Preparation Pipeline Construction"
                },
                "summary": "Automated data preparation is crucial for democratizing machine learning, yet\nexisting reinforcement learning (RL) based approaches suffer from inefficient\nexploration in the vast space of possible preprocessing pipelines. We present\nLLaPipe, a novel framework that addresses this exploration bottleneck by\nintegrating Large Language Models (LLMs) as intelligent policy advisors. Unlike\ntraditional methods that rely solely on statistical features and blind\ntrial-and-error, LLaPipe leverages the semantic understanding capabilities of\nLLMs to provide contextually relevant exploration guidance. Our framework\nintroduces three key innovations: (1) an LLM Policy Advisor that analyzes\ndataset semantics and pipeline history to suggest promising preprocessing\noperations, (2) an Experience Distillation mechanism that mines successful\npatterns from past pipelines and transfers this knowledge to guide future\nexploration, and (3) an Adaptive Advisor Triggering strategy\n(Advisor\\textsuperscript{+}) that dynamically determines when LLM intervention\nis most beneficial, balancing exploration effectiveness with computational\ncost. Through extensive experiments on 18 diverse datasets spanning multiple\ndomains, we demonstrate that LLaPipe achieves up to 22.4\\% improvement in\npipeline quality and 2.3$\\times$ faster convergence compared to\nstate-of-the-art RL-based methods, while maintaining computational efficiency\nthrough selective LLM usage (averaging only 19.0\\% of total exploration steps).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated data preparation is crucial for democratizing machine learning, yet\nexisting reinforcement learning (RL) based approaches suffer from inefficient\nexploration in the vast space of possible preprocessing pipelines. We present\nLLaPipe, a novel framework that addresses this exploration bottleneck by\nintegrating Large Language Models (LLMs) as intelligent policy advisors. Unlike\ntraditional methods that rely solely on statistical features and blind\ntrial-and-error, LLaPipe leverages the semantic understanding capabilities of\nLLMs to provide contextually relevant exploration guidance. Our framework\nintroduces three key innovations: (1) an LLM Policy Advisor that analyzes\ndataset semantics and pipeline history to suggest promising preprocessing\noperations, (2) an Experience Distillation mechanism that mines successful\npatterns from past pipelines and transfers this knowledge to guide future\nexploration, and (3) an Adaptive Advisor Triggering strategy\n(Advisor\\textsuperscript{+}) that dynamically determines when LLM intervention\nis most beneficial, balancing exploration effectiveness with computational\ncost. Through extensive experiments on 18 diverse datasets spanning multiple\ndomains, we demonstrate that LLaPipe achieves up to 22.4\\% improvement in\npipeline quality and 2.3$\\times$ faster convergence compared to\nstate-of-the-art RL-based methods, while maintaining computational efficiency\nthrough selective LLM usage (averaging only 19.0\\% of total exploration steps)."
                },
                "authors": [
                    {
                        "name": "Jing Chang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Jinbin Huang"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Jianbin Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jianbin Qin"
                },
                "author": "Jianbin Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12385v2",
                "updated": "2025-07-18T07:52:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    52,
                    14,
                    4,
                    199,
                    0
                ],
                "published": "2024-06-18T08:15:52Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    8,
                    15,
                    52,
                    1,
                    170,
                    0
                ],
                "title": "Fast Graph Vector Search via Hardware Acceleration and\n  Delayed-Synchronization Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Graph Vector Search via Hardware Acceleration and\n  Delayed-Synchronization Traversal"
                },
                "summary": "Vector search systems are indispensable in large language model (LLM)\nserving, search engines, and recommender systems, where minimizing online\nsearch latency is essential. Among various algorithms, graph-based vector\nsearch (GVS) is particularly popular due to its high search performance and\nquality. However, reducing GVS latency by intra-query parallelization remains\nchallenging due to limitations imposed by both existing hardware architectures\n(CPUs and GPUs) and the inherent difficulty of parallelizing graph traversals.\nTo efficiently serve low-latency GVS, we co-design hardware and algorithm by\nproposing Falcon and Delayed-Synchronization Traversal (DST). Falcon is a\nhardware GVS accelerator that implements efficient GVS operators, pipelines\nthese operators, and reduces memory accesses by tracking search states with an\non-chip Bloom filter. DST is an efficient graph traversal algorithm that\nsimultaneously improves search performance and quality by relaxing traversal\norders to maximize accelerator utilization. Evaluation across various graphs\nand datasets shows that Falcon, prototyped on FPGAs, together with DST,\nachieves up to 4.3x and 19.5x lower latency and up to 8.0x and 26.9x\nimprovements in energy efficiency over CPU- and GPU-based GVS systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector search systems are indispensable in large language model (LLM)\nserving, search engines, and recommender systems, where minimizing online\nsearch latency is essential. Among various algorithms, graph-based vector\nsearch (GVS) is particularly popular due to its high search performance and\nquality. However, reducing GVS latency by intra-query parallelization remains\nchallenging due to limitations imposed by both existing hardware architectures\n(CPUs and GPUs) and the inherent difficulty of parallelizing graph traversals.\nTo efficiently serve low-latency GVS, we co-design hardware and algorithm by\nproposing Falcon and Delayed-Synchronization Traversal (DST). Falcon is a\nhardware GVS accelerator that implements efficient GVS operators, pipelines\nthese operators, and reduces memory accesses by tracking search states with an\non-chip Bloom filter. DST is an efficient graph traversal algorithm that\nsimultaneously improves search performance and quality by relaxing traversal\norders to maximize accelerator utilization. Evaluation across various graphs\nand datasets shows that Falcon, prototyped on FPGAs, together with DST,\nachieves up to 4.3x and 19.5x lower latency and up to 8.0x and 26.9x\nimprovements in energy efficiency over CPU- and GPU-based GVS systems."
                },
                "authors": [
                    {
                        "name": "Wenqi Jiang"
                    },
                    {
                        "name": "Hang Hu"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Gustavo Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Alonso"
                },
                "author": "Gustavo Alonso",
                "arxiv_doi": "10.14778/3749646.3749655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3749646.3749655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.12385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by VLDB'25",
                "arxiv_journal_ref": "Proceedings of the VLDB Endowment Volume 18, 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13205v2",
                "updated": "2025-07-18T07:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    49,
                    41,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-17T15:15:43Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    15,
                    43,
                    3,
                    198,
                    0
                ],
                "title": "Automatically assessing oral narratives of Afrikaans and isiXhosa\n  children",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically assessing oral narratives of Afrikaans and isiXhosa\n  children"
                },
                "summary": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning."
                },
                "authors": [
                    {
                        "name": "Retief Louw"
                    },
                    {
                        "name": "Emma Sharratt"
                    },
                    {
                        "name": "Febe de Wet"
                    },
                    {
                        "name": "Christiaan Jacobs"
                    },
                    {
                        "name": "Annelien Smith"
                    },
                    {
                        "name": "Herman Kamper"
                    }
                ],
                "author_detail": {
                    "name": "Herman Kamper"
                },
                "author": "Herman Kamper",
                "arxiv_comment": "Accepted to SLaTE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13710v1",
                "updated": "2025-07-18T07:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    43,
                    22,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T07:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    43,
                    22,
                    4,
                    199,
                    0
                ],
                "title": "CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for\n  Automated Data Preparation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for\n  Automated Data Preparation"
                },
                "summary": "Data preparation is a foundational yet notoriously challenging component of\nthe machine learning lifecycle, characterized by a vast combinatorial search\nspace of potential operator sequences. While reinforcement learning (RL) offers\na promising direction, existing approaches are inefficient as they fail to\ncapture the structured, hierarchical nature of the problem. We argue that\nHierarchical Reinforcement Learning (HRL), a paradigm that has been successful\nin other domains, provides a conceptually ideal yet previously unexplored\nframework for this task. However, a naive HRL implementation with a `hard\nhierarchy' is prone to suboptimal, irreversible decisions. To address this, we\nintroduce CogniQ-H, the first framework to implement a soft hierarchical\nparadigm for robust, end-to-end automated data preparation. CogniQ-H formulates\naction selection as a Bayesian inference problem. A high-level strategic prior,\ngenerated by a Large Language Model (LLM), guides exploration\nprobabilistically. This prior is synergistically combined with a fine-grained\noperator quality score from a supervised Learning-to-Rank (LTR) model and a\nlong-term value estimate from the agent's own Q-function. This hybrid\narchitecture allows CogniQ-H to balance strategic guidance with adaptive,\nevidence-based decision-making. Through extensive experiments on 18 diverse\ndatasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to\n13.9\\% improvement in pipeline quality and 2.8$\\times$ faster convergence\ncompared to state-of-the-art RL-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data preparation is a foundational yet notoriously challenging component of\nthe machine learning lifecycle, characterized by a vast combinatorial search\nspace of potential operator sequences. While reinforcement learning (RL) offers\na promising direction, existing approaches are inefficient as they fail to\ncapture the structured, hierarchical nature of the problem. We argue that\nHierarchical Reinforcement Learning (HRL), a paradigm that has been successful\nin other domains, provides a conceptually ideal yet previously unexplored\nframework for this task. However, a naive HRL implementation with a `hard\nhierarchy' is prone to suboptimal, irreversible decisions. To address this, we\nintroduce CogniQ-H, the first framework to implement a soft hierarchical\nparadigm for robust, end-to-end automated data preparation. CogniQ-H formulates\naction selection as a Bayesian inference problem. A high-level strategic prior,\ngenerated by a Large Language Model (LLM), guides exploration\nprobabilistically. This prior is synergistically combined with a fine-grained\noperator quality score from a supervised Learning-to-Rank (LTR) model and a\nlong-term value estimate from the agent's own Q-function. This hybrid\narchitecture allows CogniQ-H to balance strategic guidance with adaptive,\nevidence-based decision-making. Through extensive experiments on 18 diverse\ndatasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to\n13.9\\% improvement in pipeline quality and 2.8$\\times$ faster convergence\ncompared to state-of-the-art RL-based methods."
                },
                "authors": [
                    {
                        "name": "Jing Chang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Jinbin Huang"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Jianbin Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jianbin Qin"
                },
                "author": "Jianbin Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01729v2",
                "updated": "2025-07-18T07:43:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    43,
                    15,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-03T07:51:46Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    7,
                    51,
                    46,
                    5,
                    123,
                    0
                ],
                "title": "PosePilot: Steering Camera Pose for Generative World Models with\n  Self-supervised Depth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PosePilot: Steering Camera Pose for Generative World Models with\n  Self-supervised Depth"
                },
                "summary": "Recent advancements in autonomous driving (AD) systems have highlighted the\npotential of world models in achieving robust and generalizable performance\nacross both ordinary and challenging driving conditions. However, a key\nchallenge remains: precise and flexible camera pose control, which is crucial\nfor accurate viewpoint transformation and realistic simulation of scene\ndynamics. In this paper, we introduce PosePilot, a lightweight yet powerful\nframework that significantly enhances camera pose controllability in generative\nworld models. Drawing inspiration from self-supervised depth estimation,\nPosePilot leverages structure-from-motion principles to establish a tight\ncoupling between camera pose and video generation. Specifically, we incorporate\nself-supervised depth and pose readouts, allowing the model to infer depth and\nrelative camera motion directly from video sequences. These outputs drive\npose-aware frame warping, guided by a photometric warping loss that enforces\ngeometric consistency across synthesized frames. To further refine camera pose\nestimation, we introduce a reverse warping step and a pose regression loss,\nimproving viewpoint precision and adaptability. Extensive experiments on\nautonomous driving and general-domain video datasets demonstrate that PosePilot\nsignificantly enhances structural understanding and motion reasoning in both\ndiffusion-based and auto-regressive world models. By steering camera pose with\nself-supervised depth, PosePilot sets a new benchmark for pose controllability,\nenabling physically consistent, reliable viewpoint synthesis in generative\nworld models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in autonomous driving (AD) systems have highlighted the\npotential of world models in achieving robust and generalizable performance\nacross both ordinary and challenging driving conditions. However, a key\nchallenge remains: precise and flexible camera pose control, which is crucial\nfor accurate viewpoint transformation and realistic simulation of scene\ndynamics. In this paper, we introduce PosePilot, a lightweight yet powerful\nframework that significantly enhances camera pose controllability in generative\nworld models. Drawing inspiration from self-supervised depth estimation,\nPosePilot leverages structure-from-motion principles to establish a tight\ncoupling between camera pose and video generation. Specifically, we incorporate\nself-supervised depth and pose readouts, allowing the model to infer depth and\nrelative camera motion directly from video sequences. These outputs drive\npose-aware frame warping, guided by a photometric warping loss that enforces\ngeometric consistency across synthesized frames. To further refine camera pose\nestimation, we introduce a reverse warping step and a pose regression loss,\nimproving viewpoint precision and adaptability. Extensive experiments on\nautonomous driving and general-domain video datasets demonstrate that PosePilot\nsignificantly enhances structural understanding and motion reasoning in both\ndiffusion-based and auto-regressive world models. By steering camera pose with\nself-supervised depth, PosePilot sets a new benchmark for pose controllability,\nenabling physically consistent, reliable viewpoint synthesis in generative\nworld models."
                },
                "authors": [
                    {
                        "name": "Bu Jin"
                    },
                    {
                        "name": "Weize Li"
                    },
                    {
                        "name": "Baihan Yang"
                    },
                    {
                        "name": "Zhenxin Zhu"
                    },
                    {
                        "name": "Junpeng Jiang"
                    },
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Haiyang Sun"
                    },
                    {
                        "name": "Kun Zhan"
                    },
                    {
                        "name": "Hengtong Hu"
                    },
                    {
                        "name": "Xueyang Zhang"
                    },
                    {
                        "name": "Peng Jia"
                    },
                    {
                        "name": "Hao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhao"
                },
                "author": "Hao Zhao",
                "arxiv_comment": "Accepted at IEEE/RSJ IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00691v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00691v4",
                "updated": "2025-07-18T07:40:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    40,
                    22,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-02T06:32:23Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    6,
                    32,
                    23,
                    6,
                    33,
                    0
                ],
                "title": "To Code or not to Code? Adaptive Tool Integration for Math Language\n  Models via Expectation-Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Code or not to Code? Adaptive Tool Integration for Math Language\n  Models via Expectation-Maximization"
                },
                "summary": "Recent advances in mathematical problem-solving with language models (LMs)\nintegrate chain-of-thought (CoT) reasoning and code execution to harness their\ncomplementary strengths. However, existing hybrid frameworks exhibit a critical\nlimitation: they depend on externally dictated instructions or rigid\ncode-integration templates, lacking metacognitive awareness -- the capacity to\ndynamically evaluate intrinsic capabilities and autonomously determine when and\nhow to integrate tools. This rigidity motivates our study of autonomous code\nintegration, enabling models to adapt tool-usage strategies as their reasoning\nabilities evolve during training.\n  While reinforcement learning (RL) shows promise for boosting LLM reasoning at\nscale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning\nautonomous code integration due to inadequate exploration of the vast\ncombinatorial space of CoT-code interleaving patterns. To address this\nchallenge, we propose a novel Expectation-Maximization (EM) framework that\nsynergizes structured exploration (E-step) with off-policy RL optimization\n(M-step), creating a self-reinforcing cycle between metacognitive tool-use\ndecisions and evolving capabilities. Experiments reveal our method achieves\nsuperior results through improved exploration. Notably, our 7B model improves\nover 11% on MATH500 and 9.4% on AIME without o1-like CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in mathematical problem-solving with language models (LMs)\nintegrate chain-of-thought (CoT) reasoning and code execution to harness their\ncomplementary strengths. However, existing hybrid frameworks exhibit a critical\nlimitation: they depend on externally dictated instructions or rigid\ncode-integration templates, lacking metacognitive awareness -- the capacity to\ndynamically evaluate intrinsic capabilities and autonomously determine when and\nhow to integrate tools. This rigidity motivates our study of autonomous code\nintegration, enabling models to adapt tool-usage strategies as their reasoning\nabilities evolve during training.\n  While reinforcement learning (RL) shows promise for boosting LLM reasoning at\nscale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning\nautonomous code integration due to inadequate exploration of the vast\ncombinatorial space of CoT-code interleaving patterns. To address this\nchallenge, we propose a novel Expectation-Maximization (EM) framework that\nsynergizes structured exploration (E-step) with off-policy RL optimization\n(M-step), creating a self-reinforcing cycle between metacognitive tool-use\ndecisions and evolving capabilities. Experiments reveal our method achieves\nsuperior results through improved exploration. Notably, our 7B model improves\nover 11% on MATH500 and 9.4% on AIME without o1-like CoT."
                },
                "authors": [
                    {
                        "name": "Haozhe Wang"
                    },
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Fengming Zhu"
                    },
                    {
                        "name": "Weidi Xu"
                    },
                    {
                        "name": "Wei Chu"
                    },
                    {
                        "name": "Fangzhen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhen Lin"
                },
                "author": "Fangzhen Lin",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00691v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00691v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17562v2",
                "updated": "2025-07-18T07:39:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    39,
                    50,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-21T03:13:08Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    3,
                    13,
                    8,
                    5,
                    172,
                    0
                ],
                "title": "LLM-driven Medical Report Generation via Communication-efficient\n  Heterogeneous Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven Medical Report Generation via Communication-efficient\n  Heterogeneous Federated Learning"
                },
                "summary": "LLMs have demonstrated significant potential in Medical Report Generation\n(MRG), yet their development requires large amounts of medical image-report\npairs, which are commonly scattered across multiple centers. Centralizing these\ndata is exceptionally challenging due to privacy regulations, thereby impeding\nmodel development and broader adoption of LLM-driven MRG models. To address\nthis challenge, we present FedMRG, the first framework that leverages Federated\nLearning (FL) to enable privacy-preserving, multi-center development of\nLLM-driven MRG models, specifically designed to overcome the critical challenge\nof communication-efficient LLM training under multi-modal data heterogeneity.\nTo start with, our framework tackles the fundamental challenge of communication\noverhead in FL-LLM tuning by employing low-rank factorization to efficiently\ndecompose parameter updates, significantly reducing gradient transmission costs\nand making LLM-driven MRG feasible in bandwidth-constrained FL settings.\nFurthermore, we observed the dual heterogeneity in MRG under the FL scenario:\nvarying image characteristics across medical centers, as well as diverse\nreporting styles and terminology preferences. To address this, we further\nenhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,\ncoupled with diagnosis-driven prompts, which capture both globally\ngeneralizable and locally distinctive features while maintaining diagnostic\naccuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder\nthat harmonizes generic and specialized adapters to address variations in\nreporting styles and terminology. Through extensive evaluation of our\nestablished FL-MRG benchmark, we demonstrate the generalizability and\nadaptability of FedMRG, underscoring its potential in harnessing multi-center\ndata and generating clinically accurate reports while maintaining communication\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated significant potential in Medical Report Generation\n(MRG), yet their development requires large amounts of medical image-report\npairs, which are commonly scattered across multiple centers. Centralizing these\ndata is exceptionally challenging due to privacy regulations, thereby impeding\nmodel development and broader adoption of LLM-driven MRG models. To address\nthis challenge, we present FedMRG, the first framework that leverages Federated\nLearning (FL) to enable privacy-preserving, multi-center development of\nLLM-driven MRG models, specifically designed to overcome the critical challenge\nof communication-efficient LLM training under multi-modal data heterogeneity.\nTo start with, our framework tackles the fundamental challenge of communication\noverhead in FL-LLM tuning by employing low-rank factorization to efficiently\ndecompose parameter updates, significantly reducing gradient transmission costs\nand making LLM-driven MRG feasible in bandwidth-constrained FL settings.\nFurthermore, we observed the dual heterogeneity in MRG under the FL scenario:\nvarying image characteristics across medical centers, as well as diverse\nreporting styles and terminology preferences. To address this, we further\nenhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,\ncoupled with diagnosis-driven prompts, which capture both globally\ngeneralizable and locally distinctive features while maintaining diagnostic\naccuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder\nthat harmonizes generic and specialized adapters to address variations in\nreporting styles and terminology. Through extensive evaluation of our\nestablished FL-MRG benchmark, we demonstrate the generalizability and\nadaptability of FedMRG, underscoring its potential in harnessing multi-center\ndata and generating clinically accurate reports while maintaining communication\nefficiency."
                },
                "authors": [
                    {
                        "name": "Haoxuan Che"
                    },
                    {
                        "name": "Haibo Jin"
                    },
                    {
                        "name": "Zhengrui Guo"
                    },
                    {
                        "name": "Yi Lin"
                    },
                    {
                        "name": "Cheng Jin"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "arxiv_comment": "Accepted by IEEE TMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01312v2",
                "updated": "2025-07-18T07:37:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    37,
                    35,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-03T12:41:36Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    41,
                    36,
                    0,
                    34,
                    0
                ],
                "title": "CleanPose: Category-Level Object Pose Estimation via Causal Learning and\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanPose: Category-Level Object Pose Estimation via Causal Learning and\n  Knowledge Distillation"
                },
                "summary": "Category-level object pose estimation aims to recover the rotation,\ntranslation and size of unseen instances within predefined categories. In this\ntask, deep neural network-based methods have demonstrated remarkable\nperformance. However, previous studies show they suffer from spurious\ncorrelations raised by \"unclean\" confounders in models, hindering their\nperformance on novel instances with significant variations. To address this\nissue, we propose CleanPose, a novel approach integrating causal learning and\nknowledge distillation to enhance category-level pose estimation. To mitigate\nthe negative effect of unobserved confounders, we develop a causal inference\nmodule based on front-door adjustment, which promotes unbiased estimation by\nreducing potential spurious correlations. Additionally, to further improve\ngeneralization ability, we devise a residual-based knowledge distillation\nmethod that has proven effective in providing comprehensive category\ninformation guidance. Extensive experiments across multiple benchmarks\n(REAL275, CAMERA25 and HouseCat6D) hightlight the superiority of proposed\nCleanPose over state-of-the-art methods. Code will be available at\nhttps://github.com/chrislin0621/CleanPose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category-level object pose estimation aims to recover the rotation,\ntranslation and size of unseen instances within predefined categories. In this\ntask, deep neural network-based methods have demonstrated remarkable\nperformance. However, previous studies show they suffer from spurious\ncorrelations raised by \"unclean\" confounders in models, hindering their\nperformance on novel instances with significant variations. To address this\nissue, we propose CleanPose, a novel approach integrating causal learning and\nknowledge distillation to enhance category-level pose estimation. To mitigate\nthe negative effect of unobserved confounders, we develop a causal inference\nmodule based on front-door adjustment, which promotes unbiased estimation by\nreducing potential spurious correlations. Additionally, to further improve\ngeneralization ability, we devise a residual-based knowledge distillation\nmethod that has proven effective in providing comprehensive category\ninformation guidance. Extensive experiments across multiple benchmarks\n(REAL275, CAMERA25 and HouseCat6D) hightlight the superiority of proposed\nCleanPose over state-of-the-art methods. Code will be available at\nhttps://github.com/chrislin0621/CleanPose."
                },
                "authors": [
                    {
                        "name": "Xiao Lin"
                    },
                    {
                        "name": "Yun Peng"
                    },
                    {
                        "name": "Liuyi Wang"
                    },
                    {
                        "name": "Xianyou Zhong"
                    },
                    {
                        "name": "Minghao Zhu"
                    },
                    {
                        "name": "Jingwei Yang"
                    },
                    {
                        "name": "Yi Feng"
                    },
                    {
                        "name": "Chengju Liu"
                    },
                    {
                        "name": "Qijun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qijun Chen"
                },
                "author": "Qijun Chen",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13207v2",
                "updated": "2025-07-18T07:35:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    35,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-17T15:16:30Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    16,
                    30,
                    3,
                    198,
                    0
                ],
                "title": "MoTM: Towards a Foundation Model for Time Series Imputation based on\n  Continuous Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoTM: Towards a Foundation Model for Time Series Imputation based on\n  Continuous Modeling"
                },
                "summary": "Recent years have witnessed a growing interest for time series foundation\nmodels, with a strong emphasis on the forecasting task. Yet, the crucial task\nof out-of-domain imputation of missing values remains largely underexplored. We\npropose a first step to fill this gap by leveraging implicit neural\nrepresentations (INRs). INRs model time series as continuous functions and\nnaturally handle various missing data scenarios and sampling rates. While they\nhave shown strong performance within specific distributions, they struggle\nunder distribution shifts. To address this, we introduce MoTM (Mixture of\nTimeflow Models), a step toward a foundation model for time series imputation.\nBuilding on the idea that a new time series is a mixture of previously seen\npatterns, MoTM combines a basis of INRs, each trained independently on a\ndistinct family of time series, with a ridge regressor that adapts to the\nobserved context at inference. We demonstrate robust in-domain and\nout-of-domain generalization across diverse imputation scenarios (e.g., block\nand pointwise missingness, variable sampling rates), paving the way for\nadaptable foundation imputation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed a growing interest for time series foundation\nmodels, with a strong emphasis on the forecasting task. Yet, the crucial task\nof out-of-domain imputation of missing values remains largely underexplored. We\npropose a first step to fill this gap by leveraging implicit neural\nrepresentations (INRs). INRs model time series as continuous functions and\nnaturally handle various missing data scenarios and sampling rates. While they\nhave shown strong performance within specific distributions, they struggle\nunder distribution shifts. To address this, we introduce MoTM (Mixture of\nTimeflow Models), a step toward a foundation model for time series imputation.\nBuilding on the idea that a new time series is a mixture of previously seen\npatterns, MoTM combines a basis of INRs, each trained independently on a\ndistinct family of time series, with a ridge regressor that adapts to the\nobserved context at inference. We demonstrate robust in-domain and\nout-of-domain generalization across diverse imputation scenarios (e.g., block\nand pointwise missingness, variable sampling rates), paving the way for\nadaptable foundation imputation models."
                },
                "authors": [
                    {
                        "name": "Etienne Le Naour"
                    },
                    {
                        "name": "Tahar Nabil"
                    },
                    {
                        "name": "Ghislain Agoua"
                    }
                ],
                "author_detail": {
                    "name": "Ghislain Agoua"
                },
                "author": "Ghislain Agoua",
                "arxiv_comment": "10th Workshop on Advanced Analytics and Learning on Temporal Data\n  (AALTD), ECML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17735v2",
                "updated": "2025-07-18T07:34:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    34,
                    40,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-23T10:56:06Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    56,
                    6,
                    4,
                    143,
                    0
                ],
                "title": "SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator"
                },
                "summary": "Large Language Model (LLM)-based agents are increasingly deployed in\nreal-world applications such as \"digital assistants, autonomous customer\nservice, and decision-support systems\", where their ability to \"interact in\nmulti-turn, tool-augmented environments\" makes them indispensable. However,\nensuring the safety of these agents remains a significant challenge due to the\ndiverse and complex risks arising from dynamic user interactions, external tool\nusage, and the potential for unintended harmful behaviors. To address this\ncritical issue, we propose AutoSafe, the first framework that systematically\nenhances agent safety through fully automated synthetic data generation.\nConcretely, 1) we introduce an open and extensible threat model, OTS, which\nformalizes how unsafe behaviors emerge from the interplay of user instructions,\ninteraction contexts, and agent actions. This enables precise modeling of\nsafety risks across diverse scenarios. 2) we develop a fully automated data\ngeneration pipeline that simulates unsafe user behaviors, applies\nself-reflective reasoning to generate safe responses, and constructs a\nlarge-scale, diverse, and high-quality safety training dataset-eliminating the\nneed for hazardous real-world data collection. To evaluate the effectiveness of\nour framework, we design comprehensive experiments on both synthetic and\nreal-world safety benchmarks. Results demonstrate that AutoSafe boosts safety\nscores by 45% on average and achieves a 28.91% improvement on real-world tasks,\nvalidating the generalization ability of our learned safety strategies. These\nresults highlight the practical advancement and scalability of AutoSafe in\nbuilding safer LLM-based agents for real-world deployment. We have released the\nproject page at https://auto-safe.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents are increasingly deployed in\nreal-world applications such as \"digital assistants, autonomous customer\nservice, and decision-support systems\", where their ability to \"interact in\nmulti-turn, tool-augmented environments\" makes them indispensable. However,\nensuring the safety of these agents remains a significant challenge due to the\ndiverse and complex risks arising from dynamic user interactions, external tool\nusage, and the potential for unintended harmful behaviors. To address this\ncritical issue, we propose AutoSafe, the first framework that systematically\nenhances agent safety through fully automated synthetic data generation.\nConcretely, 1) we introduce an open and extensible threat model, OTS, which\nformalizes how unsafe behaviors emerge from the interplay of user instructions,\ninteraction contexts, and agent actions. This enables precise modeling of\nsafety risks across diverse scenarios. 2) we develop a fully automated data\ngeneration pipeline that simulates unsafe user behaviors, applies\nself-reflective reasoning to generate safe responses, and constructs a\nlarge-scale, diverse, and high-quality safety training dataset-eliminating the\nneed for hazardous real-world data collection. To evaluate the effectiveness of\nour framework, we design comprehensive experiments on both synthetic and\nreal-world safety benchmarks. Results demonstrate that AutoSafe boosts safety\nscores by 45% on average and achieves a 28.91% improvement on real-world tasks,\nvalidating the generalization ability of our learned safety strategies. These\nresults highlight the practical advancement and scalability of AutoSafe in\nbuilding safer LLM-based agents for real-world deployment. We have released the\nproject page at https://auto-safe.github.io/."
                },
                "authors": [
                    {
                        "name": "Xueyang Zhou"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Lin Lu"
                    },
                    {
                        "name": "Jiawen Shi"
                    },
                    {
                        "name": "Guiyao Tie"
                    },
                    {
                        "name": "Yongtian Xu"
                    },
                    {
                        "name": "Lixing Chen"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "38 pages;12 figures;12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.14121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14121v1",
                "updated": "2025-07-18T17:50:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    50,
                    51,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:50:51Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    50,
                    51,
                    4,
                    199,
                    0
                ],
                "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical\n  Perspective"
                },
                "summary": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios."
                },
                "authors": [
                    {
                        "name": "Pankaj Yadav"
                    },
                    {
                        "name": "Vivek Vijay"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Vijay"
                },
                "author": "Vivek Vijay",
                "arxiv_comment": "9 Pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03304v2",
                "updated": "2025-07-18T17:50:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    50,
                    4,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-05T16:03:17Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    3,
                    17,
                    2,
                    36,
                    0
                ],
                "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning"
                },
                "summary": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose Divergence-driven\nZeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer\nadaptation by incorporating projections to ZO updates, generating\ndiverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning. Our code is released at\nhttps://anonymous.4open.science/r/DiZO-E86D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose Divergence-driven\nZeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer\nadaptation by incorporating projections to ZO updates, generating\ndiverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning. Our code is released at\nhttps://anonymous.4open.science/r/DiZO-E86D."
                },
                "authors": [
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Caiwei Ding"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Jaewoo Lee"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14111v1",
                "updated": "2025-07-18T17:43:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    43,
                    56,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:43:56Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    43,
                    56,
                    4,
                    199,
                    0
                ],
                "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning"
                },
                "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources."
                },
                "authors": [
                    {
                        "name": "Xiaoya Li"
                    },
                    {
                        "name": "Xiaofei Sun"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Chris Shum"
                    }
                ],
                "author_detail": {
                    "name": "Chris Shum"
                },
                "author": "Chris Shum",
                "arxiv_comment": "Preprint Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14107v1",
                "updated": "2025-07-18T17:39:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    39,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:39:03Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    39,
                    3,
                    4,
                    199,
                    0
                ],
                "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps\n  Using Large Language Models for Bridge Condition Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Interpretation of Non-Destructive Evaluation Contour Maps\n  Using Large Language Models for Bridge Condition Assessment"
                },
                "summary": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments."
                },
                "authors": [
                    {
                        "name": "Viraj Nishesh Darji"
                    },
                    {
                        "name": "Callie C. Liao"
                    },
                    {
                        "name": "Duoduo Liao"
                    }
                ],
                "author_detail": {
                    "name": "Duoduo Liao"
                },
                "author": "Duoduo Liao",
                "arxiv_doi": "10.1109/BigData62323.2024.10825532",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/BigData62323.2024.10825532",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.14107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE BigData, Year: 2024; Page: 3258-3263",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14096v1",
                "updated": "2025-07-18T17:23:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    23,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:23:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    23,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts\n  (PLABA) track",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts\n  (PLABA) track"
                },
                "summary": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools."
                },
                "authors": [
                    {
                        "name": "Brian Ondov"
                    },
                    {
                        "name": "William Xia"
                    },
                    {
                        "name": "Kush Attal"
                    },
                    {
                        "name": "Ishita Unde"
                    },
                    {
                        "name": "Jerry He"
                    },
                    {
                        "name": "Hoa Dang"
                    },
                    {
                        "name": "Ian Soboroff"
                    },
                    {
                        "name": "Dina Demner-Fushman"
                    }
                ],
                "author_detail": {
                    "name": "Dina Demner-Fushman"
                },
                "author": "Dina Demner-Fushman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02768v2",
                "updated": "2025-07-18T17:21:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    21,
                    42,
                    4,
                    199,
                    0
                ],
                "published": "2025-04-03T17:05:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    5,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs"
                },
                "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages"
                },
                "authors": [
                    {
                        "name": "Jaap Jumelet"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Joakim Nivre"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12272v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12272v5",
                "updated": "2025-07-18T17:21:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    21,
                    27,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-17T19:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    19,
                    16,
                    37,
                    0,
                    48,
                    0
                ],
                "title": "Learning to Reason at the Frontier of Learnability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason at the Frontier of Learnability"
                },
                "summary": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs."
                },
                "authors": [
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Anya Sims"
                    },
                    {
                        "name": "Johannes Forkel"
                    },
                    {
                        "name": "Mattie Fellows"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12272v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12272v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14088v1",
                "updated": "2025-07-18T17:13:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    13,
                    21,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:13:21Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    13,
                    21,
                    4,
                    199,
                    0
                ],
                "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time\n  Human-AI Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time\n  Human-AI Collaboration"
                },
                "summary": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system."
                },
                "authors": [
                    {
                        "name": "Xiyun Li"
                    },
                    {
                        "name": "Yining Ding"
                    },
                    {
                        "name": "Yuhua Jiang"
                    },
                    {
                        "name": "Yunlong Zhao"
                    },
                    {
                        "name": "Runpeng Xie"
                    },
                    {
                        "name": "Shuang Xu"
                    },
                    {
                        "name": "Yuanhua Ni"
                    },
                    {
                        "name": "Yiqin Yang"
                    },
                    {
                        "name": "Bo Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Xu"
                },
                "author": "Bo Xu",
                "arxiv_journal_ref": "cogsci-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04617v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04617v3",
                "updated": "2025-07-18T17:06:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    6,
                    0,
                    4,
                    199,
                    0
                ],
                "published": "2024-09-06T21:00:57Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    21,
                    0,
                    57,
                    4,
                    250,
                    0
                ],
                "title": "Sparse Rewards Can Self-Train Dialogue Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Rewards Can Self-Train Dialogue Agents"
                },
                "summary": "Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)\nagents, especially in multi-turn dialogue tasks, have been primarily driven by\nsupervised fine-tuning and high-quality human feedback. However, as base LLM\nmodels continue to improve, acquiring meaningful human feedback has become\nincreasingly challenging and costly. In certain domains, base LLM agents may\neventually exceed human capabilities, making traditional feedback-driven\nmethods impractical. In this paper, we introduce a novel self-improvement\nparadigm that empowers LLM agents to autonomously enhance their performance\nwithout external human feedback. Our method, Juxtaposed Outcomes for Simulation\nHarvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward\nsimulation environment to extract ideal behaviors and further train the LLM on\nits own outputs. We present ToolWOZ, a sparse reward tool-calling simulation\nenvironment derived from MultiWOZ. We demonstrate that models trained with\nJOSH, both small and frontier, significantly improve tool-based interactions\nwhile preserving general model capabilities across diverse benchmarks. Our code\nand data are publicly available on GitHub at\nhttps://github.com/asappresearch/josh-llm-simulation-training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)\nagents, especially in multi-turn dialogue tasks, have been primarily driven by\nsupervised fine-tuning and high-quality human feedback. However, as base LLM\nmodels continue to improve, acquiring meaningful human feedback has become\nincreasingly challenging and costly. In certain domains, base LLM agents may\neventually exceed human capabilities, making traditional feedback-driven\nmethods impractical. In this paper, we introduce a novel self-improvement\nparadigm that empowers LLM agents to autonomously enhance their performance\nwithout external human feedback. Our method, Juxtaposed Outcomes for Simulation\nHarvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward\nsimulation environment to extract ideal behaviors and further train the LLM on\nits own outputs. We present ToolWOZ, a sparse reward tool-calling simulation\nenvironment derived from MultiWOZ. We demonstrate that models trained with\nJOSH, both small and frontier, significantly improve tool-based interactions\nwhile preserving general model capabilities across diverse benchmarks. Our code\nand data are publicly available on GitHub at\nhttps://github.com/asappresearch/josh-llm-simulation-training"
                },
                "authors": [
                    {
                        "name": "Barrett Martin Lattimer"
                    },
                    {
                        "name": "Varun Gangal"
                    },
                    {
                        "name": "Ryan McDonald"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "Accepted to ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04617v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04617v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14079v1",
                "updated": "2025-07-18T17:00:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    0,
                    27,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T17:00:27Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    0,
                    27,
                    4,
                    199,
                    0
                ],
                "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of\n  Heterogeneous Clinical Notes Across Hospital Visits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of\n  Heterogeneous Clinical Notes Across Hospital Visits"
                },
                "summary": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings."
                },
                "authors": [
                    {
                        "name": "Garapati Keerthana"
                    },
                    {
                        "name": "Manik Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Manik Gupta"
                },
                "author": "Manik Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11200v2",
                "updated": "2025-07-18T16:56:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    56,
                    2,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-15T11:12:39Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    12,
                    39,
                    1,
                    196,
                    0
                ],
                "title": "How Far Have Medical Vision-Language Models Come? A Comprehensive\n  Benchmarking Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Have Medical Vision-Language Models Come? A Comprehensive\n  Benchmarking Study"
                },
                "summary": "Vision-Language Models (VLMs) trained on web-scale corpora excel at natural\nimage tasks and are increasingly repurposed for healthcare; however, their\ncompetence in medical tasks remains underexplored. We present a comprehensive\nevaluation of open-source general-purpose and medically specialised VLMs,\nranging from 3B to 72B parameters, across eight benchmarks: MedXpert,\nOmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model\nperformance across different aspects, we first separate it into understanding\nand reasoning components. Three salient findings emerge. First, large\ngeneral-purpose models already match or surpass medical-specific counterparts\non several benchmarks, demonstrating strong zero-shot transfer from natural to\nmedical images. Second, reasoning performance is consistently lower than\nunderstanding, highlighting a critical barrier to safe decision support. Third,\nperformance varies widely across benchmarks, reflecting differences in task\ndesign, annotation quality, and knowledge demands. No model yet reaches the\nreliability threshold for clinical deployment, underscoring the need for\nstronger multimodal alignment and more rigorous, fine-grained evaluation\nprotocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) trained on web-scale corpora excel at natural\nimage tasks and are increasingly repurposed for healthcare; however, their\ncompetence in medical tasks remains underexplored. We present a comprehensive\nevaluation of open-source general-purpose and medically specialised VLMs,\nranging from 3B to 72B parameters, across eight benchmarks: MedXpert,\nOmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model\nperformance across different aspects, we first separate it into understanding\nand reasoning components. Three salient findings emerge. First, large\ngeneral-purpose models already match or surpass medical-specific counterparts\non several benchmarks, demonstrating strong zero-shot transfer from natural to\nmedical images. Second, reasoning performance is consistently lower than\nunderstanding, highlighting a critical barrier to safe decision support. Third,\nperformance varies widely across benchmarks, reflecting differences in task\ndesign, annotation quality, and knowledge demands. No model yet reaches the\nreliability threshold for clinical deployment, underscoring the need for\nstronger multimodal alignment and more rigorous, fine-grained evaluation\nprotocols."
                },
                "authors": [
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Jiazhen Pan"
                    },
                    {
                        "name": "Weixiang Shen"
                    },
                    {
                        "name": "Wenjia Bai"
                    },
                    {
                        "name": "Daniel Rueckert"
                    },
                    {
                        "name": "Rossella Arcucci"
                    }
                ],
                "author_detail": {
                    "name": "Rossella Arcucci"
                },
                "author": "Rossella Arcucci",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14069v1",
                "updated": "2025-07-18T16:47:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    47,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:47:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    47,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "Edge Intelligence with Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Intelligence with Spiking Neural Networks"
                },
                "summary": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence."
                },
                "authors": [
                    {
                        "name": "Shuiguang Deng"
                    },
                    {
                        "name": "Di Yu"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Xin Du"
                    },
                    {
                        "name": "Linshan Jiang"
                    },
                    {
                        "name": "Xiaofan Zhao"
                    },
                    {
                        "name": "Wentao Tong"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Weijia Fang"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Gang Pan"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Albert Y. Zomaya"
                    }
                ],
                "author_detail": {
                    "name": "Albert Y. Zomaya"
                },
                "author": "Albert Y. Zomaya",
                "arxiv_comment": "This work has been submitted to Proceeding of IEEE for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12723v2",
                "updated": "2025-07-18T16:43:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    43,
                    12,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-19T05:25:29Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    25,
                    29,
                    0,
                    139,
                    0
                ],
                "title": "On-Policy Optimization with Group Equivalent Preference for\n  Multi-Programming Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Policy Optimization with Group Equivalent Preference for\n  Multi-Programming Language Understanding"
                },
                "summary": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages."
                },
                "authors": [
                    {
                        "name": "Haoyuan Wu"
                    },
                    {
                        "name": "Rui Ming"
                    },
                    {
                        "name": "Jilong Gao"
                    },
                    {
                        "name": "Hangyu Zhao"
                    },
                    {
                        "name": "Xueyi Chen"
                    },
                    {
                        "name": "Yikai Yang"
                    },
                    {
                        "name": "Haisheng Zheng"
                    },
                    {
                        "name": "Zhuolun He"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23086v2",
                "updated": "2025-07-18T16:19:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    19,
                    31,
                    4,
                    199,
                    0
                ],
                "published": "2024-10-30T15:02:54Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    2,
                    54,
                    2,
                    304,
                    0
                ],
                "title": "Towards Practical Operation of Deep Reinforcement Learning Agents in\n  Real-World Network Management at Open RAN Edges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Practical Operation of Deep Reinforcement Learning Agents in\n  Real-World Network Management at Open RAN Edges"
                },
                "summary": "Deep Reinforcement Learning (DRL) has emerged as a powerful solution for\nmeeting the growing demands for connectivity, reliability, low latency and\noperational efficiency in advanced networks. However, most research has focused\non theoretical analysis and simulations, with limited investigation into\nreal-world deployment. To bridge the gap and support practical DRL deployment\nfor network management, we first present an orchestration framework that\nintegrates ETSI Multi-access Edge Computing (MEC) with Open RAN, enabling\nseamless adoption of DRL-based strategies across different time scales while\nenhancing agent lifecycle management. We then identify three critical\nchallenges hindering DRL's real-world deployment, including (1) asynchronous\nrequests from unpredictable or bursty traffic, (2) adaptability and\ngeneralization across heterogeneous topologies and evolving service demands,\nand (3) prolonged convergence and service interruptions due to exploration in\nlive operational environments. To address these challenges, we propose a\nthree-fold solution strategy: (a) advanced time-series integration for handling\nasynchronized traffic, (b) flexible architecture design such as multi-agent DRL\nand incremental learning to support heterogeneous scenarios, and (c)\nsimulation-driven deployment with transfer learning to reduce convergence time\nand service disruptions. Lastly, the feasibility of the MEC-O-RAN architecture\nis validated on an urban-wide testing infrastructure, and two real-world use\ncases are presented, showcasing the three identified challenges and\ndemonstrating the effectiveness of the proposed solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reinforcement Learning (DRL) has emerged as a powerful solution for\nmeeting the growing demands for connectivity, reliability, low latency and\noperational efficiency in advanced networks. However, most research has focused\non theoretical analysis and simulations, with limited investigation into\nreal-world deployment. To bridge the gap and support practical DRL deployment\nfor network management, we first present an orchestration framework that\nintegrates ETSI Multi-access Edge Computing (MEC) with Open RAN, enabling\nseamless adoption of DRL-based strategies across different time scales while\nenhancing agent lifecycle management. We then identify three critical\nchallenges hindering DRL's real-world deployment, including (1) asynchronous\nrequests from unpredictable or bursty traffic, (2) adaptability and\ngeneralization across heterogeneous topologies and evolving service demands,\nand (3) prolonged convergence and service interruptions due to exploration in\nlive operational environments. To address these challenges, we propose a\nthree-fold solution strategy: (a) advanced time-series integration for handling\nasynchronized traffic, (b) flexible architecture design such as multi-agent DRL\nand incremental learning to support heterogeneous scenarios, and (c)\nsimulation-driven deployment with transfer learning to reduce convergence time\nand service disruptions. Lastly, the feasibility of the MEC-O-RAN architecture\nis validated on an urban-wide testing infrastructure, and two real-world use\ncases are presented, showcasing the three identified challenges and\ndemonstrating the effectiveness of the proposed solutions."
                },
                "authors": [
                    {
                        "name": "Haiyuan Li"
                    },
                    {
                        "name": "Hari Madhukumar"
                    },
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Yuelin Liu"
                    },
                    {
                        "name": "Yiran Teng"
                    },
                    {
                        "name": "Yulei Wu"
                    },
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Shuangyi Yan"
                    },
                    {
                        "name": "Dimitra Simeonidou"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Simeonidou"
                },
                "author": "Dimitra Simeonidou",
                "arxiv_doi": "10.1109/MCOM.001.2500207",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MCOM.001.2500207",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.23086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13394v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13394v2",
                "updated": "2025-07-18T16:14:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    14,
                    51,
                    4,
                    199,
                    0
                ],
                "published": "2024-10-17T09:45:32Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    9,
                    45,
                    32,
                    3,
                    291,
                    0
                ],
                "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs"
                },
                "summary": "Evaluating machine-generated text remains a significant challenge in NLP,\nespecially for non-English languages. Current methodologies, including\nautomated metrics, human assessments, and LLM-based evaluations, predominantly\nfocus on English, revealing a significant gap in multilingual evaluation\nframeworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an\nextensible framework that includes evaluator LLMs (Hercule) and a novel test\nset (Recon) specifically designed for multilingual evaluation. Our test set\nfeatures 500 human-annotated instructions spanning various task capabilities\nalong with human judgment scores across six languages. This would enable\nbenchmarking of general-purpose multilingual LLMs and facilitate\nmeta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a\ncross-lingual evaluation model that addresses the scarcity of reference answers\nin the target language by learning to assign scores to responses based on\neasily available reference answers in English. Our experiments demonstrate that\nHercule aligns more closely with human judgments compared to proprietary\nmodels, demonstrating the effectiveness of such cross-lingual evaluation in low\nresource scenarios. Further, it is also effective in zero-shot evaluation on\nunseen languages. This study is the first comprehensive examination of\ncross-lingual evaluation using LLMs, presenting a scalable and effective\napproach for multilingual assessment. All code, datasets, and models will be\npublicly available to enable further research in this important area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating machine-generated text remains a significant challenge in NLP,\nespecially for non-English languages. Current methodologies, including\nautomated metrics, human assessments, and LLM-based evaluations, predominantly\nfocus on English, revealing a significant gap in multilingual evaluation\nframeworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an\nextensible framework that includes evaluator LLMs (Hercule) and a novel test\nset (Recon) specifically designed for multilingual evaluation. Our test set\nfeatures 500 human-annotated instructions spanning various task capabilities\nalong with human judgment scores across six languages. This would enable\nbenchmarking of general-purpose multilingual LLMs and facilitate\nmeta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a\ncross-lingual evaluation model that addresses the scarcity of reference answers\nin the target language by learning to assign scores to responses based on\neasily available reference answers in English. Our experiments demonstrate that\nHercule aligns more closely with human judgments compared to proprietary\nmodels, demonstrating the effectiveness of such cross-lingual evaluation in low\nresource scenarios. Further, it is also effective in zero-shot evaluation on\nunseen languages. This study is the first comprehensive examination of\ncross-lingual evaluation using LLMs, presenting a scalable and effective\napproach for multilingual assessment. All code, datasets, and models will be\npublicly available to enable further research in this important area."
                },
                "authors": [
                    {
                        "name": "Sumanth Doddapaneni"
                    },
                    {
                        "name": "Mohammed Safi Ur Rahman Khan"
                    },
                    {
                        "name": "Dilip Venkatesh"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Anoop Kunchukuttan"
                    },
                    {
                        "name": "Mitesh M. Khapra"
                    }
                ],
                "author_detail": {
                    "name": "Mitesh M. Khapra"
                },
                "author": "Mitesh M. Khapra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13394v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13394v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14045v1",
                "updated": "2025-07-18T16:13:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    13,
                    35,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:13:35Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    13,
                    35,
                    4,
                    199,
                    0
                ],
                "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in\n  Benchmark Biomedical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in\n  Benchmark Biomedical Tasks"
                },
                "summary": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications."
                },
                "authors": [
                    {
                        "name": "Israt Jahan"
                    },
                    {
                        "name": "Md Tahmid Rahman Laskar"
                    },
                    {
                        "name": "Chun Peng"
                    },
                    {
                        "name": "Jimmy Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Huang"
                },
                "author": "Jimmy Huang",
                "arxiv_comment": "Accepted at Canadian AI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14034v1",
                "updated": "2025-07-18T16:06:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    6,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:06:03Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    6,
                    3,
                    4,
                    199,
                    0
                ],
                "title": "Architecting Human-AI Cocreation for Technical Services -- Interaction\n  Modes and Contingency Factors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Human-AI Cocreation for Technical Services -- Interaction\n  Modes and Contingency Factors"
                },
                "summary": "Agentic AI systems, powered by Large Language Models (LLMs), offer\ntransformative potential for value co-creation in technical services. However,\npersistent challenges like hallucinations and operational brittleness limit\ntheir autonomous use, creating a critical need for robust frameworks to guide\nhuman-AI collaboration. Drawing on established Human-AI teaming research and\nanalogies from fields like autonomous driving, this paper develops a structured\ntaxonomy of human-agent interaction. Based on case study research within\ntechnical support platforms, we propose a six-mode taxonomy that organizes\ncollaboration across a spectrum of AI autonomy. This spectrum is anchored by\nthe Human-Out-of-the-Loop (HOOTL) model for full automation and the\nHuman-Augmented Model (HAM) for passive AI assistance. Between these poles, the\nframework specifies four distinct intermediate structures. These include the\nHuman-in-Command (HIC) model, where AI proposals re-quire mandatory human\napproval, and the Human-in-the-Process (HITP) model for structured work-flows\nwith deterministic human tasks. The taxonomy further delineates the\nHuman-in-the-Loop (HITL) model, which facilitates agent-initiated escalation\nupon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables\ndiscretionary human oversight of an autonomous AI. The primary contribution of\nthis work is a comprehensive framework that connects this taxonomy to key\ncontingency factors -- such as task complexity, operational risk, and system\nreliability -- and their corresponding conceptual architectures. By providing a\nsystematic method for selecting and designing an appropriate level of human\noversight, our framework offers practitioners a crucial tool to navigate the\ntrade-offs between automation and control, thereby fostering the development of\nsafer, more effective, and context-aware technical service systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems, powered by Large Language Models (LLMs), offer\ntransformative potential for value co-creation in technical services. However,\npersistent challenges like hallucinations and operational brittleness limit\ntheir autonomous use, creating a critical need for robust frameworks to guide\nhuman-AI collaboration. Drawing on established Human-AI teaming research and\nanalogies from fields like autonomous driving, this paper develops a structured\ntaxonomy of human-agent interaction. Based on case study research within\ntechnical support platforms, we propose a six-mode taxonomy that organizes\ncollaboration across a spectrum of AI autonomy. This spectrum is anchored by\nthe Human-Out-of-the-Loop (HOOTL) model for full automation and the\nHuman-Augmented Model (HAM) for passive AI assistance. Between these poles, the\nframework specifies four distinct intermediate structures. These include the\nHuman-in-Command (HIC) model, where AI proposals re-quire mandatory human\napproval, and the Human-in-the-Process (HITP) model for structured work-flows\nwith deterministic human tasks. The taxonomy further delineates the\nHuman-in-the-Loop (HITL) model, which facilitates agent-initiated escalation\nupon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables\ndiscretionary human oversight of an autonomous AI. The primary contribution of\nthis work is a comprehensive framework that connects this taxonomy to key\ncontingency factors -- such as task complexity, operational risk, and system\nreliability -- and their corresponding conceptual architectures. By providing a\nsystematic method for selecting and designing an appropriate level of human\noversight, our framework offers practitioners a crucial tool to navigate the\ntrade-offs between automation and control, thereby fostering the development of\nsafer, more effective, and context-aware technical service systems."
                },
                "authors": [
                    {
                        "name": "Jochen Wulf"
                    },
                    {
                        "name": "Jurg Meierhofer"
                    },
                    {
                        "name": "Frank Hannich"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hannich"
                },
                "author": "Frank Hannich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14032v1",
                "updated": "2025-07-18T16:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    0,
                    11,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T16:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    0,
                    11,
                    4,
                    199,
                    0
                ],
                "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language\n  Models"
                },
                "summary": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale."
                },
                "authors": [
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Erika Barcelos"
                    },
                    {
                        "name": "Roger French"
                    },
                    {
                        "name": "Yinghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yinghui Wu"
                },
                "author": "Yinghui Wu",
                "arxiv_comment": "Accepted to the 24th International Semantic Web Conference Research\n  Track (ISWC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06049v3",
                "updated": "2025-07-18T15:52:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    52,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2023-04-11T19:52:30Z",
                "published_parsed": [
                    2023,
                    4,
                    11,
                    19,
                    52,
                    30,
                    1,
                    101,
                    0
                ],
                "title": "Equivalent and Compact Representations of Neural Network Controllers\n  With Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equivalent and Compact Representations of Neural Network Controllers\n  With Decision Trees"
                },
                "summary": "Over the past decade, neural network (NN)-based controllers have demonstrated\nremarkable efficacy in a variety of decision-making tasks. However, their\nblack-box nature and the risk of unexpected behaviors pose a challenge to their\ndeployment in real-world systems requiring strong guarantees of correctness and\nsafety. We address these limitations by investigating the transformation of\nNN-based controllers into equivalent soft decision tree (SDT)-based controllers\nand its impact on verifiability. In contrast to existing work, we focus on\ndiscrete-output NN controllers including rectified linear unit (ReLU)\nactivation functions as well as argmax operations. We then devise an exact yet\nefficient transformation algorithm which automatically prunes redundant\nbranches. We first demonstrate the practical efficacy of the transformation\nalgorithm applied to an autonomous driving NN controller within OpenAI Gym's\nCarRacing environment. Subsequently, we evaluate our approach using two\nbenchmarks from the OpenAI Gym environment. Our results indicate that the SDT\ntransformation can benefit formal verification, showing runtime improvements of\nup to $21 \\times$ and $2 \\times$ for MountainCar-v0 and CartPole-v1,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, neural network (NN)-based controllers have demonstrated\nremarkable efficacy in a variety of decision-making tasks. However, their\nblack-box nature and the risk of unexpected behaviors pose a challenge to their\ndeployment in real-world systems requiring strong guarantees of correctness and\nsafety. We address these limitations by investigating the transformation of\nNN-based controllers into equivalent soft decision tree (SDT)-based controllers\nand its impact on verifiability. In contrast to existing work, we focus on\ndiscrete-output NN controllers including rectified linear unit (ReLU)\nactivation functions as well as argmax operations. We then devise an exact yet\nefficient transformation algorithm which automatically prunes redundant\nbranches. We first demonstrate the practical efficacy of the transformation\nalgorithm applied to an autonomous driving NN controller within OpenAI Gym's\nCarRacing environment. Subsequently, we evaluate our approach using two\nbenchmarks from the OpenAI Gym environment. Our results indicate that the SDT\ntransformation can benefit formal verification, showing runtime improvements of\nup to $21 \\times$ and $2 \\times$ for MountainCar-v0 and CartPole-v1,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Kevin Chang"
                    },
                    {
                        "name": "Nathan Dahlin"
                    },
                    {
                        "name": "Rahul Jain"
                    },
                    {
                        "name": "Pierluigi Nuzzo"
                    }
                ],
                "author_detail": {
                    "name": "Pierluigi Nuzzo"
                },
                "author": "Pierluigi Nuzzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14017v1",
                "updated": "2025-07-18T15:31:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    31,
                    16,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T15:31:16Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    31,
                    16,
                    4,
                    199,
                    0
                ],
                "title": "Efficient Temporal Tokenization for Mobility Prediction with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Temporal Tokenization for Mobility Prediction with Large\n  Language Models"
                },
                "summary": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haoyu He"
                    },
                    {
                        "name": "Haozheng Luo"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Qi R. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qi R. Wang"
                },
                "author": "Qi R. Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14000v1",
                "updated": "2025-07-18T15:14:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    14,
                    56,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T15:14:56Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    14,
                    56,
                    4,
                    199,
                    0
                ],
                "title": "Photonic Fabric Platform for AI Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic Fabric Platform for AI Accelerators"
                },
                "summary": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute."
                },
                "authors": [
                    {
                        "name": "Jing Ding"
                    },
                    {
                        "name": "Trung Diep"
                    }
                ],
                "author_detail": {
                    "name": "Trung Diep"
                },
                "author": "Trung Diep",
                "arxiv_comment": "12 pages, 14 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09046v2",
                "updated": "2025-07-18T14:52:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    52,
                    19,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-10T17:59:21Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    21,
                    1,
                    161,
                    0
                ],
                "title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual\n  Backpropagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual\n  Backpropagation"
                },
                "summary": "Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework."
                },
                "authors": [
                    {
                        "name": "Xiaowen Ma"
                    },
                    {
                        "name": "Chenyang Lin"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13969v1",
                "updated": "2025-07-18T14:32:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    32,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:32:29Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    32,
                    29,
                    4,
                    199,
                    0
                ],
                "title": "A Minimalist Controller for Autonomously Self-Aggregating Robotic\n  Swarms: Enabling Compact Formations in Multitasking Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Controller for Autonomously Self-Aggregating Robotic\n  Swarms: Enabling Compact Formations in Multitasking Scenarios"
                },
                "summary": "The deployment of simple emergent behaviors in swarm robotics has been\nwell-rehearsed in the literature. A recent study has shown how self-aggregation\nis possible in a multitask approach -- where multiple self-aggregation task\ninstances occur concurrently in the same environment. The multitask approach\nposes new challenges, in special, how the dynamic of each group impacts the\nperformance of others. So far, the multitask self-aggregation of groups of\nrobots suffers from generating a circular formation -- that is not fully\ncompact -- or is not fully autonomous. In this paper, we present a multitask\nself-aggregation where groups of homogeneous robots sort themselves into\ndifferent compact clusters, relying solely on a line-of-sight sensor. Our\nmultitask self-aggregation behavior was able to scale well and achieve a\ncompact formation. We report scalability results from a series of simulation\ntrials with different configurations in the number of groups and the number of\nrobots per group. We were able to improve the multitask self-aggregation\nbehavior performance in terms of the compactness of the clusters, keeping the\nproportion of clustered robots found in other studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of simple emergent behaviors in swarm robotics has been\nwell-rehearsed in the literature. A recent study has shown how self-aggregation\nis possible in a multitask approach -- where multiple self-aggregation task\ninstances occur concurrently in the same environment. The multitask approach\nposes new challenges, in special, how the dynamic of each group impacts the\nperformance of others. So far, the multitask self-aggregation of groups of\nrobots suffers from generating a circular formation -- that is not fully\ncompact -- or is not fully autonomous. In this paper, we present a multitask\nself-aggregation where groups of homogeneous robots sort themselves into\ndifferent compact clusters, relying solely on a line-of-sight sensor. Our\nmultitask self-aggregation behavior was able to scale well and achieve a\ncompact formation. We report scalability results from a series of simulation\ntrials with different configurations in the number of groups and the number of\nrobots per group. We were able to improve the multitask self-aggregation\nbehavior performance in terms of the compactness of the clusters, keeping the\nproportion of clustered robots found in other studies."
                },
                "authors": [
                    {
                        "name": "Maria Eduarda Silva de Macedo"
                    },
                    {
                        "name": "Ana Paula Chiarelli de Souza"
                    },
                    {
                        "name": "Roberto Silvio Ubertino Rosso Jr."
                    },
                    {
                        "name": "Yuri Kaszubowski Lopes"
                    }
                ],
                "author_detail": {
                    "name": "Yuri Kaszubowski Lopes"
                },
                "author": "Yuri Kaszubowski Lopes",
                "arxiv_comment": "7 pages total (6 pages of content + 1 page of references). Short\n  paper manuscript submitted to TAROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13957v1",
                "updated": "2025-07-18T14:22:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    22,
                    5,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:22:05Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    22,
                    5,
                    4,
                    199,
                    0
                ],
                "title": "DUALRec: A Hybrid Sequential and Language Model Framework for\n  Context-Aware Movie Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DUALRec: A Hybrid Sequential and Language Model Framework for\n  Context-Aware Movie Recommendation"
                },
                "summary": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders."
                },
                "authors": [
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Raoul Grasman"
                    }
                ],
                "author_detail": {
                    "name": "Raoul Grasman"
                },
                "author": "Raoul Grasman",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68T50, 62M45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.6; H.3.4; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13956v1",
                "updated": "2025-07-18T14:21:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    21,
                    24,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:21:24Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    21,
                    24,
                    4,
                    199,
                    0
                ],
                "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction"
                },
                "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis."
                },
                "authors": [
                    {
                        "name": "Yutao Jin"
                    },
                    {
                        "name": "Haowen Xiao"
                    },
                    {
                        "name": "Jielei Chu"
                    },
                    {
                        "name": "Fengmao Lv"
                    },
                    {
                        "name": "Yuxiao Li"
                    },
                    {
                        "name": "Tianrui Li"
                    }
                ],
                "author_detail": {
                    "name": "Tianrui Li"
                },
                "author": "Tianrui Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13949v1",
                "updated": "2025-07-18T14:18:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    18,
                    18,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:18:18Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    18,
                    18,
                    4,
                    199,
                    0
                ],
                "title": "Exploiting Primacy Effect To Improve Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Primacy Effect To Improve Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications."
                },
                "authors": [
                    {
                        "name": "Bianca Raimondi"
                    },
                    {
                        "name": "Maurizio Gabbrielli"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Gabbrielli"
                },
                "author": "Maurizio Gabbrielli",
                "arxiv_comment": "Accepted by RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13938v1",
                "updated": "2025-07-18T14:10:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    10,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:10:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    10,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "Device-Free Localization Using Commercial UWB Transceivers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-Free Localization Using Commercial UWB Transceivers"
                },
                "summary": "Recently, commercial ultra-wideband (UWB) transceivers have enabled not only\nmeasuring device-to-device distance but also tracking the position of a\npedestrian who does not carry a UWB device. UWB-based device-free localization\nthat does not require dedicated radar equipment is compatible with existing\nanchor infrastructure and can be reused to reduce hardware deployment costs.\nHowever, it is difficult to estimate the target's position accurately in\nreal-world scenarios due to the low signal-to-noise ratio (SNR) and the\ncluttered environment. In this paper, we propose a deep learning (DL)-assisted\nparticle filter to overcome these challenges. First, the channel impulse\nresponse (CIR) variance is analyzed to capture the variability induced by the\ntarget's movement. Then, a DL-based one-dimensional attention U-Net is used to\nextract only the reflection components caused by the target and suppress the\nnoise components within the CIR variance profile. Finally, multiple\npreprocessed CIR variance profiles are used as input to a particle filter to\nestimate the target's position. Experimental results demonstrate that the\nproposed system is a practical and cost-effective solution for IoT and\nautomotive applications with a root mean square error (RMSE) of about 15 cm and\nan average processing time of 4 ms. Furthermore, comparisons with existing\nstate-of-the-art methods show that the proposed method provides the best\nperformance with reasonable computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, commercial ultra-wideband (UWB) transceivers have enabled not only\nmeasuring device-to-device distance but also tracking the position of a\npedestrian who does not carry a UWB device. UWB-based device-free localization\nthat does not require dedicated radar equipment is compatible with existing\nanchor infrastructure and can be reused to reduce hardware deployment costs.\nHowever, it is difficult to estimate the target's position accurately in\nreal-world scenarios due to the low signal-to-noise ratio (SNR) and the\ncluttered environment. In this paper, we propose a deep learning (DL)-assisted\nparticle filter to overcome these challenges. First, the channel impulse\nresponse (CIR) variance is analyzed to capture the variability induced by the\ntarget's movement. Then, a DL-based one-dimensional attention U-Net is used to\nextract only the reflection components caused by the target and suppress the\nnoise components within the CIR variance profile. Finally, multiple\npreprocessed CIR variance profiles are used as input to a particle filter to\nestimate the target's position. Experimental results demonstrate that the\nproposed system is a practical and cost-effective solution for IoT and\nautomotive applications with a root mean square error (RMSE) of about 15 cm and\nan average processing time of 4 ms. Furthermore, comparisons with existing\nstate-of-the-art methods show that the proposed method provides the best\nperformance with reasonable computational costs."
                },
                "authors": [
                    {
                        "name": "Hyun Seok Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Seok Lee"
                },
                "author": "Hyun Seok Lee",
                "arxiv_comment": "8 pages, 10 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12426v2",
                "updated": "2025-07-18T14:10:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    10,
                    43,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-16T17:15:06Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    15,
                    6,
                    2,
                    197,
                    0
                ],
                "title": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for\n  Spatio-Temporal Action Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for\n  Spatio-Temporal Action Recognition"
                },
                "summary": "The landscape of video recognition has evolved significantly, shifting from\ntraditional Convolutional Neural Networks (CNNs) to Transformer-based\narchitectures for improved accuracy. While 3D CNNs have been effective at\ncapturing spatiotemporal dynamics, recent Transformer models leverage\nself-attention to model long-range spatial and temporal dependencies. Despite\nachieving state-of-the-art performance on major benchmarks, Transformers remain\ncomputationally expensive, particularly with dense video data. To address this,\nwe propose a lightweight Video Focal Modulation Network, DVFL-Net, which\ndistills spatiotemporal knowledge from a large pre-trained teacher into a\ncompact nano student model, enabling efficient on-device deployment. DVFL-Net\nutilizes knowledge distillation and spatial-temporal feature modulation to\nsignificantly reduce computation while preserving high recognition performance.\nWe employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal\nfocal modulation to effectively transfer both local and global context from the\nVideo-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate\nDVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it\nagainst recent state-of-the-art methods in Human Action Recognition (HAR).\nAdditionally, we conduct a detailed ablation study analyzing the impact of\nforward KL divergence. The results confirm the superiority of DVFL-Net in\nachieving an optimal balance between performance and efficiency, demonstrating\nlower memory usage, reduced GFLOPs, and strong accuracy, making it a practical\nsolution for real-time HAR applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The landscape of video recognition has evolved significantly, shifting from\ntraditional Convolutional Neural Networks (CNNs) to Transformer-based\narchitectures for improved accuracy. While 3D CNNs have been effective at\ncapturing spatiotemporal dynamics, recent Transformer models leverage\nself-attention to model long-range spatial and temporal dependencies. Despite\nachieving state-of-the-art performance on major benchmarks, Transformers remain\ncomputationally expensive, particularly with dense video data. To address this,\nwe propose a lightweight Video Focal Modulation Network, DVFL-Net, which\ndistills spatiotemporal knowledge from a large pre-trained teacher into a\ncompact nano student model, enabling efficient on-device deployment. DVFL-Net\nutilizes knowledge distillation and spatial-temporal feature modulation to\nsignificantly reduce computation while preserving high recognition performance.\nWe employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal\nfocal modulation to effectively transfer both local and global context from the\nVideo-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate\nDVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it\nagainst recent state-of-the-art methods in Human Action Recognition (HAR).\nAdditionally, we conduct a detailed ablation study analyzing the impact of\nforward KL divergence. The results confirm the superiority of DVFL-Net in\nachieving an optimal balance between performance and efficiency, demonstrating\nlower memory usage, reduced GFLOPs, and strong accuracy, making it a practical\nsolution for real-time HAR applications."
                },
                "authors": [
                    {
                        "name": "Hayat Ullah"
                    },
                    {
                        "name": "Muhammad Ali Shafique"
                    },
                    {
                        "name": "Abbas Khan"
                    },
                    {
                        "name": "Arslan Munir"
                    }
                ],
                "author_detail": {
                    "name": "Arslan Munir"
                },
                "author": "Arslan Munir",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13937v1",
                "updated": "2025-07-18T14:09:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    9,
                    45,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:09:45Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    9,
                    45,
                    4,
                    199,
                    0
                ],
                "title": "Marcel: A Lightweight and Open-Source Conversational Agent for\n  University Student Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marcel: A Lightweight and Open-Source Conversational Agent for\n  University Student Support"
                },
                "summary": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Jan Trienes"
                    },
                    {
                        "name": "Anastasiia Derzhanskaia"
                    },
                    {
                        "name": "Roland Schwarzkopf"
                    },
                    {
                        "name": "Markus M√ºhling"
                    },
                    {
                        "name": "J√∂rg Schl√∂tterer"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13933v1",
                "updated": "2025-07-18T14:09:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    9,
                    4,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:09:04Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    9,
                    4,
                    4,
                    199,
                    0
                ],
                "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preprint: Did I Just Browse A Website Written by LLMs?"
                },
                "summary": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem."
                },
                "authors": [
                    {
                        "name": "Sichang \"Steven\" He"
                    },
                    {
                        "name": "Ramesh Govindan"
                    },
                    {
                        "name": "Harsha V. Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Harsha V. Madhyastha"
                },
                "author": "Harsha V. Madhyastha",
                "arxiv_comment": "In submission. 2 pages. 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13919v1",
                "updated": "2025-07-18T13:50:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    50,
                    9,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T13:50:09Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    50,
                    9,
                    4,
                    199,
                    0
                ],
                "title": "The Levers of Political Persuasion with Conversational AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Levers of Political Persuasion with Conversational AI"
                },
                "summary": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy."
                },
                "authors": [
                    {
                        "name": "Kobi Hackenburg"
                    },
                    {
                        "name": "Ben M. Tappin"
                    },
                    {
                        "name": "Luke Hewitt"
                    },
                    {
                        "name": "Ed Saunders"
                    },
                    {
                        "name": "Sid Black"
                    },
                    {
                        "name": "Hause Lin"
                    },
                    {
                        "name": "Catherine Fist"
                    },
                    {
                        "name": "Helen Margetts"
                    },
                    {
                        "name": "David G. Rand"
                    },
                    {
                        "name": "Christopher Summerfield"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Summerfield"
                },
                "author": "Christopher Summerfield",
                "arxiv_comment": "19 pages, 4 figures. Our supplementary materials file can be found at\n  https://github.com/kobihackenburg/scaling-conversational-AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06602v2",
                "updated": "2025-07-18T13:46:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    46,
                    26,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-09T07:22:22Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    22,
                    22,
                    2,
                    190,
                    0
                ],
                "title": "Generalization in Reinforcement Learning for Radio Access Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalization in Reinforcement Learning for Radio Access Networks"
                },
                "summary": "Modern RAN operate in highly dynamic and heterogeneous environments, where\nhand-tuned, rule-based RRM algorithms often underperform. While RL can surpass\nsuch heuristics in constrained settings, the diversity of deployments and\nunpredictable radio conditions introduce major generalization challenges.\nData-driven policies frequently overfit to training conditions, degrading\nperformance in unseen scenarios. To address this, we propose a\ngeneralization-centered RL framework for RAN control that: (i) robustly\nreconstructs dynamically varying states from partial and noisy observations,\nwhile encoding static and semi-static information, such as radio nodes, cell\nattributes, and their topology, through graph representations; (ii) applies\ndomain randomization to broaden the training distribution; and (iii)\ndistributes data generation across multiple actors while centralizing training\nin a cloud-compatible architecture aligned with O-RAN principles. Although\ngeneralization increases computational and data-management complexity, our\ndistributed design mitigates this by scaling data collection and training\nacross diverse network conditions. Applied to downlink link adaptation in five\n5G benchmarks, our policy improves average throughput and spectral efficiency\nby ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and\nby >20% under high mobility. It matches specialized RL in full-buffer traffic\nand achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,\nrespectively. In nine-cell deployments, GAT models offer 30% higher throughput\nover MLP baselines. These results, combined with our scalable architecture,\noffer a path toward AI-native 6G RAN using a single, generalizable RL agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern RAN operate in highly dynamic and heterogeneous environments, where\nhand-tuned, rule-based RRM algorithms often underperform. While RL can surpass\nsuch heuristics in constrained settings, the diversity of deployments and\nunpredictable radio conditions introduce major generalization challenges.\nData-driven policies frequently overfit to training conditions, degrading\nperformance in unseen scenarios. To address this, we propose a\ngeneralization-centered RL framework for RAN control that: (i) robustly\nreconstructs dynamically varying states from partial and noisy observations,\nwhile encoding static and semi-static information, such as radio nodes, cell\nattributes, and their topology, through graph representations; (ii) applies\ndomain randomization to broaden the training distribution; and (iii)\ndistributes data generation across multiple actors while centralizing training\nin a cloud-compatible architecture aligned with O-RAN principles. Although\ngeneralization increases computational and data-management complexity, our\ndistributed design mitigates this by scaling data collection and training\nacross diverse network conditions. Applied to downlink link adaptation in five\n5G benchmarks, our policy improves average throughput and spectral efficiency\nby ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and\nby >20% under high mobility. It matches specialized RL in full-buffer traffic\nand achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,\nrespectively. In nine-cell deployments, GAT models offer 30% higher throughput\nover MLP baselines. These results, combined with our scalable architecture,\noffer a path toward AI-native 6G RAN using a single, generalizable RL agent."
                },
                "authors": [
                    {
                        "name": "Burak Demirel"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Cristian Tatino"
                    },
                    {
                        "name": "Pablo Soldati"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Soldati"
                },
                "author": "Pablo Soldati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.09409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.09409v3",
                "updated": "2025-07-18T13:42:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    42,
                    41,
                    4,
                    199,
                    0
                ],
                "published": "2023-02-18T19:53:12Z",
                "published_parsed": [
                    2023,
                    2,
                    18,
                    19,
                    53,
                    12,
                    5,
                    49,
                    0
                ],
                "title": "LOCUS: LOcalization with Channel Uncertainty and Sporadic Energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOCUS: LOcalization with Channel Uncertainty and Sporadic Energy"
                },
                "summary": "Accurate sound source localization (SSL), such as direction-of-arrival (DoA)\nestimation, relies on consistent multichannel data. However, batteryless\nsystems often suffer from missing data due to the stochastic nature of energy\nharvesting, degrading localization performance. We propose LOCUS, a deep\nlearning framework that recovers corrupted features in such settings. LOCUS\nintegrates three modules: (1) Information-Weighted Focus (InFo) to identify\ncorrupted regions, (2) Latent Feature Synthesizer (LaFS) to reconstruct missing\nfeatures, and (3) Guided Replacement (GRep) to restore data without altering\nvalid inputs. LOCUS significantly improves DoA accuracy under missing-channel\nconditions, achieving up to 36.91% error reduction on DCASE and LargeSet, and\n25.87-59.46% gains in real-world deployments. We release a 50-hour multichannel\ndataset to support future research on localization under energy constraints.\nOur code and data are available at: https://bashlab.github.io/locus_project/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate sound source localization (SSL), such as direction-of-arrival (DoA)\nestimation, relies on consistent multichannel data. However, batteryless\nsystems often suffer from missing data due to the stochastic nature of energy\nharvesting, degrading localization performance. We propose LOCUS, a deep\nlearning framework that recovers corrupted features in such settings. LOCUS\nintegrates three modules: (1) Information-Weighted Focus (InFo) to identify\ncorrupted regions, (2) Latent Feature Synthesizer (LaFS) to reconstruct missing\nfeatures, and (3) Guided Replacement (GRep) to restore data without altering\nvalid inputs. LOCUS significantly improves DoA accuracy under missing-channel\nconditions, achieving up to 36.91% error reduction on DCASE and LargeSet, and\n25.87-59.46% gains in real-world deployments. We release a 50-hour multichannel\ndataset to support future research on localization under energy constraints.\nOur code and data are available at: https://bashlab.github.io/locus_project/"
                },
                "authors": [
                    {
                        "name": "Subrata Biswas"
                    },
                    {
                        "name": "Mohammad Nur Hossain Khan"
                    },
                    {
                        "name": "Violet Colwell"
                    },
                    {
                        "name": "Jack Adiletta"
                    },
                    {
                        "name": "Bashima Islam"
                    }
                ],
                "author_detail": {
                    "name": "Bashima Islam"
                },
                "author": "Bashima Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.09409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.09409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13895v1",
                "updated": "2025-07-18T13:20:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    20,
                    58,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T13:20:58Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    20,
                    58,
                    4,
                    199,
                    0
                ],
                "title": "Application Placement with Constraint Relaxation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application Placement with Constraint Relaxation"
                },
                "summary": "Novel utility computing paradigms rely upon the deployment of multi-service\napplications to pervasive and highly distributed cloud-edge infrastructure\nresources. Deciding onto which computational nodes to place services in\ncloud-edge networks, as per their functional and non-functional constraints,\ncan be formulated as a combinatorial optimisation problem. Most existing\nsolutions in this space are not able to deal with \\emph{unsatisfiable} problem\ninstances, nor preferences, i.e. requirements that DevOps may agree to relax to\nobtain a solution. In this article, we exploit Answer Set Programming\noptimisation capabilities to tackle this problem. Experimental results in\nsimulated settings show that our approach is effective on lifelike networks and\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel utility computing paradigms rely upon the deployment of multi-service\napplications to pervasive and highly distributed cloud-edge infrastructure\nresources. Deciding onto which computational nodes to place services in\ncloud-edge networks, as per their functional and non-functional constraints,\ncan be formulated as a combinatorial optimisation problem. Most existing\nsolutions in this space are not able to deal with \\emph{unsatisfiable} problem\ninstances, nor preferences, i.e. requirements that DevOps may agree to relax to\nobtain a solution. In this article, we exploit Answer Set Programming\noptimisation capabilities to tackle this problem. Experimental results in\nsimulated settings show that our approach is effective on lifelike networks and\napplications."
                },
                "authors": [
                    {
                        "name": "Damiano Azzolini"
                    },
                    {
                        "name": "Marco Duca"
                    },
                    {
                        "name": "Stefano Forti"
                    },
                    {
                        "name": "Francesco Gallo"
                    },
                    {
                        "name": "Antonio Ielo"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Ielo"
                },
                "author": "Antonio Ielo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13881v1",
                "updated": "2025-07-18T12:59:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    59,
                    17,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T12:59:17Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    59,
                    17,
                    4,
                    199,
                    0
                ],
                "title": "Using LLMs to identify features of personal and professional skills in\n  an open-response situational judgment test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs to identify features of personal and professional skills in\n  an open-response situational judgment test"
                },
                "summary": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills."
                },
                "authors": [
                    {
                        "name": "Cole Walsh"
                    },
                    {
                        "name": "Rodica Ivan"
                    },
                    {
                        "name": "Muhammad Zafar Iqbal"
                    },
                    {
                        "name": "Colleen Robb"
                    }
                ],
                "author_detail": {
                    "name": "Colleen Robb"
                },
                "author": "Colleen Robb",
                "arxiv_comment": "10 pages, 2 figures, 4 tables; this work was accepted for\n  presentation at the 2025 Artificial Intelligence in Measurement and Education\n  Conference in Pittsburgh, Pennsylvania, United States",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13874v1",
                "updated": "2025-07-18T12:54:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    54,
                    28,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T12:54:28Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    54,
                    28,
                    4,
                    199,
                    0
                ],
                "title": "Large Language Models as Innovators: A Framework to Leverage Latent\n  Space Exploration for Novelty Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Innovators: A Framework to Leverage Latent\n  Space Exploration for Novelty Discovery"
                },
                "summary": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration."
                },
                "authors": [
                    {
                        "name": "Mateusz Bystro≈Ñski"
                    },
                    {
                        "name": "Miko≈Çaj Ho≈Çysz"
                    },
                    {
                        "name": "Grzegorz Piotrowski"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    },
                    {
                        "name": "Tomasz Kajdanowicz"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kajdanowicz"
                },
                "author": "Tomasz Kajdanowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16247v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16247v3",
                "updated": "2025-07-18T12:37:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    37,
                    43,
                    4,
                    199,
                    0
                ],
                "published": "2024-12-20T00:01:16Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    0,
                    1,
                    16,
                    4,
                    355,
                    0
                ],
                "title": "Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models"
                },
                "summary": "Sparse dictionary learning (DL) has emerged as a powerful approach to extract\nsemantically meaningful concepts from the internals of large language models\n(LLMs) trained mainly in the text domain. In this work, we explore whether DL\ncan extract meaningful concepts from less human-interpretable scientific data,\nsuch as vision foundation models trained on cell microscopy images, where\nlimited prior knowledge exists about which high-level concepts should arise. We\npropose a novel combination of a sparse DL algorithm, Iterative Codebook\nFeature Learning (ICFL), with a PCA whitening pre-processing step derived from\ncontrol data. Using this combined approach, we successfully retrieve\nbiologically meaningful concepts, such as cell types and genetic perturbations.\nMoreover, we demonstrate how our method reveals subtle morphological changes\narising from human-interpretable interventions, offering a promising new\ndirection for scientific discovery via mechanistic interpretability in\nbioimaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse dictionary learning (DL) has emerged as a powerful approach to extract\nsemantically meaningful concepts from the internals of large language models\n(LLMs) trained mainly in the text domain. In this work, we explore whether DL\ncan extract meaningful concepts from less human-interpretable scientific data,\nsuch as vision foundation models trained on cell microscopy images, where\nlimited prior knowledge exists about which high-level concepts should arise. We\npropose a novel combination of a sparse DL algorithm, Iterative Codebook\nFeature Learning (ICFL), with a PCA whitening pre-processing step derived from\ncontrol data. Using this combined approach, we successfully retrieve\nbiologically meaningful concepts, such as cell types and genetic perturbations.\nMoreover, we demonstrate how our method reveals subtle morphological changes\narising from human-interpretable interventions, offering a promising new\ndirection for scientific discovery via mechanistic interpretability in\nbioimaging."
                },
                "authors": [
                    {
                        "name": "Konstantin Donhauser"
                    },
                    {
                        "name": "Kristina Ulicna"
                    },
                    {
                        "name": "Gemma Elyse Moran"
                    },
                    {
                        "name": "Aditya Ravuri"
                    },
                    {
                        "name": "Kian Kenyon-Dean"
                    },
                    {
                        "name": "Cian Eastwood"
                    },
                    {
                        "name": "Jason Hartford"
                    }
                ],
                "author_detail": {
                    "name": "Jason Hartford"
                },
                "author": "Jason Hartford",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16247v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16247v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04800v3",
                "updated": "2025-07-18T12:34:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    34,
                    45,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-03T06:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    6,
                    54,
                    5,
                    0,
                    62,
                    0
                ],
                "title": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated\n  Information on Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated\n  Information on Retrieval-Augmented Generation"
                },
                "summary": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH."
                },
                "authors": [
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Tingyue Pan"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Ruiran Yan"
                    },
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Jiaying Lin"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13859v1",
                "updated": "2025-07-18T12:28:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    28,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T12:28:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    28,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data\n  Memorization and Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data\n  Memorization and Knowledge Injection"
                },
                "summary": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible."
                },
                "authors": [
                    {
                        "name": "Aleksandr Gashkov"
                    },
                    {
                        "name": "Aleksandr Perevalov"
                    },
                    {
                        "name": "Maria Eltsova"
                    },
                    {
                        "name": "Andreas Both"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Both"
                },
                "author": "Andreas Both",
                "arxiv_comment": "Winner of Best Paper Award at the 25th International Conference on\n  Web Engineering (ICWE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13858v1",
                "updated": "2025-07-18T12:23:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    23,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T12:23:47Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    23,
                    47,
                    4,
                    199,
                    0
                ],
                "title": "InTraVisTo: Inside Transformer Visualisation Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InTraVisTo: Inside Transformer Visualisation Tool"
                },
                "summary": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs."
                },
                "authors": [
                    {
                        "name": "Nicol√≤ Brunello"
                    },
                    {
                        "name": "Davide Rigamonti"
                    },
                    {
                        "name": "Andrea Sassella"
                    },
                    {
                        "name": "Vincenzo Scotti"
                    },
                    {
                        "name": "Mark James Carman"
                    }
                ],
                "author_detail": {
                    "name": "Mark James Carman"
                },
                "author": "Mark James Carman",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10484v2",
                "updated": "2025-07-18T12:09:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    12,
                    9,
                    59,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-17T05:20:38Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    20,
                    38,
                    4,
                    17,
                    0
                ],
                "title": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study\n  of ChatGPT and Claude",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study\n  of ChatGPT and Claude"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled human-like\nresponses across various tasks, raising questions about their ethical\ndecision-making capabilities and potential biases. This study investigates\nprotected attributes in LLMs through systematic evaluation of their responses\nto ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5\nSonnet - we analyzed their decision-making patterns across multiple protected\nattributes including age, gender, race, appearance, and disability status.\nThrough 11,200 experimental trials involving both single-factor and two-factor\nprotected attribute combinations, we evaluated the models' ethical preferences,\nsensitivity, stability, and clustering of preferences. Our findings reveal\nsignificant protected attributeses in both models, with consistent preferences\nfor certain features (e.g., \"good-looking\") and systematic neglect of others.\nNotably, while GPT-3.5 Turbo showed stronger preferences aligned with\ntraditional power structures, Claude 3.5 Sonnet demonstrated more diverse\nprotected attribute choices. We also found that ethical sensitivity\nsignificantly decreases in more complex scenarios involving multiple protected\nattributes. Additionally, linguistic referents heavily influence the models'\nethical evaluations, as demonstrated by differing responses to racial\ndescriptors (e.g., \"Yellow\" versus \"Asian\"). These findings highlight critical\nconcerns about the potential impact of LLM biases in autonomous decision-making\nsystems and emphasize the need for careful consideration of protected\nattributes in AI development. Our study contributes to the growing body of\nresearch on AI ethics by providing a systematic framework for evaluating\nprotected attributes in LLMs' ethical decision-making capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled human-like\nresponses across various tasks, raising questions about their ethical\ndecision-making capabilities and potential biases. This study investigates\nprotected attributes in LLMs through systematic evaluation of their responses\nto ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5\nSonnet - we analyzed their decision-making patterns across multiple protected\nattributes including age, gender, race, appearance, and disability status.\nThrough 11,200 experimental trials involving both single-factor and two-factor\nprotected attribute combinations, we evaluated the models' ethical preferences,\nsensitivity, stability, and clustering of preferences. Our findings reveal\nsignificant protected attributeses in both models, with consistent preferences\nfor certain features (e.g., \"good-looking\") and systematic neglect of others.\nNotably, while GPT-3.5 Turbo showed stronger preferences aligned with\ntraditional power structures, Claude 3.5 Sonnet demonstrated more diverse\nprotected attribute choices. We also found that ethical sensitivity\nsignificantly decreases in more complex scenarios involving multiple protected\nattributes. Additionally, linguistic referents heavily influence the models'\nethical evaluations, as demonstrated by differing responses to racial\ndescriptors (e.g., \"Yellow\" versus \"Asian\"). These findings highlight critical\nconcerns about the potential impact of LLM biases in autonomous decision-making\nsystems and emphasize the need for careful consideration of protected\nattributes in AI development. Our study contributes to the growing body of\nresearch on AI ethics by providing a systematic framework for evaluating\nprotected attributes in LLMs' ethical decision-making capabilities."
                },
                "authors": [
                    {
                        "name": "Yile Yan"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Wentao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Xu"
                },
                "author": "Wentao Xu",
                "arxiv_comment": "This paper has been accepted by International AAAI Conference on Web\n  and Social Media 2026, sunny Los Angeles, California",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13841v1",
                "updated": "2025-07-18T11:55:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    55,
                    18,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T11:55:18Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    55,
                    18,
                    4,
                    199,
                    0
                ],
                "title": "Modeling Fair Play in Detective Stories with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Fair Play in Detective Stories with Language Models"
                },
                "summary": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality."
                },
                "authors": [
                    {
                        "name": "Eitan Wagner"
                    },
                    {
                        "name": "Renana Keydar"
                    },
                    {
                        "name": "Omri Abend"
                    }
                ],
                "author_detail": {
                    "name": "Omri Abend"
                },
                "author": "Omri Abend",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13833v1",
                "updated": "2025-07-18T11:41:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    41,
                    49,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T11:41:49Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    41,
                    49,
                    4,
                    199,
                    0
                ],
                "title": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient\n  LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient\n  LLM Post-Training"
                },
                "summary": "Reinforcement learning (RL) has become the pivotal post-training technique\nfor large language model. Effectively scaling reinforcement learning is now the\nkey to unlocking advanced reasoning capabilities and ensuring safe,\ngoal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually\nemploy a hybrid-controller architecture where a single-controller dispatches\nthe overall execution logic and manages overall data transfer and the\nmulti-controller executes distributed computation. For large-scale\nreinforcement learning, minor load imbalances can introduce significant\nbottlenecks, ultimately constraining the scalability of the system. To address\nthis limitation, we introduce DistFlow, a novel, fully distributed RL framework\ndesigned to break scaling barrier. We adopt a multi-controller paradigm that\ndispatches data transfer and execution tasks to all workers, which eliminates\nthe centralized node. This allows each worker to operate independently, leading\nto near-linear scalability up to thousands of GPUs and dramatic efficiency\ngains. Furthermore, our architecture decouples resource configuration from\nexecution logic, allowing each worker to have a unique execution flow, offering\nsignificant flexibility for rapid and cost-effective algorithmic\nexperimentation. Extensive experiments show that DistFlow achieves excellent\nlinear scalability and up to a 7x end-to-end throughput improvement over\nstate-of-the-art (SOTA) frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become the pivotal post-training technique\nfor large language model. Effectively scaling reinforcement learning is now the\nkey to unlocking advanced reasoning capabilities and ensuring safe,\ngoal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually\nemploy a hybrid-controller architecture where a single-controller dispatches\nthe overall execution logic and manages overall data transfer and the\nmulti-controller executes distributed computation. For large-scale\nreinforcement learning, minor load imbalances can introduce significant\nbottlenecks, ultimately constraining the scalability of the system. To address\nthis limitation, we introduce DistFlow, a novel, fully distributed RL framework\ndesigned to break scaling barrier. We adopt a multi-controller paradigm that\ndispatches data transfer and execution tasks to all workers, which eliminates\nthe centralized node. This allows each worker to operate independently, leading\nto near-linear scalability up to thousands of GPUs and dramatic efficiency\ngains. Furthermore, our architecture decouples resource configuration from\nexecution logic, allowing each worker to have a unique execution flow, offering\nsignificant flexibility for rapid and cost-effective algorithmic\nexperimentation. Extensive experiments show that DistFlow achieves excellent\nlinear scalability and up to a 7x end-to-end throughput improvement over\nstate-of-the-art (SOTA) frameworks."
                },
                "authors": [
                    {
                        "name": "Zhixin Wang"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Jiarui Hu"
                    },
                    {
                        "name": "Dian Yang"
                    },
                    {
                        "name": "Jinlong Hou"
                    },
                    {
                        "name": "Siyuan Feng"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Yuan Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Qi"
                },
                "author": "Yuan Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04295v3",
                "updated": "2025-07-18T11:37:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    37,
                    12,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-06T08:39:26Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    8,
                    39,
                    26,
                    6,
                    187,
                    0
                ],
                "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with\n  Educators in the Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with\n  Educators in the Loop"
                },
                "summary": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students."
                },
                "authors": [
                    {
                        "name": "Runcong Zhao"
                    },
                    {
                        "name": "Artem Bobrov"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01558v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01558v3",
                "updated": "2025-07-18T11:35:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    35,
                    1,
                    4,
                    199,
                    0
                ],
                "published": "2024-05-05T19:10:19Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    19,
                    10,
                    19,
                    6,
                    126,
                    0
                ],
                "title": "Visual Grounding Methods for Efficient Interaction with Desktop\n  Graphical User Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Grounding Methods for Efficient Interaction with Desktop\n  Graphical User Interfaces"
                },
                "summary": "Most visual grounding solutions primarily focus on realistic images. However,\napplications involving synthetic images, such as Graphical User Interfaces\n(GUIs), remain limited. This restricts the development of autonomous computer\nvision-powered artificial intelligence (AI) agents for automatic application\ninteraction. Enabling AI to effectively understand and interact with GUIs is\ncrucial to advancing automation in software testing, accessibility, and\nhuman-computer interaction. In this work, we explore Instruction Visual\nGrounding (IVG), a multi-modal approach to object identification within a GUI.\nMore precisely, given a natural language instruction and a GUI screen, IVG\nlocates the coordinates of the element on the screen where the instruction\nshould be executed. We propose two main methods: (1) IVGocr, which combines a\nLarge Language Model (LLM), an object detection model, and an Optical Character\nRecognition (OCR) module; and (2) IVGdirect, which uses a multimodal\narchitecture for end-to-end grounding. For each method, we introduce a\ndedicated dataset. In addition, we propose the Central Point Validation (CPV)\nmetric, a relaxed variant of the classical Central Proximity Score (CPS)\nmetric. Our final test dataset is publicly released to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most visual grounding solutions primarily focus on realistic images. However,\napplications involving synthetic images, such as Graphical User Interfaces\n(GUIs), remain limited. This restricts the development of autonomous computer\nvision-powered artificial intelligence (AI) agents for automatic application\ninteraction. Enabling AI to effectively understand and interact with GUIs is\ncrucial to advancing automation in software testing, accessibility, and\nhuman-computer interaction. In this work, we explore Instruction Visual\nGrounding (IVG), a multi-modal approach to object identification within a GUI.\nMore precisely, given a natural language instruction and a GUI screen, IVG\nlocates the coordinates of the element on the screen where the instruction\nshould be executed. We propose two main methods: (1) IVGocr, which combines a\nLarge Language Model (LLM), an object detection model, and an Optical Character\nRecognition (OCR) module; and (2) IVGdirect, which uses a multimodal\narchitecture for end-to-end grounding. For each method, we introduce a\ndedicated dataset. In addition, we propose the Central Point Validation (CPV)\nmetric, a relaxed variant of the classical Central Proximity Score (CPS)\nmetric. Our final test dataset is publicly released to support future research."
                },
                "authors": [
                    {
                        "name": "El Hassane Ettifouri"
                    },
                    {
                        "name": "Jessica L√≥pez Espejel"
                    },
                    {
                        "name": "Laura Minkova"
                    },
                    {
                        "name": "Tassnim Dardouri"
                    },
                    {
                        "name": "Walid Dahhane"
                    }
                ],
                "author_detail": {
                    "name": "Walid Dahhane"
                },
                "author": "Walid Dahhane",
                "arxiv_comment": "Preprint submitted to Engineering Applications of Artificial\n  Intelligence journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01558v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01558v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13827v1",
                "updated": "2025-07-18T11:31:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    31,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T11:31:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    31,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "Question-Answer Extraction from Scientific Articles Using Knowledge\n  Graphs and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-Answer Extraction from Scientific Articles Using Knowledge\n  Graphs and Large Language Models"
                },
                "summary": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents."
                },
                "authors": [
                    {
                        "name": "Hosein Azarbonyad"
                    },
                    {
                        "name": "Zi Long Zhu"
                    },
                    {
                        "name": "Georgios Cheirmpos"
                    },
                    {
                        "name": "Zubair Afzal"
                    },
                    {
                        "name": "Vikrant Yadav"
                    },
                    {
                        "name": "Georgios Tsatsaronis"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Tsatsaronis"
                },
                "author": "Georgios Tsatsaronis",
                "arxiv_comment": "SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13822v1",
                "updated": "2025-07-18T11:20:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    20,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T11:20:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    20,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs"
                },
                "summary": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications."
                },
                "authors": [
                    {
                        "name": "Shad Nygren"
                    },
                    {
                        "name": "Pinar Avci"
                    },
                    {
                        "name": "Andre Daniels"
                    },
                    {
                        "name": "Reza Rassol"
                    },
                    {
                        "name": "Afshin Beheshti"
                    },
                    {
                        "name": "Diego Galeano"
                    }
                ],
                "author_detail": {
                    "name": "Diego Galeano"
                },
                "author": "Diego Galeano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13820v1",
                "updated": "2025-07-18T11:12:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    12,
                    44,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T11:12:44Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    12,
                    44,
                    4,
                    199,
                    0
                ],
                "title": "Team of One: Cracking Complex Video QA with Model Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Team of One: Cracking Complex Video QA with Model Synergy"
                },
                "summary": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development."
                },
                "authors": [
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Zhaoran Zhao"
                    },
                    {
                        "name": "Xiongjun Guan"
                    },
                    {
                        "name": "Yingjian Zhu"
                    },
                    {
                        "name": "Hongzhu Yi"
                    },
                    {
                        "name": "Xinming Wang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Zhepeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhepeng Wang"
                },
                "author": "Zhepeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15761v2",
                "updated": "2025-07-18T11:10:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    11,
                    10,
                    7,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-13T20:55:48Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    20,
                    55,
                    48,
                    3,
                    44,
                    0
                ],
                "title": "AIvaluateXR: An Evaluation Framework for on-Device AI in XR with\n  Benchmarking Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIvaluateXR: An Evaluation Framework for on-Device AI in XR with\n  Benchmarking Results"
                },
                "summary": "The deployment of large language models (LLMs) on extended reality (XR)\ndevices has great potential to advance the field of human-AI interaction. In\nthe case of direct, on-device model inference, selecting the appropriate model\nand device for specific tasks remains challenging. In this paper, we present\nAIvaluateXR, a comprehensive evaluation framework for benchmarking LLMs running\non XR devices. To demonstrate the framework, we deploy 17 selected LLMs across\nfour XR platforms: Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision\nPro, and conduct an extensive evaluation. Our experimental setup measures four\nkey metrics: performance consistency, processing speed, memory usage, and\nbattery consumption. For each of the 68 model-device pairs, we assess\nperformance under varying string lengths, batch sizes, and thread counts,\nanalyzing the trade-offs for real-time XR applications. We propose a unified\nevaluation method based on the 3D Pareto Optimality theory to select the\noptimal device-model pairs from quality and speed objectives. Additionally, we\ncompare the efficiency of on-device LLMs with client-server and cloud-based\nsetups, and evaluate their accuracy on two interactive tasks. We believe our\nfindings offer valuable insight to guide future optimization efforts for LLM\ndeployment on XR devices. Our evaluation method can be used as standard\ngroundwork for further research and development in this emerging field. The\nsource code and supplementary materials are available at:\nwww.nanovis.org/AIvaluateXR.html",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) on extended reality (XR)\ndevices has great potential to advance the field of human-AI interaction. In\nthe case of direct, on-device model inference, selecting the appropriate model\nand device for specific tasks remains challenging. In this paper, we present\nAIvaluateXR, a comprehensive evaluation framework for benchmarking LLMs running\non XR devices. To demonstrate the framework, we deploy 17 selected LLMs across\nfour XR platforms: Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision\nPro, and conduct an extensive evaluation. Our experimental setup measures four\nkey metrics: performance consistency, processing speed, memory usage, and\nbattery consumption. For each of the 68 model-device pairs, we assess\nperformance under varying string lengths, batch sizes, and thread counts,\nanalyzing the trade-offs for real-time XR applications. We propose a unified\nevaluation method based on the 3D Pareto Optimality theory to select the\noptimal device-model pairs from quality and speed objectives. Additionally, we\ncompare the efficiency of on-device LLMs with client-server and cloud-based\nsetups, and evaluate their accuracy on two interactive tasks. We believe our\nfindings offer valuable insight to guide future optimization efforts for LLM\ndeployment on XR devices. Our evaluation method can be used as standard\ngroundwork for further research and development in this emerging field. The\nsource code and supplementary materials are available at:\nwww.nanovis.org/AIvaluateXR.html"
                },
                "authors": [
                    {
                        "name": "Dawar Khan"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Omar Mena"
                    },
                    {
                        "name": "Donggang Jia"
                    },
                    {
                        "name": "Alexandre Kouyoumdjian"
                    },
                    {
                        "name": "Ivan Viola"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Viola"
                },
                "author": "Ivan Viola",
                "arxiv_comment": "AIvaluateXR is updated version of LoXR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13774v2",
                "updated": "2025-07-18T10:56:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    56,
                    13,
                    4,
                    199,
                    0
                ],
                "published": "2025-04-18T16:22:20Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    20,
                    4,
                    108,
                    0
                ],
                "title": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs"
                },
                "summary": "Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information."
                },
                "authors": [
                    {
                        "name": "Tamim Al Mahmud"
                    },
                    {
                        "name": "Najeeb Jebreel"
                    },
                    {
                        "name": "Josep Domingo-Ferrer"
                    },
                    {
                        "name": "David Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "David Sanchez"
                },
                "author": "David Sanchez",
                "arxiv_doi": "10.1016/j.neunet.2025.107879",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.neunet.2025.107879",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the updated version of the preprint, revised following\n  acceptance for publication in Elsevier Neural Networks Journal. The paper is\n  now published (18 July 2025) with DOI:\n  https://doi.org/10.1016/j.neunet.2025.107879",
                "arxiv_journal_ref": "Neural Networks, 2025, Article 107879",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08102v3",
                "updated": "2025-07-18T10:53:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    53,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-14T13:19:47Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    19,
                    47,
                    1,
                    14,
                    0
                ],
                "title": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design."
                },
                "authors": [
                    {
                        "name": "Wenlu Fan"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Wentao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Xu"
                },
                "author": "Wentao Xu",
                "arxiv_comment": "This paper has been accepted by the International AAAI Conference on\n  Web and Social Media (ICWSM) 2026(Los Angeles, California, U.S.)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13814v1",
                "updated": "2025-07-18T10:52:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    52,
                    22,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T10:52:22Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    52,
                    22,
                    4,
                    199,
                    0
                ],
                "title": "CodeEdu: A Multi-Agent Collaborative Platform for Personalized Coding\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeEdu: A Multi-Agent Collaborative Platform for Personalized Coding\n  Education"
                },
                "summary": "Large Language Models (LLMs) have demonstrated considerable potential in\nimproving coding education by providing support for code writing, explanation,\nand debugging. However, existing LLM-based approaches generally fail to assess\nstudents' abilities, design learning plans, provide personalized material\naligned with individual learning goals, and enable interactive learning.\nCurrent work mostly uses single LLM agents, which limits their ability to\nunderstand complex code repositories and schedule step-by-step tutoring. Recent\nresearch has shown that multi-agent LLMs can collaborate to solve complicated\nproblems in various domains like software engineering, but their potential in\nthe field of education remains unexplored. In this work, we introduce CodeEdu,\nan innovative multi-agent collaborative platform that combines LLMs with tool\nuse to provide proactive and personalized education in coding. Unlike static\npipelines, CodeEdu dynamically allocates agents and tasks to meet student\nneeds. Various agents in CodeEdu undertake certain functions specifically,\nincluding task planning, personalized material generation, real-time QA,\nstep-by-step tutoring, code execution, debugging, and learning report\ngeneration, facilitated with extensive external tools to improve task\nefficiency. Automated evaluations reveal that CodeEdu substantially enhances\nstudents' coding performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated considerable potential in\nimproving coding education by providing support for code writing, explanation,\nand debugging. However, existing LLM-based approaches generally fail to assess\nstudents' abilities, design learning plans, provide personalized material\naligned with individual learning goals, and enable interactive learning.\nCurrent work mostly uses single LLM agents, which limits their ability to\nunderstand complex code repositories and schedule step-by-step tutoring. Recent\nresearch has shown that multi-agent LLMs can collaborate to solve complicated\nproblems in various domains like software engineering, but their potential in\nthe field of education remains unexplored. In this work, we introduce CodeEdu,\nan innovative multi-agent collaborative platform that combines LLMs with tool\nuse to provide proactive and personalized education in coding. Unlike static\npipelines, CodeEdu dynamically allocates agents and tasks to meet student\nneeds. Various agents in CodeEdu undertake certain functions specifically,\nincluding task planning, personalized material generation, real-time QA,\nstep-by-step tutoring, code execution, debugging, and learning report\ngeneration, facilitated with extensive external tools to improve task\nefficiency. Automated evaluations reveal that CodeEdu substantially enhances\nstudents' coding performance."
                },
                "authors": [
                    {
                        "name": "Jianing Zhao"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Zhiyuan Wen"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Jianing Yin"
                    },
                    {
                        "name": "Ruosong Yang"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "4 pages, 4 figures. Demo video available at:\n  https://youtu.be/9iIVmTT4CVk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19688v2",
                "updated": "2025-07-18T10:36:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    36,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-26T08:49:06Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    49,
                    6,
                    0,
                    146,
                    0
                ],
                "title": "GeoPF: Infusing Geometry into Potential Fields for Reactive Planning in\n  Non-trivial Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoPF: Infusing Geometry into Potential Fields for Reactive Planning in\n  Non-trivial Environments"
                },
                "summary": "Reactive intelligence remains one of the cornerstones of versatile robotics\noperating in cluttered, dynamic, and human-centred environments. Among reactive\napproaches, potential fields (PF) continue to be widely adopted due to their\nsimplicity and real-time applicability. However, existing PF methods typically\noversimplify environmental representations by relying on isotropic, point- or\nsphere-based obstacle approximations. In human-centred settings, this\nsimplification results in overly conservative paths, cumbersome tuning, and\ncomputational overhead -- even breaking real-time requirements. In response, we\npropose the Geometric Potential Field (GeoPF), a reactive motion-planning\nframework that explicitly infuses geometric primitives -- points, lines,\nplanes, cubes, and cylinders -- their structure and spatial relationship in\nmodulating the real-time repulsive response. Extensive quantitative analyses\nconsistently show GeoPF's higher success rates, reduced tuning complexity (a\nsingle parameter set across experiments), and substantially lower computational\ncosts (up to 2 orders of magnitude) compared to traditional PF methods.\nReal-world experiments further validate GeoPF reliability, robustness, and\npractical ease of deployment, as well as its scalability to whole-body\navoidance. GeoPF provides a fresh perspective on reactive planning problems\ndriving geometric-aware temporal motion generation, enabling flexible and\nlow-latency motion planning suitable for modern robotic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reactive intelligence remains one of the cornerstones of versatile robotics\noperating in cluttered, dynamic, and human-centred environments. Among reactive\napproaches, potential fields (PF) continue to be widely adopted due to their\nsimplicity and real-time applicability. However, existing PF methods typically\noversimplify environmental representations by relying on isotropic, point- or\nsphere-based obstacle approximations. In human-centred settings, this\nsimplification results in overly conservative paths, cumbersome tuning, and\ncomputational overhead -- even breaking real-time requirements. In response, we\npropose the Geometric Potential Field (GeoPF), a reactive motion-planning\nframework that explicitly infuses geometric primitives -- points, lines,\nplanes, cubes, and cylinders -- their structure and spatial relationship in\nmodulating the real-time repulsive response. Extensive quantitative analyses\nconsistently show GeoPF's higher success rates, reduced tuning complexity (a\nsingle parameter set across experiments), and substantially lower computational\ncosts (up to 2 orders of magnitude) compared to traditional PF methods.\nReal-world experiments further validate GeoPF reliability, robustness, and\npractical ease of deployment, as well as its scalability to whole-body\navoidance. GeoPF provides a fresh perspective on reactive planning problems\ndriving geometric-aware temporal motion generation, enabling flexible and\nlow-latency motion planning suitable for modern robotic applications."
                },
                "authors": [
                    {
                        "name": "Yuhe Gong"
                    },
                    {
                        "name": "Riddhiman Laha"
                    },
                    {
                        "name": "Luis Figueredo"
                    }
                ],
                "author_detail": {
                    "name": "Luis Figueredo"
                },
                "author": "Luis Figueredo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13803v1",
                "updated": "2025-07-18T10:30:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    30,
                    37,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T10:30:37Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    30,
                    37,
                    4,
                    199,
                    0
                ],
                "title": "GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with\n  Adaptive Low-Rank Compensation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with\n  Adaptive Low-Rank Compensation"
                },
                "summary": "Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely\ndeployed in smart homes, intelligent transport, industrial automation, and\nhealthcare. However, existing systems often face challenges: high model\ncomplexity hinders deployment in resource-constrained environments,\nunidirectional modal alignment neglects inter-modal relationships, and\nrobustness suffers when sensor data is missing. These issues impede efficient\nand robust multimodal perception in real-world IoT settings. To overcome these\nlimitations, we propose GRAM-MAMBA. This framework utilizes the\nlinear-complexity Mamba model for efficient sensor time-series processing,\ncombined with an optimized GRAM matrix strategy for pairwise alignment among\nmodalities, addressing the shortcomings of traditional single-modality\nalignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive\nlow-rank layer compensation strategy to handle missing modalities\npost-training. This strategy freezes the pre-trained model core and irrelevant\nadaptive layers, fine-tuning only those related to available modalities and the\nfusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On\nthe SPAWC2021 indoor positioning dataset, the pre-trained model shows lower\nerror than baselines; adapting to missing modalities yields a 24.5% performance\nboost by training less than 0.2% of parameters. On the USC-HAD human activity\nrecognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),\noutperforming prior work; the update strategy increases F1 by 23% while\ntraining less than 0.3% of parameters. These results highlight GRAM-MAMBA's\npotential for achieving efficient and robust multimodal perception in\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely\ndeployed in smart homes, intelligent transport, industrial automation, and\nhealthcare. However, existing systems often face challenges: high model\ncomplexity hinders deployment in resource-constrained environments,\nunidirectional modal alignment neglects inter-modal relationships, and\nrobustness suffers when sensor data is missing. These issues impede efficient\nand robust multimodal perception in real-world IoT settings. To overcome these\nlimitations, we propose GRAM-MAMBA. This framework utilizes the\nlinear-complexity Mamba model for efficient sensor time-series processing,\ncombined with an optimized GRAM matrix strategy for pairwise alignment among\nmodalities, addressing the shortcomings of traditional single-modality\nalignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive\nlow-rank layer compensation strategy to handle missing modalities\npost-training. This strategy freezes the pre-trained model core and irrelevant\nadaptive layers, fine-tuning only those related to available modalities and the\nfusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On\nthe SPAWC2021 indoor positioning dataset, the pre-trained model shows lower\nerror than baselines; adapting to missing modalities yields a 24.5% performance\nboost by training less than 0.2% of parameters. On the USC-HAD human activity\nrecognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),\noutperforming prior work; the update strategy increases F1 by 23% while\ntraining less than 0.3% of parameters. These results highlight GRAM-MAMBA's\npotential for achieving efficient and robust multimodal perception in\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Weiqi Yang"
                    },
                    {
                        "name": "Xu Zhou"
                    },
                    {
                        "name": "Jingfu Guan"
                    },
                    {
                        "name": "Hao Du"
                    },
                    {
                        "name": "Tianyu Bai"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Bai"
                },
                "author": "Tianyu Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11264v3",
                "updated": "2025-07-18T10:09:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    10,
                    9,
                    5,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-20T04:11:21Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    4,
                    11,
                    21,
                    0,
                    20,
                    0
                ],
                "title": "Code Readability in the Age of Large Language Models: An Industrial Case\n  Study from Atlassian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Readability in the Age of Large Language Models: An Industrial Case\n  Study from Atlassian"
                },
                "summary": "Software engineers spend a significant amount of time reading code during the\nsoftware development process, especially in the age of large language models\n(LLMs) that can automatically generate code. However, little is known about the\nreadability of the LLM-generated code and whether it is still important from\npractitioners' perspectives in this new era. In this paper, we conduct a survey\nto explore the practitioners' perspectives on code readability in the age of\nLLMs and investigate the readability of our LLM-based software development\nagents framework, HULA, by comparing its generated code with human-written code\nin real-world scenarios. Overall, the findings underscore that (1) readability\nremains a critical aspect of software development; (2) the readability of our\nLLM-generated code is comparable to human-written code, fostering the\nestablishment of appropriate trust and driving the broad adoption of our\nLLM-powered software development platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software engineers spend a significant amount of time reading code during the\nsoftware development process, especially in the age of large language models\n(LLMs) that can automatically generate code. However, little is known about the\nreadability of the LLM-generated code and whether it is still important from\npractitioners' perspectives in this new era. In this paper, we conduct a survey\nto explore the practitioners' perspectives on code readability in the age of\nLLMs and investigate the readability of our LLM-based software development\nagents framework, HULA, by comparing its generated code with human-written code\nin real-world scenarios. Overall, the findings underscore that (1) readability\nremains a critical aspect of software development; (2) the readability of our\nLLM-generated code is comparable to human-written code, fostering the\nestablishment of appropriate trust and driving the broad adoption of our\nLLM-powered software development platform."
                },
                "authors": [
                    {
                        "name": "Wannita Takerngsaksiri"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    },
                    {
                        "name": "Micheal Fu"
                    },
                    {
                        "name": "Jirat Pasuksmit"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Wu"
                },
                "author": "Ming Wu",
                "arxiv_comment": "11 pages, 7 figures, 8 tables, Accepted at ICSME",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03221v3",
                "updated": "2025-07-18T09:35:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    35,
                    18,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-03T09:12:52Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    12,
                    52,
                    1,
                    154,
                    0
                ],
                "title": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into\n  Structured Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into\n  Structured Knowledge"
                },
                "summary": "As the volume of scientific literature grows, efficient knowledge\norganization becomes increasingly challenging. Traditional approaches to\nstructuring scientific content are time-consuming and require significant\ndomain expertise, highlighting the need for tool support. We present\nExtracTable, a Human-in-the-Loop (HITL) workflow and framework that assists\nresearchers in transforming unstructured publications into structured\nrepresentations. The workflow combines large language models (LLMs) with\nuser-defined schemas and is designed for downstream integration into knowledge\ngraphs (KGs). Developed and evaluated in the context of the Open Research\nKnowledge Graph (ORKG), ExtracTable automates key steps such as document\npreprocessing and data extraction while ensuring user oversight through\nvalidation. In an evaluation with ORKG community participants following the\nQuality Improvement Paradigm (QIP), ExtracTable demonstrated high usability and\npractical value. Participants gave it an average System Usability Scale (SUS)\nscore of 84.17 (A+, the highest rating). The time to progress from a research\ninterest to literature-based insights was reduced from between 4 hours and 2\nweeks to an average of 24:40 minutes. By streamlining corpus creation and\nstructured data extraction for knowledge graph integration, ExtracTable\nleverages LLMs and user models to accelerate literature reviews. However, human\nvalidation remains essential to ensure quality, and future work will address\nimproving extraction accuracy and entity linking to existing knowledge\nresources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the volume of scientific literature grows, efficient knowledge\norganization becomes increasingly challenging. Traditional approaches to\nstructuring scientific content are time-consuming and require significant\ndomain expertise, highlighting the need for tool support. We present\nExtracTable, a Human-in-the-Loop (HITL) workflow and framework that assists\nresearchers in transforming unstructured publications into structured\nrepresentations. The workflow combines large language models (LLMs) with\nuser-defined schemas and is designed for downstream integration into knowledge\ngraphs (KGs). Developed and evaluated in the context of the Open Research\nKnowledge Graph (ORKG), ExtracTable automates key steps such as document\npreprocessing and data extraction while ensuring user oversight through\nvalidation. In an evaluation with ORKG community participants following the\nQuality Improvement Paradigm (QIP), ExtracTable demonstrated high usability and\npractical value. Participants gave it an average System Usability Scale (SUS)\nscore of 84.17 (A+, the highest rating). The time to progress from a research\ninterest to literature-based insights was reduced from between 4 hours and 2\nweeks to an average of 24:40 minutes. By streamlining corpus creation and\nstructured data extraction for knowledge graph integration, ExtracTable\nleverages LLMs and user models to accelerate literature reviews. However, human\nvalidation remains essential to ensure quality, and future work will address\nimproving extraction accuracy and entity linking to existing knowledge\nresources."
                },
                "authors": [
                    {
                        "name": "Lena John"
                    },
                    {
                        "name": "Ahmed Malek Ghanmi"
                    },
                    {
                        "name": "Tim Wittenborg"
                    },
                    {
                        "name": "S√∂ren Auer"
                    },
                    {
                        "name": "Oliver Karras"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Karras"
                },
                "author": "Oliver Karras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08924v2",
                "updated": "2025-07-18T09:31:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    31,
                    19,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-11T17:56:32Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    56,
                    32,
                    4,
                    192,
                    0
                ],
                "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for\n  LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for\n  LLM Evaluation"
                },
                "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available."
                },
                "authors": [
                    {
                        "name": "Seokhee Hong"
                    },
                    {
                        "name": "Sunkyoung Kim"
                    },
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Soyeon Kim"
                    },
                    {
                        "name": "Yeonjung Hong"
                    },
                    {
                        "name": "Jinsik Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinsik Lee"
                },
                "author": "Jinsik Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17328v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17328v3",
                "updated": "2025-07-18T09:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-28T22:39:03Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    22,
                    39,
                    3,
                    1,
                    28,
                    0
                ],
                "title": "SIC: Similarity-Based Interpretable Image Classification with Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC: Similarity-Based Interpretable Image Classification with Neural\n  Networks"
                },
                "summary": "The deployment of deep learning models in critical domains necessitates a\nbalance between high accuracy and interpretability. We introduce SIC, an\ninherently interpretable neural network that provides local and global\nexplanations of its decision-making process. Leveraging the concept of\ncase-based reasoning, SIC extracts class-representative support vectors from\ntraining images, ensuring they capture relevant features while suppressing\nirrelevant ones. Classification decisions are made by calculating and\naggregating similarity scores between these support vectors and the input's\nlatent feature vector. We employ B-Cos transformations, which align model\nweights with inputs, to yield coherent pixel-level explanations in addition to\nglobal explanations of case-based reasoning. We evaluate SIC on three tasks:\nfine-grained classification on Stanford Dogs and FunnyBirds, multi-label\nclassification on Pascal VOC, and pathology detection on the RSNA dataset.\nResults indicate that SIC not only achieves competitive accuracy compared to\nstate-of-the-art black-box and inherently interpretable models but also offers\ninsightful explanations verified through practical evaluation on the FunnyBirds\nbenchmark. Our theoretical analysis proves that these explanations fulfill\nestablished axioms for explanations. Our findings underscore SIC's potential\nfor applications where understanding model decisions is as critical as the\ndecisions themselves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of deep learning models in critical domains necessitates a\nbalance between high accuracy and interpretability. We introduce SIC, an\ninherently interpretable neural network that provides local and global\nexplanations of its decision-making process. Leveraging the concept of\ncase-based reasoning, SIC extracts class-representative support vectors from\ntraining images, ensuring they capture relevant features while suppressing\nirrelevant ones. Classification decisions are made by calculating and\naggregating similarity scores between these support vectors and the input's\nlatent feature vector. We employ B-Cos transformations, which align model\nweights with inputs, to yield coherent pixel-level explanations in addition to\nglobal explanations of case-based reasoning. We evaluate SIC on three tasks:\nfine-grained classification on Stanford Dogs and FunnyBirds, multi-label\nclassification on Pascal VOC, and pathology detection on the RSNA dataset.\nResults indicate that SIC not only achieves competitive accuracy compared to\nstate-of-the-art black-box and inherently interpretable models but also offers\ninsightful explanations verified through practical evaluation on the FunnyBirds\nbenchmark. Our theoretical analysis proves that these explanations fulfill\nestablished axioms for explanations. Our findings underscore SIC's potential\nfor applications where understanding model decisions is as critical as the\ndecisions themselves."
                },
                "authors": [
                    {
                        "name": "Tom Nuno Wolf"
                    },
                    {
                        "name": "Emre Kavak"
                    },
                    {
                        "name": "Fabian Bongratz"
                    },
                    {
                        "name": "Christian Wachinger"
                    }
                ],
                "author_detail": {
                    "name": "Christian Wachinger"
                },
                "author": "Christian Wachinger",
                "arxiv_comment": "Accepted at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17328v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17328v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13758v1",
                "updated": "2025-07-18T09:06:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    6,
                    10,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T09:06:10Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    9,
                    6,
                    10,
                    4,
                    199,
                    0
                ],
                "title": "The Emperor's New Chain-of-Thought: Probing Reasoning Theater Bias in\n  Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emperor's New Chain-of-Thought: Probing Reasoning Theater Bias in\n  Large Reasoning Models"
                },
                "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and o1 are increasingly used\nas automated evaluators, raising critical questions about their vulnerability\nto the aesthetics of reasoning in LLM-as-a-judge settings. We introduce\nTHEATER, a comprehensive benchmark to systematically evaluate this\nvulnerability-termed Reasoning Theater Bias (RTB)-by comparing LLMs and LRMs\nacross subjective preference and objective factual datasets. Through\ninvestigation of six bias types including Simple Cues and Fake\nChain-of-Thought, we uncover three key findings: (1) in a critical paradox,\nreasoning-specialized LRMs are consistently more susceptible to RTB than\ngeneral-purpose LLMs, particularly in subjective tasks; (2) this creates a\ntask-dependent trade-off, where LRMs show more robustness on factual tasks but\nless on subjective ones; and (3) we identify 'shallow reasoning'-plausible but\nflawed arguments-as the most potent form of RTB. To address this, we design and\nevaluate two prompting strategies: a targeted system prompt that improves\naccuracy by up to 12% on factual tasks but only 1-3% on subjective tasks, and a\nself-reflection mechanism that shows similarly limited effectiveness in the\nmore vulnerable subjective domains. Our work reveals that RTB is a deep-seated\nchallenge for LRM-based evaluation and provides a systematic framework for\ndeveloping more genuinely robust and trustworthy LRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) like DeepSeek-R1 and o1 are increasingly used\nas automated evaluators, raising critical questions about their vulnerability\nto the aesthetics of reasoning in LLM-as-a-judge settings. We introduce\nTHEATER, a comprehensive benchmark to systematically evaluate this\nvulnerability-termed Reasoning Theater Bias (RTB)-by comparing LLMs and LRMs\nacross subjective preference and objective factual datasets. Through\ninvestigation of six bias types including Simple Cues and Fake\nChain-of-Thought, we uncover three key findings: (1) in a critical paradox,\nreasoning-specialized LRMs are consistently more susceptible to RTB than\ngeneral-purpose LLMs, particularly in subjective tasks; (2) this creates a\ntask-dependent trade-off, where LRMs show more robustness on factual tasks but\nless on subjective ones; and (3) we identify 'shallow reasoning'-plausible but\nflawed arguments-as the most potent form of RTB. To address this, we design and\nevaluate two prompting strategies: a targeted system prompt that improves\naccuracy by up to 12% on factual tasks but only 1-3% on subjective tasks, and a\nself-reflection mechanism that shows similarly limited effectiveness in the\nmore vulnerable subjective domains. Our work reveals that RTB is a deep-seated\nchallenge for LRM-based evaluation and provides a systematic framework for\ndeveloping more genuinely robust and trustworthy LRMs."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yubo Fan"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13745v1",
                "updated": "2025-07-18T08:48:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    48,
                    14,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:48:14Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    48,
                    14,
                    4,
                    199,
                    0
                ],
                "title": "Time-Frequency Transfer over Optical Fiber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Frequency Transfer over Optical Fiber"
                },
                "summary": "Optical time-frequency transfer establishes the metrological linkage in\nlarge-scale clock networks, which facilitates various applications. Fiber-based\ntransfer benefits from the abundant deployment of fiber infrastructures to\nachieve this advantage. In this Review, we provide an overview of the advances\nin optical two-way time-frequency transfer, which began with characterizing the\ntime-frequency transfer stability. Then, we discuss the system configuration,\nkey modules, main challenges, and mainstream transfer methods. Finally, the\nReview concludes with an outlook on further applications toward global-scale\nhigh-precision clock networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical time-frequency transfer establishes the metrological linkage in\nlarge-scale clock networks, which facilitates various applications. Fiber-based\ntransfer benefits from the abundant deployment of fiber infrastructures to\nachieve this advantage. In this Review, we provide an overview of the advances\nin optical two-way time-frequency transfer, which began with characterizing the\ntime-frequency transfer stability. Then, we discuss the system configuration,\nkey modules, main challenges, and mainstream transfer methods. Finally, the\nReview concludes with an outlook on further applications toward global-scale\nhigh-precision clock networks."
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Bin Luo"
                    },
                    {
                        "name": "Hong Guo"
                    }
                ],
                "author_detail": {
                    "name": "Hong Guo"
                },
                "author": "Hong Guo",
                "arxiv_doi": "10.1093/nsr/nwaf236",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/nsr/nwaf236",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.13745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13743v1",
                "updated": "2025-07-18T08:44:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    44,
                    27,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:44:27Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    44,
                    27,
                    4,
                    199,
                    0
                ],
                "title": "PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for\n  Equality in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for\n  Equality in LLMs"
                },
                "summary": "Large Language Models (LLMs) frequently reproduce the gender- and\nsexual-identity prejudices embedded in their training corpora, leading to\noutputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of\ngreat importance. To achieve this, we evaluate two parameter-efficient\nfine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt\ntuning - as lightweight alternatives to full-model fine-tuning for mitigating\nsuch biases. Using the WinoQueer benchmark, we quantify bias in three\nopen-source LLMs and observe baseline bias scores reaching up to 98 (out of\n100) across a range of queer identities defined by gender and/or sexual\norientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%\nadditional parameters) on a curated QueerNews corpus reduces those scores by up\nto 50 points and raises neutrality from virtually 0% to as much as 36%.\nSoft-prompt tuning (10 virtual tokens) delivers only marginal improvements.\nThese findings show that LoRA can deliver meaningful fairness gains with\nminimal computation. We advocate broader adoption of community-informed PEFT,\nthe creation of larger queer-authored corpora, and richer evaluation suites\nbeyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) frequently reproduce the gender- and\nsexual-identity prejudices embedded in their training corpora, leading to\noutputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of\ngreat importance. To achieve this, we evaluate two parameter-efficient\nfine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt\ntuning - as lightweight alternatives to full-model fine-tuning for mitigating\nsuch biases. Using the WinoQueer benchmark, we quantify bias in three\nopen-source LLMs and observe baseline bias scores reaching up to 98 (out of\n100) across a range of queer identities defined by gender and/or sexual\norientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%\nadditional parameters) on a curated QueerNews corpus reduces those scores by up\nto 50 points and raises neutrality from virtually 0% to as much as 36%.\nSoft-prompt tuning (10 virtual tokens) delivers only marginal improvements.\nThese findings show that LoRA can deliver meaningful fairness gains with\nminimal computation. We advocate broader adoption of community-informed PEFT,\nthe creation of larger queer-authored corpora, and richer evaluation suites\nbeyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive."
                },
                "authors": [
                    {
                        "name": "Maluna Menke"
                    },
                    {
                        "name": "Thilo Hagendorff"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Hagendorff"
                },
                "author": "Thilo Hagendorff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02145v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02145v4",
                "updated": "2025-07-18T08:39:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    39,
                    33,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-04T09:19:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    19,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "From Words to Collisions: LLM-Guided Evaluation and Adversarial\n  Generation of Safety-Critical Driving Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Words to Collisions: LLM-Guided Evaluation and Adversarial\n  Generation of Safety-Critical Driving Scenarios"
                },
                "summary": "Ensuring the safety of autonomous vehicles requires virtual scenario-based\ntesting, which depends on the robust evaluation and generation of\nsafety-critical scenarios. So far, researchers have used scenario-based testing\nframeworks that rely heavily on handcrafted scenarios as safety metrics. To\nreduce the effort of human interpretation and overcome the limited scalability\nof these approaches, we combine Large Language Models (LLMs) with structured\nscenario parsing and prompt engineering to automatically evaluate and generate\nsafety-critical driving scenarios. We introduce Cartesian and Ego-centric\nprompt strategies for scenario evaluation, and an adversarial generation module\nthat modifies trajectories of risk-inducing vehicles (ego-attackers) to create\ncritical scenarios. We validate our approach using a 2D simulation framework\nand multiple pre-trained LLMs. The results show that the evaluation module\neffectively detects collision scenarios and infers scenario safety. Meanwhile,\nthe new generation module identifies high-risk agents and synthesizes\nrealistic, safety-critical scenarios. We conclude that an LLM equipped with\ndomain-informed prompting techniques can effectively evaluate and generate\nsafety-critical driving scenarios, reducing dependence on handcrafted metrics.\nWe release our open-source code and scenarios at:\nhttps://github.com/TUM-AVS/From-Words-to-Collisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety of autonomous vehicles requires virtual scenario-based\ntesting, which depends on the robust evaluation and generation of\nsafety-critical scenarios. So far, researchers have used scenario-based testing\nframeworks that rely heavily on handcrafted scenarios as safety metrics. To\nreduce the effort of human interpretation and overcome the limited scalability\nof these approaches, we combine Large Language Models (LLMs) with structured\nscenario parsing and prompt engineering to automatically evaluate and generate\nsafety-critical driving scenarios. We introduce Cartesian and Ego-centric\nprompt strategies for scenario evaluation, and an adversarial generation module\nthat modifies trajectories of risk-inducing vehicles (ego-attackers) to create\ncritical scenarios. We validate our approach using a 2D simulation framework\nand multiple pre-trained LLMs. The results show that the evaluation module\neffectively detects collision scenarios and infers scenario safety. Meanwhile,\nthe new generation module identifies high-risk agents and synthesizes\nrealistic, safety-critical scenarios. We conclude that an LLM equipped with\ndomain-informed prompting techniques can effectively evaluate and generate\nsafety-critical driving scenarios, reducing dependence on handcrafted metrics.\nWe release our open-source code and scenarios at:\nhttps://github.com/TUM-AVS/From-Words-to-Collisions."
                },
                "authors": [
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Mattia Piccinini"
                    },
                    {
                        "name": "Korbinian Moller"
                    },
                    {
                        "name": "Amr Alanwar"
                    },
                    {
                        "name": "Johannes Betz"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Betz"
                },
                "author": "Johannes Betz",
                "arxiv_comment": "Final Version and Paper Accepted at IEEE ITSC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02145v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02145v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13737v1",
                "updated": "2025-07-18T08:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    33,
                    30,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    33,
                    30,
                    4,
                    199,
                    0
                ],
                "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal\n  Sensors and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal\n  Sensors and LLMs"
                },
                "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed."
                },
                "authors": [
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Xiaoyuan Ren"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Onat Gungor"
                    },
                    {
                        "name": "Xiaofan Yu"
                    },
                    {
                        "name": "Tajana Rosing"
                    }
                ],
                "author_detail": {
                    "name": "Tajana Rosing"
                },
                "author": "Tajana Rosing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13732v1",
                "updated": "2025-07-18T08:28:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    28,
                    53,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:28:53Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    28,
                    53,
                    4,
                    199,
                    0
                ],
                "title": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction"
                },
                "summary": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Guillaume Zambrano"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Zambrano"
                },
                "author": "Guillaume Zambrano",
                "arxiv_comment": "23 pages, 24 figures shorter version submitted to JURIX 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13729v1",
                "updated": "2025-07-18T08:20:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    20,
                    16,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:20:16Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    20,
                    16,
                    4,
                    199,
                    0
                ],
                "title": "AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios\n  with an Agentic LLM Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios\n  with an Agentic LLM Framework"
                },
                "summary": "Rare, yet critical, scenarios pose a significant challenge in testing and\nevaluating autonomous driving planners. Relying solely on real-world driving\nscenes requires collecting massive datasets to capture these scenarios. While\nautomatic generation of traffic scenarios appears promising, data-driven models\nrequire extensive training data and often lack fine-grained control over the\noutput. Moreover, generating novel scenarios from scratch can introduce a\ndistributional shift from the original training scenes which undermines the\nvalidity of evaluations especially for learning-based planners. To sidestep\nthis, recent work proposes to generate challenging scenarios by augmenting\noriginal scenarios from the test set. However, this involves the manual\naugmentation of scenarios by domain experts. An approach that is unable to meet\nthe demands for scale in the evaluation of self-driving systems. Therefore,\nthis paper introduces a novel LLM-agent based framework for augmenting\nreal-world traffic scenarios using natural language descriptions, addressing\nthe limitations of existing methods. A key innovation is the use of an agentic\ndesign, enabling fine-grained control over the output and maintaining high\nperformance even with smaller, cost-effective LLMs. Extensive human expert\nevaluation demonstrates our framework's ability to accurately adhere to user\nintent, generating high quality augmented scenarios comparable to those created\nmanually.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare, yet critical, scenarios pose a significant challenge in testing and\nevaluating autonomous driving planners. Relying solely on real-world driving\nscenes requires collecting massive datasets to capture these scenarios. While\nautomatic generation of traffic scenarios appears promising, data-driven models\nrequire extensive training data and often lack fine-grained control over the\noutput. Moreover, generating novel scenarios from scratch can introduce a\ndistributional shift from the original training scenes which undermines the\nvalidity of evaluations especially for learning-based planners. To sidestep\nthis, recent work proposes to generate challenging scenarios by augmenting\noriginal scenarios from the test set. However, this involves the manual\naugmentation of scenarios by domain experts. An approach that is unable to meet\nthe demands for scale in the evaluation of self-driving systems. Therefore,\nthis paper introduces a novel LLM-agent based framework for augmenting\nreal-world traffic scenarios using natural language descriptions, addressing\nthe limitations of existing methods. A key innovation is the use of an agentic\ndesign, enabling fine-grained control over the output and maintaining high\nperformance even with smaller, cost-effective LLMs. Extensive human expert\nevaluation demonstrates our framework's ability to accurately adhere to user\nintent, generating high quality augmented scenarios comparable to those created\nmanually."
                },
                "authors": [
                    {
                        "name": "Yu Yao"
                    },
                    {
                        "name": "Salil Bhatnagar"
                    },
                    {
                        "name": "Markus Mazzola"
                    },
                    {
                        "name": "Vasileios Belagiannis"
                    },
                    {
                        "name": "Igor Gilitschenski"
                    },
                    {
                        "name": "Luigi Palmieri"
                    },
                    {
                        "name": "Simon Razniewski"
                    },
                    {
                        "name": "Marcel Hallgarten"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Hallgarten"
                },
                "author": "Marcel Hallgarten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13720v1",
                "updated": "2025-07-18T08:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    0,
                    9,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T08:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    8,
                    0,
                    9,
                    4,
                    199,
                    0
                ],
                "title": "Quantum Blockchain Survey: Foundations, Trends, and Gaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Blockchain Survey: Foundations, Trends, and Gaps"
                },
                "summary": "Quantum computing poses fundamental risks to classical blockchain systems by\nundermining widely used cryptographic primitives. In response, two major\nresearch directions have emerged: post-quantum blockchains, which integrate\nquantum-resistant algorithms, and quantum blockchains, which leverage quantum\nproperties such as entanglement and quantum key distribution. This survey\nreviews key developments in both areas, analyzing their cryptographic\nfoundations, architectural designs, and implementation challenges. This work\nprovides a comparative overview of technical proposals, highlight trade-offs in\nsecurity, scalability, and deployment, and identify open research problems\nacross hardware, consensus, and network design. The goal is to offer a\nstructured and comprehensive reference for advancing secure blockchain systems\nin the quantum era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing poses fundamental risks to classical blockchain systems by\nundermining widely used cryptographic primitives. In response, two major\nresearch directions have emerged: post-quantum blockchains, which integrate\nquantum-resistant algorithms, and quantum blockchains, which leverage quantum\nproperties such as entanglement and quantum key distribution. This survey\nreviews key developments in both areas, analyzing their cryptographic\nfoundations, architectural designs, and implementation challenges. This work\nprovides a comparative overview of technical proposals, highlight trade-offs in\nsecurity, scalability, and deployment, and identify open research problems\nacross hardware, consensus, and network design. The goal is to offer a\nstructured and comprehensive reference for advancing secure blockchain systems\nin the quantum era."
                },
                "authors": [
                    {
                        "name": "Saurav Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Saurav Ghosh"
                },
                "author": "Saurav Ghosh",
                "arxiv_comment": "12 Pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M10, 81P94, 94A60 68M10, 81P94, 94A60 68M10, 81P94, 94A60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; E.3; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13712v1",
                "updated": "2025-07-18T07:52:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    52,
                    19,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T07:52:19Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    52,
                    19,
                    4,
                    199,
                    0
                ],
                "title": "LLaPipe: LLM-Guided Reinforcement Learning for Automated Data\n  Preparation Pipeline Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaPipe: LLM-Guided Reinforcement Learning for Automated Data\n  Preparation Pipeline Construction"
                },
                "summary": "Automated data preparation is crucial for democratizing machine learning, yet\nexisting reinforcement learning (RL) based approaches suffer from inefficient\nexploration in the vast space of possible preprocessing pipelines. We present\nLLaPipe, a novel framework that addresses this exploration bottleneck by\nintegrating Large Language Models (LLMs) as intelligent policy advisors. Unlike\ntraditional methods that rely solely on statistical features and blind\ntrial-and-error, LLaPipe leverages the semantic understanding capabilities of\nLLMs to provide contextually relevant exploration guidance. Our framework\nintroduces three key innovations: (1) an LLM Policy Advisor that analyzes\ndataset semantics and pipeline history to suggest promising preprocessing\noperations, (2) an Experience Distillation mechanism that mines successful\npatterns from past pipelines and transfers this knowledge to guide future\nexploration, and (3) an Adaptive Advisor Triggering strategy\n(Advisor\\textsuperscript{+}) that dynamically determines when LLM intervention\nis most beneficial, balancing exploration effectiveness with computational\ncost. Through extensive experiments on 18 diverse datasets spanning multiple\ndomains, we demonstrate that LLaPipe achieves up to 22.4\\% improvement in\npipeline quality and 2.3$\\times$ faster convergence compared to\nstate-of-the-art RL-based methods, while maintaining computational efficiency\nthrough selective LLM usage (averaging only 19.0\\% of total exploration steps).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated data preparation is crucial for democratizing machine learning, yet\nexisting reinforcement learning (RL) based approaches suffer from inefficient\nexploration in the vast space of possible preprocessing pipelines. We present\nLLaPipe, a novel framework that addresses this exploration bottleneck by\nintegrating Large Language Models (LLMs) as intelligent policy advisors. Unlike\ntraditional methods that rely solely on statistical features and blind\ntrial-and-error, LLaPipe leverages the semantic understanding capabilities of\nLLMs to provide contextually relevant exploration guidance. Our framework\nintroduces three key innovations: (1) an LLM Policy Advisor that analyzes\ndataset semantics and pipeline history to suggest promising preprocessing\noperations, (2) an Experience Distillation mechanism that mines successful\npatterns from past pipelines and transfers this knowledge to guide future\nexploration, and (3) an Adaptive Advisor Triggering strategy\n(Advisor\\textsuperscript{+}) that dynamically determines when LLM intervention\nis most beneficial, balancing exploration effectiveness with computational\ncost. Through extensive experiments on 18 diverse datasets spanning multiple\ndomains, we demonstrate that LLaPipe achieves up to 22.4\\% improvement in\npipeline quality and 2.3$\\times$ faster convergence compared to\nstate-of-the-art RL-based methods, while maintaining computational efficiency\nthrough selective LLM usage (averaging only 19.0\\% of total exploration steps)."
                },
                "authors": [
                    {
                        "name": "Jing Chang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Jinbin Huang"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Jianbin Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jianbin Qin"
                },
                "author": "Jianbin Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12385v2",
                "updated": "2025-07-18T07:52:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    52,
                    14,
                    4,
                    199,
                    0
                ],
                "published": "2024-06-18T08:15:52Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    8,
                    15,
                    52,
                    1,
                    170,
                    0
                ],
                "title": "Fast Graph Vector Search via Hardware Acceleration and\n  Delayed-Synchronization Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Graph Vector Search via Hardware Acceleration and\n  Delayed-Synchronization Traversal"
                },
                "summary": "Vector search systems are indispensable in large language model (LLM)\nserving, search engines, and recommender systems, where minimizing online\nsearch latency is essential. Among various algorithms, graph-based vector\nsearch (GVS) is particularly popular due to its high search performance and\nquality. However, reducing GVS latency by intra-query parallelization remains\nchallenging due to limitations imposed by both existing hardware architectures\n(CPUs and GPUs) and the inherent difficulty of parallelizing graph traversals.\nTo efficiently serve low-latency GVS, we co-design hardware and algorithm by\nproposing Falcon and Delayed-Synchronization Traversal (DST). Falcon is a\nhardware GVS accelerator that implements efficient GVS operators, pipelines\nthese operators, and reduces memory accesses by tracking search states with an\non-chip Bloom filter. DST is an efficient graph traversal algorithm that\nsimultaneously improves search performance and quality by relaxing traversal\norders to maximize accelerator utilization. Evaluation across various graphs\nand datasets shows that Falcon, prototyped on FPGAs, together with DST,\nachieves up to 4.3x and 19.5x lower latency and up to 8.0x and 26.9x\nimprovements in energy efficiency over CPU- and GPU-based GVS systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector search systems are indispensable in large language model (LLM)\nserving, search engines, and recommender systems, where minimizing online\nsearch latency is essential. Among various algorithms, graph-based vector\nsearch (GVS) is particularly popular due to its high search performance and\nquality. However, reducing GVS latency by intra-query parallelization remains\nchallenging due to limitations imposed by both existing hardware architectures\n(CPUs and GPUs) and the inherent difficulty of parallelizing graph traversals.\nTo efficiently serve low-latency GVS, we co-design hardware and algorithm by\nproposing Falcon and Delayed-Synchronization Traversal (DST). Falcon is a\nhardware GVS accelerator that implements efficient GVS operators, pipelines\nthese operators, and reduces memory accesses by tracking search states with an\non-chip Bloom filter. DST is an efficient graph traversal algorithm that\nsimultaneously improves search performance and quality by relaxing traversal\norders to maximize accelerator utilization. Evaluation across various graphs\nand datasets shows that Falcon, prototyped on FPGAs, together with DST,\nachieves up to 4.3x and 19.5x lower latency and up to 8.0x and 26.9x\nimprovements in energy efficiency over CPU- and GPU-based GVS systems."
                },
                "authors": [
                    {
                        "name": "Wenqi Jiang"
                    },
                    {
                        "name": "Hang Hu"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Gustavo Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Alonso"
                },
                "author": "Gustavo Alonso",
                "arxiv_doi": "10.14778/3749646.3749655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3749646.3749655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.12385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by VLDB'25",
                "arxiv_journal_ref": "Proceedings of the VLDB Endowment Volume 18, 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13205v2",
                "updated": "2025-07-18T07:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    49,
                    41,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-17T15:15:43Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    15,
                    43,
                    3,
                    198,
                    0
                ],
                "title": "Automatically assessing oral narratives of Afrikaans and isiXhosa\n  children",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically assessing oral narratives of Afrikaans and isiXhosa\n  children"
                },
                "summary": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning."
                },
                "authors": [
                    {
                        "name": "Retief Louw"
                    },
                    {
                        "name": "Emma Sharratt"
                    },
                    {
                        "name": "Febe de Wet"
                    },
                    {
                        "name": "Christiaan Jacobs"
                    },
                    {
                        "name": "Annelien Smith"
                    },
                    {
                        "name": "Herman Kamper"
                    }
                ],
                "author_detail": {
                    "name": "Herman Kamper"
                },
                "author": "Herman Kamper",
                "arxiv_comment": "Accepted to SLaTE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13710v1",
                "updated": "2025-07-18T07:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    43,
                    22,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T07:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    43,
                    22,
                    4,
                    199,
                    0
                ],
                "title": "CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for\n  Automated Data Preparation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for\n  Automated Data Preparation"
                },
                "summary": "Data preparation is a foundational yet notoriously challenging component of\nthe machine learning lifecycle, characterized by a vast combinatorial search\nspace of potential operator sequences. While reinforcement learning (RL) offers\na promising direction, existing approaches are inefficient as they fail to\ncapture the structured, hierarchical nature of the problem. We argue that\nHierarchical Reinforcement Learning (HRL), a paradigm that has been successful\nin other domains, provides a conceptually ideal yet previously unexplored\nframework for this task. However, a naive HRL implementation with a `hard\nhierarchy' is prone to suboptimal, irreversible decisions. To address this, we\nintroduce CogniQ-H, the first framework to implement a soft hierarchical\nparadigm for robust, end-to-end automated data preparation. CogniQ-H formulates\naction selection as a Bayesian inference problem. A high-level strategic prior,\ngenerated by a Large Language Model (LLM), guides exploration\nprobabilistically. This prior is synergistically combined with a fine-grained\noperator quality score from a supervised Learning-to-Rank (LTR) model and a\nlong-term value estimate from the agent's own Q-function. This hybrid\narchitecture allows CogniQ-H to balance strategic guidance with adaptive,\nevidence-based decision-making. Through extensive experiments on 18 diverse\ndatasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to\n13.9\\% improvement in pipeline quality and 2.8$\\times$ faster convergence\ncompared to state-of-the-art RL-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data preparation is a foundational yet notoriously challenging component of\nthe machine learning lifecycle, characterized by a vast combinatorial search\nspace of potential operator sequences. While reinforcement learning (RL) offers\na promising direction, existing approaches are inefficient as they fail to\ncapture the structured, hierarchical nature of the problem. We argue that\nHierarchical Reinforcement Learning (HRL), a paradigm that has been successful\nin other domains, provides a conceptually ideal yet previously unexplored\nframework for this task. However, a naive HRL implementation with a `hard\nhierarchy' is prone to suboptimal, irreversible decisions. To address this, we\nintroduce CogniQ-H, the first framework to implement a soft hierarchical\nparadigm for robust, end-to-end automated data preparation. CogniQ-H formulates\naction selection as a Bayesian inference problem. A high-level strategic prior,\ngenerated by a Large Language Model (LLM), guides exploration\nprobabilistically. This prior is synergistically combined with a fine-grained\noperator quality score from a supervised Learning-to-Rank (LTR) model and a\nlong-term value estimate from the agent's own Q-function. This hybrid\narchitecture allows CogniQ-H to balance strategic guidance with adaptive,\nevidence-based decision-making. Through extensive experiments on 18 diverse\ndatasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to\n13.9\\% improvement in pipeline quality and 2.8$\\times$ faster convergence\ncompared to state-of-the-art RL-based methods."
                },
                "authors": [
                    {
                        "name": "Jing Chang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Jinbin Huang"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Jianbin Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jianbin Qin"
                },
                "author": "Jianbin Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00691v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00691v4",
                "updated": "2025-07-18T07:40:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    40,
                    22,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-02T06:32:23Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    6,
                    32,
                    23,
                    6,
                    33,
                    0
                ],
                "title": "To Code or not to Code? Adaptive Tool Integration for Math Language\n  Models via Expectation-Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Code or not to Code? Adaptive Tool Integration for Math Language\n  Models via Expectation-Maximization"
                },
                "summary": "Recent advances in mathematical problem-solving with language models (LMs)\nintegrate chain-of-thought (CoT) reasoning and code execution to harness their\ncomplementary strengths. However, existing hybrid frameworks exhibit a critical\nlimitation: they depend on externally dictated instructions or rigid\ncode-integration templates, lacking metacognitive awareness -- the capacity to\ndynamically evaluate intrinsic capabilities and autonomously determine when and\nhow to integrate tools. This rigidity motivates our study of autonomous code\nintegration, enabling models to adapt tool-usage strategies as their reasoning\nabilities evolve during training.\n  While reinforcement learning (RL) shows promise for boosting LLM reasoning at\nscale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning\nautonomous code integration due to inadequate exploration of the vast\ncombinatorial space of CoT-code interleaving patterns. To address this\nchallenge, we propose a novel Expectation-Maximization (EM) framework that\nsynergizes structured exploration (E-step) with off-policy RL optimization\n(M-step), creating a self-reinforcing cycle between metacognitive tool-use\ndecisions and evolving capabilities. Experiments reveal our method achieves\nsuperior results through improved exploration. Notably, our 7B model improves\nover 11% on MATH500 and 9.4% on AIME without o1-like CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in mathematical problem-solving with language models (LMs)\nintegrate chain-of-thought (CoT) reasoning and code execution to harness their\ncomplementary strengths. However, existing hybrid frameworks exhibit a critical\nlimitation: they depend on externally dictated instructions or rigid\ncode-integration templates, lacking metacognitive awareness -- the capacity to\ndynamically evaluate intrinsic capabilities and autonomously determine when and\nhow to integrate tools. This rigidity motivates our study of autonomous code\nintegration, enabling models to adapt tool-usage strategies as their reasoning\nabilities evolve during training.\n  While reinforcement learning (RL) shows promise for boosting LLM reasoning at\nscale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning\nautonomous code integration due to inadequate exploration of the vast\ncombinatorial space of CoT-code interleaving patterns. To address this\nchallenge, we propose a novel Expectation-Maximization (EM) framework that\nsynergizes structured exploration (E-step) with off-policy RL optimization\n(M-step), creating a self-reinforcing cycle between metacognitive tool-use\ndecisions and evolving capabilities. Experiments reveal our method achieves\nsuperior results through improved exploration. Notably, our 7B model improves\nover 11% on MATH500 and 9.4% on AIME without o1-like CoT."
                },
                "authors": [
                    {
                        "name": "Haozhe Wang"
                    },
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Fengming Zhu"
                    },
                    {
                        "name": "Weidi Xu"
                    },
                    {
                        "name": "Wei Chu"
                    },
                    {
                        "name": "Fangzhen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhen Lin"
                },
                "author": "Fangzhen Lin",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00691v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00691v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17562v2",
                "updated": "2025-07-18T07:39:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    39,
                    50,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-21T03:13:08Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    3,
                    13,
                    8,
                    5,
                    172,
                    0
                ],
                "title": "LLM-driven Medical Report Generation via Communication-efficient\n  Heterogeneous Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven Medical Report Generation via Communication-efficient\n  Heterogeneous Federated Learning"
                },
                "summary": "LLMs have demonstrated significant potential in Medical Report Generation\n(MRG), yet their development requires large amounts of medical image-report\npairs, which are commonly scattered across multiple centers. Centralizing these\ndata is exceptionally challenging due to privacy regulations, thereby impeding\nmodel development and broader adoption of LLM-driven MRG models. To address\nthis challenge, we present FedMRG, the first framework that leverages Federated\nLearning (FL) to enable privacy-preserving, multi-center development of\nLLM-driven MRG models, specifically designed to overcome the critical challenge\nof communication-efficient LLM training under multi-modal data heterogeneity.\nTo start with, our framework tackles the fundamental challenge of communication\noverhead in FL-LLM tuning by employing low-rank factorization to efficiently\ndecompose parameter updates, significantly reducing gradient transmission costs\nand making LLM-driven MRG feasible in bandwidth-constrained FL settings.\nFurthermore, we observed the dual heterogeneity in MRG under the FL scenario:\nvarying image characteristics across medical centers, as well as diverse\nreporting styles and terminology preferences. To address this, we further\nenhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,\ncoupled with diagnosis-driven prompts, which capture both globally\ngeneralizable and locally distinctive features while maintaining diagnostic\naccuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder\nthat harmonizes generic and specialized adapters to address variations in\nreporting styles and terminology. Through extensive evaluation of our\nestablished FL-MRG benchmark, we demonstrate the generalizability and\nadaptability of FedMRG, underscoring its potential in harnessing multi-center\ndata and generating clinically accurate reports while maintaining communication\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated significant potential in Medical Report Generation\n(MRG), yet their development requires large amounts of medical image-report\npairs, which are commonly scattered across multiple centers. Centralizing these\ndata is exceptionally challenging due to privacy regulations, thereby impeding\nmodel development and broader adoption of LLM-driven MRG models. To address\nthis challenge, we present FedMRG, the first framework that leverages Federated\nLearning (FL) to enable privacy-preserving, multi-center development of\nLLM-driven MRG models, specifically designed to overcome the critical challenge\nof communication-efficient LLM training under multi-modal data heterogeneity.\nTo start with, our framework tackles the fundamental challenge of communication\noverhead in FL-LLM tuning by employing low-rank factorization to efficiently\ndecompose parameter updates, significantly reducing gradient transmission costs\nand making LLM-driven MRG feasible in bandwidth-constrained FL settings.\nFurthermore, we observed the dual heterogeneity in MRG under the FL scenario:\nvarying image characteristics across medical centers, as well as diverse\nreporting styles and terminology preferences. To address this, we further\nenhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,\ncoupled with diagnosis-driven prompts, which capture both globally\ngeneralizable and locally distinctive features while maintaining diagnostic\naccuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder\nthat harmonizes generic and specialized adapters to address variations in\nreporting styles and terminology. Through extensive evaluation of our\nestablished FL-MRG benchmark, we demonstrate the generalizability and\nadaptability of FedMRG, underscoring its potential in harnessing multi-center\ndata and generating clinically accurate reports while maintaining communication\nefficiency."
                },
                "authors": [
                    {
                        "name": "Haoxuan Che"
                    },
                    {
                        "name": "Haibo Jin"
                    },
                    {
                        "name": "Zhengrui Guo"
                    },
                    {
                        "name": "Yi Lin"
                    },
                    {
                        "name": "Cheng Jin"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "arxiv_comment": "Accepted by IEEE TMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03068v2",
                "updated": "2025-07-18T07:38:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    38,
                    55,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-03T17:57:12Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    57,
                    12,
                    3,
                    184,
                    0
                ],
                "title": "Mitigating Goal Misgeneralization via Minimax Regret",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Goal Misgeneralization via Minimax Regret"
                },
                "summary": "Safe generalization in reinforcement learning requires not only that a\nlearned policy acts capably in new situations, but also that it uses its\ncapabilities towards the pursuit of the designer's intended goal. The latter\nrequirement may fail when a proxy goal incentivizes similar behavior to the\nintended goal within the training environment, but not in novel deployment\nenvironments. This creates the risk that policies will behave as if in pursuit\nof the proxy goal, rather than the intended goal, in deployment -- a phenomenon\nknown as goal misgeneralization. In this paper, we formalize this problem\nsetting in order to theoretically study the possibility of goal\nmisgeneralization under different training objectives. We show that goal\nmisgeneralization is possible under approximate optimization of the maximum\nexpected value (MEV) objective, but not the minimax expected regret (MMER)\nobjective. We then empirically show that the standard MEV-based training method\nof domain randomization exhibits goal misgeneralization in\nprocedurally-generated grid-world environments, whereas current regret-based\nunsupervised environment design (UED) methods are more robust to goal\nmisgeneralization (though they don't find MMER policies in all cases). Our\nfindings suggest that minimax expected regret is a promising approach to\nmitigating goal misgeneralization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe generalization in reinforcement learning requires not only that a\nlearned policy acts capably in new situations, but also that it uses its\ncapabilities towards the pursuit of the designer's intended goal. The latter\nrequirement may fail when a proxy goal incentivizes similar behavior to the\nintended goal within the training environment, but not in novel deployment\nenvironments. This creates the risk that policies will behave as if in pursuit\nof the proxy goal, rather than the intended goal, in deployment -- a phenomenon\nknown as goal misgeneralization. In this paper, we formalize this problem\nsetting in order to theoretically study the possibility of goal\nmisgeneralization under different training objectives. We show that goal\nmisgeneralization is possible under approximate optimization of the maximum\nexpected value (MEV) objective, but not the minimax expected regret (MMER)\nobjective. We then empirically show that the standard MEV-based training method\nof domain randomization exhibits goal misgeneralization in\nprocedurally-generated grid-world environments, whereas current regret-based\nunsupervised environment design (UED) methods are more robust to goal\nmisgeneralization (though they don't find MMER policies in all cases). Our\nfindings suggest that minimax expected regret is a promising approach to\nmitigating goal misgeneralization."
                },
                "authors": [
                    {
                        "name": "Karim Abdel Sadek"
                    },
                    {
                        "name": "Matthew Farrugia-Roberts"
                    },
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Hannah Erlebach"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Michael Dennis"
                    }
                ],
                "author_detail": {
                    "name": "Michael Dennis"
                },
                "author": "Michael Dennis",
                "arxiv_comment": "Published at RLC 2025. 11 pages main text. v2: no changes to PDF, fix\n  arXiv title",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17735v2",
                "updated": "2025-07-18T07:34:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    34,
                    40,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-23T10:56:06Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    56,
                    6,
                    4,
                    143,
                    0
                ],
                "title": "SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator"
                },
                "summary": "Large Language Model (LLM)-based agents are increasingly deployed in\nreal-world applications such as \"digital assistants, autonomous customer\nservice, and decision-support systems\", where their ability to \"interact in\nmulti-turn, tool-augmented environments\" makes them indispensable. However,\nensuring the safety of these agents remains a significant challenge due to the\ndiverse and complex risks arising from dynamic user interactions, external tool\nusage, and the potential for unintended harmful behaviors. To address this\ncritical issue, we propose AutoSafe, the first framework that systematically\nenhances agent safety through fully automated synthetic data generation.\nConcretely, 1) we introduce an open and extensible threat model, OTS, which\nformalizes how unsafe behaviors emerge from the interplay of user instructions,\ninteraction contexts, and agent actions. This enables precise modeling of\nsafety risks across diverse scenarios. 2) we develop a fully automated data\ngeneration pipeline that simulates unsafe user behaviors, applies\nself-reflective reasoning to generate safe responses, and constructs a\nlarge-scale, diverse, and high-quality safety training dataset-eliminating the\nneed for hazardous real-world data collection. To evaluate the effectiveness of\nour framework, we design comprehensive experiments on both synthetic and\nreal-world safety benchmarks. Results demonstrate that AutoSafe boosts safety\nscores by 45% on average and achieves a 28.91% improvement on real-world tasks,\nvalidating the generalization ability of our learned safety strategies. These\nresults highlight the practical advancement and scalability of AutoSafe in\nbuilding safer LLM-based agents for real-world deployment. We have released the\nproject page at https://auto-safe.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents are increasingly deployed in\nreal-world applications such as \"digital assistants, autonomous customer\nservice, and decision-support systems\", where their ability to \"interact in\nmulti-turn, tool-augmented environments\" makes them indispensable. However,\nensuring the safety of these agents remains a significant challenge due to the\ndiverse and complex risks arising from dynamic user interactions, external tool\nusage, and the potential for unintended harmful behaviors. To address this\ncritical issue, we propose AutoSafe, the first framework that systematically\nenhances agent safety through fully automated synthetic data generation.\nConcretely, 1) we introduce an open and extensible threat model, OTS, which\nformalizes how unsafe behaviors emerge from the interplay of user instructions,\ninteraction contexts, and agent actions. This enables precise modeling of\nsafety risks across diverse scenarios. 2) we develop a fully automated data\ngeneration pipeline that simulates unsafe user behaviors, applies\nself-reflective reasoning to generate safe responses, and constructs a\nlarge-scale, diverse, and high-quality safety training dataset-eliminating the\nneed for hazardous real-world data collection. To evaluate the effectiveness of\nour framework, we design comprehensive experiments on both synthetic and\nreal-world safety benchmarks. Results demonstrate that AutoSafe boosts safety\nscores by 45% on average and achieves a 28.91% improvement on real-world tasks,\nvalidating the generalization ability of our learned safety strategies. These\nresults highlight the practical advancement and scalability of AutoSafe in\nbuilding safer LLM-based agents for real-world deployment. We have released the\nproject page at https://auto-safe.github.io/."
                },
                "authors": [
                    {
                        "name": "Xueyang Zhou"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Lin Lu"
                    },
                    {
                        "name": "Jiawen Shi"
                    },
                    {
                        "name": "Guiyao Tie"
                    },
                    {
                        "name": "Yongtian Xu"
                    },
                    {
                        "name": "Lixing Chen"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "38 pages;12 figures;12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08208v2",
                "updated": "2025-07-18T07:31:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    31,
                    17,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-14T15:46:39Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    46,
                    39,
                    1,
                    14,
                    0
                ],
                "title": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems"
                },
                "summary": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development."
                },
                "authors": [
                    {
                        "name": "Mohita Chowdhury"
                    },
                    {
                        "name": "Yajie Vera He"
                    },
                    {
                        "name": "Jared Joselowitz"
                    },
                    {
                        "name": "Aisling Higham"
                    },
                    {
                        "name": "Ernest Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ernest Lim"
                },
                "author": "Ernest Lim",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13705v1",
                "updated": "2025-07-18T07:20:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    20,
                    52,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T07:20:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    7,
                    20,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations"
                },
                "summary": "Large Language Models (LLMs) are increasingly being implemented as joint\ndecision-makers and explanation generators for Group Recommender Systems (GRS).\nIn this paper, we evaluate these recommendations and explanations by comparing\nthem to social choice-based aggregation strategies. Our results indicate that\nLLM-generated recommendations often resembled those produced by Additive\nUtilitarian (ADD) aggregation. However, the explanations typically referred to\naveraging ratings (resembling but not identical to ADD aggregation). Group\nstructure, uniform or divergent, did not impact the recommendations.\nFurthermore, LLMs regularly claimed additional criteria such as user or item\nsimilarity, diversity, or used undefined popularity metrics or thresholds. Our\nfindings have important implications for LLMs in the GRS pipeline as well as\nstandard aggregation strategies. Additional criteria in explanations were\ndependent on the number of ratings in the group scenario, indicating potential\ninefficiency of standard aggregation methods at larger item set sizes.\nAdditionally, inconsistent and ambiguous explanations undermine transparency\nand explainability, which are key motivations behind the use of LLMs for GRS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being implemented as joint\ndecision-makers and explanation generators for Group Recommender Systems (GRS).\nIn this paper, we evaluate these recommendations and explanations by comparing\nthem to social choice-based aggregation strategies. Our results indicate that\nLLM-generated recommendations often resembled those produced by Additive\nUtilitarian (ADD) aggregation. However, the explanations typically referred to\naveraging ratings (resembling but not identical to ADD aggregation). Group\nstructure, uniform or divergent, did not impact the recommendations.\nFurthermore, LLMs regularly claimed additional criteria such as user or item\nsimilarity, diversity, or used undefined popularity metrics or thresholds. Our\nfindings have important implications for LLMs in the GRS pipeline as well as\nstandard aggregation strategies. Additional criteria in explanations were\ndependent on the number of ratings in the group scenario, indicating potential\ninefficiency of standard aggregation methods at larger item set sizes.\nAdditionally, inconsistent and ambiguous explanations undermine transparency\nand explainability, which are key motivations behind the use of LLMs for GRS."
                },
                "authors": [
                    {
                        "name": "Cedric Waterschoot"
                    },
                    {
                        "name": "Nava Tintarev"
                    },
                    {
                        "name": "Francesco Barile"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Barile"
                },
                "author": "Francesco Barile",
                "arxiv_doi": "10.1145/3705328.3748015",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3705328.3748015",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.13705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Short paper accepted at the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25). Cedric Waterschoot, Nava Tintarev, and Francesco\n  Barile. 2025. Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations. Proceedings of the Nineteenth ACM\n  Conference on Recommender Systems (RecSys '25), Prague, Czech Republic. doi:\n  10.1145/3705328.3748015",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13686v1",
                "updated": "2025-07-18T06:23:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    23,
                    31,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:23:31Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    23,
                    31,
                    4,
                    199,
                    0
                ],
                "title": "TopicAttack: An Indirect Prompt Injection Attack via Topic Transition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopicAttack: An Indirect Prompt Injection Attack via Topic Transition"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance across a range\nof NLP tasks. However, their strong instruction-following capabilities and\ninability to distinguish instructions from data content make them vulnerable to\nindirect prompt injection attacks. In such attacks, instructions with malicious\npurposes are injected into external data sources, such as web documents. When\nLLMs retrieve this injected data through tools, such as a search engine and\nexecute the injected instructions, they provide misled responses. Recent attack\nmethods have demonstrated potential, but their abrupt instruction injection\noften undermines their effectiveness. Motivated by the limitations of existing\nattack methods, we propose TopicAttack, which prompts the LLM to generate a\nfabricated conversational transition prompt that gradually shifts the topic\ntoward the injected instruction, making the injection smoother and enhancing\nthe plausibility and success of the attack. Through comprehensive experiments,\nTopicAttack achieves state-of-the-art performance, with an attack success rate\n(ASR) over 90\\% in most cases, even when various defense methods are applied.\nWe further analyze its effectiveness by examining attention scores. We find\nthat a higher injected-to-original attention ratio leads to a greater success\nprobability, and our method achieves a much higher ratio than the baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance across a range\nof NLP tasks. However, their strong instruction-following capabilities and\ninability to distinguish instructions from data content make them vulnerable to\nindirect prompt injection attacks. In such attacks, instructions with malicious\npurposes are injected into external data sources, such as web documents. When\nLLMs retrieve this injected data through tools, such as a search engine and\nexecute the injected instructions, they provide misled responses. Recent attack\nmethods have demonstrated potential, but their abrupt instruction injection\noften undermines their effectiveness. Motivated by the limitations of existing\nattack methods, we propose TopicAttack, which prompts the LLM to generate a\nfabricated conversational transition prompt that gradually shifts the topic\ntoward the injected instruction, making the injection smoother and enhancing\nthe plausibility and success of the attack. Through comprehensive experiments,\nTopicAttack achieves state-of-the-art performance, with an attack success rate\n(ASR) over 90\\% in most cases, even when various defense methods are applied.\nWe further analyze its effectiveness by examining attention scores. We find\nthat a higher injected-to-original attention ratio leads to a greater success\nprobability, and our method achieves a much higher ratio than the baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20839v3",
                "updated": "2025-07-18T06:15:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    15,
                    50,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-27T07:58:35Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    7,
                    58,
                    35,
                    1,
                    147,
                    0
                ],
                "title": "FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM\n  Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM\n  Inference Acceleration"
                },
                "summary": "As large language models become increasingly prevalent, memory bandwidth\nconstraints significantly limit inference throughput, motivating post-training\nquantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ\nframework and an INT4-FP8 matrix multiplication kernel that accelerates LLM\ninference across all linear layers. Specifically, FireQ quantizes linear layer\nweights and key-values to INT4, and activations and queries to FP8,\nsignificantly enhancing throughput. Additionally, we introduce a three-stage\npipelining for the prefill phase, which modifies the FlashAttention-3 kernel,\neffectively reducing time-to-first-token in the prefill phase. To minimize\naccuracy loss from quantization, we develop novel outlier smoothing techniques\ntailored separately for linear and attention layers. In linear layers, we\nexplicitly use per-tensor scaling to prevent underflow caused by the FP8\nquantization scaling factor of INT4 quantization, and channel-wise scaling to\ncompensate for coarse granularity of INT4. In attention layers, we address\nquantization challenges posed by rotary positional embeddings (RoPE) by\ncombining pre-RoPE and post-RoPE scaling strategies. FireQ significantly\noutperforms state-of-the-art methods, achieving 1.68x faster inference in\nfeed-forward network layers on Llama2-7B and 1.26x faster prefill phase\nperformance on Llama3-8B compared to QServe, with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models become increasingly prevalent, memory bandwidth\nconstraints significantly limit inference throughput, motivating post-training\nquantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ\nframework and an INT4-FP8 matrix multiplication kernel that accelerates LLM\ninference across all linear layers. Specifically, FireQ quantizes linear layer\nweights and key-values to INT4, and activations and queries to FP8,\nsignificantly enhancing throughput. Additionally, we introduce a three-stage\npipelining for the prefill phase, which modifies the FlashAttention-3 kernel,\neffectively reducing time-to-first-token in the prefill phase. To minimize\naccuracy loss from quantization, we develop novel outlier smoothing techniques\ntailored separately for linear and attention layers. In linear layers, we\nexplicitly use per-tensor scaling to prevent underflow caused by the FP8\nquantization scaling factor of INT4 quantization, and channel-wise scaling to\ncompensate for coarse granularity of INT4. In attention layers, we address\nquantization challenges posed by rotary positional embeddings (RoPE) by\ncombining pre-RoPE and post-RoPE scaling strategies. FireQ significantly\noutperforms state-of-the-art methods, achieving 1.68x faster inference in\nfeed-forward network layers on Llama2-7B and 1.26x faster prefill phase\nperformance on Llama3-8B compared to QServe, with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v1",
                "updated": "2025-07-18T06:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13677v1",
                "updated": "2025-07-18T06:02:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    2,
                    22,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:02:22Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    2,
                    22,
                    4,
                    199,
                    0
                ],
                "title": "HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with\n  Heterogeneous Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with\n  Heterogeneous Sensors"
                },
                "summary": "Real-world Vehicle-to-Everything (V2X) cooperative perception systems often\noperate under heterogeneous sensor configurations due to cost constraints and\ndeployment variability across vehicles and infrastructure. This heterogeneity\nposes significant challenges for feature fusion and perception reliability. To\naddress these issues, we propose HeCoFuse, a unified framework designed for\ncooperative perception across mixed sensor setups where nodes may carry Cameras\n(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that\nadaptively weights features through a combination of channel-wise and spatial\nattention, HeCoFuse can tackle critical challenges such as cross-modality\nfeature misalignment and imbalanced representation quality. In addition, an\nadaptive spatial resolution adjustment module is employed to balance\ncomputational cost and fusion effectiveness. To enhance robustness across\ndifferent configurations, we further implement a cooperative learning strategy\nthat dynamically adjusts fusion type based on available modalities. Experiments\non the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%\n3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D\nbaseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC\nscenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine\nheterogeneous sensor configurations. These results, validated by our\nfirst-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the\ncurrent state-of-the-art on TUM-Traf V2X dataset while demonstrating robust\nperformance across diverse sensor deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world Vehicle-to-Everything (V2X) cooperative perception systems often\noperate under heterogeneous sensor configurations due to cost constraints and\ndeployment variability across vehicles and infrastructure. This heterogeneity\nposes significant challenges for feature fusion and perception reliability. To\naddress these issues, we propose HeCoFuse, a unified framework designed for\ncooperative perception across mixed sensor setups where nodes may carry Cameras\n(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that\nadaptively weights features through a combination of channel-wise and spatial\nattention, HeCoFuse can tackle critical challenges such as cross-modality\nfeature misalignment and imbalanced representation quality. In addition, an\nadaptive spatial resolution adjustment module is employed to balance\ncomputational cost and fusion effectiveness. To enhance robustness across\ndifferent configurations, we further implement a cooperative learning strategy\nthat dynamically adjusts fusion type based on available modalities. Experiments\non the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%\n3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D\nbaseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC\nscenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine\nheterogeneous sensor configurations. These results, validated by our\nfirst-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the\ncurrent state-of-the-art on TUM-Traf V2X dataset while demonstrating robust\nperformance across diverse sensor deployments."
                },
                "authors": [
                    {
                        "name": "Chuheng Wei"
                    },
                    {
                        "name": "Ziye Qin"
                    },
                    {
                        "name": "Walter Zimmer"
                    },
                    {
                        "name": "Guoyuan Wu"
                    },
                    {
                        "name": "Matthew J. Barth"
                    }
                ],
                "author_detail": {
                    "name": "Matthew J. Barth"
                },
                "author": "Matthew J. Barth",
                "arxiv_comment": "Ranked first in CVPR DriveX workshop TUM-Traf V2X challenge. Accepted\n  by ITSC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16580v2",
                "updated": "2025-07-18T05:48:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    5,
                    48,
                    55,
                    4,
                    199,
                    0
                ],
                "published": "2025-02-23T14:02:16Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    14,
                    2,
                    16,
                    6,
                    54,
                    0
                ],
                "title": "Can Indirect Prompt Injection Attacks Be Detected and Removed?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Indirect Prompt Injection Attacks Be Detected and Removed?"
                },
                "summary": "Prompt injection attacks manipulate large language models (LLMs) by\nmisleading them to deviate from the original input instructions and execute\nmaliciously injected instructions, because of their instruction-following\ncapabilities and inability to distinguish between the original input\ninstructions and maliciously injected instructions. To defend against such\nattacks, recent studies have developed various detection mechanisms. If we\nrestrict ourselves specifically to works which perform detection rather than\ndirect defense, most of them focus on direct prompt injection attacks, while\nthere are few works for the indirect scenario, where injected instructions are\nindirectly from external tools, such as a search engine. Moreover, current\nworks mainly investigate injection detection methods and pay less attention to\nthe post-processing method that aims to mitigate the injection after detection.\nIn this paper, we investigate the feasibility of detecting and removing\nindirect prompt injection attacks, and we construct a benchmark dataset for\nevaluation. For detection, we assess the performance of existing LLMs and\nopen-source detection models, and we further train detection models using our\ncrafted training datasets. For removal, we evaluate two intuitive methods: (1)\nthe segmentation removal method, which segments the injected document and\nremoves parts containing injected instructions, and (2) the extraction removal\nmethod, which trains an extraction model to identify and remove injected\ninstructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection attacks manipulate large language models (LLMs) by\nmisleading them to deviate from the original input instructions and execute\nmaliciously injected instructions, because of their instruction-following\ncapabilities and inability to distinguish between the original input\ninstructions and maliciously injected instructions. To defend against such\nattacks, recent studies have developed various detection mechanisms. If we\nrestrict ourselves specifically to works which perform detection rather than\ndirect defense, most of them focus on direct prompt injection attacks, while\nthere are few works for the indirect scenario, where injected instructions are\nindirectly from external tools, such as a search engine. Moreover, current\nworks mainly investigate injection detection methods and pay less attention to\nthe post-processing method that aims to mitigate the injection after detection.\nIn this paper, we investigate the feasibility of detecting and removing\nindirect prompt injection attacks, and we construct a benchmark dataset for\nevaluation. For detection, we assess the performance of existing LLMs and\nopen-source detection models, and we further train detection models using our\ncrafted training datasets. For removal, we evaluate two intuitive methods: (1)\nthe segmentation removal method, which segments the injected document and\nremoves parts containing injected instructions, and (2) the extraction removal\nmethod, which trains an extraction model to identify and remove injected\ninstructions."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "To Appear in ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00459v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00459v4",
                "updated": "2025-07-18T05:44:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    5,
                    44,
                    32,
                    4,
                    199,
                    0
                ],
                "published": "2024-11-01T09:14:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    14,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques"
                },
                "summary": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Dekai Wu"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "To Appear in ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00459v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00459v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13666v1",
                "updated": "2025-07-18T05:34:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    5,
                    34,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T05:34:36Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    5,
                    34,
                    36,
                    4,
                    199,
                    0
                ],
                "title": "KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with\n  LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross a wide range of natural language processing tasks. However,\nhigh-performing models are typically accessible only via APIs, incurring\nsubstantial inference costs. Cascade methods address this by initially\nemploying a cheaper model and escalating to a stronger one only when necessary.\nNevertheless, existing cascade approaches struggle to select a reliable\nrepresentative response and assess the overall reliability of free-form\noutputs, as they rely on exact text matching. To overcome these limitations, we\npropose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient\nfree-form text generation. KiC identifies the most representative answer among\nmultiple outputs from a weaker model and evaluates the semantic alignment of\nother responses with it. Based on the degree of alignment, KiC determines\nwhether to accept the weaker model's output or escalate to a stronger model.\nExperiments on three free-form text generation benchmarks show that KiC\nachieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81\npercent on average, and even outperforms GPT-4 in a specific benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross a wide range of natural language processing tasks. However,\nhigh-performing models are typically accessible only via APIs, incurring\nsubstantial inference costs. Cascade methods address this by initially\nemploying a cheaper model and escalating to a stronger one only when necessary.\nNevertheless, existing cascade approaches struggle to select a reliable\nrepresentative response and assess the overall reliability of free-form\noutputs, as they rely on exact text matching. To overcome these limitations, we\npropose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient\nfree-form text generation. KiC identifies the most representative answer among\nmultiple outputs from a weaker model and evaluates the semantic alignment of\nother responses with it. Based on the degree of alignment, KiC determines\nwhether to accept the weaker model's output or escalate to a stronger model.\nExperiments on three free-form text generation benchmarks show that KiC\nachieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81\npercent on average, and even outperforms GPT-4 in a specific benchmark."
                },
                "authors": [
                    {
                        "name": "Woo-Chan Kim"
                    },
                    {
                        "name": "Ji-Hoon Park"
                    },
                    {
                        "name": "Seong-Whan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Whan Lee"
                },
                "author": "Seong-Whan Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08392v2",
                "updated": "2025-07-18T05:24:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    5,
                    24,
                    57,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-11T08:04:32Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    4,
                    32,
                    4,
                    192,
                    0
                ],
                "title": "Multi-Agent LLMs as Ethics Advocates for AI-Based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent LLMs as Ethics Advocates for AI-Based Systems"
                },
                "summary": "Incorporating ethics into the requirement elicitation process is essential\nfor creating ethically aligned systems. Although eliciting manual ethics\nrequirements is effective, it requires diverse input from multiple\nstakeholders, which can be challenging due to time and resource constraints.\nMoreover, it is often given a low priority in the requirements elicitation\nprocess. This study proposes a framework for generating ethics requirements\ndrafts by introducing an ethics advocate agent in a multi-agent LLM setting.\nThis agent critiques and provides input on ethical issues based on the system\ndescription. The proposed framework is evaluated through two case studies from\ndifferent contexts, demonstrating that it captures the majority of ethics\nrequirements identified by researchers during 30-minute interviews and\nintroduces several additional relevant requirements. However, it also\nhighlights reliability issues in generating ethics requirements, emphasizing\nthe need for human feedback in this sensitive domain. We believe this work can\nfacilitate the broader adoption of ethics in the requirements engineering\nprocess, ultimately leading to more ethically aligned products.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating ethics into the requirement elicitation process is essential\nfor creating ethically aligned systems. Although eliciting manual ethics\nrequirements is effective, it requires diverse input from multiple\nstakeholders, which can be challenging due to time and resource constraints.\nMoreover, it is often given a low priority in the requirements elicitation\nprocess. This study proposes a framework for generating ethics requirements\ndrafts by introducing an ethics advocate agent in a multi-agent LLM setting.\nThis agent critiques and provides input on ethical issues based on the system\ndescription. The proposed framework is evaluated through two case studies from\ndifferent contexts, demonstrating that it captures the majority of ethics\nrequirements identified by researchers during 30-minute interviews and\nintroduces several additional relevant requirements. However, it also\nhighlights reliability issues in generating ethics requirements, emphasizing\nthe need for human feedback in this sensitive domain. We believe this work can\nfacilitate the broader adoption of ethics in the requirements engineering\nprocess, ultimately leading to more ethically aligned products."
                },
                "authors": [
                    {
                        "name": "Asma Yamani"
                    },
                    {
                        "name": "Malak Baslyman"
                    },
                    {
                        "name": "Moataz Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Moataz Ahmed"
                },
                "author": "Moataz Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13663v1",
                "updated": "2025-07-18T05:15:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    5,
                    15,
                    4,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T05:15:04Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    5,
                    15,
                    4,
                    4,
                    199,
                    0
                ],
                "title": "Global Modeling Matters: A Fast, Lightweight and Effective Baseline for\n  Efficient Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Modeling Matters: A Fast, Lightweight and Effective Baseline for\n  Efficient Image Restoration"
                },
                "summary": "Natural image quality is often degraded by adverse weather conditions,\nsignificantly impairing the performance of downstream tasks. Image restoration\nhas emerged as a core solution to this challenge and has been widely discussed\nin the literature. Although recent transformer-based approaches have made\nremarkable progress in image restoration, their increasing system complexity\nposes significant challenges for real-time processing, particularly in\nreal-world deployment scenarios. To this end, most existing methods attempt to\nsimplify the self-attention mechanism, such as by channel self-attention or\nstate space model. However, these methods primarily focus on network\narchitecture while neglecting the inherent characteristics of image restoration\nitself. In this context, we explore a pyramid Wavelet-Fourier iterative\npipeline to demonstrate the potential of Wavelet-Fourier processing for image\nrestoration. Inspired by the above findings, we propose a novel and efficient\nrestoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).\nSpecifically, PW-FNet features two key design principles: 1) at the inter-block\nlevel, integrates a pyramid wavelet-based multi-input multi-output structure to\nachieve multi-scale and multi-frequency bands decomposition; and 2) at the\nintra-block level, incorporates Fourier transforms as an efficient alternative\nto self-attention mechanisms, effectively reducing computational complexity\nwhile preserving global modeling capability. Extensive experiments on tasks\nsuch as image deraining, raindrop removal, image super-resolution, motion\ndeblurring, image dehazing, image desnowing and underwater/low-light\nenhancement demonstrate that PW-FNet not only surpasses state-of-the-art\nmethods in restoration quality but also achieves superior efficiency, with\nsignificantly reduced parameter size, computational cost and inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural image quality is often degraded by adverse weather conditions,\nsignificantly impairing the performance of downstream tasks. Image restoration\nhas emerged as a core solution to this challenge and has been widely discussed\nin the literature. Although recent transformer-based approaches have made\nremarkable progress in image restoration, their increasing system complexity\nposes significant challenges for real-time processing, particularly in\nreal-world deployment scenarios. To this end, most existing methods attempt to\nsimplify the self-attention mechanism, such as by channel self-attention or\nstate space model. However, these methods primarily focus on network\narchitecture while neglecting the inherent characteristics of image restoration\nitself. In this context, we explore a pyramid Wavelet-Fourier iterative\npipeline to demonstrate the potential of Wavelet-Fourier processing for image\nrestoration. Inspired by the above findings, we propose a novel and efficient\nrestoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).\nSpecifically, PW-FNet features two key design principles: 1) at the inter-block\nlevel, integrates a pyramid wavelet-based multi-input multi-output structure to\nachieve multi-scale and multi-frequency bands decomposition; and 2) at the\nintra-block level, incorporates Fourier transforms as an efficient alternative\nto self-attention mechanisms, effectively reducing computational complexity\nwhile preserving global modeling capability. Extensive experiments on tasks\nsuch as image deraining, raindrop removal, image super-resolution, motion\ndeblurring, image dehazing, image desnowing and underwater/low-light\nenhancement demonstrate that PW-FNet not only surpasses state-of-the-art\nmethods in restoration quality but also achieves superior efficiency, with\nsignificantly reduced parameter size, computational cost and inference time."
                },
                "authors": [
                    {
                        "name": "Xingyu Jiang"
                    },
                    {
                        "name": "Ning Gao"
                    },
                    {
                        "name": "Hongkun Dou"
                    },
                    {
                        "name": "Xiuhui Zhang"
                    },
                    {
                        "name": "Xiaoqing Zhong"
                    },
                    {
                        "name": "Yue Deng"
                    },
                    {
                        "name": "Hongjue Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongjue Li"
                },
                "author": "Hongjue Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04834v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04834v4",
                "updated": "2025-07-18T05:02:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    5,
                    2,
                    11,
                    4,
                    199,
                    0
                ],
                "published": "2024-04-07T07:05:40Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    7,
                    5,
                    40,
                    6,
                    98,
                    0
                ],
                "title": "LLM-Based Multi-Agent Systems for Software Engineering: Literature\n  Review, Vision and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Agent Systems for Software Engineering: Literature\n  Review, Vision and the Road Ahead"
                },
                "summary": "Integrating Large Language Models (LLMs) into autonomous agents marks a\nsignificant shift in the research landscape by offering cognitive abilities\nthat are competitive with human planning and reasoning. This paper explores the\ntransformative potential of integrating Large Language Models into Multi-Agent\n(LMA) systems for addressing complex challenges in software engineering (SE).\nBy leveraging the collaborative and specialized abilities of multiple agents,\nLMA systems enable autonomous problem-solving, improve robustness, and provide\nscalable solutions for managing the complexity of real-world software projects.\nIn this paper, we conduct a systematic review of recent primary studies to map\nthe current landscape of LMA applications across various stages of the software\ndevelopment lifecycle (SDLC). To illustrate current capabilities and\nlimitations, we perform two case studies to demonstrate the effectiveness of\nstate-of-the-art LMA frameworks. Additionally, we identify critical research\ngaps and propose a comprehensive research agenda focused on enhancing\nindividual agent capabilities and optimizing agent synergy. Our work outlines a\nforward-looking vision for developing fully autonomous, scalable, and\ntrustworthy LMA systems, laying the foundation for the evolution of Software\nEngineering 2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) into autonomous agents marks a\nsignificant shift in the research landscape by offering cognitive abilities\nthat are competitive with human planning and reasoning. This paper explores the\ntransformative potential of integrating Large Language Models into Multi-Agent\n(LMA) systems for addressing complex challenges in software engineering (SE).\nBy leveraging the collaborative and specialized abilities of multiple agents,\nLMA systems enable autonomous problem-solving, improve robustness, and provide\nscalable solutions for managing the complexity of real-world software projects.\nIn this paper, we conduct a systematic review of recent primary studies to map\nthe current landscape of LMA applications across various stages of the software\ndevelopment lifecycle (SDLC). To illustrate current capabilities and\nlimitations, we perform two case studies to demonstrate the effectiveness of\nstate-of-the-art LMA frameworks. Additionally, we identify critical research\ngaps and propose a comprehensive research agenda focused on enhancing\nindividual agent capabilities and optimizing agent synergy. Our work outlines a\nforward-looking vision for developing fully autonomous, scalable, and\ntrustworthy LMA systems, laying the foundation for the evolution of Software\nEngineering 2.0."
                },
                "authors": [
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "TOSEM 2030 Special Issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04834v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04834v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06941v2",
                "updated": "2025-07-18T04:14:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    4,
                    14,
                    22,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-07T22:42:29Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    22,
                    42,
                    29,
                    5,
                    158,
                    0
                ],
                "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity"
                },
                "summary": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from contamination and does not provide\ninsights into the reasoning traces. In this work, we systematically investigate\nthese gaps with the help of controllable puzzle environments that allow precise\nmanipulation of complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal\nreasoning traces, offering insights into how LRMs think. Through extensive\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\nreasoning effort increases with problem complexity up to a point, then declines\ndespite having remaining token budget. By comparing LRMs with their standard\nLLM counterparts under same inference compute, we identify three performance\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\nhigh-complexity tasks where both models face complete collapse. We found that\nLRMs have limitations in exact computation: they fail to use explicit\nalgorithms and reason inconsistently across scales. We also investigate the\nreasoning traces in more depth, studying the patterns of explored solutions and\nanalyzing the models' computational behavior, shedding light on their\nstrengths, limitations, and raising questions about their reasoning\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from contamination and does not provide\ninsights into the reasoning traces. In this work, we systematically investigate\nthese gaps with the help of controllable puzzle environments that allow precise\nmanipulation of complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal\nreasoning traces, offering insights into how LRMs think. Through extensive\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\nreasoning effort increases with problem complexity up to a point, then declines\ndespite having remaining token budget. By comparing LRMs with their standard\nLLM counterparts under same inference compute, we identify three performance\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\nhigh-complexity tasks where both models face complete collapse. We found that\nLRMs have limitations in exact computation: they fail to use explicit\nalgorithms and reason inconsistently across scales. We also investigate the\nreasoning traces in more depth, studying the patterns of explored solutions and\nanalyzing the models' computational behavior, shedding light on their\nstrengths, limitations, and raising questions about their reasoning\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Parshin Shojaee"
                    },
                    {
                        "name": "Iman Mirzadeh"
                    },
                    {
                        "name": "Keivan Alizadeh"
                    },
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    }
                ],
                "author_detail": {
                    "name": "Mehrdad Farajtabar"
                },
                "author": "Mehrdad Farajtabar",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13629v1",
                "updated": "2025-07-18T03:41:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    3,
                    41,
                    18,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T03:41:18Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    3,
                    41,
                    18,
                    4,
                    199,
                    0
                ],
                "title": "Large Language Models in Cybersecurity: Applications, Vulnerabilities,\n  and Defense Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in Cybersecurity: Applications, Vulnerabilities,\n  and Defense Techniques"
                },
                "summary": "Large Language Models (LLMs) are transforming cybersecurity by enabling\nintelligent, adaptive, and automated approaches to threat detection,\nvulnerability assessment, and incident response. With their advanced language\nunderstanding and contextual reasoning, LLMs surpass traditional methods in\ntackling challenges across domains such as IoT, blockchain, and hardware\nsecurity. This survey provides a comprehensive overview of LLM applications in\ncybersecurity, focusing on two core areas: (1) the integration of LLMs into key\ncybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along\nwith mitigation strategies. By synthesizing recent advancements and identifying\nkey limitations, this work offers practical insights and strategic\nrecommendations for leveraging LLMs to build secure, scalable, and future-ready\ncyber defense systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming cybersecurity by enabling\nintelligent, adaptive, and automated approaches to threat detection,\nvulnerability assessment, and incident response. With their advanced language\nunderstanding and contextual reasoning, LLMs surpass traditional methods in\ntackling challenges across domains such as IoT, blockchain, and hardware\nsecurity. This survey provides a comprehensive overview of LLM applications in\ncybersecurity, focusing on two core areas: (1) the integration of LLMs into key\ncybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along\nwith mitigation strategies. By synthesizing recent advancements and identifying\nkey limitations, this work offers practical insights and strategic\nrecommendations for leveraging LLMs to build secure, scalable, and future-ready\ncyber defense systems."
                },
                "authors": [
                    {
                        "name": "Niveen O. Jaffal"
                    },
                    {
                        "name": "Mohammed Alkhanafseh"
                    },
                    {
                        "name": "David Mohaisen"
                    }
                ],
                "author_detail": {
                    "name": "David Mohaisen"
                },
                "author": "David Mohaisen",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13625v1",
                "updated": "2025-07-18T03:39:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    3,
                    39,
                    14,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T03:39:14Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    3,
                    39,
                    14,
                    4,
                    199,
                    0
                ],
                "title": "BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question\n  Answering in Construction Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question\n  Answering in Construction Safety"
                },
                "summary": "Information retrieval and question answering from safety regulations are\nessential for automated construction compliance checking but are hindered by\nthe linguistic and structural complexity of regulatory text. Many\ncompliance-related queries are multi-hop, requiring synthesis of information\nacross interlinked clauses. This poses a challenge for traditional\nretrieval-augmented generation (RAG) systems. To overcome this, we introduce\nBifrostRAG: a dual-graph RAG-integrated system that explicitly models both\nlinguistic relationships (via an Entity Network Graph) and document structure\n(via a Document Navigator Graph). This architecture powers a hybrid retrieval\nmechanism that combines graph traversal with vector-based semantic search,\nenabling large language models to reason over both the meaning and the\nstructure of the text. Evaluation on a multi-hop question dataset shows that\nBifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1\nscore of 87.3 percent. These results significantly outperform vector-only and\ngraph-only RAG baselines that represent current leading approaches. Error\nanalysis further highlights the comparative advantages of our hybrid method\nover single-modality RAGs. These findings establish BifrostRAG as a robust\nknowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid\nretrieval mechanism offers a transferable blueprint for navigating complex\ntechnical documents across knowledge-intensive engineering domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval and question answering from safety regulations are\nessential for automated construction compliance checking but are hindered by\nthe linguistic and structural complexity of regulatory text. Many\ncompliance-related queries are multi-hop, requiring synthesis of information\nacross interlinked clauses. This poses a challenge for traditional\nretrieval-augmented generation (RAG) systems. To overcome this, we introduce\nBifrostRAG: a dual-graph RAG-integrated system that explicitly models both\nlinguistic relationships (via an Entity Network Graph) and document structure\n(via a Document Navigator Graph). This architecture powers a hybrid retrieval\nmechanism that combines graph traversal with vector-based semantic search,\nenabling large language models to reason over both the meaning and the\nstructure of the text. Evaluation on a multi-hop question dataset shows that\nBifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1\nscore of 87.3 percent. These results significantly outperform vector-only and\ngraph-only RAG baselines that represent current leading approaches. Error\nanalysis further highlights the comparative advantages of our hybrid method\nover single-modality RAGs. These findings establish BifrostRAG as a robust\nknowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid\nretrieval mechanism offers a transferable blueprint for navigating complex\ntechnical documents across knowledge-intensive engineering domains."
                },
                "authors": [
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Mo Hu"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Zhang"
                },
                "arxiv_affiliation": "Department of Construction Science, College of Architecture, Texas A&M University, College Station, USA",
                "author": "Zhenyu Zhang",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01551v2",
                "updated": "2025-07-18T03:23:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    3,
                    23,
                    12,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-02T11:28:32Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    28,
                    32,
                    0,
                    153,
                    0
                ],
                "title": "EvolveNav: Self-Improving Embodied Reasoning for LLM-Based\n  Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolveNav: Self-Improving Embodied Reasoning for LLM-Based\n  Vision-Language Navigation"
                },
                "summary": "Building Vision-Language Navigation (VLN) agents which can navigate following\nnatural language instructions is a long-standing goal in human-robot\ninteraction applications. Recent studies have revealed the potential of\ntraining open-source Large Language Models (LLMs) to unleash LLMs' reasoning\nability for improving navigation, and simultaneously mitigate the domain gap\nbetween LLMs' training corpus and the VLN task. However, these approaches\nprimarily adopt direct input-output mapping paradigms, causing the mapping\nlearning difficult and the navigational decisions unexplainable.\nChain-of-Thought (CoT) training is a promising way to improve both navigational\ndecision accuracy and interpretability, while the complexity of the navigation\ntask makes the perfect CoT labels unavailable and may lead to overfitting\nthrough pure CoT supervised fine-tuning. In this paper, we propose a novel\nsElf-improving embodied reasoning framework for boosting LLM-based\nvision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two\nstages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model\nwith formalized CoT labels to both activate the model's navigational reasoning\ncapabilities and increase the reasoning speed; (2) Self-Reflective\nPost-Training, where the model is iteratively trained with its own reasoning\noutputs as self-enriched CoT labels to enhance the supervision diversity. A\nself-reflective auxiliary task is also introduced to encourage learning correct\nreasoning patterns by contrasting with wrong ones. Experimental results on the\npopular VLN benchmarks demonstrate the superiority of EvolveNav over previous\nLLM-based VLN approaches. Code is available at\nhttps://github.com/expectorlin/EvolveNav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Vision-Language Navigation (VLN) agents which can navigate following\nnatural language instructions is a long-standing goal in human-robot\ninteraction applications. Recent studies have revealed the potential of\ntraining open-source Large Language Models (LLMs) to unleash LLMs' reasoning\nability for improving navigation, and simultaneously mitigate the domain gap\nbetween LLMs' training corpus and the VLN task. However, these approaches\nprimarily adopt direct input-output mapping paradigms, causing the mapping\nlearning difficult and the navigational decisions unexplainable.\nChain-of-Thought (CoT) training is a promising way to improve both navigational\ndecision accuracy and interpretability, while the complexity of the navigation\ntask makes the perfect CoT labels unavailable and may lead to overfitting\nthrough pure CoT supervised fine-tuning. In this paper, we propose a novel\nsElf-improving embodied reasoning framework for boosting LLM-based\nvision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two\nstages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model\nwith formalized CoT labels to both activate the model's navigational reasoning\ncapabilities and increase the reasoning speed; (2) Self-Reflective\nPost-Training, where the model is iteratively trained with its own reasoning\noutputs as self-enriched CoT labels to enhance the supervision diversity. A\nself-reflective auxiliary task is also introduced to encourage learning correct\nreasoning patterns by contrasting with wrong ones. Experimental results on the\npopular VLN benchmarks demonstrate the superiority of EvolveNav over previous\nLLM-based VLN approaches. Code is available at\nhttps://github.com/expectorlin/EvolveNav."
                },
                "authors": [
                    {
                        "name": "Bingqian Lin"
                    },
                    {
                        "name": "Yunshuang Nie"
                    },
                    {
                        "name": "Khun Loun Zai"
                    },
                    {
                        "name": "Ziming Wei"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Minzhe Niu"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Cewu Lu"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13618v1",
                "updated": "2025-07-18T03:19:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    3,
                    19,
                    43,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T03:19:43Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    3,
                    19,
                    43,
                    4,
                    199,
                    0
                ],
                "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters"
                },
                "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications."
                },
                "authors": [
                    {
                        "name": "Shanbo Cheng"
                    },
                    {
                        "name": "Yu Bao"
                    },
                    {
                        "name": "Qian Cao"
                    },
                    {
                        "name": "Luyang Huang"
                    },
                    {
                        "name": "Liyan Kang"
                    },
                    {
                        "name": "Zhicheng Liu"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Wenhao Zhu"
                    },
                    {
                        "name": "Zhichao Huang"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Sitong Liu"
                    },
                    {
                        "name": "Ningxin Peng"
                    },
                    {
                        "name": "Shuaijie She"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Runsheng Yu"
                    },
                    {
                        "name": "Yiming Yu"
                    },
                    {
                        "name": "Liehao Zou"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Yonghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yonghui Wu"
                },
                "author": "Yonghui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03993v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03993v4",
                "updated": "2025-07-18T03:17:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    3,
                    17,
                    5,
                    4,
                    199,
                    0
                ],
                "published": "2024-10-05T01:18:26Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    1,
                    18,
                    26,
                    5,
                    279,
                    0
                ],
                "title": "TR-LLM: Integrating Trajectory Data for Scene-Aware LLM-Based Human\n  Action Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TR-LLM: Integrating Trajectory Data for Scene-Aware LLM-Based Human\n  Action Prediction"
                },
                "summary": "Accurate prediction of human behavior is crucial for AI systems to\neffectively support real-world applications, such as autonomous robots\nanticipating and assisting with human tasks. Real-world scenarios frequently\npresent challenges such as occlusions and incomplete scene observations, which\ncan compromise predictive accuracy. Thus, traditional video-based methods often\nstruggle due to limited temporal and spatial perspectives. Large Language\nModels (LLMs) offer a promising alternative. Having been trained on a large\ntext corpus describing human behaviors, LLMs likely encode plausible sequences\nof human actions in a home environment. However, LLMs, trained primarily on\ntext data, lack inherent spatial awareness and real-time environmental\nperception. They struggle with understanding physical constraints and spatial\ngeometry. Therefore, to be effective in a real-world spatial scenario, we\npropose a multimodal prediction framework that enhances LLM-based action\nprediction by integrating physical constraints derived from human trajectories.\nOur experiments demonstrate that combining LLM predictions with trajectory data\nsignificantly improves overall prediction performance. This enhancement is\nparticularly notable in situations where the LLM receives limited scene\ninformation, highlighting the complementary nature of linguistic knowledge and\nphysical constraints in understanding and anticipating human behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of human behavior is crucial for AI systems to\neffectively support real-world applications, such as autonomous robots\nanticipating and assisting with human tasks. Real-world scenarios frequently\npresent challenges such as occlusions and incomplete scene observations, which\ncan compromise predictive accuracy. Thus, traditional video-based methods often\nstruggle due to limited temporal and spatial perspectives. Large Language\nModels (LLMs) offer a promising alternative. Having been trained on a large\ntext corpus describing human behaviors, LLMs likely encode plausible sequences\nof human actions in a home environment. However, LLMs, trained primarily on\ntext data, lack inherent spatial awareness and real-time environmental\nperception. They struggle with understanding physical constraints and spatial\ngeometry. Therefore, to be effective in a real-world spatial scenario, we\npropose a multimodal prediction framework that enhances LLM-based action\nprediction by integrating physical constraints derived from human trajectories.\nOur experiments demonstrate that combining LLM predictions with trajectory data\nsignificantly improves overall prediction performance. This enhancement is\nparticularly notable in situations where the LLM receives limited scene\ninformation, highlighting the complementary nature of linguistic knowledge and\nphysical constraints in understanding and anticipating human behavior."
                },
                "authors": [
                    {
                        "name": "Kojiro Takeyama"
                    },
                    {
                        "name": "Yimeng Liu"
                    },
                    {
                        "name": "Misha Sra"
                    }
                ],
                "author_detail": {
                    "name": "Misha Sra"
                },
                "author": "Misha Sra",
                "arxiv_comment": "Accepted to IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03993v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03993v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00152v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00152v3",
                "updated": "2025-07-18T03:12:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    3,
                    12,
                    59,
                    4,
                    199,
                    0
                ],
                "published": "2024-12-30T21:54:33Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    21,
                    54,
                    33,
                    0,
                    365,
                    0
                ],
                "title": "Temporal reasoning for timeline summarisation in social media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal reasoning for timeline summarisation in social media"
                },
                "summary": "This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation."
                },
                "authors": [
                    {
                        "name": "Jiayu Song"
                    },
                    {
                        "name": "Mahmud Elahi Akhter"
                    },
                    {
                        "name": "Dana Atzil Slonim"
                    },
                    {
                        "name": "Maria Liakata"
                    }
                ],
                "author_detail": {
                    "name": "Maria Liakata"
                },
                "author": "Maria Liakata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00152v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00152v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13616v1",
                "updated": "2025-07-18T02:52:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    2,
                    52,
                    58,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T02:52:58Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    2,
                    52,
                    58,
                    4,
                    199,
                    0
                ],
                "title": "From Firms to Computation: AI Governance and the Evolution of\n  Institutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Firms to Computation: AI Governance and the Evolution of\n  Institutions"
                },
                "summary": "The integration of agential artificial intelligence into socioeconomic\nsystems requires us to reexamine the evolutionary processes that describe\nchanges in our economic institutions. This article synthesizes three\nframeworks: multi-level selection theory, Aoki's view of firms as computational\nprocesses, and Ostrom's design principles for robust institutions. We develop a\nframework where selection operates concurrently across organizational levels,\nfirms implement distributed inference via game-theoretic architectures, and\nOstrom-style rules evolve as alignment mechanisms that address AI-related\nrisks. This synthesis yields a multi-level Price equation expressed over nested\ngames, providing quantitative metrics for how selection and governance\nco-determine economic outcomes. We examine connections to Acemoglu's work on\ninclusive institutions, analyze how institutional structures shape AI\ndeployment, and demonstrate the framework's explanatory power via case studies.\nWe conclude by proposing a set of design principles that operationalize\nalignment between humans and AI across institutional layers, enabling scalable,\nadaptive, and inclusive governance of agential AI systems. We conclude with\npractical policy recommendations and further research to extend these\nprinciples into real-world implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of agential artificial intelligence into socioeconomic\nsystems requires us to reexamine the evolutionary processes that describe\nchanges in our economic institutions. This article synthesizes three\nframeworks: multi-level selection theory, Aoki's view of firms as computational\nprocesses, and Ostrom's design principles for robust institutions. We develop a\nframework where selection operates concurrently across organizational levels,\nfirms implement distributed inference via game-theoretic architectures, and\nOstrom-style rules evolve as alignment mechanisms that address AI-related\nrisks. This synthesis yields a multi-level Price equation expressed over nested\ngames, providing quantitative metrics for how selection and governance\nco-determine economic outcomes. We examine connections to Acemoglu's work on\ninclusive institutions, analyze how institutional structures shape AI\ndeployment, and demonstrate the framework's explanatory power via case studies.\nWe conclude by proposing a set of design principles that operationalize\nalignment between humans and AI across institutional layers, enabling scalable,\nadaptive, and inclusive governance of agential AI systems. We conclude with\npractical policy recommendations and further research to extend these\nprinciples into real-world implementation."
                },
                "authors": [
                    {
                        "name": "Michael S. Harre"
                    }
                ],
                "author_detail": {
                    "name": "Michael S. Harre"
                },
                "author": "Michael S. Harre",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; J.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13614v1",
                "updated": "2025-07-18T02:46:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    2,
                    46,
                    55,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T02:46:55Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    2,
                    46,
                    55,
                    4,
                    199,
                    0
                ],
                "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans\n  and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic and Embedding-Based Profiling of Texts generated by Humans\n  and Large Language Models"
                },
                "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts."
                },
                "authors": [
                    {
                        "name": "Sergio E. Zanotto"
                    },
                    {
                        "name": "Segun Aroyehun"
                    }
                ],
                "author_detail": {
                    "name": "Segun Aroyehun"
                },
                "author": "Segun Aroyehun",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2412.03025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13611v1",
                "updated": "2025-07-18T02:41:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    2,
                    41,
                    17,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T02:41:17Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    2,
                    41,
                    17,
                    4,
                    199,
                    0
                ],
                "title": "Dynamic Transmission Line Switching Amidst Wildfire-Prone Weather Under\n  Decision-Dependent Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Transmission Line Switching Amidst Wildfire-Prone Weather Under\n  Decision-Dependent Uncertainty"
                },
                "summary": "During dry and windy seasons, environmental conditions significantly increase\nthe risk of wildfires, exposing power grids to disruptions caused by\ntransmission line failures. Wildfire propagation exacerbates grid\nvulnerability, potentially leading to prolonged power outages. To address this\nchallenge, we propose a multi-stage optimization model that dynamically adjusts\ntransmission grid topology in response to wildfire propagation, aiming to\ndevelop an optimal response policy. By accounting for decision-dependent\nuncertainty, where line survival probabilities depend on usage, we employ\ndistributionally robust optimization to model uncertainty in line survival\ndistributions. We adapt the stochastic nested decomposition algorithm and\nderive a deterministic upper bound for its finite convergence. To enhance\ncomputational efficiency, we exploit the Lagrangian dual problem structure for\na faster generation of Lagrangian cuts. Using realistic data from the\nCalifornia transmission grid, we demonstrate the superior performance of\ndynamic response policies against two-stage alternatives through a\ncomprehensive case study. In addition, we construct easy-to-implement policies\nthat significantly reduce computational burden while maintaining good\nperformance in real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During dry and windy seasons, environmental conditions significantly increase\nthe risk of wildfires, exposing power grids to disruptions caused by\ntransmission line failures. Wildfire propagation exacerbates grid\nvulnerability, potentially leading to prolonged power outages. To address this\nchallenge, we propose a multi-stage optimization model that dynamically adjusts\ntransmission grid topology in response to wildfire propagation, aiming to\ndevelop an optimal response policy. By accounting for decision-dependent\nuncertainty, where line survival probabilities depend on usage, we employ\ndistributionally robust optimization to model uncertainty in line survival\ndistributions. We adapt the stochastic nested decomposition algorithm and\nderive a deterministic upper bound for its finite convergence. To enhance\ncomputational efficiency, we exploit the Lagrangian dual problem structure for\na faster generation of Lagrangian cuts. Using realistic data from the\nCalifornia transmission grid, we demonstrate the superior performance of\ndynamic response policies against two-stage alternatives through a\ncomprehensive case study. In addition, we construct easy-to-implement policies\nthat significantly reduce computational burden while maintaining good\nperformance in real-time deployment."
                },
                "authors": [
                    {
                        "name": "Juan-Alberto Estrada-Garcia"
                    },
                    {
                        "name": "Ruiwei Jiang"
                    },
                    {
                        "name": "Alexandre Moreira"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Moreira"
                },
                "author": "Alexandre Moreira",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18183v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18183v3",
                "updated": "2025-07-18T02:39:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    2,
                    39,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-22T21:46:42Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    21,
                    46,
                    42,
                    6,
                    173,
                    0
                ],
                "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't\n  Know?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't\n  Know?"
                },
                "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models."
                },
                "authors": [
                    {
                        "name": "Zhiting Mei"
                    },
                    {
                        "name": "Christina Zhang"
                    },
                    {
                        "name": "Tenny Yin"
                    },
                    {
                        "name": "Justin Lidard"
                    },
                    {
                        "name": "Ola Shorinwa"
                    },
                    {
                        "name": "Anirudha Majumdar"
                    }
                ],
                "author_detail": {
                    "name": "Anirudha Majumdar"
                },
                "author": "Anirudha Majumdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18183v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18183v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24068v2",
                "updated": "2025-07-18T01:26:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    26,
                    48,
                    4,
                    199,
                    0
                ],
                "published": "2025-06-30T17:21:08Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    21,
                    8,
                    0,
                    181,
                    0
                ],
                "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STACK: Adversarial Attacks on LLM Safeguard Pipelines"
                },
                "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks."
                },
                "authors": [
                    {
                        "name": "Ian R. McKenzie"
                    },
                    {
                        "name": "Oskar J. Hollinsworth"
                    },
                    {
                        "name": "Tom Tseng"
                    },
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Aaron D. Tucker"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Adam Gleave"
                    }
                ],
                "author_detail": {
                    "name": "Adam Gleave"
                },
                "author": "Adam Gleave",
                "arxiv_comment": "Fixed typos (including Figure 1), amended GPU-hours rather than days,\n  clarified ReNeLLM prompt modifications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12674v2",
                "updated": "2025-07-18T01:02:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    2,
                    16,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-16T23:12:14Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    23,
                    12,
                    14,
                    2,
                    197,
                    0
                ],
                "title": "ParaStudent: Generating and Evaluating Realistic Student Code by\n  Teaching LLMs to Struggle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaStudent: Generating and Evaluating Realistic Student Code by\n  Teaching LLMs to Struggle"
                },
                "summary": "Large Language Models (LLMs) have shown strong performance on programming\ntasks, but can they generate student-like code like real students - imperfect,\niterative, and stylistically diverse? We present ParaStudent, a systematic\nstudy of LLM-based \"student-like\" code generation in an introductory\nprogramming course setting. Using a dataset of timestamped student submissions\nacross multiple semesters, we design low- and high-resolution experiments to\nmodel student progress and evaluate code outputs along semantic, functional,\nand stylistic dimensions. Our results show that fine-tuning significantly\nimproves alignment with real student trajectories and captures error patterns,\nincremental improvements, and stylistic variations more faithfully. This study\nshows that modeling realistic student code requires capturing learning dynamics\nthrough context-aware generation, temporal modeling, and multi-dimensional\nevaluation. Code for experiments and evaluation is available at\nhttps://github.com/mmiroyan/ParaStudent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong performance on programming\ntasks, but can they generate student-like code like real students - imperfect,\niterative, and stylistically diverse? We present ParaStudent, a systematic\nstudy of LLM-based \"student-like\" code generation in an introductory\nprogramming course setting. Using a dataset of timestamped student submissions\nacross multiple semesters, we design low- and high-resolution experiments to\nmodel student progress and evaluate code outputs along semantic, functional,\nand stylistic dimensions. Our results show that fine-tuning significantly\nimproves alignment with real student trajectories and captures error patterns,\nincremental improvements, and stylistic variations more faithfully. This study\nshows that modeling realistic student code requires capturing learning dynamics\nthrough context-aware generation, temporal modeling, and multi-dimensional\nevaluation. Code for experiments and evaluation is available at\nhttps://github.com/mmiroyan/ParaStudent."
                },
                "authors": [
                    {
                        "name": "Mihran Miroyan"
                    },
                    {
                        "name": "Rose Niousha"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Gireeja Ranade"
                    },
                    {
                        "name": "Narges Norouzi"
                    }
                ],
                "author_detail": {
                    "name": "Narges Norouzi"
                },
                "author": "Narges Norouzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13591v1",
                "updated": "2025-07-18T00:50:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    0,
                    50,
                    44,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T00:50:44Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    0,
                    50,
                    44,
                    4,
                    199,
                    0
                ],
                "title": "FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning"
                },
                "summary": "Federated Learning (FL) enables collaborative model training without\ncentralizing client data, making it attractive for privacy-sensitive domains.\nWhile existing approaches employ cryptographic techniques such as homomorphic\nencryption, differential privacy, or secure multiparty computation to mitigate\ninference attacks-including model inversion, membership inference, and gradient\nleakage-they often suffer from high computational, communication, or memory\noverheads. Moreover, many methods overlook the confidentiality of the global\nmodel itself, which may be proprietary and sensitive. These challenges limit\nthe practicality of secure FL, especially in cross-silo deployments involving\nlarge datasets and strict compliance requirements.\n  We present FuSeFL, a fully secure and scalable FL scheme designed for\ncross-silo settings. FuSeFL decentralizes training across client pairs using\nlightweight secure multiparty computation (MPC), while confining the server's\nrole to secure aggregation. This design eliminates server bottlenecks, avoids\ndata offloading, and preserves full confidentiality of data, model, and updates\nthroughout training. FuSeFL defends against inference threats, achieves up to\n95% lower communication latency and 50% lower server memory usage, and improves\naccuracy over prior secure FL solutions, demonstrating strong security and\nefficiency at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training without\ncentralizing client data, making it attractive for privacy-sensitive domains.\nWhile existing approaches employ cryptographic techniques such as homomorphic\nencryption, differential privacy, or secure multiparty computation to mitigate\ninference attacks-including model inversion, membership inference, and gradient\nleakage-they often suffer from high computational, communication, or memory\noverheads. Moreover, many methods overlook the confidentiality of the global\nmodel itself, which may be proprietary and sensitive. These challenges limit\nthe practicality of secure FL, especially in cross-silo deployments involving\nlarge datasets and strict compliance requirements.\n  We present FuSeFL, a fully secure and scalable FL scheme designed for\ncross-silo settings. FuSeFL decentralizes training across client pairs using\nlightweight secure multiparty computation (MPC), while confining the server's\nrole to secure aggregation. This design eliminates server bottlenecks, avoids\ndata offloading, and preserves full confidentiality of data, model, and updates\nthroughout training. FuSeFL defends against inference threats, achieves up to\n95% lower communication latency and 50% lower server memory usage, and improves\naccuracy over prior secure FL solutions, demonstrating strong security and\nefficiency at scale."
                },
                "authors": [
                    {
                        "name": "Sahar Ghoflsaz Ghinani"
                    },
                    {
                        "name": "Elaheh Sadredini"
                    }
                ],
                "author_detail": {
                    "name": "Elaheh Sadredini"
                },
                "author": "Elaheh Sadredini",
                "arxiv_comment": "15 Pages, 12 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]