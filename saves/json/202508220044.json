[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.14468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14468v1",
                "updated": "2025-08-20T06:48:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:48:54Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "title": "Diverse Negative Sampling for Implicit Collaborative Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Negative Sampling for Implicit Collaborative Filtering"
                },
                "summary": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Yueqing Xuan"
                    },
                    {
                        "name": "Kacper Sokol"
                    },
                    {
                        "name": "Mark Sanderson"
                    },
                    {
                        "name": "Jeffrey Chan"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Chan"
                },
                "author": "Jeffrey Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14420v1",
                "updated": "2025-08-20T04:36:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T04:36:25Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "title": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan"
                },
                "summary": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform."
                },
                "authors": [
                    {
                        "name": "Shuli Wang"
                    },
                    {
                        "name": "Yinqiu Huang"
                    },
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Yonggang Liu"
                    },
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Yinhua Zhu"
                    },
                    {
                        "name": "Haitao Wang"
                    },
                    {
                        "name": "Xingxing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wang"
                },
                "author": "Xingxing Wang",
                "arxiv_doi": "10.1145/3746252.3761539",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761539",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.14420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14148v1",
                "updated": "2025-08-19T16:56:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T16:56:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPad: Efficient Diffusion Language Models with Suffix Dropout"
                },
                "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad."
                },
                "authors": [
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai \"Hellen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13935v1",
                "updated": "2025-08-19T15:26:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:26:36Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/TC.2025.3587513",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TC.2025.3587513",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Computers",
                "arxiv_journal_ref": "Year 2025, pp. 1-14,",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13909v1",
                "updated": "2025-08-19T15:08:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:08:39Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB)."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/ICDE60146.2024.00312",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE60146.2024.00312",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, accepted by 2024 IEEE 40st International Conference on Data\n  Engineering (ICDE)",
                "arxiv_journal_ref": "Year: 2024, Pages: 4072-4085",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v1",
                "updated": "2025-08-19T14:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13859v1",
                "updated": "2025-08-19T14:18:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:18:16Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "title": "Zobrist Hash-based Duplicate Detection in Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zobrist Hash-based Duplicate Detection in Symbolic Regression"
                },
                "summary": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information."
                },
                "authors": [
                    {
                        "name": "Bogdan Burlacu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Burlacu"
                },
                "author": "Bogdan Burlacu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13756v1",
                "updated": "2025-08-19T11:54:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:54:30Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "title": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video"
                },
                "summary": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems."
                },
                "authors": [
                    {
                        "name": "Ruonan Chai"
                    },
                    {
                        "name": "Yixiang Zhu"
                    },
                    {
                        "name": "Xinjiao Li"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Zili Meng"
                    },
                    {
                        "name": "Dirk Kutscher"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Kutscher"
                },
                "author": "Dirk Kutscher",
                "arxiv_comment": "9 pages, 9 figures, 2 tables. To appear in Proc. of the 33rd ACM\n  International Conference on Multimedia (MM '25), October 27--31, 2025,\n  Dublin, Ireland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.4; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13716v1",
                "updated": "2025-08-19T10:21:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:21:33Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning"
                },
                "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments."
                },
                "authors": [
                    {
                        "name": "Xianfeng Song"
                    },
                    {
                        "name": "Yi Zou"
                    },
                    {
                        "name": "Zheng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Shi"
                },
                "author": "Zheng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v3",
                "updated": "2025-08-19T09:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    13,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v1",
                "updated": "2025-08-19T05:27:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v2",
                "updated": "2025-08-19T03:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    3,
                    13,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v2",
                "updated": "2025-08-19T01:38:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    1,
                    38,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13382v1",
                "updated": "2025-08-18T21:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T21:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "title": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis"
                },
                "summary": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution."
                },
                "authors": [
                    {
                        "name": "Ayoub Ben Chaliah"
                    },
                    {
                        "name": "Hela Dellagi"
                    }
                ],
                "author_detail": {
                    "name": "Hela Dellagi"
                },
                "author": "Hela Dellagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v3",
                "updated": "2025-08-18T16:52:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    52,
                    22,
                    0,
                    230,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up"
                },
                "summary": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v2",
                "updated": "2025-08-18T16:06:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    6,
                    9,
                    0,
                    230,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12767v1",
                "updated": "2025-08-18T09:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:41:28Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "title": "Some optimization possibilities in data plane programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some optimization possibilities in data plane programming"
                },
                "summary": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality."
                },
                "authors": [
                    {
                        "name": "Altangerel Gereltsetseg"
                    },
                    {
                        "name": "Tejfel Máté"
                    }
                ],
                "author_detail": {
                    "name": "Tejfel Máté"
                },
                "author": "Tejfel Máté",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12743v1",
                "updated": "2025-08-18T09:06:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:06:49Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "title": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs"
                },
                "summary": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Ruimin Shi"
                    },
                    {
                        "name": "Edgar A. León"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "arxiv_comment": "To be published in IISWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12691v1",
                "updated": "2025-08-18T07:49:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T07:49:33Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration"
                },
                "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Yuanxin Wei"
                    },
                    {
                        "name": "Lansong Diao"
                    },
                    {
                        "name": "Bujiao Chen"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Zhengping Qian"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Jiangsu Du"
                    }
                ],
                "author_detail": {
                    "name": "Jiangsu Du"
                },
                "author": "Jiangsu Du",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12485v1",
                "updated": "2025-08-17T20:01:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T20:01:12Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "title": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX"
                },
                "summary": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs."
                },
                "authors": [
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "Arpit Bhayani"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Bhayani"
                },
                "author": "Arpit Bhayani",
                "arxiv_comment": "8 pages, 4 figures (system architecture, eviction path, training\n  pipeline, and DQN algorithm), 2 tables. Code available at\n  https://github.com/ayushgupta4897/DRL-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; C.4; D.4.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v1",
                "updated": "2025-08-17T19:07:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12407v1",
                "updated": "2025-08-17T15:48:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T15:48:50Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads"
                },
                "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance."
                },
                "authors": [
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12357v1",
                "updated": "2025-08-17T13:05:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T13:05:52Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "title": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method"
                },
                "summary": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn."
                },
                "authors": [
                    {
                        "name": "S. Khardazi"
                    },
                    {
                        "name": "Z. Gargar"
                    },
                    {
                        "name": "A. Lyubchyk"
                    },
                    {
                        "name": "O. Zakir"
                    },
                    {
                        "name": "D. Mezzane"
                    },
                    {
                        "name": "M. Amjoud"
                    },
                    {
                        "name": "A. Alimoussa"
                    },
                    {
                        "name": "Z. Kutnjak"
                    }
                ],
                "author_detail": {
                    "name": "Z. Kutnjak"
                },
                "author": "Z. Kutnjak",
                "arxiv_doi": "10.1016/j.jssc.2025.125547",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jssc.2025.125547",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.12357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v5",
                "updated": "2025-08-16T23:41:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    23,
                    41,
                    48,
                    5,
                    228,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v2",
                "updated": "2025-08-16T18:49:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    18,
                    49,
                    41,
                    5,
                    228,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. Accepted to the 2025 IEEE-RAS 24th International\n  Conference on Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10824v2",
                "updated": "2025-08-16T03:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    3,
                    17,
                    35,
                    5,
                    228,
                    0
                ],
                "published": "2025-08-14T16:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    48,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures"
                },
                "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Parsa Omidi"
                    },
                    {
                        "name": "Xingshuai Huang"
                    },
                    {
                        "name": "Axel Laborieux"
                    },
                    {
                        "name": "Bahareh Nikpour"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Armaghan Eshaghi"
                    }
                ],
                "author_detail": {
                    "name": "Armaghan Eshaghi"
                },
                "author": "Armaghan Eshaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11495v1",
                "updated": "2025-08-15T14:17:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:17:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation"
                },
                "summary": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators."
                },
                "authors": [
                    {
                        "name": "Jingnan Xu"
                    },
                    {
                        "name": "Leixia Wang"
                    },
                    {
                        "name": "Xiaofeng Meng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Meng"
                },
                "author": "Xiaofeng Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11291v1",
                "updated": "2025-08-15T07:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:55:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks"
                },
                "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks."
                },
                "authors": [
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Chen"
                },
                "author": "Zhiyong Chen",
                "arxiv_comment": "accepted by IEEE/CIC ICCC workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11260v1",
                "updated": "2025-08-15T06:53:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:53:28Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Gaurja Aeron"
                    },
                    {
                        "name": "Antara Raaghavi Bhattacharya"
                    },
                    {
                        "name": "Dang Khoa Dang Dinh"
                    },
                    {
                        "name": "Ikhlasul Akmal Hanif"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v2",
                "updated": "2025-08-15T04:27:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    27,
                    30,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10875v1",
                "updated": "2025-08-14T17:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v2",
                "updated": "2025-08-14T16:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v2",
                "updated": "2025-08-14T15:37:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    37,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache"
                },
                "summary": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10963v1",
                "updated": "2025-08-14T14:11:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:11:48Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "title": "EVCtrl: Efficient Control Adapter for Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVCtrl: Efficient Control Adapter for Visual Generation"
                },
                "summary": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Zixiang Yang"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Yinhan Zhang"
                    },
                    {
                        "name": "Shanhui Mo"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10613v1",
                "updated": "2025-08-14T13:10:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:10:43Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "title": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks"
                },
                "summary": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability."
                },
                "authors": [
                    {
                        "name": "Mengyao Li"
                    },
                    {
                        "name": "Qiaolun Zhang"
                    },
                    {
                        "name": "Zongshuai Yang"
                    },
                    {
                        "name": "Stefano Bregni"
                    },
                    {
                        "name": "Alberto Gatto"
                    },
                    {
                        "name": "Raouf Boutaba"
                    },
                    {
                        "name": "Massimo Tornatore"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Tornatore"
                },
                "author": "Massimo Tornatore",
                "arxiv_comment": "6 pages, this paper has been successfully accepted by GLOBECOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v3",
                "updated": "2025-08-14T10:26:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    26,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Fangyun Zhou"
                    },
                    {
                        "name": "Jiacheng Lv"
                    },
                    {
                        "name": "Jianqi Ma"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Junyan Lv"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Wenkai Lv"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Yewen Wang"
                    },
                    {
                        "name": "Yonghang Guan"
                    },
                    {
                        "name": "Zhihao Hu"
                    },
                    {
                        "name": "Zhongbin Fang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqian Sun"
                },
                "author": "Zhongqian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v2",
                "updated": "2025-08-14T09:04:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    4,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v1",
                "updated": "2025-08-14T08:04:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10424v1",
                "updated": "2025-08-14T07:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:54:44Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "title": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability."
                },
                "authors": [
                    {
                        "name": "Shanyuan Liu"
                    },
                    {
                        "name": "Jian Zhu"
                    },
                    {
                        "name": "Junda Lu"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Liuzhuozheng Li"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Liebucha Wu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yin"
                },
                "author": "Yuhui Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10395v1",
                "updated": "2025-08-14T06:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization"
                },
                "summary": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models."
                },
                "authors": [
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v3",
                "updated": "2025-08-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v1",
                "updated": "2025-08-13T13:54:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining."
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v1",
                "updated": "2025-08-13T07:40:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v4",
                "updated": "2025-08-13T06:13:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    6,
                    13,
                    36,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v2",
                "updated": "2025-08-13T04:24:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    24,
                    56,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v4",
                "updated": "2025-08-13T04:03:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    3,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09442v1",
                "updated": "2025-08-13T02:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference"
                },
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment."
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09419v1",
                "updated": "2025-08-13T01:39:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T01:39:09Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "title": "Design and Simulation of 6T SRAM Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Simulation of 6T SRAM Array"
                },
                "summary": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given."
                },
                "authors": [
                    {
                        "name": "Justin London"
                    }
                ],
                "author_detail": {
                    "name": "Justin London"
                },
                "author": "Justin London",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08744v2",
                "updated": "2025-08-13T01:39:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    3,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T08:39:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    39,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality."
                },
                "authors": [
                    {
                        "name": "Zhonggen Li"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Bocheng Yu"
                    },
                    {
                        "name": "Baihua Zheng"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09262v1",
                "updated": "2025-08-12T18:05:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T18:05:33Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Input-Adaptive Inference for Efficient VLN"
                },
                "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation."
                },
                "authors": [
                    {
                        "name": "Dongwoo Kang"
                    },
                    {
                        "name": "Akhil Perincherry"
                    },
                    {
                        "name": "Zachary Coalson"
                    },
                    {
                        "name": "Aiden Gabriel"
                    },
                    {
                        "name": "Stefan Lee"
                    },
                    {
                        "name": "Sanghyun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sanghyun Hong"
                },
                "author": "Sanghyun Hong",
                "arxiv_comment": "Accepted to ICCV 2025 [Poster]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v1",
                "updated": "2025-08-12T16:47:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Yi Fei"
                    },
                    {
                        "name": "Zeng Weidi"
                    }
                ],
                "author_detail": {
                    "name": "Zeng Weidi"
                },
                "author": "Zeng Weidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09001v1",
                "updated": "2025-08-12T15:11:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:11:47Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Sparse Attention for Efficient Long-Context Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%."
                },
                "authors": [
                    {
                        "name": "Seonghwan Choi"
                    },
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08978v1",
                "updated": "2025-08-12T14:40:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:40:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "TaoCache: Structure-Maintained Video Generation Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaoCache: Structure-Maintained Video Generation Acceleration"
                },
                "summary": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups."
                },
                "authors": [
                    {
                        "name": "Zhentao Fan"
                    },
                    {
                        "name": "Zongzuo Wang"
                    },
                    {
                        "name": "Weiwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Zhang"
                },
                "author": "Weiwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11488v2",
                "updated": "2025-08-12T10:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    43,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2023-11-30T16:02:04Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    2,
                    4,
                    3,
                    334,
                    0
                ],
                "title": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows"
                },
                "summary": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes."
                },
                "authors": [
                    {
                        "name": "Thiago Garrett"
                    },
                    {
                        "name": "Weijia Song"
                    },
                    {
                        "name": "Roman Vitenberg"
                    },
                    {
                        "name": "Ken Birman"
                    }
                ],
                "author_detail": {
                    "name": "Ken Birman"
                },
                "author": "Ken Birman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v3",
                "updated": "2025-08-12T05:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    5,
                    51,
                    37,
                    1,
                    224,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08600v1",
                "updated": "2025-08-12T03:33:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T03:33:15Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "title": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes"
                },
                "summary": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased."
                },
                "authors": [
                    {
                        "name": "Timur V. Tscherbul"
                    },
                    {
                        "name": "Roman V. Krems"
                    }
                ],
                "author_detail": {
                    "name": "Roman V. Krems"
                },
                "author": "Roman V. Krems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07675v2",
                "updated": "2025-08-12T02:51:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    51,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    53,
                    27,
                    0,
                    223,
                    0
                ],
                "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v2",
                "updated": "2025-08-12T02:27:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    27,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08531v1",
                "updated": "2025-08-12T00:06:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T00:06:34Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "title": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective"
                },
                "summary": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference."
                },
                "authors": [
                    {
                        "name": "Afsara Benazir"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08457v1",
                "updated": "2025-08-11T20:30:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T20:30:31Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "title": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories"
                },
                "summary": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Muhammed Ahosan Ul Karim"
                    },
                    {
                        "name": "Harsono Simka"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "7 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.3; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08438v1",
                "updated": "2025-08-11T19:55:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T19:55:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference"
                },
                "summary": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Zecheng Lin"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Jianchang Su"
                    },
                    {
                        "name": "Cheng Chu"
                    },
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Wenhui Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "17 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08081v1",
                "updated": "2025-08-11T15:28:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:28:28Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "title": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures"
                },
                "summary": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29."
                },
                "authors": [
                    {
                        "name": "Florian Naef"
                    },
                    {
                        "name": "Thomas Willwacher"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Willwacher"
                },
                "author": "Thomas Willwacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v2",
                "updated": "2025-08-11T14:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    27,
                    0,
                    223,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 14 figures; Accepted by ICCV2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v1",
                "updated": "2025-08-11T10:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving"
                },
                "summary": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Under review for a computer science conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07811v1",
                "updated": "2025-08-11T09:54:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:54:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"
                },
                "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions."
                },
                "authors": [
                    {
                        "name": "Sicheng Gao"
                    },
                    {
                        "name": "Nancy Mehta"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v2",
                "updated": "2025-08-11T08:10:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    10,
                    21,
                    0,
                    223,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07570v1",
                "updated": "2025-08-11T03:03:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T03:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios."
                },
                "authors": [
                    {
                        "name": "Khanh-Binh Nguyen"
                    },
                    {
                        "name": "Phuoc-Nguyen Bui"
                    },
                    {
                        "name": "Hyunseung Choo"
                    },
                    {
                        "name": "Duc Thanh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duc Thanh Nguyen"
                },
                "author": "Duc Thanh Nguyen",
                "arxiv_comment": "12 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09208v1",
                "updated": "2025-08-10T14:05:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "published": "2025-08-10T14:05:36Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "title": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge"
                },
                "summary": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models."
                },
                "authors": [
                    {
                        "name": "Muqing Li"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v2",
                "updated": "2025-08-09T11:31:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    31,
                    44,
                    5,
                    221,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06937v1",
                "updated": "2025-08-09T11:06:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-09T11:06:58Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing"
                },
                "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods."
                },
                "authors": [
                    {
                        "name": "Weiyan Xie"
                    },
                    {
                        "name": "Han Gao"
                    },
                    {
                        "name": "Didan Deng"
                    },
                    {
                        "name": "Kaican Li"
                    },
                    {
                        "name": "April Hua Liu"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "Project Page: vaynexie.github.io/CannyEdit/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v2",
                "updated": "2025-08-09T02:33:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    2,
                    33,
                    21,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v2",
                "updated": "2025-08-09T00:12:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    0,
                    12,
                    1,
                    5,
                    221,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v3",
                "updated": "2025-08-08T18:16:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    16,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06160v1",
                "updated": "2025-08-08T09:29:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:29:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment"
                },
                "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff."
                },
                "authors": [
                    {
                        "name": "Zhenbang Du"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lifu Wang"
                    },
                    {
                        "name": "Jiayi Qian"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v1",
                "updated": "2025-08-08T08:54:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06064v1",
                "updated": "2025-08-08T06:53:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T06:53:50Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generic Complete Anytime Beam Search for Optimal Decision Tree"
                },
                "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees."
                },
                "authors": [
                    {
                        "name": "Harold Silvère Kiossou"
                    },
                    {
                        "name": "Siegfried Nijssen"
                    },
                    {
                        "name": "Pierre Schaus"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Schaus"
                },
                "author": "Pierre Schaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2104.13123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2104.13123v3",
                "updated": "2025-08-08T06:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    38,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2021-04-27T11:55:54Z",
                "published_parsed": [
                    2021,
                    4,
                    27,
                    11,
                    55,
                    54,
                    1,
                    117,
                    0
                ],
                "title": "Affine Springer fibers and depth zero L-packets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affine Springer fibers and depth zero L-packets"
                },
                "summary": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable."
                },
                "authors": [
                    {
                        "name": "Roman Bezrukavnikov"
                    },
                    {
                        "name": "Yakov Varshavsky"
                    }
                ],
                "author_detail": {
                    "name": "Yakov Varshavsky"
                },
                "author": "Yakov Varshavsky",
                "arxiv_comment": "v.3, 96 pages, minor changes in abstract and introduction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2104.13123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2104.13123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "22E50, 22E57",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09192v1",
                "updated": "2025-08-08T04:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    4,
                    51,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T04:51:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    4,
                    51,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing"
                },
                "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing."
                },
                "authors": [
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Chenkai Xu"
                    },
                    {
                        "name": "Yijie Jin"
                    },
                    {
                        "name": "Jiachun Jin"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05904v1",
                "updated": "2025-08-07T23:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:53:31Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "title": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data"
                },
                "summary": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks."
                },
                "authors": [
                    {
                        "name": "Brandon Baker"
                    },
                    {
                        "name": "Elliott Brossard"
                    },
                    {
                        "name": "Chenwei Xie"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Deen Liu"
                    },
                    {
                        "name": "Yijun Xie"
                    },
                    {
                        "name": "Arthur Zwiegincew"
                    },
                    {
                        "name": "Nitya Kumar Sharma"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Eugene Retunsky"
                    },
                    {
                        "name": "Mike Halcrow"
                    },
                    {
                        "name": "Derek Denny-Brown"
                    },
                    {
                        "name": "Istvan Cseri"
                    },
                    {
                        "name": "Tyler Akidau"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "arxiv_comment": "12 pages, 6 figures, accepted in ICDCS 2025",
                "arxiv_journal_ref": "Proc. 45th IEEE International Conference on Distributed Computing\n  Systems (ICDCS), Glasgow, UK, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05898v1",
                "updated": "2025-08-07T23:11:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:11:33Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates"
                },
                "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA."
                },
                "authors": [
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Aijun An"
                    },
                    {
                        "name": "Ali cheraghian"
                    }
                ],
                "author_detail": {
                    "name": "Ali cheraghian"
                },
                "author": "Ali cheraghian",
                "arxiv_comment": "BMVC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10024v1",
                "updated": "2025-08-07T21:18:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    21,
                    18,
                    52,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T21:18:52Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    21,
                    18,
                    52,
                    3,
                    219,
                    0
                ],
                "title": "RTTC: Reward-Guided Collaborative Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTTC: Reward-Guided Collaborative Test-Time Compute"
                },
                "summary": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation."
                },
                "authors": [
                    {
                        "name": "J. Pablo Muñoz"
                    },
                    {
                        "name": "Jinjie Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Yuan"
                },
                "author": "Jinjie Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v1",
                "updated": "2025-08-07T09:47:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05091v1",
                "updated": "2025-08-07T07:19:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T07:19:02Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "title": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation"
                },
                "summary": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration."
                },
                "authors": [
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Busheng Su"
                    },
                    {
                        "name": "Finn Wong"
                    }
                ],
                "author_detail": {
                    "name": "Finn Wong"
                },
                "author": "Finn Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05012v1",
                "updated": "2025-08-07T03:49:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T03:49:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines"
                },
                "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion."
                },
                "authors": [
                    {
                        "name": "Ugur Cetintemel"
                    },
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Alexander W. Lee"
                    },
                    {
                        "name": "Deepti Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Raghavan"
                },
                "author": "Deepti Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v3",
                "updated": "2025-08-06T15:38:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    38,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads"
                },
                "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Ted Hart"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "Proceedings of the VLDB Endowment 18 (VLDB'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v3",
                "updated": "2025-08-06T11:46:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    46,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "arxiv_comment": "Submission under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v2",
                "updated": "2025-08-06T08:32:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    8,
                    32,
                    53,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "The data and method in the paper need to be re-audited",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11661v1",
                "updated": "2025-08-06T02:53:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    2,
                    53,
                    14,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T02:53:14Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    2,
                    53,
                    14,
                    2,
                    218,
                    0
                ],
                "title": "Sparse Attention across Multiple-context KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention across Multiple-context KV Cache"
                },
                "summary": "Large language models face significant cost challenges in long-sequence\ninference. To address this, reusing historical Key-Value (KV) Cache for\nimproved inference efficiency has become a mainstream approach. Recent advances\nfurther enhance throughput by sparse attention mechanisms to select the most\nrelevant KV Cache, thereby reducing sequence length. However, such techniques\nare limited to single-context scenarios, where historical KV Cache is computed\nsequentially with causal-attention dependencies. In retrieval-augmented\ngeneration (RAG) scenarios, where retrieved documents as context are unknown\nbeforehand, each document's KV Cache is computed and stored independently\n(termed multiple-context KV Cache), lacking cross-attention between contexts.\nThis renders existing methods ineffective. Although prior work partially\nrecomputes multiple-context KV Cache to mitigate accuracy loss from missing\ncross-attention, it requires retaining all KV Cache throughout, failing to\nreduce memory overhead. This paper presents SamKV, the first exploration of\nattention sparsification for multiple-context KV Cache. Specifically, SamKV\ntakes into account the complementary information of other contexts when\nsparsifying one context, and then locally recomputes the sparsified\ninformation. Experiments demonstrate that our method compresses sequence length\nto 15% without accuracy degradation compared with full-recompuation baselines,\nsignificantly boosting throughput in multi-context RAG scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant cost challenges in long-sequence\ninference. To address this, reusing historical Key-Value (KV) Cache for\nimproved inference efficiency has become a mainstream approach. Recent advances\nfurther enhance throughput by sparse attention mechanisms to select the most\nrelevant KV Cache, thereby reducing sequence length. However, such techniques\nare limited to single-context scenarios, where historical KV Cache is computed\nsequentially with causal-attention dependencies. In retrieval-augmented\ngeneration (RAG) scenarios, where retrieved documents as context are unknown\nbeforehand, each document's KV Cache is computed and stored independently\n(termed multiple-context KV Cache), lacking cross-attention between contexts.\nThis renders existing methods ineffective. Although prior work partially\nrecomputes multiple-context KV Cache to mitigate accuracy loss from missing\ncross-attention, it requires retaining all KV Cache throughout, failing to\nreduce memory overhead. This paper presents SamKV, the first exploration of\nattention sparsification for multiple-context KV Cache. Specifically, SamKV\ntakes into account the complementary information of other contexts when\nsparsifying one context, and then locally recomputes the sparsified\ninformation. Experiments demonstrate that our method compresses sequence length\nto 15% without accuracy degradation compared with full-recompuation baselines,\nsignificantly boosting throughput in multi-context RAG scenarios."
                },
                "authors": [
                    {
                        "name": "Ziyi Cao"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jingbin Zhang"
                    },
                    {
                        "name": "Bingquan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bingquan Liu"
                },
                "author": "Bingquan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03837v1",
                "updated": "2025-08-05T18:34:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T18:34:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems"
                },
                "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."
                },
                "authors": [
                    {
                        "name": "Davide Zoni"
                    },
                    {
                        "name": "Andrea Galimberti"
                    },
                    {
                        "name": "Adriano Guarisco"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Guarisco"
                },
                "author": "Adriano Guarisco",
                "arxiv_comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v2",
                "updated": "2025-08-05T16:17:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    17,
                    1,
                    1,
                    217,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03321v1",
                "updated": "2025-08-05T11:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T11:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios"
                },
                "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS."
                },
                "authors": [
                    {
                        "name": "Jörn Bodenhausen"
                    },
                    {
                        "name": "Simon Mangel"
                    },
                    {
                        "name": "Thomas Vogt"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03258v1",
                "updated": "2025-08-05T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T09:35:52Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%."
                },
                "authors": [
                    {
                        "name": "Yueyue Liu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yuantian Miao"
                    }
                ],
                "author_detail": {
                    "name": "Yuantian Miao"
                },
                "author": "Yuantian Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.14896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14896v1",
                "updated": "2025-08-20T17:59:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    59,
                    51,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:59:51Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    59,
                    51,
                    2,
                    232,
                    0
                ],
                "title": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs"
                },
                "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community."
                },
                "authors": [
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Haobo Xu"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Zhenan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Sun"
                },
                "author": "Zhenan Sun",
                "arxiv_comment": "Technical Report, Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14890v1",
                "updated": "2025-08-20T17:59:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    59,
                    1,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:59:01Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    59,
                    1,
                    2,
                    232,
                    0
                ],
                "title": "Estimating Initial Mass of Gaia-Enceladus Dwarf Galaxy with Chemical\n  Evolution Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Initial Mass of Gaia-Enceladus Dwarf Galaxy with Chemical\n  Evolution Model"
                },
                "summary": "This work investigates the initial mass and chemical evolution history of the\nGaia-Enceladus dwarf galaxy. We combine spectroscopic data from APOGEE with\nastrometric data from Gaia DR3 to identify Gaia-Enceladus candidate stars via a\nmachine-learning pipeline using t-SNE and HDBSCAN. By focusing on kinematic and\nchemical parameters, especially $\\mathrm{[Fe/H]}$, $\\mathrm{[Mg/Fe]}$,\n$\\mathrm{[Al/Fe]}$, and $\\mathrm{[Mn/Fe]}$, we uncover a population of\nmetal-poor, high-eccentricity stars that align with literature criteria for\nGaia-Enceladus debris. We then apply the \\textit{OMEGA+} chemical evolution\nmodel, incorporating MCMC fitting of the observed abundance trends in the\n$\\mathrm{[Mg/Fe]\\times[Fe/H]}$ plane. Our best-fitting model indicates a gas\nmass of $4.93_{-0.72}^{+0.32}\\times10^9\\,{M_{\\odot}}$ for Gaia-Enceladus,\nplacing it at the higher end of previously suggested mass ranges. The model\nscenario suggests a short star formation timescale, substantial outflows, and a\nrapid build-up of metals mainly driven by core-collapse supernovae, with a\nlesser contribution from Type~Ia supernovae. Comparison with observational data\nin other chemical planes (e.g., $\\mathrm{[Mg/Mn]\\times[Al/Fe]}$) supports this\nscenario, emphasizing a distinct evolution path relative to the Milky Way.\nAdditionally, our results provide indirect evidence that star formation in\nGaia-Enceladus likely ceased within the first 4 Gyr, consistent with earlier\ninferences of an early merger event. These findings highlight the power of\nchemical evolution modeling in reconstructing the origin and mass of ancient\naccreted systems. Overall, we show that Gaia-Enceladus, through a rapid star\nformation and strong outflows, contributed a significant fraction of the\nmetal-poor stellar halo of the Milky Way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work investigates the initial mass and chemical evolution history of the\nGaia-Enceladus dwarf galaxy. We combine spectroscopic data from APOGEE with\nastrometric data from Gaia DR3 to identify Gaia-Enceladus candidate stars via a\nmachine-learning pipeline using t-SNE and HDBSCAN. By focusing on kinematic and\nchemical parameters, especially $\\mathrm{[Fe/H]}$, $\\mathrm{[Mg/Fe]}$,\n$\\mathrm{[Al/Fe]}$, and $\\mathrm{[Mn/Fe]}$, we uncover a population of\nmetal-poor, high-eccentricity stars that align with literature criteria for\nGaia-Enceladus debris. We then apply the \\textit{OMEGA+} chemical evolution\nmodel, incorporating MCMC fitting of the observed abundance trends in the\n$\\mathrm{[Mg/Fe]\\times[Fe/H]}$ plane. Our best-fitting model indicates a gas\nmass of $4.93_{-0.72}^{+0.32}\\times10^9\\,{M_{\\odot}}$ for Gaia-Enceladus,\nplacing it at the higher end of previously suggested mass ranges. The model\nscenario suggests a short star formation timescale, substantial outflows, and a\nrapid build-up of metals mainly driven by core-collapse supernovae, with a\nlesser contribution from Type~Ia supernovae. Comparison with observational data\nin other chemical planes (e.g., $\\mathrm{[Mg/Mn]\\times[Al/Fe]}$) supports this\nscenario, emphasizing a distinct evolution path relative to the Milky Way.\nAdditionally, our results provide indirect evidence that star formation in\nGaia-Enceladus likely ceased within the first 4 Gyr, consistent with earlier\ninferences of an early merger event. These findings highlight the power of\nchemical evolution modeling in reconstructing the origin and mass of ancient\naccreted systems. Overall, we show that Gaia-Enceladus, through a rapid star\nformation and strong outflows, contributed a significant fraction of the\nmetal-poor stellar halo of the Milky Way."
                },
                "authors": [
                    {
                        "name": "Olcay Plevne"
                    },
                    {
                        "name": "Furkan Akbaba"
                    }
                ],
                "author_detail": {
                    "name": "Furkan Akbaba"
                },
                "author": "Furkan Akbaba",
                "arxiv_comment": "Accepted for publication in The Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14880v1",
                "updated": "2025-08-20T17:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    51,
                    20,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    51,
                    20,
                    2,
                    232,
                    0
                ],
                "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A\n  Knowledge-Informed Trajectory Synthesis Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A\n  Knowledge-Informed Trajectory Synthesis Framework"
                },
                "summary": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts.We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions.Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts.We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions.Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains."
                },
                "authors": [
                    {
                        "name": "Ailing Yu"
                    },
                    {
                        "name": "Lan Yao"
                    },
                    {
                        "name": "Jingnan Liu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Jiajun Yin"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xinhao Liao"
                    },
                    {
                        "name": "Zhiling Ye"
                    },
                    {
                        "name": "Ji Li"
                    },
                    {
                        "name": "Yun Yue"
                    },
                    {
                        "name": "Hansong Xiao"
                    },
                    {
                        "name": "Hualei Zhou"
                    },
                    {
                        "name": "Chunxiao Guo"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14879v1",
                "updated": "2025-08-20T17:50:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    50,
                    15,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:50:15Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    50,
                    15,
                    2,
                    232,
                    0
                ],
                "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds"
                },
                "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding."
                },
                "authors": [
                    {
                        "name": "Bingquan Dai"
                    },
                    {
                        "name": "Li Ray Luo"
                    },
                    {
                        "name": "Qihong Tang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xinyu Lian"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Minghan Qin"
                    },
                    {
                        "name": "Xudong Xu"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhaoyang Lyu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14869v1",
                "updated": "2025-08-20T17:31:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    31,
                    53,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:31:53Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    31,
                    53,
                    2,
                    232,
                    0
                ],
                "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding\n  Large Language Models"
                },
                "summary": "Prompt engineering has rapidly emerged as a critical skill for effective\ninteraction with large language models (LLMs). However, the cognitive and\nneural underpinnings of this expertise remain largely unexplored. This paper\npresents findings from a cross-sectional pilot fMRI study investigating\ndifferences in brain functional connectivity and network activity between\nexperts and intermediate prompt engineers. Our results reveal distinct neural\nsignatures associated with higher prompt engineering literacy, including\nincreased functional connectivity in brain regions such as the left middle\ntemporal gyrus and the left frontal pole, as well as altered power-frequency\ndynamics in key cognitive networks. These findings offer initial insights into\nthe neurobiological basis of prompt engineering proficiency. We discuss the\nimplications of these neurocognitive markers in Natural Language Processing\n(NLP). Understanding the neural basis of human expertise in interacting with\nLLMs can inform the design of more intuitive human-AI interfaces, contribute to\ncognitive models of LLM interaction, and potentially guide the development of\nAI systems that better align with human cognitive workflows. This\ninterdisciplinary approach aims to bridge the gap between human cognition and\nmachine intelligence, fostering a deeper understanding of how humans learn and\nadapt to complex AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering has rapidly emerged as a critical skill for effective\ninteraction with large language models (LLMs). However, the cognitive and\nneural underpinnings of this expertise remain largely unexplored. This paper\npresents findings from a cross-sectional pilot fMRI study investigating\ndifferences in brain functional connectivity and network activity between\nexperts and intermediate prompt engineers. Our results reveal distinct neural\nsignatures associated with higher prompt engineering literacy, including\nincreased functional connectivity in brain regions such as the left middle\ntemporal gyrus and the left frontal pole, as well as altered power-frequency\ndynamics in key cognitive networks. These findings offer initial insights into\nthe neurobiological basis of prompt engineering proficiency. We discuss the\nimplications of these neurocognitive markers in Natural Language Processing\n(NLP). Understanding the neural basis of human expertise in interacting with\nLLMs can inform the design of more intuitive human-AI interfaces, contribute to\ncognitive models of LLM interaction, and potentially guide the development of\nAI systems that better align with human cognitive workflows. This\ninterdisciplinary approach aims to bridge the gap between human cognition and\nmachine intelligence, fostering a deeper understanding of how humans learn and\nadapt to complex AI systems."
                },
                "authors": [
                    {
                        "name": "Hend Al-Khalifa"
                    },
                    {
                        "name": "Raneem Almansour"
                    },
                    {
                        "name": "Layan Abdulrahman Alhuasini"
                    },
                    {
                        "name": "Alanood Alsaleh"
                    },
                    {
                        "name": "Mohamad-Hani Temsah"
                    },
                    {
                        "name": "Mohamad-Hani_Temsah"
                    },
                    {
                        "name": "Ashwag Rafea S Alruwaili"
                    }
                ],
                "author_detail": {
                    "name": "Ashwag Rafea S Alruwaili"
                },
                "author": "Ashwag Rafea S Alruwaili",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02085v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02085v4",
                "updated": "2025-08-20T17:19:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    19,
                    48,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-04T05:51:55Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    5,
                    51,
                    55,
                    0,
                    216,
                    0
                ],
                "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents"
                },
                "summary": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent."
                },
                "authors": [
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Yifu Guo"
                    },
                    {
                        "name": "Yuzhen Han"
                    },
                    {
                        "name": "Sen Hu"
                    },
                    {
                        "name": "Ziyi Ni"
                    },
                    {
                        "name": "Licheng Wang"
                    },
                    {
                        "name": "Mingguang Chen"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Huacan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huacan Wang"
                },
                "author": "Huacan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02085v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02085v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18889v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18889v4",
                "updated": "2025-08-20T17:03:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    3,
                    48,
                    2,
                    232,
                    0
                ],
                "published": "2025-05-24T22:22:43Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    22,
                    22,
                    43,
                    5,
                    144,
                    0
                ],
                "title": "Security Concerns for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Concerns for Large Language Models: A Survey"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused\na revolution in natural language processing, but their capabilities also\nintroduce new security vulnerabilities. This survey provides a comprehensive\noverview of these emerging concerns, categorizing threats into several key\nareas: prompt injection and jailbreaking; adversarial attacks, including input\nperturbations and data poisoning; misuse by malicious actors to generate\ndisinformation, phishing emails, and malware; and the worrisome risks inherent\nin autonomous LLM agents. Recently, a significant focus is increasingly being\nplaced on the latter, exploring goal misalignment, emergent deception,\nself-preservation instincts, and the potential for LLMs to develop and pursue\ncovert, misaligned objectives, a behavior known as scheming, which may even\npersist through safety training. We summarize recent academic and industrial\nstudies from 2022 to 2025 that exemplify each threat, analyze proposed defenses\nand their limitations, and identify open challenges in securing LLM-based\napplications. We conclude by emphasizing the importance of advancing robust,\nmulti-layered security strategies to ensure LLMs are safe and beneficial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused\na revolution in natural language processing, but their capabilities also\nintroduce new security vulnerabilities. This survey provides a comprehensive\noverview of these emerging concerns, categorizing threats into several key\nareas: prompt injection and jailbreaking; adversarial attacks, including input\nperturbations and data poisoning; misuse by malicious actors to generate\ndisinformation, phishing emails, and malware; and the worrisome risks inherent\nin autonomous LLM agents. Recently, a significant focus is increasingly being\nplaced on the latter, exploring goal misalignment, emergent deception,\nself-preservation instincts, and the potential for LLMs to develop and pursue\ncovert, misaligned objectives, a behavior known as scheming, which may even\npersist through safety training. We summarize recent academic and industrial\nstudies from 2022 to 2025 that exemplify each threat, analyze proposed defenses\nand their limitations, and identify open challenges in securing LLM-based\napplications. We conclude by emphasizing the importance of advancing robust,\nmulti-layered security strategies to ensure LLMs are safe and beneficial."
                },
                "authors": [
                    {
                        "name": "Miles Q. Li"
                    },
                    {
                        "name": "Benjamin C. M. Fung"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin C. M. Fung"
                },
                "author": "Benjamin C. M. Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18889v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18889v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14853v1",
                "updated": "2025-08-20T17:03:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    3,
                    32,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:03:32Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    3,
                    32,
                    2,
                    232,
                    0
                ],
                "title": "Universal and Transferable Adversarial Attack on Large Language Models\n  Using Exponentiated Gradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal and Transferable Adversarial Attack on Large Language Models\n  Using Exponentiated Gradient Descent"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in critical\napplications, ensuring their robustness and safety alignment remains a major\nchallenge. Despite the overall success of alignment techniques such as\nreinforcement learning from human feedback (RLHF) on typical prompts, LLMs\nremain vulnerable to jailbreak attacks enabled by crafted adversarial triggers\nappended to user prompts. Most existing jailbreak methods either rely on\ninefficient searches over discrete token spaces or direct optimization of\ncontinuous embeddings. While continuous embeddings can be given directly to\nselected open-source models as input, doing so is not feasible for proprietary\nmodels. On the other hand, projecting these embeddings back into valid discrete\ntokens introduces additional complexity and often reduces attack effectiveness.\nWe propose an intrinsic optimization method which directly optimizes relaxed\none-hot encodings of the adversarial suffix tokens using exponentiated gradient\ndescent coupled with Bregman projection, ensuring that the optimized one-hot\nencoding of each token always remains within the probability simplex. We\nprovide theoretical proof of convergence for our proposed method and implement\nan efficient algorithm that effectively jailbreaks several widely used LLMs.\nOur method achieves higher success rates and faster convergence compared to\nthree state-of-the-art baselines, evaluated on five open-source LLMs and four\nadversarial behavior datasets curated for evaluating jailbreak methods. In\naddition to individual prompt attacks, we also generate universal adversarial\nsuffixes effective across multiple prompts and demonstrate transferability of\noptimized suffixes to different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in critical\napplications, ensuring their robustness and safety alignment remains a major\nchallenge. Despite the overall success of alignment techniques such as\nreinforcement learning from human feedback (RLHF) on typical prompts, LLMs\nremain vulnerable to jailbreak attacks enabled by crafted adversarial triggers\nappended to user prompts. Most existing jailbreak methods either rely on\ninefficient searches over discrete token spaces or direct optimization of\ncontinuous embeddings. While continuous embeddings can be given directly to\nselected open-source models as input, doing so is not feasible for proprietary\nmodels. On the other hand, projecting these embeddings back into valid discrete\ntokens introduces additional complexity and often reduces attack effectiveness.\nWe propose an intrinsic optimization method which directly optimizes relaxed\none-hot encodings of the adversarial suffix tokens using exponentiated gradient\ndescent coupled with Bregman projection, ensuring that the optimized one-hot\nencoding of each token always remains within the probability simplex. We\nprovide theoretical proof of convergence for our proposed method and implement\nan efficient algorithm that effectively jailbreaks several widely used LLMs.\nOur method achieves higher success rates and faster convergence compared to\nthree state-of-the-art baselines, evaluated on five open-source LLMs and four\nadversarial behavior datasets curated for evaluating jailbreak methods. In\naddition to individual prompt attacks, we also generate universal adversarial\nsuffixes effective across multiple prompts and demonstrate transferability of\noptimized suffixes to different LLMs."
                },
                "authors": [
                    {
                        "name": "Sajib Biswas"
                    },
                    {
                        "name": "Mao Nishino"
                    },
                    {
                        "name": "Samuel Jacob Chacko"
                    },
                    {
                        "name": "Xiuwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiuwen Liu"
                },
                "author": "Xiuwen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14846v1",
                "updated": "2025-08-20T16:57:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    57,
                    55,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T16:57:55Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    57,
                    55,
                    2,
                    232,
                    0
                ],
                "title": "Modeling tails of escaping gas in exoplanet atmospheres with Harmonica",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling tails of escaping gas in exoplanet atmospheres with Harmonica"
                },
                "summary": "Exoplanets that reside close to their host stars, and therefore receive\nsubstantial amounts of X-ray and ultraviolet radiation, are prone to suffer\nfrom strong atmospheric escape. This can lead to the creation of an envelope of\nescaping gas along the planet's orbital trajectory, often referred to as a\ntail. When transiting in front of their host star, these tails can not only\nproduce larger depths in the transit light curves, but also introduce\nsignificant asymmetries between ingress and egress. Using the publicly\navailable software Harmonica, we present a method to model the light curves of\ntransiting planets surrounded by extended envelopes of escaping gas, and\nsubsequently infer the shape and size of the latter. We apply this method to\nthe JWST NIRISS/SOSS observations of HAT-P-18b, which show pronounced helium\ntail features in its spectroscopic light curve of the metastable helium triplet\nat 10830 \\r{A}. Our model reveals that, in order to fit the observed light\ncurve of HAT-P-18b, the planet must possess a trailing helium tail of\n$15.79^{+1.14}_{-1.05}$ planetary radii. We carry out injection-recovery tests\nto validate the effectiveness of the proposed methodology. We demonstrate that,\nwith sufficient precision, we would be able to fit a multi-layer envelope to\nthe data, which would provide insight into the relative radial variations in\nthe opacity profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exoplanets that reside close to their host stars, and therefore receive\nsubstantial amounts of X-ray and ultraviolet radiation, are prone to suffer\nfrom strong atmospheric escape. This can lead to the creation of an envelope of\nescaping gas along the planet's orbital trajectory, often referred to as a\ntail. When transiting in front of their host star, these tails can not only\nproduce larger depths in the transit light curves, but also introduce\nsignificant asymmetries between ingress and egress. Using the publicly\navailable software Harmonica, we present a method to model the light curves of\ntransiting planets surrounded by extended envelopes of escaping gas, and\nsubsequently infer the shape and size of the latter. We apply this method to\nthe JWST NIRISS/SOSS observations of HAT-P-18b, which show pronounced helium\ntail features in its spectroscopic light curve of the metastable helium triplet\nat 10830 \\r{A}. Our model reveals that, in order to fit the observed light\ncurve of HAT-P-18b, the planet must possess a trailing helium tail of\n$15.79^{+1.14}_{-1.05}$ planetary radii. We carry out injection-recovery tests\nto validate the effectiveness of the proposed methodology. We demonstrate that,\nwith sufficient precision, we would be able to fit a multi-layer envelope to\nthe data, which would provide insight into the relative radial variations in\nthe opacity profile."
                },
                "authors": [
                    {
                        "name": "Carlos Gascón"
                    },
                    {
                        "name": "Mercedes López-Morales"
                    },
                    {
                        "name": "Shreyas Vissapragada"
                    },
                    {
                        "name": "Morgan MacLeod"
                    },
                    {
                        "name": "Hannah R. Wakeford"
                    },
                    {
                        "name": "David Grant"
                    },
                    {
                        "name": "Ignasi Ribas"
                    },
                    {
                        "name": "Guillem Anglada-Escudé"
                    }
                ],
                "author_detail": {
                    "name": "Guillem Anglada-Escudé"
                },
                "author": "Guillem Anglada-Escudé",
                "arxiv_comment": "15 pages, 10 figures, 3 tables. Accepted to APJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19485v2",
                "updated": "2025-08-20T16:32:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    32,
                    6,
                    2,
                    232,
                    0
                ],
                "published": "2024-11-29T05:54:41Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    54,
                    41,
                    4,
                    334,
                    0
                ],
                "title": "Action Engine: Automatic Workflow Generation in FaaS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action Engine: Automatic Workflow Generation in FaaS"
                },
                "summary": "Function as a Service (FaaS) is poised to become the foundation of the next\ngeneration of cloud systems due to its inherent advantages in scalability,\ncost-efficiency, and ease of use. However, challenges such as the need for\nspecialized knowledge, platform dependence, and difficulty in scalability in\nbuilding functional workflows persist for cloud-native application developers.\nTo overcome these challenges and mitigate the burden of developing FaaS-based\napplications, in this paper, we propose a mechanism called Action Engine, that\nmakes use of tool-augmented large language models (LLMs) at its kernel to\ninterpret human language queries and automates FaaS workflow generation,\nthereby, reducing the need for specialized expertise and manual design. Action\nEngine includes modules to identify relevant functions from the FaaS repository\nand seamlessly manage the data dependency between them, ensuring the\ndeveloper's query is processed and resolved. Beyond that, Action Engine can\nexecute the generated workflow by injecting the user-provided arguments. On\nanother front, this work addresses a gap in tool-augmented LLM research via\nadopting an Automatic FaaS Workflow Generation perspective to systematically\nevaluate methodologies across four fundamental sub-processes. Through\nbenchmarking various parameters, this research provides critical insights into\nstreamlining workflow automation for real-world applications, specifically in\nthe FaaS continuum. Our evaluations demonstrate that the Action Engine achieves\ncomparable performance to the few-shot learning approach while maintaining\nplatform- and language-agnosticism, thereby, mitigating provider-specific\ndependencies in workflow generation. We notice that Action Engine can unlock\nFaaS workflow generation for non-cloud-savvy developers and expedite the\ndevelopment cycles of cloud-native applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function as a Service (FaaS) is poised to become the foundation of the next\ngeneration of cloud systems due to its inherent advantages in scalability,\ncost-efficiency, and ease of use. However, challenges such as the need for\nspecialized knowledge, platform dependence, and difficulty in scalability in\nbuilding functional workflows persist for cloud-native application developers.\nTo overcome these challenges and mitigate the burden of developing FaaS-based\napplications, in this paper, we propose a mechanism called Action Engine, that\nmakes use of tool-augmented large language models (LLMs) at its kernel to\ninterpret human language queries and automates FaaS workflow generation,\nthereby, reducing the need for specialized expertise and manual design. Action\nEngine includes modules to identify relevant functions from the FaaS repository\nand seamlessly manage the data dependency between them, ensuring the\ndeveloper's query is processed and resolved. Beyond that, Action Engine can\nexecute the generated workflow by injecting the user-provided arguments. On\nanother front, this work addresses a gap in tool-augmented LLM research via\nadopting an Automatic FaaS Workflow Generation perspective to systematically\nevaluate methodologies across four fundamental sub-processes. Through\nbenchmarking various parameters, this research provides critical insights into\nstreamlining workflow automation for real-world applications, specifically in\nthe FaaS continuum. Our evaluations demonstrate that the Action Engine achieves\ncomparable performance to the few-shot learning approach while maintaining\nplatform- and language-agnosticism, thereby, mitigating provider-specific\ndependencies in workflow generation. We notice that Action Engine can unlock\nFaaS workflow generation for non-cloud-savvy developers and expedite the\ndevelopment cycles of cloud-native applications."
                },
                "authors": [
                    {
                        "name": "Akiharu Esashi"
                    },
                    {
                        "name": "Pawissanutt Lertpongrujikorn"
                    },
                    {
                        "name": "Shinji Kato"
                    },
                    {
                        "name": "Mohsen Amini Salehi"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Amini Salehi"
                },
                "author": "Mohsen Amini Salehi",
                "arxiv_comment": "Published in the Future Generation Computer Systems (FGCS) journal;\n  Source code is available at: https://github.com/hpcclab/action_engine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08239v2",
                "updated": "2025-08-20T16:27:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    27,
                    42,
                    2,
                    232,
                    0
                ],
                "published": "2024-09-12T17:39:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    39,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources"
                },
                "summary": "Synthetic data generation has recently emerged as a promising approach for\nenhancing the capabilities of large language models (LLMs) without the need for\nexpensive human annotations. However, existing methods often generate data that\ncan be low quality or contrived. In this paper, we introduce Source2Synth, a\nscalable approach for synthetic data generation and curation that is grounded\nin real-world data sources. Source2Synth takes as input a custom data source\nand produces synthetic data examples with intermediate reasoning steps. Our\nmethod improves the dataset quality by discarding low-quality generations based\non their answerability. We demonstrate the generality of this approach by\napplying it to two tasks that leverage two different types of data: multi-hop\nquestion answering (MHQA), where we test complex reasoning abilities leveraging\ndocuments, and tabular question answering (TQA), where we test tool usage\nleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQL\nand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data generation has recently emerged as a promising approach for\nenhancing the capabilities of large language models (LLMs) without the need for\nexpensive human annotations. However, existing methods often generate data that\ncan be low quality or contrived. In this paper, we introduce Source2Synth, a\nscalable approach for synthetic data generation and curation that is grounded\nin real-world data sources. Source2Synth takes as input a custom data source\nand produces synthetic data examples with intermediate reasoning steps. Our\nmethod improves the dataset quality by discarding low-quality generations based\non their answerability. We demonstrate the generality of this approach by\napplying it to two tasks that leverage two different types of data: multi-hop\nquestion answering (MHQA), where we test complex reasoning abilities leveraging\ndocuments, and tabular question answering (TQA), where we test tool usage\nleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQL\nand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines."
                },
                "authors": [
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Carlos Gemmell"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Jane Dwivedi-Yu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Maria Lomeli"
                    }
                ],
                "author_detail": {
                    "name": "Maria Lomeli"
                },
                "author": "Maria Lomeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14828v1",
                "updated": "2025-08-20T16:22:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    22,
                    51,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T16:22:51Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    22,
                    51,
                    2,
                    232,
                    0
                ],
                "title": "Long Chain-of-Thought Reasoning Across Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Chain-of-Thought Reasoning Across Languages"
                },
                "summary": "Scaling inference through long chains-of-thought (CoTs) has unlocked\nimpressive reasoning capabilities in large language models (LLMs), yet the\nreasoning process remains almost exclusively English-centric. We construct\ntranslated versions of two popular English reasoning datasets, fine-tune Qwen\n2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT\ngeneration across French, Japanese, Latvian, and Swahili. Our experiments\nreveal three key findings. First, the efficacy of using English as a pivot\nlanguage varies by language: it provides no benefit for French, improves\nperformance when used as the reasoning language for Japanese and Latvian, and\nproves insufficient for Swahili where both task comprehension and reasoning\nremain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but\ndoes not eliminate the cross-lingual performance gap. A lightweight fine-tune\nusing only 1k traces still improves performance by over 30\\% in Swahili. Third,\ndata quality versus scale trade-offs are language dependent: small, carefully\ncurated datasets suffice for English and French, whereas larger but noisier\ncorpora prove more effective for Swahili and Latvian. Together, these results\nclarify when and why long CoTs transfer across languages and provide translated\ndatasets to foster equitable multilingual reasoning research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference through long chains-of-thought (CoTs) has unlocked\nimpressive reasoning capabilities in large language models (LLMs), yet the\nreasoning process remains almost exclusively English-centric. We construct\ntranslated versions of two popular English reasoning datasets, fine-tune Qwen\n2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT\ngeneration across French, Japanese, Latvian, and Swahili. Our experiments\nreveal three key findings. First, the efficacy of using English as a pivot\nlanguage varies by language: it provides no benefit for French, improves\nperformance when used as the reasoning language for Japanese and Latvian, and\nproves insufficient for Swahili where both task comprehension and reasoning\nremain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but\ndoes not eliminate the cross-lingual performance gap. A lightweight fine-tune\nusing only 1k traces still improves performance by over 30\\% in Swahili. Third,\ndata quality versus scale trade-offs are language dependent: small, carefully\ncurated datasets suffice for English and French, whereas larger but noisier\ncorpora prove more effective for Swahili and Latvian. Together, these results\nclarify when and why long CoTs transfer across languages and provide translated\ndatasets to foster equitable multilingual reasoning research."
                },
                "authors": [
                    {
                        "name": "Josh Barua"
                    },
                    {
                        "name": "Seun Eisape"
                    },
                    {
                        "name": "Kayo Yin"
                    },
                    {
                        "name": "Alane Suhr"
                    }
                ],
                "author_detail": {
                    "name": "Alane Suhr"
                },
                "author": "Alane Suhr",
                "arxiv_comment": "Accepted to SCALR @ COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14823v1",
                "updated": "2025-08-20T16:15:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    15,
                    59,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T16:15:59Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    15,
                    59,
                    2,
                    232,
                    0
                ],
                "title": "Using an LLM to Investigate Students' Explanations on Conceptual Physics\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using an LLM to Investigate Students' Explanations on Conceptual Physics\n  Questions"
                },
                "summary": "Analyzing students' written solutions to physics questions is a major area in\nPER. However, gauging student understanding in college courses is bottlenecked\nby large class sizes, which limits assessments to a multiple-choice (MC) format\nfor ease of grading. Although sufficient in quantifying scientifically correct\nconceptions, MC assessments do not uncover students' deeper ways of\nunderstanding physics. Large language models (LLMs) offer a promising approach\nfor assessing students' written responses at scale. Our study used an LLM,\nvalidated by human graders, to classify students' written explanations to three\nquestions on the Energy and Momentum Conceptual Survey as correct or incorrect,\nand organized students' incorrect explanations into emergent categories. We\nfound that the LLM (GPT-4o) can fairly assess students' explanations,\ncomparable to human graders (0-3% discrepancy). Furthermore, the categories of\nincorrect explanations were different from corresponding MC distractors,\nallowing for different and deeper conceptions to become accessible to\neducators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing students' written solutions to physics questions is a major area in\nPER. However, gauging student understanding in college courses is bottlenecked\nby large class sizes, which limits assessments to a multiple-choice (MC) format\nfor ease of grading. Although sufficient in quantifying scientifically correct\nconceptions, MC assessments do not uncover students' deeper ways of\nunderstanding physics. Large language models (LLMs) offer a promising approach\nfor assessing students' written responses at scale. Our study used an LLM,\nvalidated by human graders, to classify students' written explanations to three\nquestions on the Energy and Momentum Conceptual Survey as correct or incorrect,\nand organized students' incorrect explanations into emergent categories. We\nfound that the LLM (GPT-4o) can fairly assess students' explanations,\ncomparable to human graders (0-3% discrepancy). Furthermore, the categories of\nincorrect explanations were different from corresponding MC distractors,\nallowing for different and deeper conceptions to become accessible to\neducators."
                },
                "authors": [
                    {
                        "name": "Sean Savage"
                    },
                    {
                        "name": "N. Sanjay Rebello"
                    }
                ],
                "author_detail": {
                    "name": "N. Sanjay Rebello"
                },
                "author": "N. Sanjay Rebello",
                "arxiv_comment": "5 pages, 3 figures and Physics Education Research Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14817v1",
                "updated": "2025-08-20T16:09:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    9,
                    37,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T16:09:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    9,
                    37,
                    2,
                    232,
                    0
                ],
                "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for\n  Clinical Reasoning over EHRs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for\n  Clinical Reasoning over EHRs"
                },
                "summary": "Electronic health records (EHRs) are long, noisy, and often redundant, posing\na major challenge for the clinicians who must navigate them. Large language\nmodels (LLMs) offer a promising solution for extracting and reasoning over this\nunstructured text, but the length of clinical notes often exceeds even\nstate-of-the-art models' extended context windows. Retrieval-augmented\ngeneration (RAG) offers an alternative by retrieving task-relevant passages\nfrom across the entire EHR, potentially reducing the amount of required input\ntokens. In this work, we propose three clinical tasks designed to be replicable\nacross health systems with minimal effort: 1) extracting imaging procedures, 2)\ngenerating timelines of antibiotic use, and 3) identifying key diagnoses. Using\nEHRs from actual hospitalized patients, we test three state-of-the-art LLMs\nwith varying amounts of provided context, using either targeted text retrieval\nor the most recent clinical notes. We find that RAG closely matches or exceeds\nthe performance of using recent notes, and approaches the performance of using\nthe models' full context while requiring drastically fewer input tokens. Our\nresults suggest that RAG remains a competitive and efficient approach even as\nnewer models become capable of handling increasingly longer amounts of text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic health records (EHRs) are long, noisy, and often redundant, posing\na major challenge for the clinicians who must navigate them. Large language\nmodels (LLMs) offer a promising solution for extracting and reasoning over this\nunstructured text, but the length of clinical notes often exceeds even\nstate-of-the-art models' extended context windows. Retrieval-augmented\ngeneration (RAG) offers an alternative by retrieving task-relevant passages\nfrom across the entire EHR, potentially reducing the amount of required input\ntokens. In this work, we propose three clinical tasks designed to be replicable\nacross health systems with minimal effort: 1) extracting imaging procedures, 2)\ngenerating timelines of antibiotic use, and 3) identifying key diagnoses. Using\nEHRs from actual hospitalized patients, we test three state-of-the-art LLMs\nwith varying amounts of provided context, using either targeted text retrieval\nor the most recent clinical notes. We find that RAG closely matches or exceeds\nthe performance of using recent notes, and approaches the performance of using\nthe models' full context while requiring drastically fewer input tokens. Our\nresults suggest that RAG remains a competitive and efficient approach even as\nnewer models become capable of handling increasingly longer amounts of text."
                },
                "authors": [
                    {
                        "name": "Skatje Myers"
                    },
                    {
                        "name": "Dmitriy Dligach"
                    },
                    {
                        "name": "Timothy A. Miller"
                    },
                    {
                        "name": "Samantha Barr"
                    },
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Matthew Churpek"
                    },
                    {
                        "name": "Anoop Mayampurath"
                    },
                    {
                        "name": "Majid Afshar"
                    }
                ],
                "author_detail": {
                    "name": "Majid Afshar"
                },
                "author": "Majid Afshar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14816v1",
                "updated": "2025-08-20T16:09:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    9,
                    4,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T16:09:04Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    9,
                    4,
                    2,
                    232,
                    0
                ],
                "title": "Joint estimation of asymmetric community numbers in directed networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint estimation of asymmetric community numbers in directed networks"
                },
                "summary": "Community detection in directed networks is a central task in network\nanalysis. Unlike undirected networks, directed networks encode inherently\nasymmetric relationships, giving rise to sender and receiver roles that may\neach follow distinct community organizations with possibly different numbers of\ncommunities. Estimating these two community counts simultaneously is therefore\nconsiderably more challenging than in the undirected setting, yet it is\nessential for faithful model specification and reliable downstream inference.\nThis work addresses this challenge within the stochastic co-block model (ScBM),\na powerful statistical framework for capturing asymmetric relational structures\ninherent in directed networks. We propose a novel goodness-of-fit test based on\nthe deviation of the largest singular value of a normalized residual matrix\nfrom the constant value 2. We show that the upper bound of this test statistic\nconverges to zero under the null hypothesis, while this statistic goes to\ninfinity if the true model has finer communities than hypothesized. Leveraging\nthis tail bounds behavior, we develop an efficient sequential testing algorithm\nthat lexicographically explores candidate community number pairs. To enhance\nrobustness in practical settings, we further introduce a ratio-based variant\nthat detects the transition point in the test statistic sequence. We rigorously\nshow both algorithms' consistency in recovering the true sender and receiver\ncommunity counts under ScBM. Numerical experiments demonstrate the accuracy and\nrobustness of our methods in estimating community numbers across diverse ScBM\nsettings. %To our knowledge, this work presents the first theoretically\nguaranteed approach for jointly estimating the numbers of sender and receiver\ncommunities within the ScBM framework, providing a critical tool for reliable\ndirected network analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community detection in directed networks is a central task in network\nanalysis. Unlike undirected networks, directed networks encode inherently\nasymmetric relationships, giving rise to sender and receiver roles that may\neach follow distinct community organizations with possibly different numbers of\ncommunities. Estimating these two community counts simultaneously is therefore\nconsiderably more challenging than in the undirected setting, yet it is\nessential for faithful model specification and reliable downstream inference.\nThis work addresses this challenge within the stochastic co-block model (ScBM),\na powerful statistical framework for capturing asymmetric relational structures\ninherent in directed networks. We propose a novel goodness-of-fit test based on\nthe deviation of the largest singular value of a normalized residual matrix\nfrom the constant value 2. We show that the upper bound of this test statistic\nconverges to zero under the null hypothesis, while this statistic goes to\ninfinity if the true model has finer communities than hypothesized. Leveraging\nthis tail bounds behavior, we develop an efficient sequential testing algorithm\nthat lexicographically explores candidate community number pairs. To enhance\nrobustness in practical settings, we further introduce a ratio-based variant\nthat detects the transition point in the test statistic sequence. We rigorously\nshow both algorithms' consistency in recovering the true sender and receiver\ncommunity counts under ScBM. Numerical experiments demonstrate the accuracy and\nrobustness of our methods in estimating community numbers across diverse ScBM\nsettings. %To our knowledge, this work presents the first theoretically\nguaranteed approach for jointly estimating the numbers of sender and receiver\ncommunities within the ScBM framework, providing a critical tool for reliable\ndirected network analysis."
                },
                "authors": [
                    {
                        "name": "Huan Qing"
                    }
                ],
                "author_detail": {
                    "name": "Huan Qing"
                },
                "author": "Huan Qing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14812v1",
                "updated": "2025-08-20T16:03:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    3,
                    56,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T16:03:56Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    3,
                    56,
                    2,
                    232,
                    0
                ],
                "title": "Repeating Words for Video-Language Retrieval with Coarse-to-Fine\n  Objectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repeating Words for Video-Language Retrieval with Coarse-to-Fine\n  Objectives"
                },
                "summary": "The explosive growth of video streaming presents challenges in achieving high\naccuracy and low training costs for video-language retrieval. However, existing\nmethods rely on large-scale pre-training to improve video retrieval\nperformance, resulting in significant computational demands. Additionally, the\nfine-grained information in videos and texts remains underexplored. To\nalleviate these problems, we propose a novel framework to learn fine-grained\nfeatures for better alignment and introduce an inference pipeline to improve\nperformance without additional training. Specifically, we employ coarse-to-fine\nobjectives to understand the semantic information of video-text pairs,\nincluding contrastive and matching learning. The fine-grained data used for\ntraining is obtained through the Granularity-Aware Representation module, which\nis designed based on similarity analysis between video frames and words in\ncaptions. Furthermore, we observe that the repetition of keywords in the\noriginal captions, referred to as \"Repetition\", can enhance retrieval\nperformance and improve alignment between video and text. Based on this\ninsight, we propose a novel and effective inference pipeline that incorporates\na voting mechanism and a new Matching Entropy metric to achieve better\nretrieval performance without requiring additional pre-training. Experimental\nresults on four benchmarks demonstrate that the proposed method outperforms\nprevious approaches. Additionally, our inference pipeline achieves significant\nperformance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT\ndataset and a 1.6% increase on the DiDeMo dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive growth of video streaming presents challenges in achieving high\naccuracy and low training costs for video-language retrieval. However, existing\nmethods rely on large-scale pre-training to improve video retrieval\nperformance, resulting in significant computational demands. Additionally, the\nfine-grained information in videos and texts remains underexplored. To\nalleviate these problems, we propose a novel framework to learn fine-grained\nfeatures for better alignment and introduce an inference pipeline to improve\nperformance without additional training. Specifically, we employ coarse-to-fine\nobjectives to understand the semantic information of video-text pairs,\nincluding contrastive and matching learning. The fine-grained data used for\ntraining is obtained through the Granularity-Aware Representation module, which\nis designed based on similarity analysis between video frames and words in\ncaptions. Furthermore, we observe that the repetition of keywords in the\noriginal captions, referred to as \"Repetition\", can enhance retrieval\nperformance and improve alignment between video and text. Based on this\ninsight, we propose a novel and effective inference pipeline that incorporates\na voting mechanism and a new Matching Entropy metric to achieve better\nretrieval performance without requiring additional pre-training. Experimental\nresults on four benchmarks demonstrate that the proposed method outperforms\nprevious approaches. Additionally, our inference pipeline achieves significant\nperformance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT\ndataset and a 1.6% increase on the DiDeMo dataset."
                },
                "authors": [
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Jiaxi Gu"
                    },
                    {
                        "name": "Shicong Wang"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00050v2",
                "updated": "2025-08-20T16:01:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    1,
                    52,
                    2,
                    232,
                    0
                ],
                "published": "2025-03-31T02:18:51Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    18,
                    51,
                    0,
                    90,
                    0
                ],
                "title": "JudgeLRM: Large Reasoning Models as a Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JudgeLRM: Large Reasoning Models as a Judge"
                },
                "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Qingyun Zou"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14802v1",
                "updated": "2025-08-20T15:52:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    52,
                    34,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:52:34Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    52,
                    34,
                    2,
                    232,
                    0
                ],
                "title": "Privileged Self-Access Matters for Introspection in AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privileged Self-Access Matters for Introspection in AI"
                },
                "summary": "Whether AI models can introspect is an increasingly important practical\nquestion. But there is no consensus on how introspection is to be defined.\nBeginning from a recently proposed ''lightweight'' definition, we argue instead\nfor a thicker one. According to our proposal, introspection in AI is any\nprocess which yields information about internal states through a process more\nreliable than one with equal or lower computational cost available to a third\nparty. Using experiments where LLMs reason about their internal temperature\nparameters, we show they can appear to have lightweight introspection while\nfailing to meaningfully introspect per our proposed definition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether AI models can introspect is an increasingly important practical\nquestion. But there is no consensus on how introspection is to be defined.\nBeginning from a recently proposed ''lightweight'' definition, we argue instead\nfor a thicker one. According to our proposal, introspection in AI is any\nprocess which yields information about internal states through a process more\nreliable than one with equal or lower computational cost available to a third\nparty. Using experiments where LLMs reason about their internal temperature\nparameters, we show they can appear to have lightweight introspection while\nfailing to meaningfully introspect per our proposed definition."
                },
                "authors": [
                    {
                        "name": "Siyuan Song"
                    },
                    {
                        "name": "Harvey Lederman"
                    },
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Kyle Mahowald"
                    }
                ],
                "author_detail": {
                    "name": "Kyle Mahowald"
                },
                "author": "Kyle Mahowald",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21755v2",
                "updated": "2025-08-20T15:49:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    49,
                    30,
                    2,
                    232,
                    0
                ],
                "published": "2025-03-27T17:57:01Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    1,
                    3,
                    86,
                    0
                ],
                "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness"
                },
                "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nto individual dimensions, our evaluation framework integrates generalists such\nas SOTA VLMs and LLMs, and specialists, including anomaly detection methods\nproposed for video generation. We conduct extensive human annotations to ensure\nevaluation alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nto individual dimensions, our evaluation framework integrates generalists such\nas SOTA VLMs and LLMs, and specialists, including anomaly detection methods\nproposed for video generation. We conduct extensive human annotations to ensure\nevaluation alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness."
                },
                "authors": [
                    {
                        "name": "Dian Zheng"
                    },
                    {
                        "name": "Ziqi Huang"
                    },
                    {
                        "name": "Hongbo Liu"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Lulu Gu"
                    },
                    {
                        "name": "Yuanhan Zhang"
                    },
                    {
                        "name": "Jingwen He"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Equal contributions from first two authors. Project page:\n  https://vchitect.github.io/VBench-2.0-project/ Code:\n  https://github.com/Vchitect/VBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11370v2",
                "updated": "2025-08-20T15:45:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    45,
                    11,
                    2,
                    232,
                    0
                ],
                "published": "2023-12-18T17:36:20Z",
                "published_parsed": [
                    2023,
                    12,
                    18,
                    17,
                    36,
                    20,
                    0,
                    352,
                    0
                ],
                "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model"
                },
                "summary": "Large language models (LLMs) have shown remarkable proficiency in human-level\nreasoning and generation capabilities, which encourages extensive research on\ntheir application in mathematical problem solving. However, current work has\nbeen largely focused on text-based mathematical problems, with limited\ninvestigation in problems involving geometric information. Addressing this gap,\nwe aim to enable LLMs to solve geometric problems by understanding image input.\nWe first analyze the limitations of current Multimodal Large Language Models\n(MLLMs) in this area: they struggle to accurately comprehending basic geometric\nelements and their relationships. To overcome these challenges, we take\nadvantage of the unique characteristics of geometric problems (such as unique\ngeometric logical form, and geometric scalability) and the capacity of the\ntextual LLMs to build an enriched multimodal geometry dataset based on existing\ndata. The augmented dataset, Geo170K, contains more than 170K geometric\nimage-caption and question-answer pairs. Utilizing our constructed Geo170K\ndataset, we develop G-LLaVA, which demonstrates exceptional performance in\nsolving geometric problems, significantly outperforming GPT-4-V on the\nMathVista benchmark with only 7B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable proficiency in human-level\nreasoning and generation capabilities, which encourages extensive research on\ntheir application in mathematical problem solving. However, current work has\nbeen largely focused on text-based mathematical problems, with limited\ninvestigation in problems involving geometric information. Addressing this gap,\nwe aim to enable LLMs to solve geometric problems by understanding image input.\nWe first analyze the limitations of current Multimodal Large Language Models\n(MLLMs) in this area: they struggle to accurately comprehending basic geometric\nelements and their relationships. To overcome these challenges, we take\nadvantage of the unique characteristics of geometric problems (such as unique\ngeometric logical form, and geometric scalability) and the capacity of the\ntextual LLMs to build an enriched multimodal geometry dataset based on existing\ndata. The augmented dataset, Geo170K, contains more than 170K geometric\nimage-caption and question-answer pairs. Utilizing our constructed Geo170K\ndataset, we develop G-LLaVA, which demonstrates exceptional performance in\nsolving geometric problems, significantly outperforming GPT-4-V on the\nMathVista benchmark with only 7B parameters."
                },
                "authors": [
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Jiacheng Ye"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01427v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01427v4",
                "updated": "2025-08-20T15:44:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    44,
                    10,
                    2,
                    232,
                    0
                ],
                "published": "2024-10-02T11:24:22Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    11,
                    24,
                    22,
                    2,
                    276,
                    0
                ],
                "title": "Regularized e-processes: anytime valid inference with knowledge-based\n  efficiency gains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularized e-processes: anytime valid inference with knowledge-based\n  efficiency gains"
                },
                "summary": "Classical statistical methods have theoretical justification when the sample\nsize is predetermined. In applications, however, it's often the case that\nsample sizes are data-dependent rather than predetermined. The aforementioned\nmethods aren't reliable in this latter case, hence the recent interest in\ne-processes and methods that are anytime valid, i.e., reliable for any dynamic\ndata-collection plan. But if the investigator has relevant-yet-incomplete prior\ninformation about the quantity of interest, then there's an opportunity for\nefficiency gain. This paper proposes a regularized e-process framework\nfeaturing a knowledge-based, imprecise-probabilistic regularization with\nimproved efficiency. A generalized version of Ville's inequality is\nestablished, ensuring that inference based on the regularized e-process are\nanytime valid in a novel, knowledge-dependent sense. Regularized e-processes\nalso facilitate possibility-theoretic uncertainty quantification with strong\nfrequentist-like calibration properties and other Bayesian-like properties:\nsatisfies the likelihood principle, avoids sure-loss, and offers formal\ndecision-making with reliability guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical statistical methods have theoretical justification when the sample\nsize is predetermined. In applications, however, it's often the case that\nsample sizes are data-dependent rather than predetermined. The aforementioned\nmethods aren't reliable in this latter case, hence the recent interest in\ne-processes and methods that are anytime valid, i.e., reliable for any dynamic\ndata-collection plan. But if the investigator has relevant-yet-incomplete prior\ninformation about the quantity of interest, then there's an opportunity for\nefficiency gain. This paper proposes a regularized e-process framework\nfeaturing a knowledge-based, imprecise-probabilistic regularization with\nimproved efficiency. A generalized version of Ville's inequality is\nestablished, ensuring that inference based on the regularized e-process are\nanytime valid in a novel, knowledge-dependent sense. Regularized e-processes\nalso facilitate possibility-theoretic uncertainty quantification with strong\nfrequentist-like calibration properties and other Bayesian-like properties:\nsatisfies the likelihood principle, avoids sure-loss, and offers formal\ndecision-making with reliability guarantees."
                },
                "authors": [
                    {
                        "name": "Ryan Martin"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Martin"
                },
                "author": "Ryan Martin",
                "arxiv_comment": "Comments welcome (via email or) at\n  https://researchers.one/articles/24.09.00003",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01427v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01427v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19292v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19292v2",
                "updated": "2025-08-20T15:41:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    41,
                    42,
                    2,
                    232,
                    0
                ],
                "published": "2025-05-25T19:54:37Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    19,
                    54,
                    37,
                    6,
                    145,
                    0
                ],
                "title": "A likelihood-based Bayesian inference framework for the calibration of\n  and selection between stochastic velocity-jump models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A likelihood-based Bayesian inference framework for the calibration of\n  and selection between stochastic velocity-jump models"
                },
                "summary": "Advances in experimental techniques allow the collection of high-resolution\nspatio-temporal data that track individual motile entities. These tracking data\ncan be used to calibrate mathematical models describing the motility of\nindividual entities. The challenges in calibrating models for single-agent\nmotion derive from the intrinsic characteristics of experimental data,\ncollected at discrete time steps and with measurement noise. We consider motion\nof individual agents that can be described by velocity-jump models in one\nspatial dimension. These agents transition between a network of \\textit{n}\nstates, in which each state is associated with a fixed velocity and fixed rates\nof switching to every other state. Exploiting approximate solutions to the\nresultant stochastic process, we develop a Bayesian inference framework to\ncalibrate these models to discrete-time noisy data. We first demonstrate that\nthe framework can be used to effectively recover the model parameters of data\nsimulated from two-state and three-state models. Finally, we explore the\nquestion of model selection first using simulated data and then using\nexperimental data tracking mRNA transport inside \\textit{Drosophila} neurons.\nOverall, our results demonstrate that the framework is effective and efficient\nin calibrating and selecting between velocity-jump models and it can be applied\nto a range of motion processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in experimental techniques allow the collection of high-resolution\nspatio-temporal data that track individual motile entities. These tracking data\ncan be used to calibrate mathematical models describing the motility of\nindividual entities. The challenges in calibrating models for single-agent\nmotion derive from the intrinsic characteristics of experimental data,\ncollected at discrete time steps and with measurement noise. We consider motion\nof individual agents that can be described by velocity-jump models in one\nspatial dimension. These agents transition between a network of \\textit{n}\nstates, in which each state is associated with a fixed velocity and fixed rates\nof switching to every other state. Exploiting approximate solutions to the\nresultant stochastic process, we develop a Bayesian inference framework to\ncalibrate these models to discrete-time noisy data. We first demonstrate that\nthe framework can be used to effectively recover the model parameters of data\nsimulated from two-state and three-state models. Finally, we explore the\nquestion of model selection first using simulated data and then using\nexperimental data tracking mRNA transport inside \\textit{Drosophila} neurons.\nOverall, our results demonstrate that the framework is effective and efficient\nin calibrating and selecting between velocity-jump models and it can be applied\nto a range of motion processes."
                },
                "authors": [
                    {
                        "name": "Arianna Ceccarelli"
                    },
                    {
                        "name": "Alexander P. Browning"
                    },
                    {
                        "name": "Tai Chaiamarit"
                    },
                    {
                        "name": "Ilan Davis"
                    },
                    {
                        "name": "Ruth E. Baker"
                    }
                ],
                "author_detail": {
                    "name": "Ruth E. Baker"
                },
                "author": "Ruth E. Baker",
                "arxiv_comment": "30 pages, 8 figures SI: 21 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19292v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19292v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02934v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02934v4",
                "updated": "2025-08-20T15:38:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    38,
                    15,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-04T22:20:34Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    22,
                    20,
                    34,
                    0,
                    216,
                    0
                ],
                "title": "Hubble Space Telescope Observations of the Interstellar Interloper\n  3I/ATLAS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hubble Space Telescope Observations of the Interstellar Interloper\n  3I/ATLAS"
                },
                "summary": "We present high angular resolution observations of the third known\ninterstellar interloper, 3I/ATLAS, from the Hubble Space Telescope. The object\nis clearly active at 3.8 au pre-perihelion, showing dust emitted from the hot\nSun-facing side of the nucleus and a weak, radiation pressure swept tail away\nfrom the Sun. We apply a simple model to estimate the mass loss rate in dust as\ndM/dt = 12 sqrt(a) kg/s, where a is the mean particle size in microns. With 1 <\na < 100, we infer dM/dt = 12 to 120 kg/s. A fit to the surface brightness\ndistribution of the inner coma limits the effective radius of the nucleus to be\nr < 2.8 km, assuming red geometric albedo 0.04. Conversely, the nucleus cannot\nbe smaller than 0.22 km in radius if its coma is supplied by sublimation of\ncarbon monoxide, and must be larger if a less volatile molecule drives the mass\nloss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present high angular resolution observations of the third known\ninterstellar interloper, 3I/ATLAS, from the Hubble Space Telescope. The object\nis clearly active at 3.8 au pre-perihelion, showing dust emitted from the hot\nSun-facing side of the nucleus and a weak, radiation pressure swept tail away\nfrom the Sun. We apply a simple model to estimate the mass loss rate in dust as\ndM/dt = 12 sqrt(a) kg/s, where a is the mean particle size in microns. With 1 <\na < 100, we infer dM/dt = 12 to 120 kg/s. A fit to the surface brightness\ndistribution of the inner coma limits the effective radius of the nucleus to be\nr < 2.8 km, assuming red geometric albedo 0.04. Conversely, the nucleus cannot\nbe smaller than 0.22 km in radius if its coma is supplied by sublimation of\ncarbon monoxide, and must be larger if a less volatile molecule drives the mass\nloss."
                },
                "authors": [
                    {
                        "name": "David Jewitt"
                    },
                    {
                        "name": "Man-To Hui"
                    },
                    {
                        "name": "Max Mutchler"
                    },
                    {
                        "name": "Yoonyoung Kim"
                    },
                    {
                        "name": "Jessica Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Jessica Agarwal"
                },
                "author": "Jessica Agarwal",
                "arxiv_comment": "13 pages, 4 figures, 2 tables: Corrected binning error in the x-axis\n  of Figure 3 and updated computed dust speed and mass loss rate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02934v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02934v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14782v1",
                "updated": "2025-08-20T15:27:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    27,
                    49,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:27:49Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    27,
                    49,
                    2,
                    232,
                    0
                ],
                "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban\n  Transportation via Learnable Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransLLM: A Unified Multi-Task Foundation Framework for Urban\n  Transportation via Learnable Prompting"
                },
                "summary": "Urban transportation systems encounter diverse challenges across multiple\ntasks, such as traffic forecasting, electric vehicle (EV) charging demand\nprediction, and taxi dispatch. Existing approaches suffer from two key\nlimitations: small-scale deep learning models are task-specific and\ndata-hungry, limiting their generalizability across diverse scenarios, while\nlarge language models (LLMs), despite offering flexibility through natural\nlanguage interfaces, struggle with structured spatiotemporal data and numerical\nreasoning in transportation domains. To address these limitations, we propose\nTransLLM, a unified foundation framework that integrates spatiotemporal\nmodeling with large language models through learnable prompt composition. Our\napproach features a lightweight spatiotemporal encoder that captures complex\ndependencies via dilated temporal convolutions and dual-adjacency graph\nattention networks, seamlessly interfacing with LLMs through structured\nembeddings. A novel instance-level prompt routing mechanism, trained via\nreinforcement learning, dynamically personalizes prompts based on input\ncharacteristics, moving beyond fixed task-specific templates. The framework\noperates by encoding spatiotemporal patterns into contextual representations,\ndynamically composing personalized prompts to guide LLM reasoning, and\nprojecting the resulting representations through specialized output layers to\ngenerate task-specific predictions. Experiments across seven datasets and three\ntasks demonstrate the exceptional effectiveness of TransLLM in both supervised\nand zero-shot settings. Compared to ten baseline models, it delivers\ncompetitive performance on both regression and planning problems, showing\nstrong generalization and cross-task adaptability. Our code is available at\nhttps://github.com/BiYunying/TransLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban transportation systems encounter diverse challenges across multiple\ntasks, such as traffic forecasting, electric vehicle (EV) charging demand\nprediction, and taxi dispatch. Existing approaches suffer from two key\nlimitations: small-scale deep learning models are task-specific and\ndata-hungry, limiting their generalizability across diverse scenarios, while\nlarge language models (LLMs), despite offering flexibility through natural\nlanguage interfaces, struggle with structured spatiotemporal data and numerical\nreasoning in transportation domains. To address these limitations, we propose\nTransLLM, a unified foundation framework that integrates spatiotemporal\nmodeling with large language models through learnable prompt composition. Our\napproach features a lightweight spatiotemporal encoder that captures complex\ndependencies via dilated temporal convolutions and dual-adjacency graph\nattention networks, seamlessly interfacing with LLMs through structured\nembeddings. A novel instance-level prompt routing mechanism, trained via\nreinforcement learning, dynamically personalizes prompts based on input\ncharacteristics, moving beyond fixed task-specific templates. The framework\noperates by encoding spatiotemporal patterns into contextual representations,\ndynamically composing personalized prompts to guide LLM reasoning, and\nprojecting the resulting representations through specialized output layers to\ngenerate task-specific predictions. Experiments across seven datasets and three\ntasks demonstrate the exceptional effectiveness of TransLLM in both supervised\nand zero-shot settings. Compared to ten baseline models, it delivers\ncompetitive performance on both regression and planning problems, showing\nstrong generalization and cross-task adaptability. Our code is available at\nhttps://github.com/BiYunying/TransLLM."
                },
                "authors": [
                    {
                        "name": "Jiaming Leng"
                    },
                    {
                        "name": "Yunying Bi"
                    },
                    {
                        "name": "Chuan Qin"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Chao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Wang"
                },
                "author": "Chao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14765v1",
                "updated": "2025-08-20T15:13:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    13,
                    52,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:13:52Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    13,
                    52,
                    2,
                    232,
                    0
                ],
                "title": "PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT\n  SFT and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT\n  SFT and Reinforcement Learning"
                },
                "summary": "Designing therapeutic peptides with tailored properties is hindered by the\nvastness of sequence space, limited experimental data, and poor\ninterpretability of current generative models. To address these challenges, we\nintroduce PepThink-R1, a generative framework that integrates large language\nmodels (LLMs) with chain-of-thought (CoT) supervised fine-tuning and\nreinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly\nreasons about monomer-level modifications during sequence generation, enabling\ninterpretable design choices while optimizing for multiple pharmacological\nproperties. Guided by a tailored reward function balancing chemical validity\nand property improvements, the model autonomously explores diverse sequence\nvariants. We demonstrate that PepThink-R1 generates cyclic peptides with\nsignificantly enhanced lipophilicity, stability, and exposure, outperforming\nexisting general LLMs (e.g., GPT-5) and domain-specific baseline in both\noptimization success and interpretability. To our knowledge, this is the first\nLLM-based peptide design framework that combines explicit reasoning with\nRL-driven property control, marking a step toward reliable and transparent\npeptide optimization for therapeutic discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing therapeutic peptides with tailored properties is hindered by the\nvastness of sequence space, limited experimental data, and poor\ninterpretability of current generative models. To address these challenges, we\nintroduce PepThink-R1, a generative framework that integrates large language\nmodels (LLMs) with chain-of-thought (CoT) supervised fine-tuning and\nreinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly\nreasons about monomer-level modifications during sequence generation, enabling\ninterpretable design choices while optimizing for multiple pharmacological\nproperties. Guided by a tailored reward function balancing chemical validity\nand property improvements, the model autonomously explores diverse sequence\nvariants. We demonstrate that PepThink-R1 generates cyclic peptides with\nsignificantly enhanced lipophilicity, stability, and exposure, outperforming\nexisting general LLMs (e.g., GPT-5) and domain-specific baseline in both\noptimization success and interpretability. To our knowledge, this is the first\nLLM-based peptide design framework that combines explicit reasoning with\nRL-driven property control, marking a step toward reliable and transparent\npeptide optimization for therapeutic discovery."
                },
                "authors": [
                    {
                        "name": "Ruheng Wang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Trieu Nguyen"
                    },
                    {
                        "name": "Shasha Feng"
                    },
                    {
                        "name": "Hao-Wei Pang"
                    },
                    {
                        "name": "Xiang Yu"
                    },
                    {
                        "name": "Li Xiao"
                    },
                    {
                        "name": "Peter Zhiping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Peter Zhiping Zhang"
                },
                "author": "Peter Zhiping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14764v1",
                "updated": "2025-08-20T15:12:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    12,
                    52,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:12:52Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    12,
                    52,
                    2,
                    232,
                    0
                ],
                "title": "Investigation of the Inter-Rater Reliability between Large Language\n  Models and Human Raters in Qualitative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigation of the Inter-Rater Reliability between Large Language\n  Models and Human Raters in Qualitative Analysis"
                },
                "summary": "Qualitative analysis is typically limited to small datasets because it is\ntime-intensive. Moreover, a second human rater is required to ensure reliable\nfindings. Artificial intelligence tools may replace human raters if we\ndemonstrate high reliability compared to human ratings. We investigated the\ninter-rater reliability of state-of-the-art Large Language Models (LLMs),\nChatGPT-4o and ChatGPT-4.5-preview, in rating audio transcripts coded manually.\nWe explored prompts and hyperparameters to optimize model performance. The\nparticipants were 14 undergraduate student groups from a university in the\nmidwestern United States who discussed problem-solving strategies for a\nproject. We prompted an LLM to replicate manual coding, and calculated Cohen's\nKappa for inter-rater reliability. After optimizing model hyperparameters and\nprompts, the results showed substantial agreement (${\\kappa}>0.6$) for three\nthemes and moderate agreement on one. Our findings demonstrate the potential of\nGPT-4o and GPT-4.5 for efficient, scalable qualitative analysis in physics\neducation and identify their limitations in rating domain-general constructs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative analysis is typically limited to small datasets because it is\ntime-intensive. Moreover, a second human rater is required to ensure reliable\nfindings. Artificial intelligence tools may replace human raters if we\ndemonstrate high reliability compared to human ratings. We investigated the\ninter-rater reliability of state-of-the-art Large Language Models (LLMs),\nChatGPT-4o and ChatGPT-4.5-preview, in rating audio transcripts coded manually.\nWe explored prompts and hyperparameters to optimize model performance. The\nparticipants were 14 undergraduate student groups from a university in the\nmidwestern United States who discussed problem-solving strategies for a\nproject. We prompted an LLM to replicate manual coding, and calculated Cohen's\nKappa for inter-rater reliability. After optimizing model hyperparameters and\nprompts, the results showed substantial agreement (${\\kappa}>0.6$) for three\nthemes and moderate agreement on one. Our findings demonstrate the potential of\nGPT-4o and GPT-4.5 for efficient, scalable qualitative analysis in physics\neducation and identify their limitations in rating domain-general constructs."
                },
                "authors": [
                    {
                        "name": "Nikhil Sanjay Borse"
                    },
                    {
                        "name": "Ravishankar Chatta Subramaniam"
                    },
                    {
                        "name": "N. Sanjay Rebello"
                    }
                ],
                "author_detail": {
                    "name": "N. Sanjay Rebello"
                },
                "author": "N. Sanjay Rebello",
                "arxiv_comment": "7 pages, 4 figures, Physics Education Research Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02866v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02866v3",
                "updated": "2025-08-20T15:00:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    0,
                    50,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-04T19:54:40Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    19,
                    54,
                    40,
                    0,
                    216,
                    0
                ],
                "title": "PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in\n  Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in\n  Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) and other foundation models are increasingly\nused as the core of AI agents. In agentic workflows, these agents plan tasks,\ninteract with humans and peers, and influence scientific outcomes across\nfederated and heterogeneous environments. However, agents can hallucinate or\nreason incorrectly, propagating errors when one agent's output becomes\nanother's input. Thus, assuring that agents' actions are transparent,\ntraceable, reproducible, and reliable is critical to assess hallucination risks\nand mitigate their workflow impacts. While provenance techniques have long\nsupported these principles, existing methods fail to capture and relate\nagent-centric metadata such as prompts, responses, and decisions with the\nbroader workflow context and downstream outcomes. In this paper, we introduce\nPROV-AGENT, a provenance model that extends W3C PROV and leverages the Model\nContext Protocol (MCP) and data observability to integrate agent interactions\ninto end-to-end workflow provenance. Our contributions include: (1) a\nprovenance model tailored for agentic workflows, (2) a near real-time,\nopen-source system for capturing agentic provenance, and (3) a cross-facility\nevaluation spanning edge, cloud, and HPC environments, demonstrating support\nfor critical provenance queries and agent reliability analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and other foundation models are increasingly\nused as the core of AI agents. In agentic workflows, these agents plan tasks,\ninteract with humans and peers, and influence scientific outcomes across\nfederated and heterogeneous environments. However, agents can hallucinate or\nreason incorrectly, propagating errors when one agent's output becomes\nanother's input. Thus, assuring that agents' actions are transparent,\ntraceable, reproducible, and reliable is critical to assess hallucination risks\nand mitigate their workflow impacts. While provenance techniques have long\nsupported these principles, existing methods fail to capture and relate\nagent-centric metadata such as prompts, responses, and decisions with the\nbroader workflow context and downstream outcomes. In this paper, we introduce\nPROV-AGENT, a provenance model that extends W3C PROV and leverages the Model\nContext Protocol (MCP) and data observability to integrate agent interactions\ninto end-to-end workflow provenance. Our contributions include: (1) a\nprovenance model tailored for agentic workflows, (2) a near real-time,\nopen-source system for capturing agentic provenance, and (3) a cross-facility\nevaluation spanning edge, cloud, and HPC environments, demonstrating support\nfor critical provenance queries and agent reliability analysis."
                },
                "authors": [
                    {
                        "name": "Renan Souza"
                    },
                    {
                        "name": "Amal Gueroudji"
                    },
                    {
                        "name": "Stephen DeWitt"
                    },
                    {
                        "name": "Daniel Rosendo"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    },
                    {
                        "name": "Robert Ross"
                    },
                    {
                        "name": "Prasanna Balaprakash"
                    },
                    {
                        "name": "Rafael Ferreira da Silva"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Ferreira da Silva"
                },
                "author": "Rafael Ferreira da Silva",
                "arxiv_comment": "Paper accepted for publication in the Proceedings of the 2025 IEEE\n  21st International Conference on e-Science. Cite it as: R. Souza, A.\n  Gueroudji, S. DeWitt, D. Rosendo, T. Ghosal, R. Ross, P. Balaprakash, R. F.\n  da Silva, \"PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions\n  in Agentic Workflows,\" IEEE International Conference on e-Science, Chicago,\n  IL, USA, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02866v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02866v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 68T30, 68P20, 68Q85, 68M14,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.12; H.2.4; I.2.11; C.2.4; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14755v1",
                "updated": "2025-08-20T14:58:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    58,
                    5,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:58:05Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    58,
                    5,
                    2,
                    232,
                    0
                ],
                "title": "Reliable generation of isomorphic physics problems using ChatGPT with\n  prompt-chaining and tool use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable generation of isomorphic physics problems using ChatGPT with\n  prompt-chaining and tool use"
                },
                "summary": "We present a method for generating large numbers of isomorphic physics\nproblems using ChatGPT through prompt chaining and tool use. This approach\nenables precise control over structural variations-such as numeric values and\nspatial relations-while supporting diverse contextual variations in the problem\nbody. By utilizing the Python code interpreter, the method supports automatic\nsolution validation and simple diagram generation, addressing key limitations\nin existing LLM-based methods. We generated two example isomorphic problem\nbanks and compared the outcome against simpler prompt-based approaches. Results\nshow that prompt-chaining produces significantly higher quality and more\nconsistent outputs than simpler, non-chaining prompts. This work demonstrates a\npromising method for efficient problem creation accessible to the average\ninstructor, which opens new possibilities for personalized adaptive testing and\nautomated content development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a method for generating large numbers of isomorphic physics\nproblems using ChatGPT through prompt chaining and tool use. This approach\nenables precise control over structural variations-such as numeric values and\nspatial relations-while supporting diverse contextual variations in the problem\nbody. By utilizing the Python code interpreter, the method supports automatic\nsolution validation and simple diagram generation, addressing key limitations\nin existing LLM-based methods. We generated two example isomorphic problem\nbanks and compared the outcome against simpler prompt-based approaches. Results\nshow that prompt-chaining produces significantly higher quality and more\nconsistent outputs than simpler, non-chaining prompts. This work demonstrates a\npromising method for efficient problem creation accessible to the average\ninstructor, which opens new possibilities for personalized adaptive testing and\nautomated content development."
                },
                "authors": [
                    {
                        "name": "Zhongzhou Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhongzhou Chen"
                },
                "author": "Zhongzhou Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03881v2",
                "updated": "2025-08-20T14:55:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    55,
                    16,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-05T19:46:13Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    19,
                    46,
                    13,
                    1,
                    217,
                    0
                ],
                "title": "From App Features to Explanation Needs: Analyzing Correlations and\n  Predictive Potential",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From App Features to Explanation Needs: Analyzing Correlations and\n  Predictive Potential"
                },
                "summary": "In today's digitized world, software systems must support users in\nunderstanding both how to interact with a system and why certain behaviors\noccur. This study investigates whether explanation needs, classified from user\nreviews, can be predicted based on app properties, enabling early consideration\nduring development and large-scale requirements mining. We analyzed a gold\nstandard dataset of 4,495 app reviews enriched with metadata (e.g., app\nversion, ratings, age restriction, in-app purchases). Correlation analyses\nidentified mostly weak associations between app properties and explanation\nneeds, with moderate correlations only for specific features such as app\nversion, number of reviews, and star ratings. Linear regression models showed\nlimited predictive power, with no reliable forecasts across configurations.\nValidation on a manually labeled dataset of 495 reviews confirmed these\nfindings. Categories such as Security & Privacy and System Behavior showed\nslightly higher predictive potential, while Interaction and User Interface\nremained most difficult to predict. Overall, our results highlight that\nexplanation needs are highly context-dependent and cannot be precisely inferred\nfrom app metadata alone. Developers and requirements engineers should therefore\nsupplement metadata analysis with direct user feedback to effectively design\nexplainable and user-centered software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's digitized world, software systems must support users in\nunderstanding both how to interact with a system and why certain behaviors\noccur. This study investigates whether explanation needs, classified from user\nreviews, can be predicted based on app properties, enabling early consideration\nduring development and large-scale requirements mining. We analyzed a gold\nstandard dataset of 4,495 app reviews enriched with metadata (e.g., app\nversion, ratings, age restriction, in-app purchases). Correlation analyses\nidentified mostly weak associations between app properties and explanation\nneeds, with moderate correlations only for specific features such as app\nversion, number of reviews, and star ratings. Linear regression models showed\nlimited predictive power, with no reliable forecasts across configurations.\nValidation on a manually labeled dataset of 495 reviews confirmed these\nfindings. Categories such as Security & Privacy and System Behavior showed\nslightly higher predictive potential, while Interaction and User Interface\nremained most difficult to predict. Overall, our results highlight that\nexplanation needs are highly context-dependent and cannot be precisely inferred\nfrom app metadata alone. Developers and requirements engineers should therefore\nsupplement metadata analysis with direct user feedback to effectively design\nexplainable and user-centered software systems."
                },
                "authors": [
                    {
                        "name": "Martin Obaidi"
                    },
                    {
                        "name": "Kushtrim Qengaj"
                    },
                    {
                        "name": "Jakob Droste"
                    },
                    {
                        "name": "Hannah Deters"
                    },
                    {
                        "name": "Marc Herrmann"
                    },
                    {
                        "name": "Jil Klünder"
                    },
                    {
                        "name": "Elisa Schmid"
                    },
                    {
                        "name": "Kurt Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Schneider"
                },
                "author": "Kurt Schneider",
                "arxiv_comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06039v2",
                "updated": "2025-08-20T14:53:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    53,
                    26,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-08T13:39:39Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    39,
                    39,
                    1,
                    98,
                    0
                ],
                "title": "Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning\n  Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning\n  Strategies"
                },
                "summary": "Capsule endoscopy is a method to capture images of the gastrointestinal tract\nand screen for diseases which might remain hidden if investigated with standard\nendoscopes. Due to the limited size of a video capsule, embedding AI models\ndirectly into the capsule demands careful consideration of the model size and\nthus complicates anomaly detection in this field. Furthermore, the scarcity of\navailable data in this domain poses an ongoing challenge to achieving effective\nanomaly detection. Thus, this work introduces an ensemble strategy to address\nthis challenge in anomaly detection tasks in video capsule endoscopies,\nrequiring only a small number of individual neural networks during both the\ntraining and inference phases. Ensemble learning combines the predictions of\nmultiple independently trained neural networks. This has shown to be highly\neffective in enhancing both the accuracy and robustness of machine learning\nmodels. However, this comes at the cost of higher memory usage and increased\ncomputational effort, which quickly becomes prohibitive in many real-world\napplications. Instead of applying the same training algorithm to each\nindividual network, we propose using various loss functions, drawn from the\nanomaly detection field, to train each network. The methods are validated on\nthe two largest publicly available datasets for video capsule endoscopy images,\nthe Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on\nthe Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our\napproach outperforms current baselines with significantly fewer parameters\nacross all models, which is a crucial step towards incorporating artificial\nintelligence into capsule endoscopies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capsule endoscopy is a method to capture images of the gastrointestinal tract\nand screen for diseases which might remain hidden if investigated with standard\nendoscopes. Due to the limited size of a video capsule, embedding AI models\ndirectly into the capsule demands careful consideration of the model size and\nthus complicates anomaly detection in this field. Furthermore, the scarcity of\navailable data in this domain poses an ongoing challenge to achieving effective\nanomaly detection. Thus, this work introduces an ensemble strategy to address\nthis challenge in anomaly detection tasks in video capsule endoscopies,\nrequiring only a small number of individual neural networks during both the\ntraining and inference phases. Ensemble learning combines the predictions of\nmultiple independently trained neural networks. This has shown to be highly\neffective in enhancing both the accuracy and robustness of machine learning\nmodels. However, this comes at the cost of higher memory usage and increased\ncomputational effort, which quickly becomes prohibitive in many real-world\napplications. Instead of applying the same training algorithm to each\nindividual network, we propose using various loss functions, drawn from the\nanomaly detection field, to train each network. The methods are validated on\nthe two largest publicly available datasets for video capsule endoscopy images,\nthe Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on\nthe Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our\napproach outperforms current baselines with significantly fewer parameters\nacross all models, which is a crucial step towards incorporating artificial\nintelligence into capsule endoscopies."
                },
                "authors": [
                    {
                        "name": "Julia Werner"
                    },
                    {
                        "name": "Christoph Gerum"
                    },
                    {
                        "name": "Jorg Nick"
                    },
                    {
                        "name": "Maxime Le Floch"
                    },
                    {
                        "name": "Franz Brinkmann"
                    },
                    {
                        "name": "Jochen Hampe"
                    },
                    {
                        "name": "Oliver Bringmann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Bringmann"
                },
                "author": "Oliver Bringmann",
                "arxiv_comment": "Accepted at the 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBS EMBC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14751v1",
                "updated": "2025-08-20T14:50:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    50,
                    28,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:50:28Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    50,
                    28,
                    2,
                    232,
                    0
                ],
                "title": "HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents"
                },
                "summary": "Open-ended AI agents need to be able to learn efficiently goals of increasing\ncomplexity, abstraction and heterogeneity over their lifetime. Beyond sampling\nefficiently their own goals, autotelic agents specifically need to be able to\nkeep the growing complexity of goals under control, limiting the associated\ngrowth in sample and computational complexity. To adress this challenge, recent\napproaches have leveraged hierarchical reinforcement learning (HRL) and\nlanguage, capitalizing on its compositional and combinatorial generalization\ncapabilities to acquire temporally extended reusable behaviours. Existing\napproaches use expert defined spaces of subgoals over which they instantiate a\nhierarchy, and often assume pre-trained associated low-level policies. Such\ndesigns are inadequate in open-ended scenarios, where goal spaces naturally\ndiversify across a broad spectrum of difficulties. We introduce HERAKLES, a\nframework that enables a two-level hierarchical autotelic agent to continuously\ncompile mastered goals into the low-level policy, executed by a small, fast\nneural network, dynamically expanding the set of subgoals available to the\nhigh-level policy. We train a Large Language Model (LLM) to serve as the\nhigh-level controller, exploiting its strengths in goal decomposition and\ngeneralization to operate effectively over this evolving subgoal space. We\nevaluate HERAKLES in the open-ended Crafter environment and show that it scales\neffectively with goal complexity, improves sample efficiency through skill\ncompilation, and enables the agent to adapt robustly to novel challenges over\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended AI agents need to be able to learn efficiently goals of increasing\ncomplexity, abstraction and heterogeneity over their lifetime. Beyond sampling\nefficiently their own goals, autotelic agents specifically need to be able to\nkeep the growing complexity of goals under control, limiting the associated\ngrowth in sample and computational complexity. To adress this challenge, recent\napproaches have leveraged hierarchical reinforcement learning (HRL) and\nlanguage, capitalizing on its compositional and combinatorial generalization\ncapabilities to acquire temporally extended reusable behaviours. Existing\napproaches use expert defined spaces of subgoals over which they instantiate a\nhierarchy, and often assume pre-trained associated low-level policies. Such\ndesigns are inadequate in open-ended scenarios, where goal spaces naturally\ndiversify across a broad spectrum of difficulties. We introduce HERAKLES, a\nframework that enables a two-level hierarchical autotelic agent to continuously\ncompile mastered goals into the low-level policy, executed by a small, fast\nneural network, dynamically expanding the set of subgoals available to the\nhigh-level policy. We train a Large Language Model (LLM) to serve as the\nhigh-level controller, exploiting its strengths in goal decomposition and\ngeneralization to operate effectively over this evolving subgoal space. We\nevaluate HERAKLES in the open-ended Crafter environment and show that it scales\neffectively with goal complexity, improves sample efficiency through skill\ncompilation, and enables the agent to adapt robustly to novel challenges over\ntime."
                },
                "authors": [
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Clément Romac"
                    },
                    {
                        "name": "Loris Gaven"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Lamprier"
                },
                "author": "Sylvain Lamprier",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14748v1",
                "updated": "2025-08-20T14:48:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    48,
                    44,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:48:44Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    48,
                    44,
                    2,
                    232,
                    0
                ],
                "title": "Cross-Modality Controlled Molecule Generation with Diffusion Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Modality Controlled Molecule Generation with Diffusion Language\n  Model"
                },
                "summary": "Current SMILES-based diffusion models for molecule generation typically\nsupport only unimodal constraint. They inject conditioning signals at the start\nof the training process and require retraining a new model from scratch\nwhenever the constraint changes. However, real-world applications often involve\nmultiple constraints across different modalities, and additional constraints\nmay emerge over the course of a study. This raises a challenge: how to extend a\npre-trained diffusion model not only to support cross-modality constraints but\nalso to incorporate new ones without retraining. To tackle this problem, we\npropose the Cross-Modality Controlled Molecule Generation with Diffusion\nLanguage Model (CMCM-DLM), demonstrated by two distinct cross modalities:\nmolecular structure and chemical properties. Our approach builds upon a\npre-trained diffusion model, incorporating two trainable modules, the Structure\nControl Module (SCM) and the Property Control Module (PCM), and operates in two\ndistinct phases during the generation process. In Phase I, we employs the SCM\nto inject structural constraints during the early diffusion steps, effectively\nanchoring the molecular backbone. Phase II builds on this by further\nintroducing PCM to guide the later stages of inference to refine the generated\nmolecules, ensuring their chemical properties match the specified targets.\nExperimental results on multiple datasets demonstrate the efficiency and\nadaptability of our approach, highlighting CMCM-DLM's significant advancement\nin molecular generation for drug discovery applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current SMILES-based diffusion models for molecule generation typically\nsupport only unimodal constraint. They inject conditioning signals at the start\nof the training process and require retraining a new model from scratch\nwhenever the constraint changes. However, real-world applications often involve\nmultiple constraints across different modalities, and additional constraints\nmay emerge over the course of a study. This raises a challenge: how to extend a\npre-trained diffusion model not only to support cross-modality constraints but\nalso to incorporate new ones without retraining. To tackle this problem, we\npropose the Cross-Modality Controlled Molecule Generation with Diffusion\nLanguage Model (CMCM-DLM), demonstrated by two distinct cross modalities:\nmolecular structure and chemical properties. Our approach builds upon a\npre-trained diffusion model, incorporating two trainable modules, the Structure\nControl Module (SCM) and the Property Control Module (PCM), and operates in two\ndistinct phases during the generation process. In Phase I, we employs the SCM\nto inject structural constraints during the early diffusion steps, effectively\nanchoring the molecular backbone. Phase II builds on this by further\nintroducing PCM to guide the later stages of inference to refine the generated\nmolecules, ensuring their chemical properties match the specified targets.\nExperimental results on multiple datasets demonstrate the efficiency and\nadaptability of our approach, highlighting CMCM-DLM's significant advancement\nin molecular generation for drug discovery applications."
                },
                "authors": [
                    {
                        "name": "Yunzhe Zhang"
                    },
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Khanh Vinh Nguyen"
                    },
                    {
                        "name": "Pengyu Hong"
                    }
                ],
                "author_detail": {
                    "name": "Pengyu Hong"
                },
                "author": "Pengyu Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14746v1",
                "updated": "2025-08-20T14:43:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    43,
                    4,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:43:04Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    43,
                    4,
                    2,
                    232,
                    0
                ],
                "title": "MissionHD: Data-Driven Refinement of Reasoning Graph Structure through\n  Hyperdimensional Causal Path Encoding and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MissionHD: Data-Driven Refinement of Reasoning Graph Structure through\n  Hyperdimensional Causal Path Encoding and Decoding"
                },
                "summary": "Reasoning graphs from Large Language Models (LLMs) are often misaligned with\ndownstream visual tasks such as video anomaly detection (VAD). Existing Graph\nStructure Refinement (GSR) methods are ill-suited for these novel, dataset-less\ngraphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly\noptimizes graph structure using downstream task data, and propose MissionHD, a\nhyperdimensional computing (HDC) framework to operationalize it. MissionHD uses\nan efficient encode-decode process to refine the graph, guided by the\ndownstream task signal. Experiments on challenging VAD and VAR benchmarks show\nsignificant performance improvements when using our refined graphs, validating\nour approach as an effective pre-processing step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning graphs from Large Language Models (LLMs) are often misaligned with\ndownstream visual tasks such as video anomaly detection (VAD). Existing Graph\nStructure Refinement (GSR) methods are ill-suited for these novel, dataset-less\ngraphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly\noptimizes graph structure using downstream task data, and propose MissionHD, a\nhyperdimensional computing (HDC) framework to operationalize it. MissionHD uses\nan efficient encode-decode process to refine the graph, guided by the\ndownstream task signal. Experiments on challenging VAD and VAR benchmarks show\nsignificant performance improvements when using our refined graphs, validating\nour approach as an effective pre-processing step."
                },
                "authors": [
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Raheeb Hassan"
                    },
                    {
                        "name": "Ryozo Masukawa"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14735v1",
                "updated": "2025-08-20T14:30:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    30,
                    34,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:30:34Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    30,
                    34,
                    2,
                    232,
                    0
                ],
                "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via\n  Synthetic Natural Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Multilingual and Code-Switched Alignment in LLMs via\n  Synthetic Natural Language Inference"
                },
                "summary": "Large language models (LLMs) are increasingly applied in multilingual\ncontexts, yet their capacity for consistent, logically grounded alignment\nacross languages remains underexplored. We present a controlled evaluation\nframework for multilingual natural language inference (NLI) that generates\nsynthetic, logic-based premise-hypothesis pairs and translates them into a\ntypologically diverse set of languages. This design enables precise control\nover semantic relations and allows testing in both monolingual and\nmixed-language (code-switched) conditions. Surprisingly, code-switching does\nnot degrade, and can even improve, performance, suggesting that\ntranslation-induced lexical variation may serve as a regularization signal. We\nvalidate semantic preservation through embedding-based similarity analyses and\ncross-lingual alignment visualizations, confirming the fidelity of translated\npairs. Our findings expose both the potential and the brittleness of current\nLLM cross-lingual reasoning, and identify code-switching as a promising lever\nfor improving multilingual robustness. Code available at:\nhttps://github.com/KurbanIntelligenceLab/nli-stress-testing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied in multilingual\ncontexts, yet their capacity for consistent, logically grounded alignment\nacross languages remains underexplored. We present a controlled evaluation\nframework for multilingual natural language inference (NLI) that generates\nsynthetic, logic-based premise-hypothesis pairs and translates them into a\ntypologically diverse set of languages. This design enables precise control\nover semantic relations and allows testing in both monolingual and\nmixed-language (code-switched) conditions. Surprisingly, code-switching does\nnot degrade, and can even improve, performance, suggesting that\ntranslation-induced lexical variation may serve as a regularization signal. We\nvalidate semantic preservation through embedding-based similarity analyses and\ncross-lingual alignment visualizations, confirming the fidelity of translated\npairs. Our findings expose both the potential and the brittleness of current\nLLM cross-lingual reasoning, and identify code-switching as a promising lever\nfor improving multilingual robustness. Code available at:\nhttps://github.com/KurbanIntelligenceLab/nli-stress-testing"
                },
                "authors": [
                    {
                        "name": "Samir Abdaljalil"
                    },
                    {
                        "name": "Erchin Serpedin"
                    },
                    {
                        "name": "Khalid Qaraqe"
                    },
                    {
                        "name": "Hasan Kurban"
                    }
                ],
                "author_detail": {
                    "name": "Hasan Kurban"
                },
                "author": "Hasan Kurban",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19254v3",
                "updated": "2025-08-20T14:26:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    26,
                    48,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-27T14:24:45Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    24,
                    45,
                    6,
                    117,
                    0
                ],
                "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box,\n  White-Box, LLM Judge, and Ensemble Scorers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification for Language Models: A Suite of Black-Box,\n  White-Box, LLM Judge, and Ensemble Scorers"
                },
                "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we outline a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we propose a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we outline a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we propose a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Dylan Bouchard"
                    },
                    {
                        "name": "Mohit Singh Chauhan"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Singh Chauhan"
                },
                "author": "Mohit Singh Chauhan",
                "arxiv_comment": "UQLM repository: https://github.com/cvs-health/uqlm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19061v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19061v3",
                "updated": "2025-08-20T14:24:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    24,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-27T00:39:12Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    0,
                    39,
                    12,
                    6,
                    117,
                    0
                ],
                "title": "Hallucinations and Key Information Extraction in Medical Texts: A\n  Comprehensive Assessment of Open-Source Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations and Key Information Extraction in Medical Texts: A\n  Comprehensive Assessment of Open-Source Large Language Models"
                },
                "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, including admission reasons, major in-hospital events, and\ncritical follow-up actions. In addition, we also assess the prevalence of\nvarious types of hallucinations in the summaries produced by these models.\nDetecting hallucinations is vital as it directly influences the reliability of\nthe information, potentially affecting patient care and treatment outcomes. We\nconduct comprehensive simulations to rigorously evaluate the performance of\nthese models, further probing the accuracy and fidelity of the extracted\ncontent in clinical summarization. Our results reveal that while the LLMs\n(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission\nreasons and hospitalization events, they are generally less consistent when it\ncomes to identifying follow-up recommendations, highlighting broader challenges\nin leveraging LLMs for comprehensive summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, including admission reasons, major in-hospital events, and\ncritical follow-up actions. In addition, we also assess the prevalence of\nvarious types of hallucinations in the summaries produced by these models.\nDetecting hallucinations is vital as it directly influences the reliability of\nthe information, potentially affecting patient care and treatment outcomes. We\nconduct comprehensive simulations to rigorously evaluate the performance of\nthese models, further probing the accuracy and fidelity of the extracted\ncontent in clinical summarization. Our results reveal that while the LLMs\n(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission\nreasons and hospitalization events, they are generally less consistent when it\ncomes to identifying follow-up recommendations, highlighting broader challenges\nin leveraging LLMs for comprehensive summarization."
                },
                "authors": [
                    {
                        "name": "Anindya Bijoy Das"
                    },
                    {
                        "name": "Shibbir Ahmed"
                    },
                    {
                        "name": "Shahnewaz Karim Sakib"
                    }
                ],
                "author_detail": {
                    "name": "Shahnewaz Karim Sakib"
                },
                "author": "Shahnewaz Karim Sakib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19061v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19061v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14727v1",
                "updated": "2025-08-20T14:16:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    16,
                    21,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:16:21Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    16,
                    21,
                    2,
                    232,
                    0
                ],
                "title": "Assessing the Quality and Security of AI-Generated Code: A Quantitative\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Quality and Security of AI-Generated Code: A Quantitative\n  Analysis"
                },
                "summary": "This study presents a quantitative evaluation of the code quality and\nsecurity of five prominent Large Language Models (LLMs): Claude Sonnet 4,\nClaude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior\nresearch has assessed the functional performance of LLM-generated code, this\nresearch tested LLM output from 4,442 Java coding assignments through\ncomprehensive static analysis using SonarQube. The findings suggest that\nalthough LLMs can generate functional code, they also introduce a range of\nsoftware defects, including bugs, security vulnerabilities, and code smells.\nThese defects do not appear to be isolated; rather, they may represent shared\nweaknesses stemming from systemic limitations within current LLM code\ngeneration methods. In particular, critically severe issues, such as hard-coded\npasswords and path traversal vulnerabilities, were observed across multiple\nmodels. These results indicate that LLM-generated code requires verification in\norder to be considered production-ready. This study found no direct correlation\nbetween a model's functional performance (measured by Pass@1 rate of unit\ntests) and the overall quality and security of its generated code, measured by\nthe number of SonarQube issues in benchmark solutions that passed the\nfunctional tests. This suggests that functional benchmark performance score is\nnot a good indicator of overall code quality and security. The goal of this\nstudy is not to rank LLM performance but to highlight that all evaluated models\nappear to share certain weaknesses. Consequently, these findings support the\nview that static analysis can be a valuable instrument for detecting latent\ndefects and an important safeguard for organizations that deploy AI in software\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a quantitative evaluation of the code quality and\nsecurity of five prominent Large Language Models (LLMs): Claude Sonnet 4,\nClaude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior\nresearch has assessed the functional performance of LLM-generated code, this\nresearch tested LLM output from 4,442 Java coding assignments through\ncomprehensive static analysis using SonarQube. The findings suggest that\nalthough LLMs can generate functional code, they also introduce a range of\nsoftware defects, including bugs, security vulnerabilities, and code smells.\nThese defects do not appear to be isolated; rather, they may represent shared\nweaknesses stemming from systemic limitations within current LLM code\ngeneration methods. In particular, critically severe issues, such as hard-coded\npasswords and path traversal vulnerabilities, were observed across multiple\nmodels. These results indicate that LLM-generated code requires verification in\norder to be considered production-ready. This study found no direct correlation\nbetween a model's functional performance (measured by Pass@1 rate of unit\ntests) and the overall quality and security of its generated code, measured by\nthe number of SonarQube issues in benchmark solutions that passed the\nfunctional tests. This suggests that functional benchmark performance score is\nnot a good indicator of overall code quality and security. The goal of this\nstudy is not to rank LLM performance but to highlight that all evaluated models\nappear to share certain weaknesses. Consequently, these findings support the\nview that static analysis can be a valuable instrument for detecting latent\ndefects and an important safeguard for organizations that deploy AI in software\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Abbas Sabra"
                    },
                    {
                        "name": "Olivier Schmitt"
                    },
                    {
                        "name": "Joseph Tyler"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Tyler"
                },
                "author": "Joseph Tyler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14723v1",
                "updated": "2025-08-20T14:05:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    5,
                    18,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:05:18Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    5,
                    18,
                    2,
                    232,
                    0
                ],
                "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation"
                },
                "summary": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows."
                },
                "authors": [
                    {
                        "name": "Guangzhan Wang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Beijun Shen"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13415v2",
                "updated": "2025-08-20T13:57:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    57,
                    38,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-19T00:26:07Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    0,
                    26,
                    7,
                    1,
                    231,
                    0
                ],
                "title": "MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed across diverse\napplications that demand balancing multiple, often conflicting, objectives --\nsuch as helpfulness, harmlessness, or humor. Aligning outputs to user-specific\npreferences in such multi-objective settings typically requires fine-tuning\nmodels for each objective or preference configuration, which is computationally\nexpensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via\nValue-Guided Inference-Time Search -- a lightweight inference-time alignment\nframework that enables dynamic control over LLM behavior without modifying the\nbase model's weights. MAVIS trains a set of small value models, each\ncorresponding to a distinct objective. At inference time, these value models\nare combined using user-specified weights to produce a tilting function that\nadjusts the base model's output distribution toward desired trade-offs. The\nvalue models are trained using a simple iterative algorithm that ensures\nmonotonic improvement of the KL-regularized policy. We show empirically that\nMAVIS outperforms baselines that fine-tune per-objective models and combine\nthem post hoc, and even approaches the performance of the idealized setting\nwhere models are fine-tuned for a user's exact preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed across diverse\napplications that demand balancing multiple, often conflicting, objectives --\nsuch as helpfulness, harmlessness, or humor. Aligning outputs to user-specific\npreferences in such multi-objective settings typically requires fine-tuning\nmodels for each objective or preference configuration, which is computationally\nexpensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via\nValue-Guided Inference-Time Search -- a lightweight inference-time alignment\nframework that enables dynamic control over LLM behavior without modifying the\nbase model's weights. MAVIS trains a set of small value models, each\ncorresponding to a distinct objective. At inference time, these value models\nare combined using user-specified weights to produce a tilting function that\nadjusts the base model's output distribution toward desired trade-offs. The\nvalue models are trained using a simple iterative algorithm that ensures\nmonotonic improvement of the KL-regularized policy. We show empirically that\nMAVIS outperforms baselines that fine-tune per-objective models and combine\nthem post hoc, and even approaches the performance of the idealized setting\nwhere models are fine-tuned for a user's exact preferences."
                },
                "authors": [
                    {
                        "name": "Jeremy Carleton"
                    },
                    {
                        "name": "Debajoy Mukherjee"
                    },
                    {
                        "name": "Srinivas Shakkottai"
                    },
                    {
                        "name": "Dileep Kalathil"
                    }
                ],
                "author_detail": {
                    "name": "Dileep Kalathil"
                },
                "author": "Dileep Kalathil",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04996v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04996v4",
                "updated": "2025-08-20T13:50:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    50,
                    10,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-07T13:34:49Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    34,
                    49,
                    0,
                    188,
                    0
                ],
                "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems"
                },
                "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems."
                },
                "authors": [
                    {
                        "name": "Jiangbo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangbo Yu"
                },
                "author": "Jiangbo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04996v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04996v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13790v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13790v3",
                "updated": "2025-08-20T13:47:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    47,
                    47,
                    2,
                    232,
                    0
                ],
                "published": "2025-06-11T11:40:11Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    40,
                    11,
                    2,
                    162,
                    0
                ],
                "title": "The NordDRG AI Benchmark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NordDRG AI Benchmark for Large Language Models"
                },
                "summary": "Large language models (LLMs) are being piloted for clinical coding and\ndecision support, yet no open benchmark targets the hospital-funding layer\nwhere Diagnosis-Related Groups (DRGs) determine reimbursement. In most OECD\nsystems, DRGs route a substantial share of multi-trillion-dollar health\nspending through governed grouper software, making transparency and\nauditability first-order concerns. We release NordDRG-AI-Benchmark, the first\npublic, rule-complete test bed for DRG reasoning. The package includes (i)\nmachine-readable approximately 20-sheet NordDRG definition tables and (ii)\nexpert manuals and change-log templates that capture governance workflows. It\nexposes two suites: a 13-task Logic benchmark (code lookup, cross-table\ninference, grouping features, multilingual terminology, and CC/MCC validity\nchecks) and a 13-task Grouper benchmark that requires full DRG grouper\nemulation with strict exact-match scoring on both the DRG and the triggering\ndrg_logic.id. Lightweight reference agents (LogicAgent, GrouperAgent) enable\nartefact-only evaluation. Under an artefact-only (no web) setting, on the 13\nLogic tasks GPT-5 Thinking and Opus 4.1 score 13/13, o3 scores 12/13; mid-tier\nmodels (GPT-5 Thinking Mini, o4-mini, GPT-5 Fast) achieve 6-8/13, and remaining\nmodels score 5/13 or below. On full grouper emulation across 13 tasks, GPT-5\nThinking solves 7/13, o3 6/13, o4-mini 3/13; GPT-5 Thinking Mini solves 1/13,\nand all other tested endpoints score 0/13. To our knowledge, this is the first\npublic report of an LLM partially emulating the complete NordDRG grouper logic\nwith governance-grade traceability. Coupling a rule-complete release with\nexact-match tasks and open scoring provides a reproducible yardstick for\nhead-to-head and longitudinal evaluation in hospital funding. Benchmark\nmaterials available in Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being piloted for clinical coding and\ndecision support, yet no open benchmark targets the hospital-funding layer\nwhere Diagnosis-Related Groups (DRGs) determine reimbursement. In most OECD\nsystems, DRGs route a substantial share of multi-trillion-dollar health\nspending through governed grouper software, making transparency and\nauditability first-order concerns. We release NordDRG-AI-Benchmark, the first\npublic, rule-complete test bed for DRG reasoning. The package includes (i)\nmachine-readable approximately 20-sheet NordDRG definition tables and (ii)\nexpert manuals and change-log templates that capture governance workflows. It\nexposes two suites: a 13-task Logic benchmark (code lookup, cross-table\ninference, grouping features, multilingual terminology, and CC/MCC validity\nchecks) and a 13-task Grouper benchmark that requires full DRG grouper\nemulation with strict exact-match scoring on both the DRG and the triggering\ndrg_logic.id. Lightweight reference agents (LogicAgent, GrouperAgent) enable\nartefact-only evaluation. Under an artefact-only (no web) setting, on the 13\nLogic tasks GPT-5 Thinking and Opus 4.1 score 13/13, o3 scores 12/13; mid-tier\nmodels (GPT-5 Thinking Mini, o4-mini, GPT-5 Fast) achieve 6-8/13, and remaining\nmodels score 5/13 or below. On full grouper emulation across 13 tasks, GPT-5\nThinking solves 7/13, o3 6/13, o4-mini 3/13; GPT-5 Thinking Mini solves 1/13,\nand all other tested endpoints score 0/13. To our knowledge, this is the first\npublic report of an LLM partially emulating the complete NordDRG grouper logic\nwith governance-grade traceability. Coupling a rule-complete release with\nexact-match tasks and open scoring provides a reproducible yardstick for\nhead-to-head and longitudinal evaluation in hospital funding. Benchmark\nmaterials available in Github."
                },
                "authors": [
                    {
                        "name": "Tapio Pitkäranta"
                    }
                ],
                "author_detail": {
                    "name": "Tapio Pitkäranta"
                },
                "author": "Tapio Pitkäranta",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13790v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13790v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05836v2",
                "updated": "2025-08-20T13:43:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    43,
                    59,
                    2,
                    232,
                    0
                ],
                "published": "2025-01-10T10:23:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    23,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "Treatment Effect Estimation in Causal Survival Analysis: Practical\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treatment Effect Estimation in Causal Survival Analysis: Practical\n  Recommendations"
                },
                "summary": "Causal survival analysis combines survival analysis and causal inference to\nevaluate the effect of a treatment or intervention on a time-to-event outcome,\nsuch as survival time. It offers an alternative to relying solely on Cox models\nfor assessing these effects. In this paper, we present a comprehensive review\nof estimators for the average treatment effect measured with the restricted\nmean survival time, including regression-based methods, weighting approaches,\nand hybrid techniques. We investigate their theoretical properties and compare\ntheir performance through extensive numerical experiments. Our analysis focuses\non the finite-sample behavior of these estimators, the influence of nuisance\nparameter selection, and their robustness and stability under model\nmisspecification. By bridging theoretical insights with practical evaluation,\nwe aim to equip practitioners with both state-of-the-art implementations of\nthese methods and practical guidelines for selecting appropriate estimators for\ntreatment effect estimation. Among the approaches considered, G-formula\ntwo-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival\nforests emerge as particularly promising.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal survival analysis combines survival analysis and causal inference to\nevaluate the effect of a treatment or intervention on a time-to-event outcome,\nsuch as survival time. It offers an alternative to relying solely on Cox models\nfor assessing these effects. In this paper, we present a comprehensive review\nof estimators for the average treatment effect measured with the restricted\nmean survival time, including regression-based methods, weighting approaches,\nand hybrid techniques. We investigate their theoretical properties and compare\ntheir performance through extensive numerical experiments. Our analysis focuses\non the finite-sample behavior of these estimators, the influence of nuisance\nparameter selection, and their robustness and stability under model\nmisspecification. By bridging theoretical insights with practical evaluation,\nwe aim to equip practitioners with both state-of-the-art implementations of\nthese methods and practical guidelines for selecting appropriate estimators for\ntreatment effect estimation. Among the approaches considered, G-formula\ntwo-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival\nforests emerge as particularly promising."
                },
                "authors": [
                    {
                        "name": "Charlotte Voinot"
                    },
                    {
                        "name": "Clément Berenfeld"
                    },
                    {
                        "name": "Imke Mayer"
                    },
                    {
                        "name": "Bernard Sebastien"
                    },
                    {
                        "name": "Julie Josse"
                    }
                ],
                "author_detail": {
                    "name": "Julie Josse"
                },
                "arxiv_affiliation": "PREMEDICAL",
                "author": "Julie Josse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14713v1",
                "updated": "2025-08-20T13:43:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    43,
                    49,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T13:43:49Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    43,
                    49,
                    2,
                    232,
                    0
                ],
                "title": "Long-Context Speech Synthesis with Context-Aware Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Speech Synthesis with Context-Aware Memory"
                },
                "summary": "In long-text speech synthesis, current approaches typically convert text to\nspeech at the sentence-level and concatenate the results to form\npseudo-paragraph-level speech. These methods overlook the contextual coherence\nof paragraphs, leading to reduced naturalness and inconsistencies in style and\ntimbre across the long-form speech. To address these issues, we propose a\nContext-Aware Memory (CAM)-based long-context Text-to-Speech (TTS) model. The\nCAM block integrates and retrieves both long-term memory and local context\ndetails, enabling dynamic memory updates and transfers within long paragraphs\nto guide sentence-level speech synthesis. Furthermore, the prefix mask enhances\nthe in-context learning ability by enabling bidirectional attention on prefix\ntokens while maintaining unidirectional generation. Experimental results\ndemonstrate that the proposed method outperforms baseline and state-of-the-art\nlong-context methods in terms of prosody expressiveness, coherence and context\ninference cost across paragraph-level speech.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In long-text speech synthesis, current approaches typically convert text to\nspeech at the sentence-level and concatenate the results to form\npseudo-paragraph-level speech. These methods overlook the contextual coherence\nof paragraphs, leading to reduced naturalness and inconsistencies in style and\ntimbre across the long-form speech. To address these issues, we propose a\nContext-Aware Memory (CAM)-based long-context Text-to-Speech (TTS) model. The\nCAM block integrates and retrieves both long-term memory and local context\ndetails, enabling dynamic memory updates and transfers within long paragraphs\nto guide sentence-level speech synthesis. Furthermore, the prefix mask enhances\nthe in-context learning ability by enabling bidirectional attention on prefix\ntokens while maintaining unidirectional generation. Experimental results\ndemonstrate that the proposed method outperforms baseline and state-of-the-art\nlong-context methods in terms of prosody expressiveness, coherence and context\ninference cost across paragraph-level speech."
                },
                "authors": [
                    {
                        "name": "Zhipeng Li"
                    },
                    {
                        "name": "Xiaofen Xing"
                    },
                    {
                        "name": "Jingyuan Xing"
                    },
                    {
                        "name": "Hangrui Hu"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "arxiv_comment": "Accepted by Interspeech25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14706v1",
                "updated": "2025-08-20T13:30:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    30,
                    20,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T13:30:20Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    30,
                    20,
                    2,
                    232,
                    0
                ],
                "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine"
                },
                "summary": "Despite the success of large language models (LLMs) in various domains, their\npotential in Traditional Chinese Medicine (TCM) remains largely underexplored\ndue to two critical barriers: (1) the scarcity of high-quality TCM data and (2)\nthe inherently multimodal nature of TCM diagnostics, which involve looking,\nlistening, smelling, and pulse-taking. These sensory-rich modalities are beyond\nthe scope of conventional LLMs. To address these challenges, we present\nShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data\nscarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text\nand 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and\nphysiological signals. ShizhenGPT is pretrained and instruction-tuned to\nachieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect\nrecent national TCM qualification exams and build a visual benchmark for\nMedicinal Recognition and Visual Diagnosis. Experiments demonstrate that\nShizhenGPT outperforms comparable-scale LLMs and competes with larger\nproprietary models. Moreover, it leads in TCM visual understanding among\nexisting multimodal LLMs and demonstrates unified perception across modalities\nlike sound, pulse, smell, and vision, paving the way toward holistic multimodal\nperception and diagnosis in TCM. Datasets, models, and code are publicly\navailable. We hope this work will inspire further exploration in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of large language models (LLMs) in various domains, their\npotential in Traditional Chinese Medicine (TCM) remains largely underexplored\ndue to two critical barriers: (1) the scarcity of high-quality TCM data and (2)\nthe inherently multimodal nature of TCM diagnostics, which involve looking,\nlistening, smelling, and pulse-taking. These sensory-rich modalities are beyond\nthe scope of conventional LLMs. To address these challenges, we present\nShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data\nscarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text\nand 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and\nphysiological signals. ShizhenGPT is pretrained and instruction-tuned to\nachieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect\nrecent national TCM qualification exams and build a visual benchmark for\nMedicinal Recognition and Visual Diagnosis. Experiments demonstrate that\nShizhenGPT outperforms comparable-scale LLMs and competes with larger\nproprietary models. Moreover, it leads in TCM visual understanding among\nexisting multimodal LLMs and demonstrates unified perception across modalities\nlike sound, pulse, smell, and vision, paving the way toward holistic multimodal\nperception and diagnosis in TCM. Datasets, models, and code are publicly\navailable. We hope this work will inspire further exploration in this field."
                },
                "authors": [
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Zhenyang Cai"
                    },
                    {
                        "name": "Zhiheng Liu"
                    },
                    {
                        "name": "Yunjin Yang"
                    },
                    {
                        "name": "Rongsheng Wang"
                    },
                    {
                        "name": "Qingying Xiao"
                    },
                    {
                        "name": "Xiangyi Feng"
                    },
                    {
                        "name": "Zhan Su"
                    },
                    {
                        "name": "Jing Guo"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14705v1",
                "updated": "2025-08-20T13:29:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    29,
                    24,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T13:29:24Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    29,
                    24,
                    2,
                    232,
                    0
                ],
                "title": "Learning in Repeated Multi-Objective Stackelberg Games with Payoff\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Repeated Multi-Objective Stackelberg Games with Payoff\n  Manipulation"
                },
                "summary": "We study payoff manipulation in repeated multi-objective Stackelberg games,\nwhere a leader may strategically influence a follower's deterministic best\nresponse, e.g., by offering a share of their own payoff. We assume that the\nfollower's utility function, representing preferences over multiple objectives,\nis unknown but linear, and its weight parameter must be inferred through\ninteraction. This introduces a sequential decision-making challenge for the\nleader, who must balance preference elicitation with immediate utility\nmaximisation. We formalise this problem and propose manipulation policies based\non expected utility (EU) and long-term expected utility (longEU), which guide\nthe leader in selecting actions and offering incentives that trade off\nshort-term gains with long-term impact. We prove that under infinite repeated\ninteractions, longEU converges to the optimal manipulation. Empirical results\nacross benchmark environments demonstrate that our approach improves cumulative\nleader utility while promoting mutually beneficial outcomes, all without\nrequiring explicit negotiation or prior knowledge of the follower's utility\nfunction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study payoff manipulation in repeated multi-objective Stackelberg games,\nwhere a leader may strategically influence a follower's deterministic best\nresponse, e.g., by offering a share of their own payoff. We assume that the\nfollower's utility function, representing preferences over multiple objectives,\nis unknown but linear, and its weight parameter must be inferred through\ninteraction. This introduces a sequential decision-making challenge for the\nleader, who must balance preference elicitation with immediate utility\nmaximisation. We formalise this problem and propose manipulation policies based\non expected utility (EU) and long-term expected utility (longEU), which guide\nthe leader in selecting actions and offering incentives that trade off\nshort-term gains with long-term impact. We prove that under infinite repeated\ninteractions, longEU converges to the optimal manipulation. Empirical results\nacross benchmark environments demonstrate that our approach improves cumulative\nleader utility while promoting mutually beneficial outcomes, all without\nrequiring explicit negotiation or prior knowledge of the follower's utility\nfunction."
                },
                "authors": [
                    {
                        "name": "Phurinut Srisawad"
                    },
                    {
                        "name": "Juergen Branke"
                    },
                    {
                        "name": "Long Tran-Thanh"
                    }
                ],
                "author_detail": {
                    "name": "Long Tran-Thanh"
                },
                "author": "Long Tran-Thanh",
                "arxiv_comment": "Extended version of the paper accepted at the 28th European\n  Conference on Artificial Intelligence (ECAI 2025); Paper ID: M2635",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14704v1",
                "updated": "2025-08-20T13:28:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    28,
                    58,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T13:28:58Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    28,
                    58,
                    2,
                    232,
                    0
                ],
                "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model\n  Context Protocol Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP-Universe: Benchmarking Large Language Models with Real-World Model\n  Context Protocol Servers"
                },
                "summary": "The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem."
                },
                "authors": [
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Zhiqi Shen"
                    },
                    {
                        "name": "Wenzhuo Yang"
                    },
                    {
                        "name": "Zirui Zhao"
                    },
                    {
                        "name": "Prathyusha Jwalapuram"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Doyen Sahoo"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Junnan Li"
                    }
                ],
                "author_detail": {
                    "name": "Junnan Li"
                },
                "author": "Junnan Li",
                "arxiv_comment": "Website: https://mcp-universe.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14703v1",
                "updated": "2025-08-20T13:28:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    28,
                    39,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T13:28:39Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    28,
                    39,
                    2,
                    232,
                    0
                ],
                "title": "A Lightweight Incentive-Based Privacy-Preserving Smart Metering Protocol\n  for Value-Added Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Incentive-Based Privacy-Preserving Smart Metering Protocol\n  for Value-Added Services"
                },
                "summary": "The emergence of smart grids and advanced metering infrastructure (AMI) has\nrevolutionized energy management. Unlike traditional power grids, smart grids\nbenefit from two-way communication through AMI, which surpasses earlier\nautomated meter reading (AMR). AMI enables diverse demand- and supply-side\nutilities such as accurate billing, outage detection, real-time grid control,\nload forecasting, and value-added services. Smart meters play a key role by\ndelivering consumption values at predefined intervals to the utility provider\n(UP). However, such reports may raise privacy concerns, as adversaries can\ninfer lifestyle patterns, political orientations, and the types of electrical\ndevices in a household, or even sell the data to third parties (TP) such as\ninsurers. In this paper, we propose a lightweight, privacy-preserving smart\nmetering protocol for incentive-based value-added services. The scheme employs\nlocal differential privacy, hash chains, blind digital signatures, pseudonyms,\ntemporal aggregation, and anonymous overlay networks to report coarse-grained\nvalues with adjustable granularity to the UP. This protects consumers' privacy\nwhile preserving data utility. The scheme prevents identity disclosure while\nenabling automatic token redemption. From a performance perspective, our\nresults show that with a 1024-bit RSA key, a 7-day duration, and four reports\nper day, our protocol runs in approximately 0.51s and consumes about 4.5 MB of\nmemory. From a privacy perspective, the protocol resists semi-trusted and\nuntrusted adversaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of smart grids and advanced metering infrastructure (AMI) has\nrevolutionized energy management. Unlike traditional power grids, smart grids\nbenefit from two-way communication through AMI, which surpasses earlier\nautomated meter reading (AMR). AMI enables diverse demand- and supply-side\nutilities such as accurate billing, outage detection, real-time grid control,\nload forecasting, and value-added services. Smart meters play a key role by\ndelivering consumption values at predefined intervals to the utility provider\n(UP). However, such reports may raise privacy concerns, as adversaries can\ninfer lifestyle patterns, political orientations, and the types of electrical\ndevices in a household, or even sell the data to third parties (TP) such as\ninsurers. In this paper, we propose a lightweight, privacy-preserving smart\nmetering protocol for incentive-based value-added services. The scheme employs\nlocal differential privacy, hash chains, blind digital signatures, pseudonyms,\ntemporal aggregation, and anonymous overlay networks to report coarse-grained\nvalues with adjustable granularity to the UP. This protects consumers' privacy\nwhile preserving data utility. The scheme prevents identity disclosure while\nenabling automatic token redemption. From a performance perspective, our\nresults show that with a 1024-bit RSA key, a 7-day duration, and four reports\nper day, our protocol runs in approximately 0.51s and consumes about 4.5 MB of\nmemory. From a privacy perspective, the protocol resists semi-trusted and\nuntrusted adversaries."
                },
                "authors": [
                    {
                        "name": "Farid Zaredar"
                    },
                    {
                        "name": "Morteza Amini"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Amini"
                },
                "author": "Morteza Amini",
                "arxiv_comment": "18 Pages, 7 Figures, 6 Tables,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10207v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10207v3",
                "updated": "2025-08-20T13:15:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    15,
                    18,
                    2,
                    232,
                    0
                ],
                "published": "2024-12-13T15:30:20Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    30,
                    20,
                    4,
                    348,
                    0
                ],
                "title": "Retrieval-Augmented Semantic Parsing: Improving Generalization with\n  Lexical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Semantic Parsing: Improving Generalization with\n  Lexical Knowledge"
                },
                "summary": "Open-domain semantic parsing remains a challenging task, as neural models\noften rely on heuristics and struggle to handle unseen concepts. In this paper,\nwe investigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external symbolic knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-domain semantic parsing remains a challenging task, as neural models\noften rely on heuristics and struggle to handle unseen concepts. In this paper,\nwe investigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external symbolic knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing."
                },
                "authors": [
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Qianru Meng"
                    },
                    {
                        "name": "Johan Bos"
                    }
                ],
                "author_detail": {
                    "name": "Johan Bos"
                },
                "author": "Johan Bos",
                "arxiv_comment": "Accpted by 16th IWCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10207v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10207v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13953v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13953v2",
                "updated": "2025-08-20T13:10:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    10,
                    14,
                    2,
                    232,
                    0
                ],
                "published": "2025-02-19T18:53:16Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    53,
                    16,
                    2,
                    50,
                    0
                ],
                "title": "Benchmarking graph construction by large language models for\n  coherence-driven inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking graph construction by large language models for\n  coherence-driven inference"
                },
                "summary": "We devise an algorithm to generate propositions that objectively instantiate\ngraphs supporting coherence-driven inference. We also benchmark the ability of\nlarge language models (LLMs) to reconstruct coherence graphs from (a simple\ntransformation of) propositions expressed in natural language, with promising\nresults from a single prompt to reasoning-optimized LLMs. For example,\no1/3/4-mini achieve perfect reconstruction half of the time on sparse graphs.\nCoherence-driven inference on consistency evaluations by LLMs may advance\nmachine cognition capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We devise an algorithm to generate propositions that objectively instantiate\ngraphs supporting coherence-driven inference. We also benchmark the ability of\nlarge language models (LLMs) to reconstruct coherence graphs from (a simple\ntransformation of) propositions expressed in natural language, with promising\nresults from a single prompt to reasoning-optimized LLMs. For example,\no1/3/4-mini achieve perfect reconstruction half of the time on sparse graphs.\nCoherence-driven inference on consistency evaluations by LLMs may advance\nmachine cognition capabilities."
                },
                "authors": [
                    {
                        "name": "Steve Huntsman"
                    },
                    {
                        "name": "Jewell Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Jewell Thomas"
                },
                "author": "Jewell Thomas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13953v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01872v2",
                "updated": "2025-08-20T13:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    7,
                    2,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-02T16:27:44Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    27,
                    44,
                    2,
                    92,
                    0
                ],
                "title": "CoMatcher: Multi-View Collaborative Feature Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMatcher: Multi-View Collaborative Feature Matching"
                },
                "summary": "This paper proposes a multi-view collaborative matching strategy for reliable\ntrack construction in complex scenarios. We observe that the pairwise matching\nparadigms applied to image set matching often result in ambiguous estimation\nwhen the selected independent pairs exhibit significant occlusions or extreme\nviewpoint changes. This challenge primarily stems from the inherent uncertainty\nin interpreting intricate 3D structures based on limited two-view observations,\nas the 3D-to-2D projection leads to significant information loss. To address\nthis, we introduce CoMatcher, a deep multi-view matcher to (i) leverage\ncomplementary context cues from different views to form a holistic 3D scene\nunderstanding and (ii) utilize cross-view projection consistency to infer a\nreliable global solution. Building on CoMatcher, we develop a groupwise\nframework that fully exploits cross-view relationships for large-scale matching\ntasks. Extensive experiments on various complex scenarios demonstrate the\nsuperiority of our method over the mainstream two-view matching paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a multi-view collaborative matching strategy for reliable\ntrack construction in complex scenarios. We observe that the pairwise matching\nparadigms applied to image set matching often result in ambiguous estimation\nwhen the selected independent pairs exhibit significant occlusions or extreme\nviewpoint changes. This challenge primarily stems from the inherent uncertainty\nin interpreting intricate 3D structures based on limited two-view observations,\nas the 3D-to-2D projection leads to significant information loss. To address\nthis, we introduce CoMatcher, a deep multi-view matcher to (i) leverage\ncomplementary context cues from different views to form a holistic 3D scene\nunderstanding and (ii) utilize cross-view projection consistency to infer a\nreliable global solution. Building on CoMatcher, we develop a groupwise\nframework that fully exploits cross-view relationships for large-scale matching\ntasks. Extensive experiments on various complex scenarios demonstrate the\nsuperiority of our method over the mainstream two-view matching paradigm."
                },
                "authors": [
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Zimin Xia"
                    },
                    {
                        "name": "Mingyue Dong"
                    },
                    {
                        "name": "Shuhan Shen"
                    },
                    {
                        "name": "Linwei Yue"
                    },
                    {
                        "name": "Xianwei Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xianwei Zheng"
                },
                "author": "Xianwei Zheng",
                "arxiv_comment": "15 pages, 7 figures, to be published in CVPR 2025",
                "arxiv_journal_ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.2.10; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14686v1",
                "updated": "2025-08-20T13:03:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    3,
                    4,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T13:03:04Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    3,
                    4,
                    2,
                    232,
                    0
                ],
                "title": "Optimal Unpredictable Control for Linear Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Unpredictable Control for Linear Systems"
                },
                "summary": "In this paper, we investigate how to achieve the unpredictability against\nmalicious inferences for linear systems. The key idea is to add stochastic\ncontrol inputs, named as unpredictable control, to make the outputs irregular.\nThe future outputs thus become unpredictable and the performance of inferences\nis degraded. The major challenges lie in: i) how to formulate optimization\nproblems to obtain an optimal distribution of stochastic input, under unknown\nprediction accuracy of the adversary; and ii) how to achieve the trade-off\nbetween the unpredictability and control performance. We first utilize both\nvariance and confidence probability of prediction error to quantify\nunpredictability, then formulate two two-stage stochastic optimization\nproblems, respectively. Under variance metric, the analytic optimal\ndistribution of control input is provided. With probability metric, it is a\nnon-convex optimization problem, thus we present a novel numerical method and\nconvert the problem into a solvable linear optimization problem. Last, we\nquantify the control performance under unpredictable control, and accordingly\ndesign the unpredictable LQR and cooperative control. Simulations demonstrate\nthe unpredictability of our control algorithm. The obtained optimal\ndistribution outperforms Gaussian and Laplace distributions commonly used in\ndifferential privacy under proposed metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate how to achieve the unpredictability against\nmalicious inferences for linear systems. The key idea is to add stochastic\ncontrol inputs, named as unpredictable control, to make the outputs irregular.\nThe future outputs thus become unpredictable and the performance of inferences\nis degraded. The major challenges lie in: i) how to formulate optimization\nproblems to obtain an optimal distribution of stochastic input, under unknown\nprediction accuracy of the adversary; and ii) how to achieve the trade-off\nbetween the unpredictability and control performance. We first utilize both\nvariance and confidence probability of prediction error to quantify\nunpredictability, then formulate two two-stage stochastic optimization\nproblems, respectively. Under variance metric, the analytic optimal\ndistribution of control input is provided. With probability metric, it is a\nnon-convex optimization problem, thus we present a novel numerical method and\nconvert the problem into a solvable linear optimization problem. Last, we\nquantify the control performance under unpredictable control, and accordingly\ndesign the unpredictable LQR and cooperative control. Simulations demonstrate\nthe unpredictability of our control algorithm. The obtained optimal\ndistribution outperforms Gaussian and Laplace distributions commonly used in\ndifferential privacy under proposed metrics."
                },
                "authors": [
                    {
                        "name": "Chendi Qu"
                    },
                    {
                        "name": "Jianping He"
                    },
                    {
                        "name": "Jialun Li"
                    },
                    {
                        "name": "Xiaoming Duan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Duan"
                },
                "author": "Xiaoming Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14685v1",
                "updated": "2025-08-20T13:01:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    1,
                    34,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T13:01:34Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    1,
                    34,
                    2,
                    232,
                    0
                ],
                "title": "Improving in-context learning with a better scoring function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving in-context learning with a better scoring function"
                },
                "summary": "Large language models (LLMs) exhibit a remarkable capacity to learn by\nanalogy, known as in-context learning (ICL). However, recent studies have\nrevealed limitations in this ability. In this paper, we examine these\nlimitations on tasks involving first-order quantifiers such as {\\em all} and\n{\\em some}, as well as on ICL with linear functions. We identify Softmax, the\nscoring function in attention mechanism, as a contributing factor to these\nconstraints. To address this, we propose \\textbf{scaled signed averaging\n(SSA)}, a novel alternative to Softmax. Empirical results show that SSA\ndramatically improves performance on our target tasks. Furthermore, we evaluate\nboth encoder-only and decoder-only transformers models with SSA, demonstrating\nthat they match or exceed their Softmax-based counterparts across a variety of\nlinguistic probing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit a remarkable capacity to learn by\nanalogy, known as in-context learning (ICL). However, recent studies have\nrevealed limitations in this ability. In this paper, we examine these\nlimitations on tasks involving first-order quantifiers such as {\\em all} and\n{\\em some}, as well as on ICL with linear functions. We identify Softmax, the\nscoring function in attention mechanism, as a contributing factor to these\nconstraints. To address this, we propose \\textbf{scaled signed averaging\n(SSA)}, a novel alternative to Softmax. Empirical results show that SSA\ndramatically improves performance on our target tasks. Furthermore, we evaluate\nboth encoder-only and decoder-only transformers models with SSA, demonstrating\nthat they match or exceed their Softmax-based counterparts across a variety of\nlinguistic probing tasks."
                },
                "authors": [
                    {
                        "name": "Omar Naim"
                    },
                    {
                        "name": "Swarnadeep Bhar"
                    },
                    {
                        "name": "Jérôme Bolte"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14681v1",
                "updated": "2025-08-20T12:54:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    54,
                    58,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T12:54:58Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    54,
                    58,
                    2,
                    232,
                    0
                ],
                "title": "Virtual Multiplex Staining for Histological Images using a Marker-wise\n  Conditioned Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Multiplex Staining for Histological Images using a Marker-wise\n  Conditioned Diffusion Model"
                },
                "summary": "Multiplex imaging is revolutionizing pathology by enabling the simultaneous\nvisualization of multiple biomarkers within tissue samples, providing\nmolecular-level insights that traditional hematoxylin and eosin (H&E) staining\ncannot provide. However, the complexity and cost of multiplex data acquisition\nhave hindered its widespread adoption. Additionally, most existing large\nrepositories of H&E images lack corresponding multiplex images, limiting\nopportunities for multimodal analysis. To address these challenges, we leverage\nrecent advances in latent diffusion models (LDMs), which excel at modeling\ncomplex data distributions utilizing their powerful priors for fine-tuning to a\ntarget domain. In this paper, we introduce a novel framework for virtual\nmultiplex staining that utilizes pretrained LDM parameters to generate\nmultiplex images from H&E images using a conditional diffusion model. Our\napproach enables marker-by-marker generation by conditioning the diffusion\nmodel on each marker, while sharing the same architecture across all markers.\nTo tackle the challenge of varying pixel value distributions across different\nmarker stains and to improve inference speed, we fine-tune the model for\nsingle-step sampling, enhancing both color contrast fidelity and inference\nefficiency through pixel-level loss functions. We validate our framework on two\npublicly available datasets, notably demonstrating its effectiveness in\ngenerating up to 18 different marker types with improved accuracy, a\nsubstantial increase over the 2-3 marker types achieved in previous approaches.\nThis validation highlights the potential of our framework, pioneering virtual\nmultiplex staining. Finally, this paper bridges the gap between H&E and\nmultiplex imaging, potentially enabling retrospective studies and large-scale\nanalyses of existing H&E image repositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiplex imaging is revolutionizing pathology by enabling the simultaneous\nvisualization of multiple biomarkers within tissue samples, providing\nmolecular-level insights that traditional hematoxylin and eosin (H&E) staining\ncannot provide. However, the complexity and cost of multiplex data acquisition\nhave hindered its widespread adoption. Additionally, most existing large\nrepositories of H&E images lack corresponding multiplex images, limiting\nopportunities for multimodal analysis. To address these challenges, we leverage\nrecent advances in latent diffusion models (LDMs), which excel at modeling\ncomplex data distributions utilizing their powerful priors for fine-tuning to a\ntarget domain. In this paper, we introduce a novel framework for virtual\nmultiplex staining that utilizes pretrained LDM parameters to generate\nmultiplex images from H&E images using a conditional diffusion model. Our\napproach enables marker-by-marker generation by conditioning the diffusion\nmodel on each marker, while sharing the same architecture across all markers.\nTo tackle the challenge of varying pixel value distributions across different\nmarker stains and to improve inference speed, we fine-tune the model for\nsingle-step sampling, enhancing both color contrast fidelity and inference\nefficiency through pixel-level loss functions. We validate our framework on two\npublicly available datasets, notably demonstrating its effectiveness in\ngenerating up to 18 different marker types with improved accuracy, a\nsubstantial increase over the 2-3 marker types achieved in previous approaches.\nThis validation highlights the potential of our framework, pioneering virtual\nmultiplex staining. Finally, this paper bridges the gap between H&E and\nmultiplex imaging, potentially enabling retrospective studies and large-scale\nanalyses of existing H&E image repositories."
                },
                "authors": [
                    {
                        "name": "Hyun-Jic Oh"
                    },
                    {
                        "name": "Junsik Kim"
                    },
                    {
                        "name": "Zhiyi Shi"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Yu-An Chen"
                    },
                    {
                        "name": "Peter K. Sorger"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    },
                    {
                        "name": "Won-Ki Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Won-Ki Jeong"
                },
                "author": "Won-Ki Jeong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07773v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07773v4",
                "updated": "2025-08-20T12:20:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    20,
                    55,
                    2,
                    232,
                    0
                ],
                "published": "2025-05-12T17:23:34Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    23,
                    34,
                    0,
                    132,
                    0
                ],
                "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving"
                },
                "summary": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}."
                },
                "authors": [
                    {
                        "name": "Xinji Mai"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Xing W"
                    },
                    {
                        "name": "Weinong Wang"
                    },
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Wenqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Zhang"
                },
                "author": "Wenqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07773v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07773v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13628v2",
                "updated": "2025-08-20T12:14:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    14,
                    16,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-19T08:41:49Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    41,
                    49,
                    1,
                    231,
                    0
                ],
                "title": "DiffIER: Optimizing Diffusion Models with Iterative Error Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffIER: Optimizing Diffusion Models with Iterative Error Reduction"
                },
                "summary": "Diffusion models have demonstrated remarkable capabilities in generating\nhigh-quality samples and enhancing performance across diverse domains through\nClassifier-Free Guidance (CFG). However, the quality of generated samples is\nhighly sensitive to the selection of the guidance weight. In this work, we\nidentify a critical ``training-inference gap'' and we argue that it is the\npresence of this gap that undermines the performance of conditional generation\nand renders outputs highly sensitive to the guidance weight. We quantify this\ngap by measuring the accumulated error during the inference stage and establish\na correlation between the selection of guidance weight and minimizing this gap.\nFurthermore, to mitigate this gap, we propose DiffIER, an optimization-based\nmethod for high-quality generation. We demonstrate that the accumulated error\ncan be effectively reduced by an iterative error minimization at each step\nduring inference. By introducing this novel plug-and-play optimization\nframework, we enable the optimization of errors at every single inference step\nand enhance generation quality. Empirical results demonstrate that our proposed\nmethod outperforms baseline approaches in conditional generation tasks.\nFurthermore, the method achieves consistent success in text-to-image\ngeneration, image super-resolution, and text-to-speech generation, underscoring\nits versatility and potential for broad applications in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable capabilities in generating\nhigh-quality samples and enhancing performance across diverse domains through\nClassifier-Free Guidance (CFG). However, the quality of generated samples is\nhighly sensitive to the selection of the guidance weight. In this work, we\nidentify a critical ``training-inference gap'' and we argue that it is the\npresence of this gap that undermines the performance of conditional generation\nand renders outputs highly sensitive to the guidance weight. We quantify this\ngap by measuring the accumulated error during the inference stage and establish\na correlation between the selection of guidance weight and minimizing this gap.\nFurthermore, to mitigate this gap, we propose DiffIER, an optimization-based\nmethod for high-quality generation. We demonstrate that the accumulated error\ncan be effectively reduced by an iterative error minimization at each step\nduring inference. By introducing this novel plug-and-play optimization\nframework, we enable the optimization of errors at every single inference step\nand enhance generation quality. Empirical results demonstrate that our proposed\nmethod outperforms baseline approaches in conditional generation tasks.\nFurthermore, the method achieves consistent success in text-to-image\ngeneration, image super-resolution, and text-to-speech generation, underscoring\nits versatility and potential for broad applications in future research."
                },
                "authors": [
                    {
                        "name": "Ao Chen"
                    },
                    {
                        "name": "Lihe Ding"
                    },
                    {
                        "name": "Tianfan Xue"
                    }
                ],
                "author_detail": {
                    "name": "Tianfan Xue"
                },
                "author": "Tianfan Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14655v1",
                "updated": "2025-08-20T12:13:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    13,
                    48,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T12:13:48Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    13,
                    48,
                    2,
                    232,
                    0
                ],
                "title": "Identifying Monochromatic Signals in LISA and Taiji via Spectral Split:\n  Gravitational Waves versus Ultralight Dark Matter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Monochromatic Signals in LISA and Taiji via Spectral Split:\n  Gravitational Waves versus Ultralight Dark Matter"
                },
                "summary": "The detection of gravitational waves (GWs) has opened a new window to explore\nthe dark Universe. Ultralight dark matter (ULDM), an attractive candidate for\ndark matter, might induce monochromatic signals in gravitational-wave (GW)\nlaser interferometers. However it is not clear how such signals are\ndisentangled from the GWs emitted by galactic compact binaries. Here we\ninitiate the investigation on the spectral split of monochromatic signals\ncaused by detector's heliocentric motion in space and show the annual\nmodulation can induce distinct structures in the spectral harmonics for GWs and\nULDM, which would enable to clearly identify the nature of the signal. We show\nthe physical parameters can be inferred with high precision using the Fisher\nmatrix formalism. Our results provide a practical algorithm for probing ULDM\nand broaden the scientific objectives of future GW detectors in space, such as\nLISA and Taiji.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of gravitational waves (GWs) has opened a new window to explore\nthe dark Universe. Ultralight dark matter (ULDM), an attractive candidate for\ndark matter, might induce monochromatic signals in gravitational-wave (GW)\nlaser interferometers. However it is not clear how such signals are\ndisentangled from the GWs emitted by galactic compact binaries. Here we\ninitiate the investigation on the spectral split of monochromatic signals\ncaused by detector's heliocentric motion in space and show the annual\nmodulation can induce distinct structures in the spectral harmonics for GWs and\nULDM, which would enable to clearly identify the nature of the signal. We show\nthe physical parameters can be inferred with high precision using the Fisher\nmatrix formalism. Our results provide a practical algorithm for probing ULDM\nand broaden the scientific objectives of future GW detectors in space, such as\nLISA and Taiji."
                },
                "authors": [
                    {
                        "name": "Yue-Hui Yao"
                    },
                    {
                        "name": "Tingyuan Jiang"
                    },
                    {
                        "name": "Wenyan Ren"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Yong Tang"
                    },
                    {
                        "name": "Yu-Feng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Feng Zhou"
                },
                "author": "Yu-Feng Zhou",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14654v1",
                "updated": "2025-08-20T12:13:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    13,
                    3,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T12:13:03Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    13,
                    3,
                    2,
                    232,
                    0
                ],
                "title": "Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent\n  Framework with LLM and Knowledge Graph Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent\n  Framework with LLM and Knowledge Graph Integration"
                },
                "summary": "In recent years, the increasing frequency of extreme urban rainfall events\nhas posed significant challenges to emergency scheduling systems. Urban\nflooding often leads to severe traffic congestion and service disruptions,\nthreatening public safety and mobility. However, effective decision making\nremains hindered by three key challenges: (1) managing trade-offs among\ncompeting goals (e.g., traffic flow, task completion, and risk mitigation)\nrequires dynamic, context-aware strategies; (2) rapidly evolving environmental\nconditions render static rules inadequate; and (3) LLM-generated strategies\nfrequently suffer from semantic instability and execution inconsistency.\nExisting methods fail to align perception, global optimization, and multi-agent\ncoordination within a unified framework. To tackle these challenges, we\nintroduce H-J, a hierarchical multi-agent framework that integrates\nknowledge-guided prompting, entropy-constrained generation, and feedback-driven\noptimization. The framework establishes a closed-loop pipeline spanning from\nmulti-source perception to strategic execution and continuous refinement. We\nevaluate H-J on real-world urban topology and rainfall data under three\nrepresentative conditions: extreme rainfall, intermittent bursts, and daily\nlight rain. Experiments show that H-J outperforms rule-based and\nreinforcement-learning baselines in traffic smoothness, task success rate, and\nsystem robustness. These findings highlight the promise of uncertainty-aware,\nknowledge-constrained LLM-based approaches for enhancing resilience in urban\nflood response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the increasing frequency of extreme urban rainfall events\nhas posed significant challenges to emergency scheduling systems. Urban\nflooding often leads to severe traffic congestion and service disruptions,\nthreatening public safety and mobility. However, effective decision making\nremains hindered by three key challenges: (1) managing trade-offs among\ncompeting goals (e.g., traffic flow, task completion, and risk mitigation)\nrequires dynamic, context-aware strategies; (2) rapidly evolving environmental\nconditions render static rules inadequate; and (3) LLM-generated strategies\nfrequently suffer from semantic instability and execution inconsistency.\nExisting methods fail to align perception, global optimization, and multi-agent\ncoordination within a unified framework. To tackle these challenges, we\nintroduce H-J, a hierarchical multi-agent framework that integrates\nknowledge-guided prompting, entropy-constrained generation, and feedback-driven\noptimization. The framework establishes a closed-loop pipeline spanning from\nmulti-source perception to strategic execution and continuous refinement. We\nevaluate H-J on real-world urban topology and rainfall data under three\nrepresentative conditions: extreme rainfall, intermittent bursts, and daily\nlight rain. Experiments show that H-J outperforms rule-based and\nreinforcement-learning baselines in traffic smoothness, task success rate, and\nsystem robustness. These findings highlight the promise of uncertainty-aware,\nknowledge-constrained LLM-based approaches for enhancing resilience in urban\nflood response."
                },
                "authors": [
                    {
                        "name": "Peilin Ji"
                    },
                    {
                        "name": "Xiao Xue"
                    },
                    {
                        "name": "Simeng Wang"
                    },
                    {
                        "name": "Wenhao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Yan"
                },
                "author": "Wenhao Yan",
                "arxiv_comment": "17 pages including appendix, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14635v1",
                "updated": "2025-08-20T11:44:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    11,
                    44,
                    10,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T11:44:10Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    11,
                    44,
                    10,
                    2,
                    232,
                    0
                ],
                "title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware\n  Planning and Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware\n  Planning and Coordination"
                },
                "summary": "The ability to coordinate actions across multiple agents is critical for\nsolving complex, real-world problems. Large Language Models (LLMs) have shown\nstrong capabilities in communication, planning, and reasoning, raising the\nquestion of whether they can also support effective collaboration in\nmulti-agent settings. In this work, we investigate the use of LLM agents to\nsolve a structured victim rescue task that requires division of labor,\nprioritization, and cooperative planning. Agents operate in a fully known\ngraph-based environment and must allocate resources to victims with varying\nneeds and urgency levels. We systematically evaluate their performance using a\nsuite of coordination-sensitive metrics, including task success rate, redundant\nactions, room conflicts, and urgency-weighted efficiency. This study offers new\ninsights into the strengths and failure modes of LLMs in physically grounded\nmulti-agent collaboration tasks, contributing to future benchmarks and\narchitectural improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to coordinate actions across multiple agents is critical for\nsolving complex, real-world problems. Large Language Models (LLMs) have shown\nstrong capabilities in communication, planning, and reasoning, raising the\nquestion of whether they can also support effective collaboration in\nmulti-agent settings. In this work, we investigate the use of LLM agents to\nsolve a structured victim rescue task that requires division of labor,\nprioritization, and cooperative planning. Agents operate in a fully known\ngraph-based environment and must allocate resources to victims with varying\nneeds and urgency levels. We systematically evaluate their performance using a\nsuite of coordination-sensitive metrics, including task success rate, redundant\nactions, room conflicts, and urgency-weighted efficiency. This study offers new\ninsights into the strengths and failure modes of LLMs in physically grounded\nmulti-agent collaboration tasks, contributing to future benchmarks and\narchitectural improvements."
                },
                "authors": [
                    {
                        "name": "João Vitor de Carvalho Silva"
                    },
                    {
                        "name": "Douglas G. Macharet"
                    }
                ],
                "author_detail": {
                    "name": "Douglas G. Macharet"
                },
                "author": "Douglas G. Macharet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11198v2",
                "updated": "2025-08-20T10:52:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    52,
                    47,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-15T04:14:01Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    14,
                    1,
                    4,
                    227,
                    0
                ],
                "title": "Learning to Restore Heisenberg Limit in Noisy Quantum Sensing via\n  Quantum Digital Twin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Restore Heisenberg Limit in Noisy Quantum Sensing via\n  Quantum Digital Twin"
                },
                "summary": "Quantum sensors leverage nonclassical resources to achieve sensing precision\nat the Heisenberg limit, surpassing the standard quantum limit attainable\nthrough classical strategies. However, a critical issue is that the\nenvironmental noise induces rapid decoherence, fundamentally limiting the\nrealizability of the Heisenberg limit. In this Letter, we propose a quantum\ndigital twin protocol to overcome this issue. The protocol first establishes\nobservable-constrained state reconstruction to infer random errors in the\ndecoherence process, and then utilizes reinforcement learning to derive\nadaptive compensatory control strategies. Demonstrated across discrete,\ncontinuous variable and multi-qubit circuit systems, our approach bypasses\nquantum state tomography's exponential overhead and discovers optimal control\nschemes to restore the Heisenberg limit. Unlike quantum error correction or\nmitigation schemes requiring precise noise characterization and ancillary\nqubits, our autonomous protocol achieves noise-resilient sensing through\nenvironment-adaptive control sequencing. This work establishes quantum digital\ntwin as a generic methodology for quantum control, proposing a noise-immune\nparadigm for next-generation quantum sensors compatible with NISQ-era\nexperimental constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum sensors leverage nonclassical resources to achieve sensing precision\nat the Heisenberg limit, surpassing the standard quantum limit attainable\nthrough classical strategies. However, a critical issue is that the\nenvironmental noise induces rapid decoherence, fundamentally limiting the\nrealizability of the Heisenberg limit. In this Letter, we propose a quantum\ndigital twin protocol to overcome this issue. The protocol first establishes\nobservable-constrained state reconstruction to infer random errors in the\ndecoherence process, and then utilizes reinforcement learning to derive\nadaptive compensatory control strategies. Demonstrated across discrete,\ncontinuous variable and multi-qubit circuit systems, our approach bypasses\nquantum state tomography's exponential overhead and discovers optimal control\nschemes to restore the Heisenberg limit. Unlike quantum error correction or\nmitigation schemes requiring precise noise characterization and ancillary\nqubits, our autonomous protocol achieves noise-resilient sensing through\nenvironment-adaptive control sequencing. This work establishes quantum digital\ntwin as a generic methodology for quantum control, proposing a noise-immune\nparadigm for next-generation quantum sensors compatible with NISQ-era\nexperimental constraints."
                },
                "authors": [
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Tailong Xiao"
                    },
                    {
                        "name": "Jingzheng Huang"
                    },
                    {
                        "name": "Jianping Fan"
                    },
                    {
                        "name": "Guihua Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Guihua Zeng"
                },
                "author": "Guihua Zeng",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03082v2",
                "updated": "2025-08-20T10:33:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    33,
                    40,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-05T04:55:03Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    4,
                    55,
                    3,
                    1,
                    217,
                    0
                ],
                "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic\n  Design"
                },
                "summary": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yilu Liu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14596v1",
                "updated": "2025-08-20T10:27:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    27,
                    23,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T10:27:23Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    27,
                    23,
                    2,
                    232,
                    0
                ],
                "title": "Sequential Correct Screening and Post-Screening Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Correct Screening and Post-Screening Inference"
                },
                "summary": "Selecting the top-$m$ variables with the $m$ largest population parameters\nfrom a larger set of candidates is a fundamental problem in statistics. In this\npaper, we propose a novel methodology called Sequential Correct Screening\n(SCS), which sequentially screens out variables that are not among the top-$m$.\nA key feature of our method is its anytime validity; it provides a sequence of\nvariable subsets that, with high probability, always contain the true top-$m$\nvariables. Furthermore, we develop a post-screening inference (PSI) procedure\nto construct confidence intervals for the selected parameters. Importantly,\nthis procedure is designed to control the false coverage rate (FCR) whenever it\nis conducted -- an aspect that has been largely overlooked in the existing\nliterature. We establish theoretical guarantees for both SCS and PSI, and\ndemonstrate their performance through simulation studies and an application to\na real-world dataset on suicide rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting the top-$m$ variables with the $m$ largest population parameters\nfrom a larger set of candidates is a fundamental problem in statistics. In this\npaper, we propose a novel methodology called Sequential Correct Screening\n(SCS), which sequentially screens out variables that are not among the top-$m$.\nA key feature of our method is its anytime validity; it provides a sequence of\nvariable subsets that, with high probability, always contain the true top-$m$\nvariables. Furthermore, we develop a post-screening inference (PSI) procedure\nto construct confidence intervals for the selected parameters. Importantly,\nthis procedure is designed to control the false coverage rate (FCR) whenever it\nis conducted -- an aspect that has been largely overlooked in the existing\nliterature. We establish theoretical guarantees for both SCS and PSI, and\ndemonstrate their performance through simulation studies and an application to\na real-world dataset on suicide rates."
                },
                "authors": [
                    {
                        "name": "Masaki Toyoda"
                    },
                    {
                        "name": "Yoshimasa Uematsu"
                    }
                ],
                "author_detail": {
                    "name": "Yoshimasa Uematsu"
                },
                "author": "Yoshimasa Uematsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12581v2",
                "updated": "2025-08-20T10:21:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    21,
                    22,
                    2,
                    232,
                    0
                ],
                "published": "2024-05-21T08:24:38Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    8,
                    24,
                    38,
                    1,
                    142,
                    0
                ],
                "title": "Spectral analysis for the inference of noisy Hawkes processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral analysis for the inference of noisy Hawkes processes"
                },
                "summary": "Classic estimation methods for Hawkes processes rely on the assumption that\nobserved event times are indeed a realisation of a Hawkes process, without\nconsidering any potential perturbation of the model. However, in practice,\nobservations are often altered by some noise, the form of which depends on the\ncontext. It is then required to model the alteration mechanism in order to\ninfer accurately such a noisy Hawkes process. While several models exist, we\nconsider, in this work, the observations to be the indistinguishable union of\nevent times coming from a Hawkes process and from an independent Poisson\nprocess. Since standard inference methods (such as maximum likelihood or\nExpectation-Maximisation) are either unworkable or numerically prohibitive in\nthis context, we propose an estimation procedure based on the spectral analysis\nof second order properties of the noisy Hawkes process. Novel results include\nsufficient conditions for identifiability of the ensuing statistical model with\nexponential interaction functions for both univariate and bivariate processes,\nalong with consistency and asymptotic normality guarantees of our estimator in\nthe univariate case. Although we mainly focus on the exponential scenario,\nother types of kernels are investigated and discussed. A new estimator based on\nmaximising the spectral log-likelihood is then described, and its behaviour is\nnumerically illustrated on both synthetic data and neuronal data. Besides being\nfree from knowing the source of each observed time (Hawkes or Poisson process),\nthe proposed estimator is shown to perform accurately in estimating both\nprocesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classic estimation methods for Hawkes processes rely on the assumption that\nobserved event times are indeed a realisation of a Hawkes process, without\nconsidering any potential perturbation of the model. However, in practice,\nobservations are often altered by some noise, the form of which depends on the\ncontext. It is then required to model the alteration mechanism in order to\ninfer accurately such a noisy Hawkes process. While several models exist, we\nconsider, in this work, the observations to be the indistinguishable union of\nevent times coming from a Hawkes process and from an independent Poisson\nprocess. Since standard inference methods (such as maximum likelihood or\nExpectation-Maximisation) are either unworkable or numerically prohibitive in\nthis context, we propose an estimation procedure based on the spectral analysis\nof second order properties of the noisy Hawkes process. Novel results include\nsufficient conditions for identifiability of the ensuing statistical model with\nexponential interaction functions for both univariate and bivariate processes,\nalong with consistency and asymptotic normality guarantees of our estimator in\nthe univariate case. Although we mainly focus on the exponential scenario,\nother types of kernels are investigated and discussed. A new estimator based on\nmaximising the spectral log-likelihood is then described, and its behaviour is\nnumerically illustrated on both synthetic data and neuronal data. Besides being\nfree from knowing the source of each observed time (Hawkes or Poisson process),\nthe proposed estimator is shown to perform accurately in estimating both\nprocesses."
                },
                "authors": [
                    {
                        "name": "Anna Bonnet"
                    },
                    {
                        "name": "Felix Cheysson"
                    },
                    {
                        "name": "Miguel Martinez Herrera"
                    },
                    {
                        "name": "Maxime Sangnier"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Sangnier"
                },
                "arxiv_affiliation": "LPSM",
                "author": "Maxime Sangnier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03984v2",
                "updated": "2025-08-20T10:14:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    14,
                    24,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-05T10:23:40Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    23,
                    40,
                    5,
                    186,
                    0
                ],
                "title": "CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via\n  Chain-of-Thought Reasoning"
                },
                "summary": "Effective Out-of-Distribution (OOD) detection is criti-cal for ensuring the\nreliability of semantic segmentation models, particularly in complex road\nenvironments where safety and accuracy are paramount. Despite recent\nadvancements in large language models (LLMs), notably GPT-4, which\nsignificantly enhanced multimodal reasoning through Chain-of-Thought (CoT)\nprompting, the application of CoT-based visual reasoning for OOD semantic\nsegmentation remains largely unexplored. In this paper, through extensive\nanalyses of the road scene anomalies, we identify three challenging scenarios\nwhere current state-of-the-art OOD segmentation methods consistently struggle:\n(1) densely packed and overlapping objects, (2) distant scenes with small\nobjects, and (3) large foreground-dominant objects. To address the presented\nchallenges, we propose a novel CoT-based framework targeting OOD detection in\nroad anomaly scenes. Our method leverages the extensive knowledge and reasoning\ncapabilities of foundation models, such as GPT-4, to enhance OOD detection\nthrough improved image understanding and prompt-based reasoning aligned with\nobserved problematic scene attributes. Extensive experiments show that our\nframework consistently outperforms state-of-the-art methods on both standard\nbenchmarks and our newly defined challenging subset of the RoadAnomaly dataset,\noffering a robust and interpretable solution for OOD semantic segmentation in\ncomplex driving environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Out-of-Distribution (OOD) detection is criti-cal for ensuring the\nreliability of semantic segmentation models, particularly in complex road\nenvironments where safety and accuracy are paramount. Despite recent\nadvancements in large language models (LLMs), notably GPT-4, which\nsignificantly enhanced multimodal reasoning through Chain-of-Thought (CoT)\nprompting, the application of CoT-based visual reasoning for OOD semantic\nsegmentation remains largely unexplored. In this paper, through extensive\nanalyses of the road scene anomalies, we identify three challenging scenarios\nwhere current state-of-the-art OOD segmentation methods consistently struggle:\n(1) densely packed and overlapping objects, (2) distant scenes with small\nobjects, and (3) large foreground-dominant objects. To address the presented\nchallenges, we propose a novel CoT-based framework targeting OOD detection in\nroad anomaly scenes. Our method leverages the extensive knowledge and reasoning\ncapabilities of foundation models, such as GPT-4, to enhance OOD detection\nthrough improved image understanding and prompt-based reasoning aligned with\nobserved problematic scene attributes. Extensive experiments show that our\nframework consistently outperforms state-of-the-art methods on both standard\nbenchmarks and our newly defined challenging subset of the RoadAnomaly dataset,\noffering a robust and interpretable solution for OOD semantic segmentation in\ncomplex driving environments."
                },
                "authors": [
                    {
                        "name": "Jeonghyo Song"
                    },
                    {
                        "name": "Kimin Yun"
                    },
                    {
                        "name": "DaeUng Jo"
                    },
                    {
                        "name": "Jinyoung Kim"
                    },
                    {
                        "name": "Youngjoon Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoon Yoo"
                },
                "author": "Youngjoon Yoo",
                "arxiv_comment": "6 pages, 3 figures. Accepted at IEEE International Conference on\n  Advanced Visual and Signal-Based Systems 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12096v2",
                "updated": "2025-08-20T09:52:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    52,
                    0,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-16T16:36:43Z",
                "published_parsed": [
                    2025,
                    8,
                    16,
                    16,
                    36,
                    43,
                    5,
                    228,
                    0
                ],
                "title": "STEM: Efficient Relative Capability Evaluation of LLMs through\n  Structured Transition Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STEM: Efficient Relative Capability Evaluation of LLMs through\n  Structured Transition Samples"
                },
                "summary": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Haiquan Hu"
                    },
                    {
                        "name": "Jiazhi Jiang"
                    },
                    {
                        "name": "Shiyou Xu"
                    },
                    {
                        "name": "Ruhan Zeng"
                    },
                    {
                        "name": "Tian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tian Wang"
                },
                "author": "Tian Wang",
                "arxiv_comment": "Submit to AAAI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14496v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14496v3",
                "updated": "2025-08-20T09:42:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    42,
                    24,
                    2,
                    232,
                    0
                ],
                "published": "2025-02-20T12:26:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    26,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for\n  Interactive Environment Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for\n  Interactive Environment Generalization"
                },
                "summary": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Published in COLM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14496v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14496v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14564v1",
                "updated": "2025-08-20T09:36:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    36,
                    53,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T09:36:53Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    36,
                    53,
                    2,
                    232,
                    0
                ],
                "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic\n  Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Sees What? Structured Thought-Action Sequences for Epistemic\n  Reasoning in LLMs"
                },
                "summary": "Recent advances in large language models (LLMs) and reasoning frameworks have\nopened new possibilities for improving the perspective -taking capabilities of\nautonomous agents. However, tasks that involve active perception, collaborative\nreasoning, and perspective taking (understanding what another agent can see or\nknows) pose persistent challenges for current LLM-based systems. This study\ninvestigates the potential of structured examples derived from transformed\nsolution graphs generated by the Fast Downward planner to improve the\nperformance of LLM-based agents within a ReAct framework. We propose a\nstructured solution-processing pipeline that generates three distinct\ncategories of examples: optimal goal paths (G-type), informative node paths\n(E-type), and step-by-step optimal decision sequences contrasting alternative\nactions (L-type). These solutions are further converted into ``thought-action''\nexamples by prompting an LLM to explicitly articulate the reasoning behind each\ndecision. While L-type examples slightly reduce clarification requests and\noverall action steps, they do not yield consistent improvements. Agents are\nsuccessful in tasks requiring basic attentional filtering but struggle in\nscenarios that required mentalising about occluded spaces or weighing the costs\nof epistemic actions. These findings suggest that structured examples alone are\ninsufficient for robust perspective-taking, underscoring the need for explicit\nbelief tracking, cost modelling, and richer environments to enable socially\ngrounded collaboration in LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and reasoning frameworks have\nopened new possibilities for improving the perspective -taking capabilities of\nautonomous agents. However, tasks that involve active perception, collaborative\nreasoning, and perspective taking (understanding what another agent can see or\nknows) pose persistent challenges for current LLM-based systems. This study\ninvestigates the potential of structured examples derived from transformed\nsolution graphs generated by the Fast Downward planner to improve the\nperformance of LLM-based agents within a ReAct framework. We propose a\nstructured solution-processing pipeline that generates three distinct\ncategories of examples: optimal goal paths (G-type), informative node paths\n(E-type), and step-by-step optimal decision sequences contrasting alternative\nactions (L-type). These solutions are further converted into ``thought-action''\nexamples by prompting an LLM to explicitly articulate the reasoning behind each\ndecision. While L-type examples slightly reduce clarification requests and\noverall action steps, they do not yield consistent improvements. Agents are\nsuccessful in tasks requiring basic attentional filtering but struggle in\nscenarios that required mentalising about occluded spaces or weighing the costs\nof epistemic actions. These findings suggest that structured examples alone are\ninsufficient for robust perspective-taking, underscoring the need for explicit\nbelief tracking, cost modelling, and richer environments to enable socially\ngrounded collaboration in LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Luca Annese"
                    },
                    {
                        "name": "Sabrina Patania"
                    },
                    {
                        "name": "Silvia Serino"
                    },
                    {
                        "name": "Tom Foulsham"
                    },
                    {
                        "name": "Silvia Rossi"
                    },
                    {
                        "name": "Azzurra Ruggeri"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    }
                ],
                "author_detail": {
                    "name": "Dimitri Ognibene"
                },
                "author": "Dimitri Ognibene",
                "arxiv_comment": "Accepted at ICSR25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.10; I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17792v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17792v4",
                "updated": "2025-08-20T09:29:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    29,
                    46,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-23T10:35:37Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    35,
                    37,
                    2,
                    204,
                    0
                ],
                "title": "Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple\n  Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple\n  Domains"
                },
                "summary": "To gain deeper insights into a complex sensor system through the lens of\ncausality, we present common and individual causal mechanism estimation\n(CICME), a novel three-step approach to inferring causal mechanisms from\nheterogeneous data collected across multiple domains. By leveraging the\nprinciple of Causal Transfer Learning (CTL), CICME is able to reliably detect\ndomain-invariant causal mechanisms when provided with sufficient samples. The\nidentified common causal mechanisms are further used to guide the estimation of\nthe remaining causal mechanisms in each domain individually. The performance of\nCICME is evaluated on linear Gaussian models under scenarios inspired from a\nmanufacturing process. Building upon existing continuous optimization-based\ncausal discovery methods, we show that CICME leverages the benefits of applying\ncausal discovery on the pooled data and repeatedly on data from individual\ndomains, and it even outperforms both baseline methods under certain scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To gain deeper insights into a complex sensor system through the lens of\ncausality, we present common and individual causal mechanism estimation\n(CICME), a novel three-step approach to inferring causal mechanisms from\nheterogeneous data collected across multiple domains. By leveraging the\nprinciple of Causal Transfer Learning (CTL), CICME is able to reliably detect\ndomain-invariant causal mechanisms when provided with sufficient samples. The\nidentified common causal mechanisms are further used to guide the estimation of\nthe remaining causal mechanisms in each domain individually. The performance of\nCICME is evaluated on linear Gaussian models under scenarios inspired from a\nmanufacturing process. Building upon existing continuous optimization-based\ncausal discovery methods, we show that CICME leverages the benefits of applying\ncausal discovery on the pooled data and repeatedly on data from individual\ndomains, and it even outperforms both baseline methods under certain scenarios."
                },
                "authors": [
                    {
                        "name": "Jingyi Yu"
                    },
                    {
                        "name": "Tim Pychynski"
                    },
                    {
                        "name": "Marco F. Huber"
                    }
                ],
                "author_detail": {
                    "name": "Marco F. Huber"
                },
                "author": "Marco F. Huber",
                "arxiv_comment": "To appear in 2025 28th International Conference on Information Fusion\n  (FUSION)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17792v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17792v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14553v1",
                "updated": "2025-08-20T09:14:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    14,
                    48,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T09:14:48Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    14,
                    48,
                    2,
                    232,
                    0
                ],
                "title": "Towards LLM-generated explanations for Component-based Knowledge Graph\n  Question Answering Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM-generated explanations for Component-based Knowledge Graph\n  Question Answering Systems"
                },
                "summary": "Over time, software systems have reached a level of complexity that makes it\ndifficult for their developers and users to explain particular decisions made\nby them. In this paper, we focus on the explainability of component-based\nsystems for Question Answering (QA). These components often conduct processes\ndriven by AI methods, in which behavior and decisions cannot be clearly\nexplained or justified, s.t., even for QA experts interpreting the executed\nprocess and its results is hard. To address this challenge, we present an\napproach that considers the components' input and output data flows as a source\nfor representing the behavior and provide explanations for the components,\nenabling users to comprehend what happened. In the QA framework used here, the\ndata flows of the components are represented as SPARQL queries (inputs) and RDF\ntriples (outputs). Hence, we are also providing valuable insights on\nverbalization regarding these data types. In our experiments, the approach\ngenerates explanations while following template-based settings (baseline) or\nvia the use of Large Language Models (LLMs) with different configurations\n(automatic generation). Our evaluation shows that the explanations generated\nvia LLMs achieve high quality and mostly outperform template-based approaches\naccording to the users' ratings. Therefore, it enables us to automatically\nexplain the behavior and decisions of QA components to humans while using RDF\nand SPARQL as a context for explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over time, software systems have reached a level of complexity that makes it\ndifficult for their developers and users to explain particular decisions made\nby them. In this paper, we focus on the explainability of component-based\nsystems for Question Answering (QA). These components often conduct processes\ndriven by AI methods, in which behavior and decisions cannot be clearly\nexplained or justified, s.t., even for QA experts interpreting the executed\nprocess and its results is hard. To address this challenge, we present an\napproach that considers the components' input and output data flows as a source\nfor representing the behavior and provide explanations for the components,\nenabling users to comprehend what happened. In the QA framework used here, the\ndata flows of the components are represented as SPARQL queries (inputs) and RDF\ntriples (outputs). Hence, we are also providing valuable insights on\nverbalization regarding these data types. In our experiments, the approach\ngenerates explanations while following template-based settings (baseline) or\nvia the use of Large Language Models (LLMs) with different configurations\n(automatic generation). Our evaluation shows that the explanations generated\nvia LLMs achieve high quality and mostly outperform template-based approaches\naccording to the users' ratings. Therefore, it enables us to automatically\nexplain the behavior and decisions of QA components to humans while using RDF\nand SPARQL as a context for explanations."
                },
                "authors": [
                    {
                        "name": "Dennis Schiese"
                    },
                    {
                        "name": "Aleksandr Perevalov"
                    },
                    {
                        "name": "Andreas Both"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Both"
                },
                "author": "Andreas Both",
                "arxiv_comment": "Presented at ICWI 2024, Zagreb. Released with ISBN:\n  978-989-8704-62-7. Data source:\n  https://figshare.com/articles/dataset/Towards_LLM-generated_explanations_for_component-based_knowledge_graph_question_answering_systems/27079687",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03106v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03106v5",
                "updated": "2025-08-20T09:10:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    10,
                    5,
                    2,
                    232,
                    0
                ],
                "published": "2025-06-03T17:39:02Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    39,
                    2,
                    1,
                    154,
                    0
                ],
                "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback"
                },
                "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of spontaneous self-reflection, and\npersistent failures. We then demonstrate that RL-finetuned models, even after\nexhibiting performance plateaus, can generate correct refinements on\npersistently failed problems by leveraging natural language feedback in the\nform of critiques. Building on this insight, we propose Critique-GRPO, an\nonline RL framework that integrates both natural language and numerical\nfeedback for effective policy optimization. Critique-GRPO enables LLMs to learn\nfrom initial responses and critique-guided self-refinements simultaneously\nwhile maintaining exploration. Additionally, we employ a shaping function to\namplify learning from correct, especially unfamiliar, refinements and penalize\nincorrect ones. Extensive experiments with Qwen2.5-7B-Base,\nQwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently\noutperforms supervised learning and RL-based fine-tuning methods across eight\nchallenging mathematical, STEM, and general reasoning tasks. Specifically,\nCritique-GRPO improves average pass@1 scores across all compared methods by\napproximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably,\nCritique-GRPO enables effective self-improvement through self-critiquing,\nachieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME\n2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of spontaneous self-reflection, and\npersistent failures. We then demonstrate that RL-finetuned models, even after\nexhibiting performance plateaus, can generate correct refinements on\npersistently failed problems by leveraging natural language feedback in the\nform of critiques. Building on this insight, we propose Critique-GRPO, an\nonline RL framework that integrates both natural language and numerical\nfeedback for effective policy optimization. Critique-GRPO enables LLMs to learn\nfrom initial responses and critique-guided self-refinements simultaneously\nwhile maintaining exploration. Additionally, we employ a shaping function to\namplify learning from correct, especially unfamiliar, refinements and penalize\nincorrect ones. Extensive experiments with Qwen2.5-7B-Base,\nQwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently\noutperforms supervised learning and RL-based fine-tuning methods across eight\nchallenging mathematical, STEM, and general reasoning tasks. Specifically,\nCritique-GRPO improves average pass@1 scores across all compared methods by\napproximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably,\nCritique-GRPO enables effective self-improvement through self-critiquing,\nachieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME\n2024."
                },
                "authors": [
                    {
                        "name": "Xiaoying Zhang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "52 pages, updated with new experimental results and implementation\n  details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03106v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03106v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03047v2",
                "updated": "2025-08-20T09:09:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    9,
                    56,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-03T10:11:35Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    11,
                    35,
                    3,
                    184,
                    0
                ],
                "title": "Enhancing Temporal Sensitivity of Large Language Model for\n  Recommendation with Counterfactual Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Temporal Sensitivity of Large Language Model for\n  Recommendation with Counterfactual Tuning"
                },
                "summary": "Recent advances have applied large language models (LLMs) to sequential\nrecommendation, leveraging their pre-training knowledge and reasoning\ncapabilities to provide more personalized user experiences. However, existing\nLLM-based methods fail to sufficiently leverage the rich temporal information\ninherent in users' historical interaction sequences, stemming from fundamental\narchitectural constraints: LLMs process information through self-attention\nmechanisms that lack inherent sequence ordering and rely on position embeddings\ndesigned primarily for natural language rather than user interaction sequences.\nThis limitation significantly impairs their ability to capture the evolution of\nuser preferences over time and predict future interests accurately.\n  To address this critical gap, we propose \\underline{C}ounterfactual\n\\underline{E}nhanced \\underline{T}emporal Framework for LLM-Based\n\\underline{Rec}ommendation (CETRec). CETRec is grounded in causal inference\nprinciples, which allow it to isolate and measure the specific impact of\ntemporal information on recommendation outcomes. Combined with our\ncounterfactual tuning task derived from causal analysis, CETRec effectively\nenhances LLMs' awareness of both absolute order (how recently items were\ninteracted with) and relative order (the sequential relationships between\nitems). Extensive experiments on real-world datasets demonstrate the\neffectiveness of our CETRec. Our code is available at\nhttps://anonymous.4open.science/r/CETRec-B9CE/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have applied large language models (LLMs) to sequential\nrecommendation, leveraging their pre-training knowledge and reasoning\ncapabilities to provide more personalized user experiences. However, existing\nLLM-based methods fail to sufficiently leverage the rich temporal information\ninherent in users' historical interaction sequences, stemming from fundamental\narchitectural constraints: LLMs process information through self-attention\nmechanisms that lack inherent sequence ordering and rely on position embeddings\ndesigned primarily for natural language rather than user interaction sequences.\nThis limitation significantly impairs their ability to capture the evolution of\nuser preferences over time and predict future interests accurately.\n  To address this critical gap, we propose \\underline{C}ounterfactual\n\\underline{E}nhanced \\underline{T}emporal Framework for LLM-Based\n\\underline{Rec}ommendation (CETRec). CETRec is grounded in causal inference\nprinciples, which allow it to isolate and measure the specific impact of\ntemporal information on recommendation outcomes. Combined with our\ncounterfactual tuning task derived from causal analysis, CETRec effectively\nenhances LLMs' awareness of both absolute order (how recently items were\ninteracted with) and relative order (the sequential relationships between\nitems). Extensive experiments on real-world datasets demonstrate the\neffectiveness of our CETRec. Our code is available at\nhttps://anonymous.4open.science/r/CETRec-B9CE/."
                },
                "authors": [
                    {
                        "name": "Yutian Liu"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14544v1",
                "updated": "2025-08-20T08:55:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    55,
                    26,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:55:26Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    55,
                    26,
                    2,
                    232,
                    0
                ],
                "title": "Adaptively Robust LLM Inference Optimization under Prediction\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptively Robust LLM Inference Optimization under Prediction\n  Uncertainty"
                },
                "summary": "We study the problem of optimizing Large Language Model (LLM) inference\nscheduling to minimize total latency. LLM inference is an online and multi-task\nservice process and also heavily energy consuming by which a pre-trained LLM\nprocesses input requests and generates output tokens sequentially. Therefore,\nit is vital to improve its scheduling efficiency and reduce the power\nconsumption while a great amount of prompt requests are arriving. A key\nchallenge in LLM inference scheduling is that while the prompt length is known\nupon arrival, the output length, which critically impacts memory usage and\nprocessing time, is unknown. To address this uncertainty, we propose algorithms\nthat leverage machine learning to predict output lengths, assuming the\nprediction provides an interval classification (min-max range) for each\nrequest.\n  We first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which\nschedules requests based on the upper bound of predicted output lengths to\nprevent memory overflow. However, this approach is overly conservative: as\nprediction accuracy decreases, performance degrades significantly due to\npotential overestimation. To overcome this limitation, we propose\n$\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted\nlower bound as the output length and dynamically refines this estimate during\ninferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale\ncompetitive ratio. Through numerical simulations, we demonstrate that\n$\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler,\nhighlighting both its efficiency and robustness in practical scenarios.\nMoreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the\nprediction interval--an advantageous design choice since upper bounds on output\nlength are typically more challenging to predict accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of optimizing Large Language Model (LLM) inference\nscheduling to minimize total latency. LLM inference is an online and multi-task\nservice process and also heavily energy consuming by which a pre-trained LLM\nprocesses input requests and generates output tokens sequentially. Therefore,\nit is vital to improve its scheduling efficiency and reduce the power\nconsumption while a great amount of prompt requests are arriving. A key\nchallenge in LLM inference scheduling is that while the prompt length is known\nupon arrival, the output length, which critically impacts memory usage and\nprocessing time, is unknown. To address this uncertainty, we propose algorithms\nthat leverage machine learning to predict output lengths, assuming the\nprediction provides an interval classification (min-max range) for each\nrequest.\n  We first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which\nschedules requests based on the upper bound of predicted output lengths to\nprevent memory overflow. However, this approach is overly conservative: as\nprediction accuracy decreases, performance degrades significantly due to\npotential overestimation. To overcome this limitation, we propose\n$\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted\nlower bound as the output length and dynamically refines this estimate during\ninferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale\ncompetitive ratio. Through numerical simulations, we demonstrate that\n$\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler,\nhighlighting both its efficiency and robustness in practical scenarios.\nMoreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the\nprediction interval--an advantageous design choice since upper bounds on output\nlength are typically more challenging to predict accurately."
                },
                "authors": [
                    {
                        "name": "Zixi Chen"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14540v1",
                "updated": "2025-08-20T08:45:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    45,
                    53,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:45:53Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    45,
                    53,
                    2,
                    232,
                    0
                ],
                "title": "Post-hoc LLM-Supported Debugging of Distributed Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-hoc LLM-Supported Debugging of Distributed Processes"
                },
                "summary": "In this paper, we address the problem of manual debugging, which nowadays\nremains resource-intensive and in some parts archaic. This problem is\nespecially evident in increasingly complex and distributed software systems.\nTherefore, our objective of this work is to introduce an approach that can\npossibly be applied to any system, at both the macro- and micro-level, to ease\nthis debugging process. This approach utilizes a system's process data, in\nconjunction with generative AI, to generate natural-language explanations.\nThese explanations are generated from the actual process data, interface\ninformation, and documentation to guide the developers more efficiently to\nunderstand the behavior and possible errors of a process and its sub-processes.\nHere, we present a demonstrator that employs this approach on a component-based\nJava system. However, our approach is language-agnostic. Ideally, the generated\nexplanations will provide a good understanding of the process, even if\ndevelopers are not familiar with all the details of the considered system. Our\ndemonstrator is provided as an open-source web application that is freely\naccessible to all users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the problem of manual debugging, which nowadays\nremains resource-intensive and in some parts archaic. This problem is\nespecially evident in increasingly complex and distributed software systems.\nTherefore, our objective of this work is to introduce an approach that can\npossibly be applied to any system, at both the macro- and micro-level, to ease\nthis debugging process. This approach utilizes a system's process data, in\nconjunction with generative AI, to generate natural-language explanations.\nThese explanations are generated from the actual process data, interface\ninformation, and documentation to guide the developers more efficiently to\nunderstand the behavior and possible errors of a process and its sub-processes.\nHere, we present a demonstrator that employs this approach on a component-based\nJava system. However, our approach is language-agnostic. Ideally, the generated\nexplanations will provide a good understanding of the process, even if\ndevelopers are not familiar with all the details of the considered system. Our\ndemonstrator is provided as an open-source web application that is freely\naccessible to all users."
                },
                "authors": [
                    {
                        "name": "Dennis Schiese"
                    },
                    {
                        "name": "Andreas Both"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Both"
                },
                "author": "Andreas Both",
                "arxiv_comment": "Presented at ICWE 2025, Delft (30 June - 03 July 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05662v2",
                "updated": "2025-08-20T08:44:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    44,
                    29,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-08T04:23:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    23,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Reconstruction-Free Anomaly Detection with Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction-Free Anomaly Detection with Diffusion Models"
                },
                "summary": "Despite the remarkable success, recent reconstruction-based anomaly detection\n(AD) methods via diffusion modeling still involve fine-grained noise-strength\ntuning and computationally expensive multi-step denoising, leading to a\nfundamental tension between fidelity and efficiency. In this paper, we propose\na novel inversion-based AD approach - detection via noising in latent space -\nwhich circumvents explicit reconstruction. Importantly, we contend that the\nlimitations in prior reconstruction-based methods originate from the prevailing\ndetection via denoising in RGB space paradigm. To address this, we model AD\nunder a reconstruction-free formulation, which directly infers the final latent\nvariable corresponding to the input image via DDIM inversion, and then measures\nthe deviation based on the known prior distribution for anomaly scoring.\nSpecifically, in approximating the original probability flow ODE using the\nEuler method, we only enforce very few inversion steps to noise the clean image\nto pursue inference efficiency. As the added noise is adaptively derived with\nthe learned diffusion model, the original features for the clean testing image\ncan still be leveraged to yield high detection accuracy. We perform extensive\nexperiments and detailed analysis across three widely used image AD datasets\nunder the unsupervised unified setting to demonstrate the effectiveness of our\nmodel, regarding state-of-the-art AD performance, and about 2 times inference\ntime speedup without diffusion distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable success, recent reconstruction-based anomaly detection\n(AD) methods via diffusion modeling still involve fine-grained noise-strength\ntuning and computationally expensive multi-step denoising, leading to a\nfundamental tension between fidelity and efficiency. In this paper, we propose\na novel inversion-based AD approach - detection via noising in latent space -\nwhich circumvents explicit reconstruction. Importantly, we contend that the\nlimitations in prior reconstruction-based methods originate from the prevailing\ndetection via denoising in RGB space paradigm. To address this, we model AD\nunder a reconstruction-free formulation, which directly infers the final latent\nvariable corresponding to the input image via DDIM inversion, and then measures\nthe deviation based on the known prior distribution for anomaly scoring.\nSpecifically, in approximating the original probability flow ODE using the\nEuler method, we only enforce very few inversion steps to noise the clean image\nto pursue inference efficiency. As the added noise is adaptively derived with\nthe learned diffusion model, the original features for the clean testing image\ncan still be leveraged to yield high detection accuracy. We perform extensive\nexperiments and detailed analysis across three widely used image AD datasets\nunder the unsupervised unified setting to demonstrate the effectiveness of our\nmodel, regarding state-of-the-art AD performance, and about 2 times inference\ntime speedup without diffusion distillation."
                },
                "authors": [
                    {
                        "name": "Shunsuke Sakai"
                    },
                    {
                        "name": "Xiangteng He"
                    },
                    {
                        "name": "Chunzhi Gu"
                    },
                    {
                        "name": "Leonid Sigal"
                    },
                    {
                        "name": "Tatsuhito Hasegawa"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuhito Hasegawa"
                },
                "author": "Tatsuhito Hasegawa",
                "arxiv_comment": "Code is available at https://github.com/SkyShunsuke/InversionAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14537v1",
                "updated": "2025-08-20T08:41:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    41,
                    19,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:41:19Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    41,
                    19,
                    2,
                    232,
                    0
                ],
                "title": "WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch\n  Selection with VLM and LLM Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch\n  Selection with VLM and LLM Knowledge Fusion"
                },
                "summary": "Whole slide images (WSIs) in computational pathology (CPath) pose a major\ncomputational challenge due to their gigapixel scale, often requiring the\nprocessing of tens to hundreds of thousands of high-resolution patches per\nslide. This results in prohibitive encoding costs, with preprocessing and\ntraining times extending to days or even weeks-making WSI encoding the most\nsignificant bottleneck in real-world deployment. In this work, we propose\nWISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain\nvision-language models and large language models to address this challenge by\nselectively processing diagnostically relevant regions. WISE-FUSE first\ncomputes similarity scores between low-resolution patches and class-specific\ntextual descriptions using a knowledge distillation mechanism that preserves\nfine-grained diagnostic features. Based on these similarity scores, we select a\nsmall subset of informative regions for the target task, which quickly\neliminates irrelevant patches at the coarse level. The corresponding\nhigh-resolution patches are then selectively encoded and fused with textual\nembeddings to reinforce diagnostic context. Extensive experiments demonstrate\nthat WISE-FUSE reduces WSI encoding time by over threefold while achieving\ndiagnostic performance comparable to or surpassing that of exhaustive patch\nprocessing, offering a scalable and practical solution for CPath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole slide images (WSIs) in computational pathology (CPath) pose a major\ncomputational challenge due to their gigapixel scale, often requiring the\nprocessing of tens to hundreds of thousands of high-resolution patches per\nslide. This results in prohibitive encoding costs, with preprocessing and\ntraining times extending to days or even weeks-making WSI encoding the most\nsignificant bottleneck in real-world deployment. In this work, we propose\nWISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain\nvision-language models and large language models to address this challenge by\nselectively processing diagnostically relevant regions. WISE-FUSE first\ncomputes similarity scores between low-resolution patches and class-specific\ntextual descriptions using a knowledge distillation mechanism that preserves\nfine-grained diagnostic features. Based on these similarity scores, we select a\nsmall subset of informative regions for the target task, which quickly\neliminates irrelevant patches at the coarse level. The corresponding\nhigh-resolution patches are then selectively encoded and fused with textual\nembeddings to reinforce diagnostic context. Extensive experiments demonstrate\nthat WISE-FUSE reduces WSI encoding time by over threefold while achieving\ndiagnostic performance comparable to or surpassing that of exhaustive patch\nprocessing, offering a scalable and practical solution for CPath."
                },
                "authors": [
                    {
                        "name": "Yonghan Shin"
                    },
                    {
                        "name": "SeungKyu Kim"
                    },
                    {
                        "name": "Won-Ki Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Won-Ki Jeong"
                },
                "author": "Won-Ki Jeong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14532v1",
                "updated": "2025-08-20T08:40:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    40,
                    22,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:40:22Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    40,
                    22,
                    2,
                    232,
                    0
                ],
                "title": "Preguss: It Analyzes, It Specifies, It Verifies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preguss: It Analyzes, It Specifies, It Verifies"
                },
                "summary": "Fully automated verification of large-scale software and hardware systems is\narguably the holy grail of formal methods. Large language models (LLMs) have\nrecently demonstrated their potential for enhancing the degree of automation in\nformal verification by, e.g., generating formal specifications as essential to\ndeductive verification, yet exhibit poor scalability due to context-length\nlimitations and, more importantly, the difficulty of inferring complex,\ninterprocedural specifications. This paper outlines Preguss - a modular,\nfine-grained framework for automating the generation and refinement of formal\nspecifications. Preguss synergizes between static analysis and deductive\nverification by orchestrating two components: (i) potential runtime error\n(RTE)-guided construction and prioritization of verification units, and (ii)\nLLM-aided synthesis of interprocedural specifications at the unit level. We\nenvisage that Preguss paves a compelling path towards the automated\nverification of large-scale programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully automated verification of large-scale software and hardware systems is\narguably the holy grail of formal methods. Large language models (LLMs) have\nrecently demonstrated their potential for enhancing the degree of automation in\nformal verification by, e.g., generating formal specifications as essential to\ndeductive verification, yet exhibit poor scalability due to context-length\nlimitations and, more importantly, the difficulty of inferring complex,\ninterprocedural specifications. This paper outlines Preguss - a modular,\nfine-grained framework for automating the generation and refinement of formal\nspecifications. Preguss synergizes between static analysis and deductive\nverification by orchestrating two components: (i) potential runtime error\n(RTE)-guided construction and prioritization of verification units, and (ii)\nLLM-aided synthesis of interprocedural specifications at the unit level. We\nenvisage that Preguss paves a compelling path towards the automated\nverification of large-scale programs."
                },
                "authors": [
                    {
                        "name": "Zhongyi Wang"
                    },
                    {
                        "name": "Tengjie Lin"
                    },
                    {
                        "name": "Mingshuai Chen"
                    },
                    {
                        "name": "Mingqi Yang"
                    },
                    {
                        "name": "Haokun Li"
                    },
                    {
                        "name": "Xiao Yi"
                    },
                    {
                        "name": "Shengchao Qin"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "arxiv_comment": "Position paper to appear in the 1st International Workshop on\n  Language Models and Programming Languages (LMPL '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03608v2",
                "updated": "2025-08-20T08:37:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    37,
                    28,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-04T14:31:30Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    14,
                    31,
                    30,
                    4,
                    185,
                    0
                ],
                "title": "Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation\n  (RAG) Pipelines for Open Radio Access Networks (ORAN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation\n  (RAG) Pipelines for Open Radio Access Networks (ORAN)"
                },
                "summary": "Generative AI (GenAI) is expected to play a pivotal role in enabling\nautonomous optimization in future wireless networks. Within the ORAN\narchitecture, Large Language Models (LLMs) can be specialized to generate xApps\nand rApps by leveraging specifications and API definitions from the RAN\nIntelligent Controller (RIC) platform. However, fine-tuning base LLMs for\ntelecom-specific tasks remains expensive and resource-intensive.\nRetrieval-Augmented Generation (RAG) offers a practical alternative through\nin-context learning, enabling domain adaptation without full retraining. While\ntraditional RAG systems rely on vector-based retrieval, emerging variants such\nas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval\nstrategies to support multi-hop reasoning and improve factual grounding.\nDespite their promise, these methods lack systematic, metric-driven\nevaluations, particularly in high-stakes domains such as ORAN. In this study,\nwe conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid\nGraphRAG using ORAN specifications. We assess performance across varying\nquestion complexities using established generation metrics: faithfulness,\nanswer relevance, context relevance, and factual correctness. Results show that\nboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG\nimproves factual correctness by 8%, while GraphRAG improves context relevance\nby 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) is expected to play a pivotal role in enabling\nautonomous optimization in future wireless networks. Within the ORAN\narchitecture, Large Language Models (LLMs) can be specialized to generate xApps\nand rApps by leveraging specifications and API definitions from the RAN\nIntelligent Controller (RIC) platform. However, fine-tuning base LLMs for\ntelecom-specific tasks remains expensive and resource-intensive.\nRetrieval-Augmented Generation (RAG) offers a practical alternative through\nin-context learning, enabling domain adaptation without full retraining. While\ntraditional RAG systems rely on vector-based retrieval, emerging variants such\nas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval\nstrategies to support multi-hop reasoning and improve factual grounding.\nDespite their promise, these methods lack systematic, metric-driven\nevaluations, particularly in high-stakes domains such as ORAN. In this study,\nwe conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid\nGraphRAG using ORAN specifications. We assess performance across varying\nquestion complexities using established generation metrics: faithfulness,\nanswer relevance, context relevance, and factual correctness. Results show that\nboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG\nimproves factual correctness by 8%, while GraphRAG improves context relevance\nby 11%."
                },
                "authors": [
                    {
                        "name": "Sarat Ahmad"
                    },
                    {
                        "name": "Zeinab Nezami"
                    },
                    {
                        "name": "Maryam Hafeez"
                    },
                    {
                        "name": "Syed Ali Raza Zaidi"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ali Raza Zaidi"
                },
                "author": "Syed Ali Raza Zaidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14527v1",
                "updated": "2025-08-20T08:36:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    36,
                    57,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:36:57Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    36,
                    57,
                    2,
                    232,
                    0
                ],
                "title": "Adversarial Generation and Collaborative Evolution of Safety-Critical\n  Scenarios for Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Generation and Collaborative Evolution of Safety-Critical\n  Scenarios for Autonomous Vehicles"
                },
                "summary": "The generation of safety-critical scenarios in simulation has become\nincreasingly crucial for safety evaluation in autonomous vehicles prior to road\ndeployment in society. However, current approaches largely rely on predefined\nthreat patterns or rule-based strategies, which limit their ability to expose\ndiverse and unforeseen failure modes. To overcome these, we propose ScenGE, a\nframework that can generate plentiful safety-critical scenarios by reasoning\nnovel adversarial cases and then amplifying them with complex traffic flows.\nGiven a simple prompt of a benign scene, it first performs Meta-Scenario\nGeneration, where a large language model, grounded in structured driving\nknowledge, infers an adversarial agent whose behavior poses a threat that is\nboth plausible and deliberately challenging. This meta-scenario is then\nspecified in executable code for precise in-simulator control. Subsequently,\nComplex Scenario Evolution uses background vehicles to amplify the core threat\nintroduced by Meta-Scenario. It builds an adversarial collaborator graph to\nidentify key agent trajectories for optimization. These perturbations are\ndesigned to simultaneously reduce the ego vehicle's maneuvering space and\ncreate critical occlusions. Extensive experiments conducted on multiple\nreinforcement learning based AV models show that ScenGE uncovers more severe\ncollision cases (+31.96%) on average than SoTA baselines. Additionally, our\nScenGE can be applied to large model based AV systems and deployed on different\nsimulators; we further observe that adversarial training on our scenarios\nimproves the model robustness. Finally, we validate our framework through\nreal-world vehicle tests and human evaluation, confirming that the generated\nscenarios are both plausible and critical. We hope our paper can build up a\ncritical step towards building public trust and ensuring their safe deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation of safety-critical scenarios in simulation has become\nincreasingly crucial for safety evaluation in autonomous vehicles prior to road\ndeployment in society. However, current approaches largely rely on predefined\nthreat patterns or rule-based strategies, which limit their ability to expose\ndiverse and unforeseen failure modes. To overcome these, we propose ScenGE, a\nframework that can generate plentiful safety-critical scenarios by reasoning\nnovel adversarial cases and then amplifying them with complex traffic flows.\nGiven a simple prompt of a benign scene, it first performs Meta-Scenario\nGeneration, where a large language model, grounded in structured driving\nknowledge, infers an adversarial agent whose behavior poses a threat that is\nboth plausible and deliberately challenging. This meta-scenario is then\nspecified in executable code for precise in-simulator control. Subsequently,\nComplex Scenario Evolution uses background vehicles to amplify the core threat\nintroduced by Meta-Scenario. It builds an adversarial collaborator graph to\nidentify key agent trajectories for optimization. These perturbations are\ndesigned to simultaneously reduce the ego vehicle's maneuvering space and\ncreate critical occlusions. Extensive experiments conducted on multiple\nreinforcement learning based AV models show that ScenGE uncovers more severe\ncollision cases (+31.96%) on average than SoTA baselines. Additionally, our\nScenGE can be applied to large model based AV systems and deployed on different\nsimulators; we further observe that adversarial training on our scenarios\nimproves the model robustness. Finally, we validate our framework through\nreal-world vehicle tests and human evaluation, confirming that the generated\nscenarios are both plausible and critical. We hope our paper can build up a\ncritical step towards building public trust and ensuring their safe deployment."
                },
                "authors": [
                    {
                        "name": "Jiangfan Liu"
                    },
                    {
                        "name": "Yongkang Guo"
                    },
                    {
                        "name": "Fangzhi Zhong"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Zonglei Jing"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Jiakai Wang"
                    },
                    {
                        "name": "Mingchuan Zhang"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11317v2",
                "updated": "2025-08-20T08:32:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    32,
                    11,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-15T13:49:03Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    13,
                    49,
                    3,
                    1,
                    196,
                    0
                ],
                "title": "Species-Dependent Electron Emission from Nanoparticles under Gamma\n  Irradiation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Species-Dependent Electron Emission from Nanoparticles under Gamma\n  Irradiation"
                },
                "summary": "In this study, various nanoparticle species-including Au and Gd$_2$O$_3$-were\nirradiated with low-energy gamma rays, such as 59.5 keV photons from\n$^{241}$Am. Pulse-height spectra were recorded using a liquid-scintillation\ncounting system before and after dispersing the nanoparticles into the\nscintillator, and the differences between them were analyzed to infer the\ninteraction outcomes. Gd$_2$O$_3$ nanoparticles emitted numerous electrons;\nhowever, under identical experimental conditions, no detectable electron\nemission was observed from Au nanoparticles (AuNPs). Here, \"detectable electron\nemission\" refers to electrons with energies high enough to be registered by the\nliquid-scintillation detector used (approx 100 eV and, more typically, >= 1-2\nkeV); however, it excludes electrons that may be emitted at lower energies.\nThus, a species-dependent radiation-nanoparticle interaction was observed.\nRigorous controls and falsification tests excluded various artefacts-including\ndetector insensitivity, surface contamination, aggregation, quenching, and\nself-absorption-as causes for the absence of detectable electron emission from\nAuNPs. This observation potentially prompts a re-evaluation of the common\nassumption that nanoparticles behave like their bulk counterparts and emit\nelectrons upon gamma-irradiation. Instead, our results suggest that the\ndistinct internal environment of nanoparticles influences their interaction\nwith radiation. These findings offer significant insights for practical\napplications, including a better mechanistic understanding of nanoparticle\nradiosensitization in cancer therapy, enhanced gamma-detection efficiency of\norganic scintillators, and the development of lightweight radiation shields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, various nanoparticle species-including Au and Gd$_2$O$_3$-were\nirradiated with low-energy gamma rays, such as 59.5 keV photons from\n$^{241}$Am. Pulse-height spectra were recorded using a liquid-scintillation\ncounting system before and after dispersing the nanoparticles into the\nscintillator, and the differences between them were analyzed to infer the\ninteraction outcomes. Gd$_2$O$_3$ nanoparticles emitted numerous electrons;\nhowever, under identical experimental conditions, no detectable electron\nemission was observed from Au nanoparticles (AuNPs). Here, \"detectable electron\nemission\" refers to electrons with energies high enough to be registered by the\nliquid-scintillation detector used (approx 100 eV and, more typically, >= 1-2\nkeV); however, it excludes electrons that may be emitted at lower energies.\nThus, a species-dependent radiation-nanoparticle interaction was observed.\nRigorous controls and falsification tests excluded various artefacts-including\ndetector insensitivity, surface contamination, aggregation, quenching, and\nself-absorption-as causes for the absence of detectable electron emission from\nAuNPs. This observation potentially prompts a re-evaluation of the common\nassumption that nanoparticles behave like their bulk counterparts and emit\nelectrons upon gamma-irradiation. Instead, our results suggest that the\ndistinct internal environment of nanoparticles influences their interaction\nwith radiation. These findings offer significant insights for practical\napplications, including a better mechanistic understanding of nanoparticle\nradiosensitization in cancer therapy, enhanced gamma-detection efficiency of\norganic scintillators, and the development of lightweight radiation shields."
                },
                "authors": [
                    {
                        "name": "Darukesha B H M"
                    }
                ],
                "author_detail": {
                    "name": "Darukesha B H M"
                },
                "author": "Darukesha B H M",
                "arxiv_comment": "Version 2 (Uncertainty and Error Analysis are brought together in one\n  place). Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04255v2",
                "updated": "2025-08-20T08:20:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    20,
                    59,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-05T19:28:25Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    19,
                    28,
                    25,
                    5,
                    95,
                    0
                ],
                "title": "nonprobsvy -- An R package for modern methods for non-probability\n  surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "nonprobsvy -- An R package for modern methods for non-probability\n  surveys"
                },
                "summary": "The following paper presents nonprobsvy -- an R package for inference based\non non-probability samples. The package implements various approaches that can\nbe categorized into three groups: prediction-based approach, inverse\nprobability weighting and doubly robust approach. In the package, we assume the\nexistence of either population-level data or probability-based population\ninformation and leverage the survey package for inference. The package\nimplements both analytical and bootstrap variance estimation for the proposed\nestimators. In the paper we present the theory behind the package, its\nfunctionalities and case study that showcases the usage of the package. The\npackage is aimed at scientists and researchers who would like to use\nnon-probability samples (e.g.big data, opt-in web panels, social media) to\naccurately estimate population characteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The following paper presents nonprobsvy -- an R package for inference based\non non-probability samples. The package implements various approaches that can\nbe categorized into three groups: prediction-based approach, inverse\nprobability weighting and doubly robust approach. In the package, we assume the\nexistence of either population-level data or probability-based population\ninformation and leverage the survey package for inference. The package\nimplements both analytical and bootstrap variance estimation for the proposed\nestimators. In the paper we present the theory behind the package, its\nfunctionalities and case study that showcases the usage of the package. The\npackage is aimed at scientists and researchers who would like to use\nnon-probability samples (e.g.big data, opt-in web panels, social media) to\naccurately estimate population characteristics."
                },
                "authors": [
                    {
                        "name": "Łukasz Chrostowski"
                    },
                    {
                        "name": "Piotr Chlebicki"
                    },
                    {
                        "name": "Maciej Beręsewicz"
                    }
                ],
                "author_detail": {
                    "name": "Maciej Beręsewicz"
                },
                "author": "Maciej Beręsewicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12769v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12769v3",
                "updated": "2025-08-20T08:11:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    11,
                    10,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-18T09:43:07Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    43,
                    7,
                    0,
                    230,
                    0
                ],
                "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing\n  through Cluster Retrieval and Execution Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing\n  through Cluster Retrieval and Execution Description"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git"
                },
                "authors": [
                    {
                        "name": "Shaoming Duan"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Chuanyi Liu"
                    },
                    {
                        "name": "Zhibin Zhu"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Peiyi Han"
                    },
                    {
                        "name": "Liang Yan"
                    },
                    {
                        "name": "Zewu Peng"
                    }
                ],
                "author_detail": {
                    "name": "Zewu Peng"
                },
                "author": "Zewu Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12769v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12769v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24773v2",
                "updated": "2025-08-20T08:08:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    8,
                    3,
                    2,
                    232,
                    0
                ],
                "published": "2025-05-30T16:35:32Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    35,
                    32,
                    4,
                    150,
                    0
                ],
                "title": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with\n  Resource-Aware Low-Rank Adaption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with\n  Resource-Aware Low-Rank Adaption"
                },
                "summary": "Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world."
                },
                "authors": [
                    {
                        "name": "Yajie Zhou"
                    },
                    {
                        "name": "Xiaoyi Pang"
                    },
                    {
                        "name": "Zhibo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Wang"
                },
                "author": "Zhibo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14511v1",
                "updated": "2025-08-20T08:03:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    3,
                    0,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:03:00Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    3,
                    0,
                    2,
                    232,
                    0
                ],
                "title": "What You See Is What It Does: A Structural Pattern for Legible Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What You See Is What It Does: A Structural Pattern for Legible Software"
                },
                "summary": "The opportunities offered by LLM coders (and their current limitations)\ndemand a reevaluation of how software is structured. Software today is often\n\"illegible\" - lacking a direct correspondence between code and observed\nbehavior - and insufficiently modular, leading to a failure of three key\nrequirements of robust coding: incrementality (the ability to deliver small\nincrements by making localized changes), integrity (avoiding breaking prior\nincrements) and transparency (making clear what has changed at build time, and\nwhat actions have happened at runtime).\n  A new structural pattern offers improved legibility and modularity. Its\nelements are concepts and synchronizations: fully independent services and\nevent-based rules that mediate between them. A domain-specific language for\nsynchronizations allows behavioral features to be expressed in a granular and\ndeclarative way (and thus readily generated by an LLM). A case study of the\nRealWorld benchmark is used to illustrate and evaluate the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The opportunities offered by LLM coders (and their current limitations)\ndemand a reevaluation of how software is structured. Software today is often\n\"illegible\" - lacking a direct correspondence between code and observed\nbehavior - and insufficiently modular, leading to a failure of three key\nrequirements of robust coding: incrementality (the ability to deliver small\nincrements by making localized changes), integrity (avoiding breaking prior\nincrements) and transparency (making clear what has changed at build time, and\nwhat actions have happened at runtime).\n  A new structural pattern offers improved legibility and modularity. Its\nelements are concepts and synchronizations: fully independent services and\nevent-based rules that mediate between them. A domain-specific language for\nsynchronizations allows behavioral features to be expressed in a granular and\ndeclarative way (and thus readily generated by an LLM). A case study of the\nRealWorld benchmark is used to illustrate and evaluate the approach."
                },
                "authors": [
                    {
                        "name": "Eagon Meng"
                    },
                    {
                        "name": "Daniel Jackson"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Jackson"
                },
                "author": "Daniel Jackson",
                "arxiv_comment": "16 pages. Appearing in Onward! at SPLASH 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08113v2",
                "updated": "2025-08-20T07:59:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    59,
                    8,
                    2,
                    232,
                    0
                ],
                "published": "2025-06-09T18:10:00Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    18,
                    10,
                    0,
                    0,
                    160,
                    0
                ],
                "title": "Benchmarking Pre-Trained Time Series Models for Electricity Price\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Pre-Trained Time Series Models for Electricity Price\n  Forecasting"
                },
                "summary": "Accurate electricity price forecasting (EPF) is crucial for effective\ndecision-making in power trading on the spot market. While recent advances in\ngenerative artificial intelligence (GenAI) and pre-trained large language\nmodels (LLMs) have inspired the development of numerous time series foundation\nmodels (TSFMs) for time series forecasting, their effectiveness in EPF remains\nuncertain. To address this gap, we benchmark several state-of-the-art\npretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and\nTimeGPT--against established statistical and machine learning (ML) methods for\nEPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,\nFrance, the Netherlands, Austria, and Belgium, we generate daily forecasts with\na one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the\nTSFMs, performing on par with traditional models. However, the biseasonal MSTL\nmodel, which captures daily and weekly seasonality, stands out for its\nconsistent performance across countries and evaluation metrics, with no TSFM\nstatistically outperforming it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate electricity price forecasting (EPF) is crucial for effective\ndecision-making in power trading on the spot market. While recent advances in\ngenerative artificial intelligence (GenAI) and pre-trained large language\nmodels (LLMs) have inspired the development of numerous time series foundation\nmodels (TSFMs) for time series forecasting, their effectiveness in EPF remains\nuncertain. To address this gap, we benchmark several state-of-the-art\npretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and\nTimeGPT--against established statistical and machine learning (ML) methods for\nEPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,\nFrance, the Netherlands, Austria, and Belgium, we generate daily forecasts with\na one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the\nTSFMs, performing on par with traditional models. However, the biseasonal MSTL\nmodel, which captures daily and weekly seasonality, stands out for its\nconsistent performance across countries and evaluation metrics, with no TSFM\nstatistically outperforming it."
                },
                "authors": [
                    {
                        "name": "Timothée Hornek Amir Sartipi"
                    },
                    {
                        "name": "Igor Tchappi"
                    },
                    {
                        "name": "Gilbert Fridgen"
                    }
                ],
                "author_detail": {
                    "name": "Gilbert Fridgen"
                },
                "author": "Gilbert Fridgen",
                "arxiv_doi": "10.1109/eem64765.2025.11050326",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/eem64765.2025.11050326",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.08113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14507v1",
                "updated": "2025-08-20T07:56:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    56,
                    13,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T07:56:13Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    56,
                    13,
                    2,
                    232,
                    0
                ],
                "title": "DeepTelecom: A Digital-Twin Deep Learning Dataset for Channel and MIMO\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTelecom: A Digital-Twin Deep Learning Dataset for Channel and MIMO\n  Applications"
                },
                "summary": "Domain-specific datasets are the foundation for unleashing artificial\nintelligence (AI)-driven wireless innovation. Yet existing wireless AI corpora\nare slow to produce, offer limited modeling fidelity, and cover only narrow\nscenario types. To address the challenges, we create DeepTelecom, a\nthree-dimension (3D) digital-twin channel dataset. Specifically, a large\nlanguage model (LLM)-assisted pipeline first builds the third level of details\n(LoD3) outdoor and indoor scenes with segmentable material-parameterizable\nsurfaces. Then, DeepTelecom simulates full radio-wave propagation effects based\non Sionna's ray-tracing engine. Leveraging GPU acceleration, DeepTelecom\nstreams ray-path trajectories and real-time signal-strength heat maps, compiles\nthem into high-frame-rate videos, and simultaneously outputs synchronized\nmulti-view images, channel tensors, and multi-scale fading traces. By\nefficiently streaming large-scale, high-fidelity, and multimodal channel data,\nDeepTelecom not only furnishes a unified benchmark for wireless AI research but\nalso supplies the domain-rich training substrate that enables foundation models\nto tightly fuse large model intelligence with future communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-specific datasets are the foundation for unleashing artificial\nintelligence (AI)-driven wireless innovation. Yet existing wireless AI corpora\nare slow to produce, offer limited modeling fidelity, and cover only narrow\nscenario types. To address the challenges, we create DeepTelecom, a\nthree-dimension (3D) digital-twin channel dataset. Specifically, a large\nlanguage model (LLM)-assisted pipeline first builds the third level of details\n(LoD3) outdoor and indoor scenes with segmentable material-parameterizable\nsurfaces. Then, DeepTelecom simulates full radio-wave propagation effects based\non Sionna's ray-tracing engine. Leveraging GPU acceleration, DeepTelecom\nstreams ray-path trajectories and real-time signal-strength heat maps, compiles\nthem into high-frame-rate videos, and simultaneously outputs synchronized\nmulti-view images, channel tensors, and multi-scale fading traces. By\nefficiently streaming large-scale, high-fidelity, and multimodal channel data,\nDeepTelecom not only furnishes a unified benchmark for wireless AI research but\nalso supplies the domain-rich training substrate that enables foundation models\nto tightly fuse large model intelligence with future communication systems."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Zehua Jiang"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Chongwen Huang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Siming Jiang"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Richeng Jin"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Sami Muhaidat"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09586v2",
                "updated": "2025-08-20T07:50:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    50,
                    49,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-13T07:59:29Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    59,
                    29,
                    2,
                    225,
                    0
                ],
                "title": "EvoCurr: Self-evolving Curriculum with Behavior Code Generation for\n  Complex Decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCurr: Self-evolving Curriculum with Behavior Code Generation for\n  Complex Decision-making"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse domains, including programming, planning, and decision-making. However,\ntheir performance often degrades when faced with highly complex problem\ninstances that require deep reasoning over long horizons. In such cases, direct\nproblem-solving approaches can lead to inefficiency or failure due to the lack\nof structured intermediate guidance. To address this, we propose a novel\nself-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM\nconstructs a sequence of problem instances with gradually increasing\ndifficulty, tailored to the solver LLM's learning progress. The curriculum\ndynamically adapts easing challenges when the solver struggles and escalating\nthem when success is consistent, thus maintaining an optimal learning\ntrajectory. This approach enables the solver LLM, implemented as a\ncode-generation model producing Python decision-tree scripts, to progressively\nacquire the skills needed for complex decision-making tasks. Experimental\nresults on challenging decision-making benchmarks show that our method\nsignificantly improves task success rates and solution efficiency compared to\ndirect-solving baselines. These findings suggest that LLM-driven curriculum\nlearning holds strong potential for enhancing automated reasoning in\nreal-world, high-complexity domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse domains, including programming, planning, and decision-making. However,\ntheir performance often degrades when faced with highly complex problem\ninstances that require deep reasoning over long horizons. In such cases, direct\nproblem-solving approaches can lead to inefficiency or failure due to the lack\nof structured intermediate guidance. To address this, we propose a novel\nself-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM\nconstructs a sequence of problem instances with gradually increasing\ndifficulty, tailored to the solver LLM's learning progress. The curriculum\ndynamically adapts easing challenges when the solver struggles and escalating\nthem when success is consistent, thus maintaining an optimal learning\ntrajectory. This approach enables the solver LLM, implemented as a\ncode-generation model producing Python decision-tree scripts, to progressively\nacquire the skills needed for complex decision-making tasks. Experimental\nresults on challenging decision-making benchmarks show that our method\nsignificantly improves task success rates and solution efficiency compared to\ndirect-solving baselines. These findings suggest that LLM-driven curriculum\nlearning holds strong potential for enhancing automated reasoning in\nreal-world, high-complexity domains."
                },
                "authors": [
                    {
                        "name": "Yang Cheng"
                    },
                    {
                        "name": "Zilai Wang"
                    },
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Wenhui Zhu"
                    },
                    {
                        "name": "Yue Deng"
                    },
                    {
                        "name": "Jian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhao"
                },
                "author": "Jian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02329v2",
                "updated": "2025-08-20T07:44:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    44,
                    52,
                    2,
                    232,
                    0
                ],
                "published": "2025-02-04T14:00:32Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    0,
                    32,
                    1,
                    35,
                    0
                ],
                "title": "ReSpark: Leveraging Previous Data Reports as References to Generate New\n  Reports with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSpark: Leveraging Previous Data Reports as References to Generate New\n  Reports with LLMs"
                },
                "summary": "Creating data reports is a labor-intensive task involving iterative data\nexploration, insight extraction, and narrative construction. A key challenge\nlies in composing the analysis logic-from defining objectives and transforming\ndata to identifying and communicating insights. Manually crafting this logic\ncan be cognitively demanding. While experienced analysts often reuse scripts\nfrom past projects, finding a perfect match for a new dataset is rare. Even\nwhen similar analyses are available online, they usually share only results or\nvisualizations, not the underlying code, making reuse difficult. To address\nthis, we present ReSpark, a system that leverages large language models (LLMs)\nto reverse-engineer analysis logic from existing reports and adapt it to new\ndatasets. By generating draft analysis steps, ReSpark provides a warm start for\nusers. It also supports interactive refinement, allowing users to inspect\nintermediate outputs, insert objectives, and revise content. We evaluate\nReSpark through comparative and user studies, demonstrating its effectiveness\nin lowering the barrier to generating data reports without relying on existing\nanalysis code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating data reports is a labor-intensive task involving iterative data\nexploration, insight extraction, and narrative construction. A key challenge\nlies in composing the analysis logic-from defining objectives and transforming\ndata to identifying and communicating insights. Manually crafting this logic\ncan be cognitively demanding. While experienced analysts often reuse scripts\nfrom past projects, finding a perfect match for a new dataset is rare. Even\nwhen similar analyses are available online, they usually share only results or\nvisualizations, not the underlying code, making reuse difficult. To address\nthis, we present ReSpark, a system that leverages large language models (LLMs)\nto reverse-engineer analysis logic from existing reports and adapt it to new\ndatasets. By generating draft analysis steps, ReSpark provides a warm start for\nusers. It also supports interactive refinement, allowing users to inspect\nintermediate outputs, insert objectives, and revise content. We evaluate\nReSpark through comparative and user studies, demonstrating its effectiveness\nin lowering the barrier to generating data reports without relying on existing\nanalysis code."
                },
                "authors": [
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Chuhan Zhang"
                    },
                    {
                        "name": "Xiaotong Wang"
                    },
                    {
                        "name": "Sitong Pan"
                    },
                    {
                        "name": "Weiwei Cui"
                    },
                    {
                        "name": "Haidong Zhang"
                    },
                    {
                        "name": "Dazhen Deng"
                    },
                    {
                        "name": "Yingcai Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yingcai Wu"
                },
                "author": "Yingcai Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24157v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24157v3",
                "updated": "2025-08-20T07:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    35,
                    22,
                    2,
                    232,
                    0
                ],
                "published": "2025-03-31T14:40:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    40,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "LLM4FS: Leveraging Large Language Models for Feature Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4FS: Leveraging Large Language Models for Feature Selection"
                },
                "summary": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a new hybrid strategy called LLM4FS\nthat integrates LLMs with traditional data-driven methods. Specifically, input\ndata samples into LLMs, and directly call traditional data-driven techniques\nsuch as random forest and forward sequential selection. Notably, our analysis\nreveals that the hybrid strategy leverages the contextual understanding of LLMs\nand the high statistical reliability of traditional data-driven methods to\nachieve excellent feature selection performance, even surpassing LLMs and\ntraditional data-driven methods. Finally, we point out the limitations of its\napplication in decision-making. Our code is available at\nhttps://github.com/xianchaoxiu/LLM4FS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a new hybrid strategy called LLM4FS\nthat integrates LLMs with traditional data-driven methods. Specifically, input\ndata samples into LLMs, and directly call traditional data-driven techniques\nsuch as random forest and forward sequential selection. Notably, our analysis\nreveals that the hybrid strategy leverages the contextual understanding of LLMs\nand the high statistical reliability of traditional data-driven methods to\nachieve excellent feature selection performance, even surpassing LLMs and\ntraditional data-driven methods. Finally, we point out the limitations of its\napplication in decision-making. Our code is available at\nhttps://github.com/xianchaoxiu/LLM4FS."
                },
                "authors": [
                    {
                        "name": "Jianhao Li"
                    },
                    {
                        "name": "Xianchao Xiu"
                    }
                ],
                "author_detail": {
                    "name": "Xianchao Xiu"
                },
                "author": "Xianchao Xiu",
                "arxiv_comment": "CAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24157v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24157v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14496v1",
                "updated": "2025-08-20T07:33:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    33,
                    50,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T07:33:50Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    33,
                    50,
                    2,
                    232,
                    0
                ],
                "title": "Semantic Energy: Detecting LLM Hallucination Beyond Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Energy: Detecting LLM Hallucination Beyond Entropy"
                },
                "summary": "Large Language Models (LLMs) are being increasingly deployed in real-world\napplications, but they remain susceptible to hallucinations, which produce\nfluent yet incorrect responses and lead to erroneous decision-making.\nUncertainty estimation is a feasible approach to detect such hallucinations.\nFor example, semantic entropy estimates uncertainty by considering the semantic\ndiversity across multiple sampled responses, thus identifying hallucinations.\nHowever, semantic entropy relies on post-softmax probabilities and fails to\ncapture the model's inherent uncertainty, causing it to be ineffective in\ncertain scenarios. To address this issue, we introduce Semantic Energy, a novel\nuncertainty estimation framework that leverages the inherent confidence of LLMs\nby operating directly on logits of penultimate layer. By combining semantic\nclustering with a Boltzmann-inspired energy distribution, our method better\ncaptures uncertainty in cases where semantic entropy fails. Experiments across\nmultiple benchmarks show that Semantic Energy significantly improves\nhallucination detection and uncertainty estimation, offering more reliable\nsignals for downstream applications such as hallucination detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being increasingly deployed in real-world\napplications, but they remain susceptible to hallucinations, which produce\nfluent yet incorrect responses and lead to erroneous decision-making.\nUncertainty estimation is a feasible approach to detect such hallucinations.\nFor example, semantic entropy estimates uncertainty by considering the semantic\ndiversity across multiple sampled responses, thus identifying hallucinations.\nHowever, semantic entropy relies on post-softmax probabilities and fails to\ncapture the model's inherent uncertainty, causing it to be ineffective in\ncertain scenarios. To address this issue, we introduce Semantic Energy, a novel\nuncertainty estimation framework that leverages the inherent confidence of LLMs\nby operating directly on logits of penultimate layer. By combining semantic\nclustering with a Boltzmann-inspired energy distribution, our method better\ncaptures uncertainty in cases where semantic entropy fails. Experiments across\nmultiple benchmarks show that Semantic Energy significantly improves\nhallucination detection and uncertainty estimation, offering more reliable\nsignals for downstream applications such as hallucination detection."
                },
                "authors": [
                    {
                        "name": "Huan Ma"
                    },
                    {
                        "name": "Jiadong Pan"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Changqing Zhang"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14493v1",
                "updated": "2025-08-20T07:31:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    31,
                    37,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T07:31:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    31,
                    37,
                    2,
                    232,
                    0
                ],
                "title": "Global-Distribution Aware Scenario-Specific Variational Representation\n  Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global-Distribution Aware Scenario-Specific Variational Representation\n  Learning Framework"
                },
                "summary": "With the emergence of e-commerce, the recommendations provided by commercial\nplatforms must adapt to diverse scenarios to accommodate users' varying\nshopping preferences. Current methods typically use a unified framework to\noffer personalized recommendations for different scenarios. However, they often\nemploy shared bottom representations, which partially hinders the model's\ncapacity to capture scenario uniqueness. Ideally, users and items should\nexhibit specific characteristics in different scenarios, prompting the need to\nlearn scenario-specific representations to differentiate scenarios. Yet,\nvariations in user and item interactions across scenarios lead to data sparsity\nissues, impeding the acquisition of scenario-specific representations. To learn\nrobust scenario-specific representations, we introduce a Global-Distribution\nAware Scenario-Specific Variational Representation Learning Framework (GSVR)\nthat can be directly applied to existing multi-scenario methods. Specifically,\nconsidering the uncertainty stemming from limited samples, our approach employs\na probabilistic model to generate scenario-specific distributions for each user\nand item in each scenario, estimated through variational inference (VI).\nAdditionally, we introduce the global knowledge-aware multinomial distributions\nas prior knowledge to regulate the learning of the posterior user and item\ndistributions, ensuring similarities among distributions for users with akin\ninterests and items with similar side information. This mitigates the risk of\nusers or items with fewer records being overwhelmed in sparse scenarios.\nExtensive experimental results affirm the efficacy of GSVR in assisting\nexisting multi-scenario recommendation methods in learning more robust\nrepresentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of e-commerce, the recommendations provided by commercial\nplatforms must adapt to diverse scenarios to accommodate users' varying\nshopping preferences. Current methods typically use a unified framework to\noffer personalized recommendations for different scenarios. However, they often\nemploy shared bottom representations, which partially hinders the model's\ncapacity to capture scenario uniqueness. Ideally, users and items should\nexhibit specific characteristics in different scenarios, prompting the need to\nlearn scenario-specific representations to differentiate scenarios. Yet,\nvariations in user and item interactions across scenarios lead to data sparsity\nissues, impeding the acquisition of scenario-specific representations. To learn\nrobust scenario-specific representations, we introduce a Global-Distribution\nAware Scenario-Specific Variational Representation Learning Framework (GSVR)\nthat can be directly applied to existing multi-scenario methods. Specifically,\nconsidering the uncertainty stemming from limited samples, our approach employs\na probabilistic model to generate scenario-specific distributions for each user\nand item in each scenario, estimated through variational inference (VI).\nAdditionally, we introduce the global knowledge-aware multinomial distributions\nas prior knowledge to regulate the learning of the posterior user and item\ndistributions, ensuring similarities among distributions for users with akin\ninterests and items with similar side information. This mitigates the risk of\nusers or items with fewer records being overwhelmed in sparse scenarios.\nExtensive experimental results affirm the efficacy of GSVR in assisting\nexisting multi-scenario recommendation methods in learning more robust\nrepresentations."
                },
                "authors": [
                    {
                        "name": "Moyu Zhang"
                    },
                    {
                        "name": "Yujun Jin"
                    },
                    {
                        "name": "Jinxin Hu"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "arxiv_doi": "10.1145/3746252.3760866",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3760866",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.14493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025, 6 pages, 1 figures, 5 tables",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06569v2",
                "updated": "2025-08-20T07:24:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    24,
                    46,
                    2,
                    232,
                    0
                ],
                "published": "2024-08-13T02:08:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    8,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "Social Debiasing for Fair Multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Debiasing for Fair Multi-modal LLMs"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) have dramatically advanced the\nresearch field and delivered powerful vision-language understanding\ncapabilities. However, these models often inherit deep-rooted social biases\nfrom their training data, leading to uncomfortable responses with respect to\nattributes such as race and gender. This paper addresses the issue of social\nbiases in MLLMs by i) introducing a comprehensive counterfactual dataset with\nmultiple social concepts (CMSC), which complements existing datasets by\nproviding 18 diverse and balanced social concepts; and ii) proposing a\ncounter-stereotype debiasing (CSD) strategy that mitigates social biases in\nMLLMs by leveraging the opposites of prevalent stereotypes. CSD incorporates\nboth a novel bias-aware data sampling method and a loss rescaling method,\nenabling the model to effectively reduce biases. We conduct extensive\nexperiments with four prevalent MLLM architectures. The results demonstrate the\nadvantage of the CMSC dataset and the edge of CSD strategy in reducing social\nbiases compared to existing competing methods, without compromising the overall\nperformance on general multi-modal reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) have dramatically advanced the\nresearch field and delivered powerful vision-language understanding\ncapabilities. However, these models often inherit deep-rooted social biases\nfrom their training data, leading to uncomfortable responses with respect to\nattributes such as race and gender. This paper addresses the issue of social\nbiases in MLLMs by i) introducing a comprehensive counterfactual dataset with\nmultiple social concepts (CMSC), which complements existing datasets by\nproviding 18 diverse and balanced social concepts; and ii) proposing a\ncounter-stereotype debiasing (CSD) strategy that mitigates social biases in\nMLLMs by leveraging the opposites of prevalent stereotypes. CSD incorporates\nboth a novel bias-aware data sampling method and a loss rescaling method,\nenabling the model to effectively reduce biases. We conduct extensive\nexperiments with four prevalent MLLM architectures. The results demonstrate the\nadvantage of the CMSC dataset and the edge of CSD strategy in reducing social\nbiases compared to existing competing methods, without compromising the overall\nperformance on general multi-modal reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Harry Cheng"
                    },
                    {
                        "name": "Yangyang Guo"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Ming Yang"
                    },
                    {
                        "name": "Tian Gan"
                    },
                    {
                        "name": "Weili Guan"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Project page:\n  https://github.com/xaCheng1996/Social_Debiasing_For_Fair_MLLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14487v1",
                "updated": "2025-08-20T07:23:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    23,
                    45,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T07:23:45Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    23,
                    45,
                    2,
                    232,
                    0
                ],
                "title": "Bridge Sampling Diagnostics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge Sampling Diagnostics"
                },
                "summary": "In Bayesian statistics, the marginal likelihood is used for model selection\nand averaging, yet it is often challenging to compute accurately for complex\nmodels. Approaches such as bridge sampling, while effective, may suffer from\nissues of high variability of the estimates. We present how to estimate Monte\nCarlo standard error (MCSE) for bridge sampling, and how to diagnose the\nreliability of MCSE estimates using Pareto-$\\hat{k}$ and block reshuffling\ndiagnostics without the need to repeatedly re-run full posterior inference. We\ndemonstrate the behavior with increasingly more difficult simulated posteriors\nand many real posteriors from the posteriordb database.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Bayesian statistics, the marginal likelihood is used for model selection\nand averaging, yet it is often challenging to compute accurately for complex\nmodels. Approaches such as bridge sampling, while effective, may suffer from\nissues of high variability of the estimates. We present how to estimate Monte\nCarlo standard error (MCSE) for bridge sampling, and how to diagnose the\nreliability of MCSE estimates using Pareto-$\\hat{k}$ and block reshuffling\ndiagnostics without the need to repeatedly re-run full posterior inference. We\ndemonstrate the behavior with increasingly more difficult simulated posteriors\nand many real posteriors from the posteriordb database."
                },
                "authors": [
                    {
                        "name": "Giorgio Micaletto"
                    },
                    {
                        "name": "Aki Vehtari"
                    }
                ],
                "author_detail": {
                    "name": "Aki Vehtari"
                },
                "author": "Aki Vehtari",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14486v1",
                "updated": "2025-08-20T07:21:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    21,
                    52,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T07:21:52Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    21,
                    52,
                    2,
                    232,
                    0
                ],
                "title": "WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation,\n  and Growth Stage Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation,\n  and Growth Stage Classification"
                },
                "summary": "Weed management represents a critical challenge in agriculture, significantly\nimpacting crop yields and requiring substantial resources for control.\nEffective weed monitoring and analysis strategies are crucial for implementing\nsustainable agricultural practices and site-specific management approaches. We\nintroduce WeedSense, a novel multi-task learning architecture for comprehensive\nweed analysis that jointly performs semantic segmentation, height estimation,\nand growth stage classification. We present a unique dataset capturing 16 weed\nspecies over an 11-week growth cycle with pixel-level annotations, height\nmeasurements, and temporal labels. WeedSense leverages a dual-path encoder\nincorporating Universal Inverted Bottleneck blocks and a Multi-Task Bifurcated\nDecoder with transformer-based feature fusion to generate multi-scale features\nand enable simultaneous prediction across multiple tasks. WeedSense outperforms\nother state-of-the-art models on our comprehensive evaluation. On our\nmulti-task dataset, WeedSense achieves mIoU of 89.78% for segmentation, 1.67cm\nMAE for height estimation, and 99.99% accuracy for growth stage classification\nwhile maintaining real-time inference at 160 FPS. Our multitask approach\nachieves 3$\\times$ faster inference than sequential single-task execution and\nuses 32.4% fewer parameters. Please see our project page at\nweedsense.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weed management represents a critical challenge in agriculture, significantly\nimpacting crop yields and requiring substantial resources for control.\nEffective weed monitoring and analysis strategies are crucial for implementing\nsustainable agricultural practices and site-specific management approaches. We\nintroduce WeedSense, a novel multi-task learning architecture for comprehensive\nweed analysis that jointly performs semantic segmentation, height estimation,\nand growth stage classification. We present a unique dataset capturing 16 weed\nspecies over an 11-week growth cycle with pixel-level annotations, height\nmeasurements, and temporal labels. WeedSense leverages a dual-path encoder\nincorporating Universal Inverted Bottleneck blocks and a Multi-Task Bifurcated\nDecoder with transformer-based feature fusion to generate multi-scale features\nand enable simultaneous prediction across multiple tasks. WeedSense outperforms\nother state-of-the-art models on our comprehensive evaluation. On our\nmulti-task dataset, WeedSense achieves mIoU of 89.78% for segmentation, 1.67cm\nMAE for height estimation, and 99.99% accuracy for growth stage classification\nwhile maintaining real-time inference at 160 FPS. Our multitask approach\nachieves 3$\\times$ faster inference than sequential single-task execution and\nuses 32.4% fewer parameters. Please see our project page at\nweedsense.github.io."
                },
                "authors": [
                    {
                        "name": "Toqi Tahamid Sarker"
                    },
                    {
                        "name": "Khaled R Ahmed"
                    },
                    {
                        "name": "Taminul Islam"
                    },
                    {
                        "name": "Cristiana Bernardi Rankrape"
                    },
                    {
                        "name": "Karla Gage"
                    }
                ],
                "author_detail": {
                    "name": "Karla Gage"
                },
                "author": "Karla Gage",
                "arxiv_comment": "This paper has been submitted and accepted for publication at ICCVW\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10016v2",
                "updated": "2025-08-20T07:04:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    4,
                    41,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-14T07:51:56Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    51,
                    56,
                    0,
                    195,
                    0
                ],
                "title": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling\n  via Multimodal Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling\n  via Multimodal Large Language Model Agents"
                },
                "summary": "Our research uncovers a novel privacy risk associated with multimodal large\nlanguage models (MLLMs): the ability to infer sensitive personal attributes\nfrom audio data -- a technique we term audio private attribute profiling. This\ncapability poses a significant threat, as audio can be covertly captured\nwithout direct interaction or visibility. Moreover, compared to images and\ntext, audio carries unique characteristics, such as tone and pitch, which can\nbe exploited for more detailed profiling. However, two key challenges exist in\nunderstanding MLLM-employed private attribute profiling from audio: (1) the\nlack of audio benchmark datasets with sensitive attribute annotations and (2)\nthe limited ability of current MLLMs to infer such attributes directly from\naudio. To address these challenges, we introduce AP^2, an audio benchmark\ndataset that consists of two subsets collected and composed from real-world\ndata, and both are annotated with sensitive attribute labels. Additionally, we\npropose Gifts, a hybrid multi-agent framework that leverages the complementary\nstrengths of audio-language models (ALMs) and large language models (LLMs) to\nenhance inference capabilities. Gifts employs an LLM to guide the ALM in\ninferring sensitive attributes, then forensically analyzes and consolidates the\nALM's inferences, overcoming severe hallucinations of existing ALMs in\ngenerating long-context responses. Our evaluations demonstrate that Gifts\nsignificantly outperforms baseline approaches in inferring sensitive\nattributes. Finally, we investigate model-level and data-level defense\nstrategies to mitigate the risks of audio private attribute profiling. Our work\nvalidates the feasibility of audio-based privacy attacks using MLLMs,\nhighlighting the need for robust defenses, and provides a dataset and framework\nto facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our research uncovers a novel privacy risk associated with multimodal large\nlanguage models (MLLMs): the ability to infer sensitive personal attributes\nfrom audio data -- a technique we term audio private attribute profiling. This\ncapability poses a significant threat, as audio can be covertly captured\nwithout direct interaction or visibility. Moreover, compared to images and\ntext, audio carries unique characteristics, such as tone and pitch, which can\nbe exploited for more detailed profiling. However, two key challenges exist in\nunderstanding MLLM-employed private attribute profiling from audio: (1) the\nlack of audio benchmark datasets with sensitive attribute annotations and (2)\nthe limited ability of current MLLMs to infer such attributes directly from\naudio. To address these challenges, we introduce AP^2, an audio benchmark\ndataset that consists of two subsets collected and composed from real-world\ndata, and both are annotated with sensitive attribute labels. Additionally, we\npropose Gifts, a hybrid multi-agent framework that leverages the complementary\nstrengths of audio-language models (ALMs) and large language models (LLMs) to\nenhance inference capabilities. Gifts employs an LLM to guide the ALM in\ninferring sensitive attributes, then forensically analyzes and consolidates the\nALM's inferences, overcoming severe hallucinations of existing ALMs in\ngenerating long-context responses. Our evaluations demonstrate that Gifts\nsignificantly outperforms baseline approaches in inferring sensitive\nattributes. Finally, we investigate model-level and data-level defense\nstrategies to mitigate the risks of audio private attribute profiling. Our work\nvalidates the feasibility of audio-based privacy attacks using MLLMs,\nhighlighting the need for robust defenses, and provides a dataset and framework\nto facilitate future research."
                },
                "authors": [
                    {
                        "name": "Lixu Wang"
                    },
                    {
                        "name": "Kaixiang Yao"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Dong Yang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Wei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Dong"
                },
                "author": "Wei Dong",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01174v2",
                "updated": "2025-08-20T07:00:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    0,
                    32,
                    2,
                    232,
                    0
                ],
                "published": "2024-03-02T10:56:27Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    10,
                    56,
                    27,
                    5,
                    62,
                    0
                ],
                "title": "Consistent and Optimal Solution to Camera Motion Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistent and Optimal Solution to Camera Motion Estimation"
                },
                "summary": "Given 2D point correspondences between an image pair, inferring the camera\nmotion is a fundamental issue in the computer vision community. The existing\nworks generally set out from the epipolar constraint and estimate the essential\nmatrix, which is not optimal in the maximum likelihood (ML) sense. In this\npaper, we dive into the original measurement model with respect to the rotation\nmatrix and normalized translation vector and formulate the ML problem. We then\npropose a two-step algorithm to solve it: In the first step, we estimate the\nvariance of measurement noises and devise a consistent estimator based on bias\nelimination; In the second step, we execute a one-step Gauss-Newton iteration\non manifold to refine the consistent estimate. We prove that the proposed\nestimate owns the same asymptotic statistical properties as the ML estimate:\nThe first is consistency, i.e., the estimate converges to the ground truth as\nthe point number increases; The second is asymptotic efficiency, i.e., the mean\nsquared error of the estimate converges to the theoretical lower bound --\nCramer-Rao bound. In addition, we show that our algorithm has linear time\ncomplexity. These appealing characteristics endow our estimator with a great\nadvantage in the case of dense point correspondences. Experiments on both\nsynthetic data and real images demonstrate that when the point number reaches\nthe order of hundreds, our estimator outperforms the state-of-the-art ones in\nterms of estimation accuracy and CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given 2D point correspondences between an image pair, inferring the camera\nmotion is a fundamental issue in the computer vision community. The existing\nworks generally set out from the epipolar constraint and estimate the essential\nmatrix, which is not optimal in the maximum likelihood (ML) sense. In this\npaper, we dive into the original measurement model with respect to the rotation\nmatrix and normalized translation vector and formulate the ML problem. We then\npropose a two-step algorithm to solve it: In the first step, we estimate the\nvariance of measurement noises and devise a consistent estimator based on bias\nelimination; In the second step, we execute a one-step Gauss-Newton iteration\non manifold to refine the consistent estimate. We prove that the proposed\nestimate owns the same asymptotic statistical properties as the ML estimate:\nThe first is consistency, i.e., the estimate converges to the ground truth as\nthe point number increases; The second is asymptotic efficiency, i.e., the mean\nsquared error of the estimate converges to the theoretical lower bound --\nCramer-Rao bound. In addition, we show that our algorithm has linear time\ncomplexity. These appealing characteristics endow our estimator with a great\nadvantage in the case of dense point correspondences. Experiments on both\nsynthetic data and real images demonstrate that when the point number reaches\nthe order of hundreds, our estimator outperforms the state-of-the-art ones in\nterms of estimation accuracy and CPU time."
                },
                "authors": [
                    {
                        "name": "Guangyang Zeng"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Xinghan Li"
                    },
                    {
                        "name": "Biqiang Mu"
                    },
                    {
                        "name": "Jiming Chen"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Junfeng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Wu"
                },
                "author": "Junfeng Wu",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09482v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09482v3",
                "updated": "2025-08-20T06:57:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    57,
                    17,
                    2,
                    232,
                    0
                ],
                "published": "2025-06-11T07:50:31Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    7,
                    50,
                    31,
                    2,
                    162,
                    0
                ],
                "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference\n  Autoregression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference\n  Autoregression"
                },
                "summary": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Frechet Inception Distance (FID) of\n1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Frechet Inception Distance (FID) of\n1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation."
                },
                "authors": [
                    {
                        "name": "Dingcheng Zhen"
                    },
                    {
                        "name": "Qian Qiao"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Tan Yu"
                    },
                    {
                        "name": "Kangxi Wu"
                    },
                    {
                        "name": "Ziwei Zhang"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Shunshun Yin"
                    },
                    {
                        "name": "Ming Tao"
                    }
                ],
                "author_detail": {
                    "name": "Ming Tao"
                },
                "author": "Ming Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09482v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09482v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14472v1",
                "updated": "2025-08-20T06:52:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    52,
                    42,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:52:42Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    52,
                    42,
                    2,
                    232,
                    0
                ],
                "title": "In2x at WMT25 Translation Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In2x at WMT25 Translation Task"
                },
                "summary": "This paper presents the open-system submission by the In2x research team for\nthe WMT25 General Machine Translation Shared Task. Our submission focuses on\nJapanese-related translation tasks, aiming to explore a generalizable paradigm\nfor extending large language models (LLMs) to other languages. This paradigm\nencompasses aspects such as data construction methods and reward model design.\nThe ultimate goal is to enable large language model systems to achieve\nexceptional performance in low-resource or less commonly spoken languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the open-system submission by the In2x research team for\nthe WMT25 General Machine Translation Shared Task. Our submission focuses on\nJapanese-related translation tasks, aiming to explore a generalizable paradigm\nfor extending large language models (LLMs) to other languages. This paradigm\nencompasses aspects such as data construction methods and reward model design.\nThe ultimate goal is to enable large language model systems to achieve\nexceptional performance in low-resource or less commonly spoken languages."
                },
                "authors": [
                    {
                        "name": "Lei Pang"
                    },
                    {
                        "name": "Hanyi Mao"
                    },
                    {
                        "name": "Quanjia Xiao"
                    },
                    {
                        "name": "HaiXiao Liu"
                    },
                    {
                        "name": "Xiangyi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyi Li"
                },
                "author": "Xiangyi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17442v2",
                "updated": "2025-08-20T06:44:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    44,
                    38,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-23T12:03:54Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    12,
                    3,
                    54,
                    2,
                    204,
                    0
                ],
                "title": "Each to Their Own: Exploring the Optimal Embedding in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Each to Their Own: Exploring the Optimal Embedding in RAG"
                },
                "summary": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication."
                },
                "authors": [
                    {
                        "name": "Shiting Chen"
                    },
                    {
                        "name": "Zijian Zhao"
                    },
                    {
                        "name": "Jinsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Chen"
                },
                "author": "Jinsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13654v2",
                "updated": "2025-08-20T06:41:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    41,
                    59,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-19T09:04:13Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    4,
                    13,
                    1,
                    231,
                    0
                ],
                "title": "Input Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Input Time Scaling"
                },
                "summary": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we combine\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\nfind a new phenomenon, training-testing co-design there. We need to apply query\nstrategies during both training and testing. Only applying strategies on\ntraining or testing would seriously degrade the performance. We are also\nsurprised to find that seemingly low data quality datasets can gain high\nperformance. Adding irrelevant information to the queries, randomly selecting\nexamples from a minimally filtered dataset, can even perform the best. These\nfindings contradict the widely held inductive bias, \"garbage in, garbage out\".\nCurating datasets with seemingly high-quality data can even potentially limit\nthe performance ceiling. In addition, models trained on more data with similar\nquality (15k VS 1k) perform worse, simple dataset size scaling should also be\ncarefully inspected. The good news is that our findings are compatible with the\nLess is More phenomenon. A small set of examples is enough to evoke high-level\nreasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,\nwe are able to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we combine\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\nfind a new phenomenon, training-testing co-design there. We need to apply query\nstrategies during both training and testing. Only applying strategies on\ntraining or testing would seriously degrade the performance. We are also\nsurprised to find that seemingly low data quality datasets can gain high\nperformance. Adding irrelevant information to the queries, randomly selecting\nexamples from a minimally filtered dataset, can even perform the best. These\nfindings contradict the widely held inductive bias, \"garbage in, garbage out\".\nCurating datasets with seemingly high-quality data can even potentially limit\nthe performance ceiling. In addition, models trained on more data with similar\nquality (15k VS 1k) perform worse, simple dataset size scaling should also be\ncarefully inspected. The good news is that our findings are compatible with the\nLess is More phenomenon. A small set of examples is enough to evoke high-level\nreasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,\nwe are able to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints."
                },
                "authors": [
                    {
                        "name": "Rapheal Huang"
                    },
                    {
                        "name": "Weilong Guo"
                    }
                ],
                "author_detail": {
                    "name": "Weilong Guo"
                },
                "arxiv_affiliation": "Yuming",
                "author": "Weilong Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05846v2",
                "updated": "2025-08-20T06:37:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    37,
                    23,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-08T09:25:21Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    25,
                    21,
                    1,
                    98,
                    0
                ],
                "title": "PathGPT: Reframing Path Recommendation as a Natural Language Generation\n  Task with Retrieval-Augmented Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PathGPT: Reframing Path Recommendation as a Natural Language Generation\n  Task with Retrieval-Augmented Language Models"
                },
                "summary": "Path recommendation (PR) aims to generate travel paths that are customized to\na user's specific preferences and constraints. Conventional approaches often\nemploy explicit optimization objectives or specialized machine learning\narchitectures; however, these methods typically exhibit limited flexibility and\ngeneralizability, necessitating costly retraining to accommodate new scenarios.\nThis paper introduces an alternative paradigm that conceptualizes PR as a\nnatural language generation task. We present PathGPT, a retrieval-augmented\nlarge language model (LLM) system that leverages historical trajectory data and\nnatural language user constraints to generate plausible paths. The proposed\nmethodology first converts raw trajectory data into a human-interpretable\ntextual format, which is then stored in a database. Subsequently, a hybrid\nretrieval system extracts path-specific context from this database to inform a\npretrained LLM. The primary contribution of this work is a novel framework that\ndemonstrates how integrating established information retrieval and generative\nmodel components can enable adaptive, zero-shot path generation across diverse\nscenarios. Extensive experiments on large-scale trajectory datasets indicate\nthat PathGPT's performance is competitive with specialized, learning-based\nmethods, underscoring its potential as a flexible and generalizable path\ngeneration system that avoids the need for retraining inherent in previous\ndata-driven models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path recommendation (PR) aims to generate travel paths that are customized to\na user's specific preferences and constraints. Conventional approaches often\nemploy explicit optimization objectives or specialized machine learning\narchitectures; however, these methods typically exhibit limited flexibility and\ngeneralizability, necessitating costly retraining to accommodate new scenarios.\nThis paper introduces an alternative paradigm that conceptualizes PR as a\nnatural language generation task. We present PathGPT, a retrieval-augmented\nlarge language model (LLM) system that leverages historical trajectory data and\nnatural language user constraints to generate plausible paths. The proposed\nmethodology first converts raw trajectory data into a human-interpretable\ntextual format, which is then stored in a database. Subsequently, a hybrid\nretrieval system extracts path-specific context from this database to inform a\npretrained LLM. The primary contribution of this work is a novel framework that\ndemonstrates how integrating established information retrieval and generative\nmodel components can enable adaptive, zero-shot path generation across diverse\nscenarios. Extensive experiments on large-scale trajectory datasets indicate\nthat PathGPT's performance is competitive with specialized, learning-based\nmethods, underscoring its potential as a flexible and generalizable path\ngeneration system that avoids the need for retraining inherent in previous\ndata-driven models."
                },
                "authors": [
                    {
                        "name": "Steeve Cuthbert Marcelyn"
                    },
                    {
                        "name": "Yucen Gao"
                    },
                    {
                        "name": "Yuzhe Zhang"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Gao"
                },
                "author": "Xiaofeng Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14461v1",
                "updated": "2025-08-20T06:32:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    32,
                    44,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:32:44Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    32,
                    44,
                    2,
                    232,
                    0
                ],
                "title": "Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and\n  Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and\n  Inverse Rendering"
                },
                "summary": "While multi-step diffusion models have advanced both forward and inverse\nrendering, existing approaches often treat these problems independently,\nleading to cycle inconsistency and slow inference speed. In this work, we\npresent Ouroboros, a framework composed of two single-step diffusion models\nthat handle forward and inverse rendering with mutual reinforcement. Our\napproach extends intrinsic decomposition to both indoor and outdoor scenes and\nintroduces a cycle consistency mechanism that ensures coherence between forward\nand inverse rendering outputs. Experimental results demonstrate\nstate-of-the-art performance across diverse scenes while achieving\nsubstantially faster inference speed compared to other diffusion-based methods.\nWe also demonstrate that Ouroboros can transfer to video decomposition in a\ntraining-free manner, reducing temporal inconsistency in video sequences while\nmaintaining high-quality per-frame inverse rendering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-step diffusion models have advanced both forward and inverse\nrendering, existing approaches often treat these problems independently,\nleading to cycle inconsistency and slow inference speed. In this work, we\npresent Ouroboros, a framework composed of two single-step diffusion models\nthat handle forward and inverse rendering with mutual reinforcement. Our\napproach extends intrinsic decomposition to both indoor and outdoor scenes and\nintroduces a cycle consistency mechanism that ensures coherence between forward\nand inverse rendering outputs. Experimental results demonstrate\nstate-of-the-art performance across diverse scenes while achieving\nsubstantially faster inference speed compared to other diffusion-based methods.\nWe also demonstrate that Ouroboros can transfer to video decomposition in a\ntraining-free manner, reducing temporal inconsistency in video sequences while\nmaintaining high-quality per-frame inverse rendering."
                },
                "authors": [
                    {
                        "name": "Shanlin Sun"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Yifeng Xiong"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Ruogu Fang"
                    },
                    {
                        "name": "Xiaohui Xie"
                    },
                    {
                        "name": "Chenyu You"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu You"
                },
                "author": "Chenyu You",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.14896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14896v1",
                "updated": "2025-08-20T17:59:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    59,
                    51,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:59:51Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    59,
                    51,
                    2,
                    232,
                    0
                ],
                "title": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs"
                },
                "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community."
                },
                "authors": [
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Haobo Xu"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Zhenan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Sun"
                },
                "author": "Zhenan Sun",
                "arxiv_comment": "Technical Report, Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14880v1",
                "updated": "2025-08-20T17:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    51,
                    20,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    51,
                    20,
                    2,
                    232,
                    0
                ],
                "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A\n  Knowledge-Informed Trajectory Synthesis Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A\n  Knowledge-Informed Trajectory Synthesis Framework"
                },
                "summary": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts.We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions.Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts.We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions.Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains."
                },
                "authors": [
                    {
                        "name": "Ailing Yu"
                    },
                    {
                        "name": "Lan Yao"
                    },
                    {
                        "name": "Jingnan Liu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Jiajun Yin"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xinhao Liao"
                    },
                    {
                        "name": "Zhiling Ye"
                    },
                    {
                        "name": "Ji Li"
                    },
                    {
                        "name": "Yun Yue"
                    },
                    {
                        "name": "Hansong Xiao"
                    },
                    {
                        "name": "Hualei Zhou"
                    },
                    {
                        "name": "Chunxiao Guo"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14879v1",
                "updated": "2025-08-20T17:50:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    50,
                    15,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:50:15Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    50,
                    15,
                    2,
                    232,
                    0
                ],
                "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds"
                },
                "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding."
                },
                "authors": [
                    {
                        "name": "Bingquan Dai"
                    },
                    {
                        "name": "Li Ray Luo"
                    },
                    {
                        "name": "Qihong Tang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xinyu Lian"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Minghan Qin"
                    },
                    {
                        "name": "Xudong Xu"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhaoyang Lyu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14869v1",
                "updated": "2025-08-20T17:31:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    31,
                    53,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:31:53Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    31,
                    53,
                    2,
                    232,
                    0
                ],
                "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding\n  Large Language Models"
                },
                "summary": "Prompt engineering has rapidly emerged as a critical skill for effective\ninteraction with large language models (LLMs). However, the cognitive and\nneural underpinnings of this expertise remain largely unexplored. This paper\npresents findings from a cross-sectional pilot fMRI study investigating\ndifferences in brain functional connectivity and network activity between\nexperts and intermediate prompt engineers. Our results reveal distinct neural\nsignatures associated with higher prompt engineering literacy, including\nincreased functional connectivity in brain regions such as the left middle\ntemporal gyrus and the left frontal pole, as well as altered power-frequency\ndynamics in key cognitive networks. These findings offer initial insights into\nthe neurobiological basis of prompt engineering proficiency. We discuss the\nimplications of these neurocognitive markers in Natural Language Processing\n(NLP). Understanding the neural basis of human expertise in interacting with\nLLMs can inform the design of more intuitive human-AI interfaces, contribute to\ncognitive models of LLM interaction, and potentially guide the development of\nAI systems that better align with human cognitive workflows. This\ninterdisciplinary approach aims to bridge the gap between human cognition and\nmachine intelligence, fostering a deeper understanding of how humans learn and\nadapt to complex AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering has rapidly emerged as a critical skill for effective\ninteraction with large language models (LLMs). However, the cognitive and\nneural underpinnings of this expertise remain largely unexplored. This paper\npresents findings from a cross-sectional pilot fMRI study investigating\ndifferences in brain functional connectivity and network activity between\nexperts and intermediate prompt engineers. Our results reveal distinct neural\nsignatures associated with higher prompt engineering literacy, including\nincreased functional connectivity in brain regions such as the left middle\ntemporal gyrus and the left frontal pole, as well as altered power-frequency\ndynamics in key cognitive networks. These findings offer initial insights into\nthe neurobiological basis of prompt engineering proficiency. We discuss the\nimplications of these neurocognitive markers in Natural Language Processing\n(NLP). Understanding the neural basis of human expertise in interacting with\nLLMs can inform the design of more intuitive human-AI interfaces, contribute to\ncognitive models of LLM interaction, and potentially guide the development of\nAI systems that better align with human cognitive workflows. This\ninterdisciplinary approach aims to bridge the gap between human cognition and\nmachine intelligence, fostering a deeper understanding of how humans learn and\nadapt to complex AI systems."
                },
                "authors": [
                    {
                        "name": "Hend Al-Khalifa"
                    },
                    {
                        "name": "Raneem Almansour"
                    },
                    {
                        "name": "Layan Abdulrahman Alhuasini"
                    },
                    {
                        "name": "Alanood Alsaleh"
                    },
                    {
                        "name": "Mohamad-Hani Temsah"
                    },
                    {
                        "name": "Mohamad-Hani_Temsah"
                    },
                    {
                        "name": "Ashwag Rafea S Alruwaili"
                    }
                ],
                "author_detail": {
                    "name": "Ashwag Rafea S Alruwaili"
                },
                "author": "Ashwag Rafea S Alruwaili",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02085v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02085v4",
                "updated": "2025-08-20T17:19:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    19,
                    48,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-04T05:51:55Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    5,
                    51,
                    55,
                    0,
                    216,
                    0
                ],
                "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents"
                },
                "summary": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent."
                },
                "authors": [
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Yifu Guo"
                    },
                    {
                        "name": "Yuzhen Han"
                    },
                    {
                        "name": "Sen Hu"
                    },
                    {
                        "name": "Ziyi Ni"
                    },
                    {
                        "name": "Licheng Wang"
                    },
                    {
                        "name": "Mingguang Chen"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Huacan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huacan Wang"
                },
                "author": "Huacan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02085v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02085v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18889v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18889v4",
                "updated": "2025-08-20T17:03:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    3,
                    48,
                    2,
                    232,
                    0
                ],
                "published": "2025-05-24T22:22:43Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    22,
                    22,
                    43,
                    5,
                    144,
                    0
                ],
                "title": "Security Concerns for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Concerns for Large Language Models: A Survey"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused\na revolution in natural language processing, but their capabilities also\nintroduce new security vulnerabilities. This survey provides a comprehensive\noverview of these emerging concerns, categorizing threats into several key\nareas: prompt injection and jailbreaking; adversarial attacks, including input\nperturbations and data poisoning; misuse by malicious actors to generate\ndisinformation, phishing emails, and malware; and the worrisome risks inherent\nin autonomous LLM agents. Recently, a significant focus is increasingly being\nplaced on the latter, exploring goal misalignment, emergent deception,\nself-preservation instincts, and the potential for LLMs to develop and pursue\ncovert, misaligned objectives, a behavior known as scheming, which may even\npersist through safety training. We summarize recent academic and industrial\nstudies from 2022 to 2025 that exemplify each threat, analyze proposed defenses\nand their limitations, and identify open challenges in securing LLM-based\napplications. We conclude by emphasizing the importance of advancing robust,\nmulti-layered security strategies to ensure LLMs are safe and beneficial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused\na revolution in natural language processing, but their capabilities also\nintroduce new security vulnerabilities. This survey provides a comprehensive\noverview of these emerging concerns, categorizing threats into several key\nareas: prompt injection and jailbreaking; adversarial attacks, including input\nperturbations and data poisoning; misuse by malicious actors to generate\ndisinformation, phishing emails, and malware; and the worrisome risks inherent\nin autonomous LLM agents. Recently, a significant focus is increasingly being\nplaced on the latter, exploring goal misalignment, emergent deception,\nself-preservation instincts, and the potential for LLMs to develop and pursue\ncovert, misaligned objectives, a behavior known as scheming, which may even\npersist through safety training. We summarize recent academic and industrial\nstudies from 2022 to 2025 that exemplify each threat, analyze proposed defenses\nand their limitations, and identify open challenges in securing LLM-based\napplications. We conclude by emphasizing the importance of advancing robust,\nmulti-layered security strategies to ensure LLMs are safe and beneficial."
                },
                "authors": [
                    {
                        "name": "Miles Q. Li"
                    },
                    {
                        "name": "Benjamin C. M. Fung"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin C. M. Fung"
                },
                "author": "Benjamin C. M. Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18889v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18889v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14853v1",
                "updated": "2025-08-20T17:03:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    3,
                    32,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T17:03:32Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    3,
                    32,
                    2,
                    232,
                    0
                ],
                "title": "Universal and Transferable Adversarial Attack on Large Language Models\n  Using Exponentiated Gradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal and Transferable Adversarial Attack on Large Language Models\n  Using Exponentiated Gradient Descent"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in critical\napplications, ensuring their robustness and safety alignment remains a major\nchallenge. Despite the overall success of alignment techniques such as\nreinforcement learning from human feedback (RLHF) on typical prompts, LLMs\nremain vulnerable to jailbreak attacks enabled by crafted adversarial triggers\nappended to user prompts. Most existing jailbreak methods either rely on\ninefficient searches over discrete token spaces or direct optimization of\ncontinuous embeddings. While continuous embeddings can be given directly to\nselected open-source models as input, doing so is not feasible for proprietary\nmodels. On the other hand, projecting these embeddings back into valid discrete\ntokens introduces additional complexity and often reduces attack effectiveness.\nWe propose an intrinsic optimization method which directly optimizes relaxed\none-hot encodings of the adversarial suffix tokens using exponentiated gradient\ndescent coupled with Bregman projection, ensuring that the optimized one-hot\nencoding of each token always remains within the probability simplex. We\nprovide theoretical proof of convergence for our proposed method and implement\nan efficient algorithm that effectively jailbreaks several widely used LLMs.\nOur method achieves higher success rates and faster convergence compared to\nthree state-of-the-art baselines, evaluated on five open-source LLMs and four\nadversarial behavior datasets curated for evaluating jailbreak methods. In\naddition to individual prompt attacks, we also generate universal adversarial\nsuffixes effective across multiple prompts and demonstrate transferability of\noptimized suffixes to different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in critical\napplications, ensuring their robustness and safety alignment remains a major\nchallenge. Despite the overall success of alignment techniques such as\nreinforcement learning from human feedback (RLHF) on typical prompts, LLMs\nremain vulnerable to jailbreak attacks enabled by crafted adversarial triggers\nappended to user prompts. Most existing jailbreak methods either rely on\ninefficient searches over discrete token spaces or direct optimization of\ncontinuous embeddings. While continuous embeddings can be given directly to\nselected open-source models as input, doing so is not feasible for proprietary\nmodels. On the other hand, projecting these embeddings back into valid discrete\ntokens introduces additional complexity and often reduces attack effectiveness.\nWe propose an intrinsic optimization method which directly optimizes relaxed\none-hot encodings of the adversarial suffix tokens using exponentiated gradient\ndescent coupled with Bregman projection, ensuring that the optimized one-hot\nencoding of each token always remains within the probability simplex. We\nprovide theoretical proof of convergence for our proposed method and implement\nan efficient algorithm that effectively jailbreaks several widely used LLMs.\nOur method achieves higher success rates and faster convergence compared to\nthree state-of-the-art baselines, evaluated on five open-source LLMs and four\nadversarial behavior datasets curated for evaluating jailbreak methods. In\naddition to individual prompt attacks, we also generate universal adversarial\nsuffixes effective across multiple prompts and demonstrate transferability of\noptimized suffixes to different LLMs."
                },
                "authors": [
                    {
                        "name": "Sajib Biswas"
                    },
                    {
                        "name": "Mao Nishino"
                    },
                    {
                        "name": "Samuel Jacob Chacko"
                    },
                    {
                        "name": "Xiuwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiuwen Liu"
                },
                "author": "Xiuwen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19485v2",
                "updated": "2025-08-20T16:32:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    32,
                    6,
                    2,
                    232,
                    0
                ],
                "published": "2024-11-29T05:54:41Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    54,
                    41,
                    4,
                    334,
                    0
                ],
                "title": "Action Engine: Automatic Workflow Generation in FaaS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action Engine: Automatic Workflow Generation in FaaS"
                },
                "summary": "Function as a Service (FaaS) is poised to become the foundation of the next\ngeneration of cloud systems due to its inherent advantages in scalability,\ncost-efficiency, and ease of use. However, challenges such as the need for\nspecialized knowledge, platform dependence, and difficulty in scalability in\nbuilding functional workflows persist for cloud-native application developers.\nTo overcome these challenges and mitigate the burden of developing FaaS-based\napplications, in this paper, we propose a mechanism called Action Engine, that\nmakes use of tool-augmented large language models (LLMs) at its kernel to\ninterpret human language queries and automates FaaS workflow generation,\nthereby, reducing the need for specialized expertise and manual design. Action\nEngine includes modules to identify relevant functions from the FaaS repository\nand seamlessly manage the data dependency between them, ensuring the\ndeveloper's query is processed and resolved. Beyond that, Action Engine can\nexecute the generated workflow by injecting the user-provided arguments. On\nanother front, this work addresses a gap in tool-augmented LLM research via\nadopting an Automatic FaaS Workflow Generation perspective to systematically\nevaluate methodologies across four fundamental sub-processes. Through\nbenchmarking various parameters, this research provides critical insights into\nstreamlining workflow automation for real-world applications, specifically in\nthe FaaS continuum. Our evaluations demonstrate that the Action Engine achieves\ncomparable performance to the few-shot learning approach while maintaining\nplatform- and language-agnosticism, thereby, mitigating provider-specific\ndependencies in workflow generation. We notice that Action Engine can unlock\nFaaS workflow generation for non-cloud-savvy developers and expedite the\ndevelopment cycles of cloud-native applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function as a Service (FaaS) is poised to become the foundation of the next\ngeneration of cloud systems due to its inherent advantages in scalability,\ncost-efficiency, and ease of use. However, challenges such as the need for\nspecialized knowledge, platform dependence, and difficulty in scalability in\nbuilding functional workflows persist for cloud-native application developers.\nTo overcome these challenges and mitigate the burden of developing FaaS-based\napplications, in this paper, we propose a mechanism called Action Engine, that\nmakes use of tool-augmented large language models (LLMs) at its kernel to\ninterpret human language queries and automates FaaS workflow generation,\nthereby, reducing the need for specialized expertise and manual design. Action\nEngine includes modules to identify relevant functions from the FaaS repository\nand seamlessly manage the data dependency between them, ensuring the\ndeveloper's query is processed and resolved. Beyond that, Action Engine can\nexecute the generated workflow by injecting the user-provided arguments. On\nanother front, this work addresses a gap in tool-augmented LLM research via\nadopting an Automatic FaaS Workflow Generation perspective to systematically\nevaluate methodologies across four fundamental sub-processes. Through\nbenchmarking various parameters, this research provides critical insights into\nstreamlining workflow automation for real-world applications, specifically in\nthe FaaS continuum. Our evaluations demonstrate that the Action Engine achieves\ncomparable performance to the few-shot learning approach while maintaining\nplatform- and language-agnosticism, thereby, mitigating provider-specific\ndependencies in workflow generation. We notice that Action Engine can unlock\nFaaS workflow generation for non-cloud-savvy developers and expedite the\ndevelopment cycles of cloud-native applications."
                },
                "authors": [
                    {
                        "name": "Akiharu Esashi"
                    },
                    {
                        "name": "Pawissanutt Lertpongrujikorn"
                    },
                    {
                        "name": "Shinji Kato"
                    },
                    {
                        "name": "Mohsen Amini Salehi"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Amini Salehi"
                },
                "author": "Mohsen Amini Salehi",
                "arxiv_comment": "Published in the Future Generation Computer Systems (FGCS) journal;\n  Source code is available at: https://github.com/hpcclab/action_engine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08239v2",
                "updated": "2025-08-20T16:27:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    27,
                    42,
                    2,
                    232,
                    0
                ],
                "published": "2024-09-12T17:39:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    39,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources"
                },
                "summary": "Synthetic data generation has recently emerged as a promising approach for\nenhancing the capabilities of large language models (LLMs) without the need for\nexpensive human annotations. However, existing methods often generate data that\ncan be low quality or contrived. In this paper, we introduce Source2Synth, a\nscalable approach for synthetic data generation and curation that is grounded\nin real-world data sources. Source2Synth takes as input a custom data source\nand produces synthetic data examples with intermediate reasoning steps. Our\nmethod improves the dataset quality by discarding low-quality generations based\non their answerability. We demonstrate the generality of this approach by\napplying it to two tasks that leverage two different types of data: multi-hop\nquestion answering (MHQA), where we test complex reasoning abilities leveraging\ndocuments, and tabular question answering (TQA), where we test tool usage\nleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQL\nand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data generation has recently emerged as a promising approach for\nenhancing the capabilities of large language models (LLMs) without the need for\nexpensive human annotations. However, existing methods often generate data that\ncan be low quality or contrived. In this paper, we introduce Source2Synth, a\nscalable approach for synthetic data generation and curation that is grounded\nin real-world data sources. Source2Synth takes as input a custom data source\nand produces synthetic data examples with intermediate reasoning steps. Our\nmethod improves the dataset quality by discarding low-quality generations based\non their answerability. We demonstrate the generality of this approach by\napplying it to two tasks that leverage two different types of data: multi-hop\nquestion answering (MHQA), where we test complex reasoning abilities leveraging\ndocuments, and tabular question answering (TQA), where we test tool usage\nleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQL\nand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines."
                },
                "authors": [
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Carlos Gemmell"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Jane Dwivedi-Yu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Maria Lomeli"
                    }
                ],
                "author_detail": {
                    "name": "Maria Lomeli"
                },
                "author": "Maria Lomeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14830v1",
                "updated": "2025-08-20T16:25:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    25,
                    37,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T16:25:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    25,
                    37,
                    2,
                    232,
                    0
                ],
                "title": "MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and\n  Fair Resource Allocation in IoT Ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and\n  Fair Resource Allocation in IoT Ecosystems"
                },
                "summary": "The rapid growth of Internet of Things (IoT) ecosystems has intensified the\nchallenge of efficiently allocating heterogeneous resources in highly dynamic,\ndistributed environments. Conventional centralized mechanisms and\nsingle-objective auction models, focusing solely on metrics such as cost\nminimization or revenue maximization, struggle to deliver balanced system\nperformance. This paper proposes the Multi-Objective Hierarchical Auction\nFramework (MOHAF), a distributed resource allocation mechanism that jointly\noptimizes cost, Quality of Service (QoS), energy efficiency, and fairness.\nMOHAF integrates hierarchical clustering to reduce computational complexity\nwith a greedy, submodular optimization strategy that guarantees a (1-1/e)\napproximation ratio. A dynamic pricing mechanism adapts in real time to\nresource utilization, enhancing market stability and allocation quality.\nExtensive experiments on the Google Cluster Data trace, comprising 3,553\nrequests and 888 resources, demonstrate MOHAF's superior allocation efficiency\n(0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101)\nauctions, while achieving perfect fairness (Jain's index = 1.000). Ablation\nstudies reveal the critical influence of cost and QoS components in sustaining\nbalanced multi-objective outcomes. With near-linear scalability, theoretical\nguarantees, and robust empirical performance, MOHAF offers a practical and\nadaptable solution for large-scale IoT deployments, effectively reconciling\nefficiency, equity, and sustainability in distributed resource coordination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Internet of Things (IoT) ecosystems has intensified the\nchallenge of efficiently allocating heterogeneous resources in highly dynamic,\ndistributed environments. Conventional centralized mechanisms and\nsingle-objective auction models, focusing solely on metrics such as cost\nminimization or revenue maximization, struggle to deliver balanced system\nperformance. This paper proposes the Multi-Objective Hierarchical Auction\nFramework (MOHAF), a distributed resource allocation mechanism that jointly\noptimizes cost, Quality of Service (QoS), energy efficiency, and fairness.\nMOHAF integrates hierarchical clustering to reduce computational complexity\nwith a greedy, submodular optimization strategy that guarantees a (1-1/e)\napproximation ratio. A dynamic pricing mechanism adapts in real time to\nresource utilization, enhancing market stability and allocation quality.\nExtensive experiments on the Google Cluster Data trace, comprising 3,553\nrequests and 888 resources, demonstrate MOHAF's superior allocation efficiency\n(0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101)\nauctions, while achieving perfect fairness (Jain's index = 1.000). Ablation\nstudies reveal the critical influence of cost and QoS components in sustaining\nbalanced multi-objective outcomes. With near-linear scalability, theoretical\nguarantees, and robust empirical performance, MOHAF offers a practical and\nadaptable solution for large-scale IoT deployments, effectively reconciling\nefficiency, equity, and sustainability in distributed resource coordination."
                },
                "authors": [
                    {
                        "name": "Kushagra Agrawal"
                    },
                    {
                        "name": "Polat Goktas"
                    },
                    {
                        "name": "Anjan Bandopadhyay"
                    },
                    {
                        "name": "Debolina Ghosh"
                    },
                    {
                        "name": "Junali Jasmine Jena"
                    },
                    {
                        "name": "Mahendra Kumar Gourisaria"
                    }
                ],
                "author_detail": {
                    "name": "Mahendra Kumar Gourisaria"
                },
                "author": "Mahendra Kumar Gourisaria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14828v1",
                "updated": "2025-08-20T16:22:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    22,
                    51,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T16:22:51Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    22,
                    51,
                    2,
                    232,
                    0
                ],
                "title": "Long Chain-of-Thought Reasoning Across Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Chain-of-Thought Reasoning Across Languages"
                },
                "summary": "Scaling inference through long chains-of-thought (CoTs) has unlocked\nimpressive reasoning capabilities in large language models (LLMs), yet the\nreasoning process remains almost exclusively English-centric. We construct\ntranslated versions of two popular English reasoning datasets, fine-tune Qwen\n2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT\ngeneration across French, Japanese, Latvian, and Swahili. Our experiments\nreveal three key findings. First, the efficacy of using English as a pivot\nlanguage varies by language: it provides no benefit for French, improves\nperformance when used as the reasoning language for Japanese and Latvian, and\nproves insufficient for Swahili where both task comprehension and reasoning\nremain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but\ndoes not eliminate the cross-lingual performance gap. A lightweight fine-tune\nusing only 1k traces still improves performance by over 30\\% in Swahili. Third,\ndata quality versus scale trade-offs are language dependent: small, carefully\ncurated datasets suffice for English and French, whereas larger but noisier\ncorpora prove more effective for Swahili and Latvian. Together, these results\nclarify when and why long CoTs transfer across languages and provide translated\ndatasets to foster equitable multilingual reasoning research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference through long chains-of-thought (CoTs) has unlocked\nimpressive reasoning capabilities in large language models (LLMs), yet the\nreasoning process remains almost exclusively English-centric. We construct\ntranslated versions of two popular English reasoning datasets, fine-tune Qwen\n2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT\ngeneration across French, Japanese, Latvian, and Swahili. Our experiments\nreveal three key findings. First, the efficacy of using English as a pivot\nlanguage varies by language: it provides no benefit for French, improves\nperformance when used as the reasoning language for Japanese and Latvian, and\nproves insufficient for Swahili where both task comprehension and reasoning\nremain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but\ndoes not eliminate the cross-lingual performance gap. A lightweight fine-tune\nusing only 1k traces still improves performance by over 30\\% in Swahili. Third,\ndata quality versus scale trade-offs are language dependent: small, carefully\ncurated datasets suffice for English and French, whereas larger but noisier\ncorpora prove more effective for Swahili and Latvian. Together, these results\nclarify when and why long CoTs transfer across languages and provide translated\ndatasets to foster equitable multilingual reasoning research."
                },
                "authors": [
                    {
                        "name": "Josh Barua"
                    },
                    {
                        "name": "Seun Eisape"
                    },
                    {
                        "name": "Kayo Yin"
                    },
                    {
                        "name": "Alane Suhr"
                    }
                ],
                "author_detail": {
                    "name": "Alane Suhr"
                },
                "author": "Alane Suhr",
                "arxiv_comment": "Accepted to SCALR @ COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14823v1",
                "updated": "2025-08-20T16:15:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    15,
                    59,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T16:15:59Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    15,
                    59,
                    2,
                    232,
                    0
                ],
                "title": "Using an LLM to Investigate Students' Explanations on Conceptual Physics\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using an LLM to Investigate Students' Explanations on Conceptual Physics\n  Questions"
                },
                "summary": "Analyzing students' written solutions to physics questions is a major area in\nPER. However, gauging student understanding in college courses is bottlenecked\nby large class sizes, which limits assessments to a multiple-choice (MC) format\nfor ease of grading. Although sufficient in quantifying scientifically correct\nconceptions, MC assessments do not uncover students' deeper ways of\nunderstanding physics. Large language models (LLMs) offer a promising approach\nfor assessing students' written responses at scale. Our study used an LLM,\nvalidated by human graders, to classify students' written explanations to three\nquestions on the Energy and Momentum Conceptual Survey as correct or incorrect,\nand organized students' incorrect explanations into emergent categories. We\nfound that the LLM (GPT-4o) can fairly assess students' explanations,\ncomparable to human graders (0-3% discrepancy). Furthermore, the categories of\nincorrect explanations were different from corresponding MC distractors,\nallowing for different and deeper conceptions to become accessible to\neducators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing students' written solutions to physics questions is a major area in\nPER. However, gauging student understanding in college courses is bottlenecked\nby large class sizes, which limits assessments to a multiple-choice (MC) format\nfor ease of grading. Although sufficient in quantifying scientifically correct\nconceptions, MC assessments do not uncover students' deeper ways of\nunderstanding physics. Large language models (LLMs) offer a promising approach\nfor assessing students' written responses at scale. Our study used an LLM,\nvalidated by human graders, to classify students' written explanations to three\nquestions on the Energy and Momentum Conceptual Survey as correct or incorrect,\nand organized students' incorrect explanations into emergent categories. We\nfound that the LLM (GPT-4o) can fairly assess students' explanations,\ncomparable to human graders (0-3% discrepancy). Furthermore, the categories of\nincorrect explanations were different from corresponding MC distractors,\nallowing for different and deeper conceptions to become accessible to\neducators."
                },
                "authors": [
                    {
                        "name": "Sean Savage"
                    },
                    {
                        "name": "N. Sanjay Rebello"
                    }
                ],
                "author_detail": {
                    "name": "N. Sanjay Rebello"
                },
                "author": "N. Sanjay Rebello",
                "arxiv_comment": "5 pages, 3 figures and Physics Education Research Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14817v1",
                "updated": "2025-08-20T16:09:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    9,
                    37,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T16:09:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    9,
                    37,
                    2,
                    232,
                    0
                ],
                "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for\n  Clinical Reasoning over EHRs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for\n  Clinical Reasoning over EHRs"
                },
                "summary": "Electronic health records (EHRs) are long, noisy, and often redundant, posing\na major challenge for the clinicians who must navigate them. Large language\nmodels (LLMs) offer a promising solution for extracting and reasoning over this\nunstructured text, but the length of clinical notes often exceeds even\nstate-of-the-art models' extended context windows. Retrieval-augmented\ngeneration (RAG) offers an alternative by retrieving task-relevant passages\nfrom across the entire EHR, potentially reducing the amount of required input\ntokens. In this work, we propose three clinical tasks designed to be replicable\nacross health systems with minimal effort: 1) extracting imaging procedures, 2)\ngenerating timelines of antibiotic use, and 3) identifying key diagnoses. Using\nEHRs from actual hospitalized patients, we test three state-of-the-art LLMs\nwith varying amounts of provided context, using either targeted text retrieval\nor the most recent clinical notes. We find that RAG closely matches or exceeds\nthe performance of using recent notes, and approaches the performance of using\nthe models' full context while requiring drastically fewer input tokens. Our\nresults suggest that RAG remains a competitive and efficient approach even as\nnewer models become capable of handling increasingly longer amounts of text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic health records (EHRs) are long, noisy, and often redundant, posing\na major challenge for the clinicians who must navigate them. Large language\nmodels (LLMs) offer a promising solution for extracting and reasoning over this\nunstructured text, but the length of clinical notes often exceeds even\nstate-of-the-art models' extended context windows. Retrieval-augmented\ngeneration (RAG) offers an alternative by retrieving task-relevant passages\nfrom across the entire EHR, potentially reducing the amount of required input\ntokens. In this work, we propose three clinical tasks designed to be replicable\nacross health systems with minimal effort: 1) extracting imaging procedures, 2)\ngenerating timelines of antibiotic use, and 3) identifying key diagnoses. Using\nEHRs from actual hospitalized patients, we test three state-of-the-art LLMs\nwith varying amounts of provided context, using either targeted text retrieval\nor the most recent clinical notes. We find that RAG closely matches or exceeds\nthe performance of using recent notes, and approaches the performance of using\nthe models' full context while requiring drastically fewer input tokens. Our\nresults suggest that RAG remains a competitive and efficient approach even as\nnewer models become capable of handling increasingly longer amounts of text."
                },
                "authors": [
                    {
                        "name": "Skatje Myers"
                    },
                    {
                        "name": "Dmitriy Dligach"
                    },
                    {
                        "name": "Timothy A. Miller"
                    },
                    {
                        "name": "Samantha Barr"
                    },
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Matthew Churpek"
                    },
                    {
                        "name": "Anoop Mayampurath"
                    },
                    {
                        "name": "Majid Afshar"
                    }
                ],
                "author_detail": {
                    "name": "Majid Afshar"
                },
                "author": "Majid Afshar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00050v2",
                "updated": "2025-08-20T16:01:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    16,
                    1,
                    52,
                    2,
                    232,
                    0
                ],
                "published": "2025-03-31T02:18:51Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    18,
                    51,
                    0,
                    90,
                    0
                ],
                "title": "JudgeLRM: Large Reasoning Models as a Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JudgeLRM: Large Reasoning Models as a Judge"
                },
                "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Qingyun Zou"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14802v1",
                "updated": "2025-08-20T15:52:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    52,
                    34,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:52:34Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    52,
                    34,
                    2,
                    232,
                    0
                ],
                "title": "Privileged Self-Access Matters for Introspection in AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privileged Self-Access Matters for Introspection in AI"
                },
                "summary": "Whether AI models can introspect is an increasingly important practical\nquestion. But there is no consensus on how introspection is to be defined.\nBeginning from a recently proposed ''lightweight'' definition, we argue instead\nfor a thicker one. According to our proposal, introspection in AI is any\nprocess which yields information about internal states through a process more\nreliable than one with equal or lower computational cost available to a third\nparty. Using experiments where LLMs reason about their internal temperature\nparameters, we show they can appear to have lightweight introspection while\nfailing to meaningfully introspect per our proposed definition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether AI models can introspect is an increasingly important practical\nquestion. But there is no consensus on how introspection is to be defined.\nBeginning from a recently proposed ''lightweight'' definition, we argue instead\nfor a thicker one. According to our proposal, introspection in AI is any\nprocess which yields information about internal states through a process more\nreliable than one with equal or lower computational cost available to a third\nparty. Using experiments where LLMs reason about their internal temperature\nparameters, we show they can appear to have lightweight introspection while\nfailing to meaningfully introspect per our proposed definition."
                },
                "authors": [
                    {
                        "name": "Siyuan Song"
                    },
                    {
                        "name": "Harvey Lederman"
                    },
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Kyle Mahowald"
                    }
                ],
                "author_detail": {
                    "name": "Kyle Mahowald"
                },
                "author": "Kyle Mahowald",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21755v2",
                "updated": "2025-08-20T15:49:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    49,
                    30,
                    2,
                    232,
                    0
                ],
                "published": "2025-03-27T17:57:01Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    1,
                    3,
                    86,
                    0
                ],
                "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness"
                },
                "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nto individual dimensions, our evaluation framework integrates generalists such\nas SOTA VLMs and LLMs, and specialists, including anomaly detection methods\nproposed for video generation. We conduct extensive human annotations to ensure\nevaluation alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nto individual dimensions, our evaluation framework integrates generalists such\nas SOTA VLMs and LLMs, and specialists, including anomaly detection methods\nproposed for video generation. We conduct extensive human annotations to ensure\nevaluation alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness."
                },
                "authors": [
                    {
                        "name": "Dian Zheng"
                    },
                    {
                        "name": "Ziqi Huang"
                    },
                    {
                        "name": "Hongbo Liu"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Lulu Gu"
                    },
                    {
                        "name": "Yuanhan Zhang"
                    },
                    {
                        "name": "Jingwen He"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Equal contributions from first two authors. Project page:\n  https://vchitect.github.io/VBench-2.0-project/ Code:\n  https://github.com/Vchitect/VBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11370v2",
                "updated": "2025-08-20T15:45:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    45,
                    11,
                    2,
                    232,
                    0
                ],
                "published": "2023-12-18T17:36:20Z",
                "published_parsed": [
                    2023,
                    12,
                    18,
                    17,
                    36,
                    20,
                    0,
                    352,
                    0
                ],
                "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model"
                },
                "summary": "Large language models (LLMs) have shown remarkable proficiency in human-level\nreasoning and generation capabilities, which encourages extensive research on\ntheir application in mathematical problem solving. However, current work has\nbeen largely focused on text-based mathematical problems, with limited\ninvestigation in problems involving geometric information. Addressing this gap,\nwe aim to enable LLMs to solve geometric problems by understanding image input.\nWe first analyze the limitations of current Multimodal Large Language Models\n(MLLMs) in this area: they struggle to accurately comprehending basic geometric\nelements and their relationships. To overcome these challenges, we take\nadvantage of the unique characteristics of geometric problems (such as unique\ngeometric logical form, and geometric scalability) and the capacity of the\ntextual LLMs to build an enriched multimodal geometry dataset based on existing\ndata. The augmented dataset, Geo170K, contains more than 170K geometric\nimage-caption and question-answer pairs. Utilizing our constructed Geo170K\ndataset, we develop G-LLaVA, which demonstrates exceptional performance in\nsolving geometric problems, significantly outperforming GPT-4-V on the\nMathVista benchmark with only 7B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable proficiency in human-level\nreasoning and generation capabilities, which encourages extensive research on\ntheir application in mathematical problem solving. However, current work has\nbeen largely focused on text-based mathematical problems, with limited\ninvestigation in problems involving geometric information. Addressing this gap,\nwe aim to enable LLMs to solve geometric problems by understanding image input.\nWe first analyze the limitations of current Multimodal Large Language Models\n(MLLMs) in this area: they struggle to accurately comprehending basic geometric\nelements and their relationships. To overcome these challenges, we take\nadvantage of the unique characteristics of geometric problems (such as unique\ngeometric logical form, and geometric scalability) and the capacity of the\ntextual LLMs to build an enriched multimodal geometry dataset based on existing\ndata. The augmented dataset, Geo170K, contains more than 170K geometric\nimage-caption and question-answer pairs. Utilizing our constructed Geo170K\ndataset, we develop G-LLaVA, which demonstrates exceptional performance in\nsolving geometric problems, significantly outperforming GPT-4-V on the\nMathVista benchmark with only 7B parameters."
                },
                "authors": [
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Jiacheng Ye"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14783v1",
                "updated": "2025-08-20T15:29:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    29,
                    0,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:29:00Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    29,
                    0,
                    2,
                    232,
                    0
                ],
                "title": "Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge\n  Distillation Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge\n  Distillation Method"
                },
                "summary": "Model distillation enables the transfer of knowledge from large-scale models\nto compact student models, facilitating deployment in resource-constrained\nenvironments. However, conventional distillation approaches often suffer from\ncomputational overhead and limited generalization. We propose a novel adaptive\ndistillation framework that dynamically augments training data in regions of\nhigh student model loss. Using UMAP-based dimensionality reduction and nearest\nneighbor sampling, our method identifies underperforming regions in the\nembedding space and generates targeted synthetic examples to guide student\nlearning. To further improve efficiency, we introduce a lightweight\nteacher-student interface that bypasses the teacher's input layer, enabling\ndirect distillation on vectorized representations. Experiments across standard\nNLP benchmarks demonstrate that our 66M-parameter student model consistently\nmatches or surpasses established baselines, achieving 91.2% on QNLI and 92.3%\non SST-2, while training with fewer epochs. These results highlight the promise\nof loss-aware data augmentation and vectorized distillation for efficient and\neffective model compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model distillation enables the transfer of knowledge from large-scale models\nto compact student models, facilitating deployment in resource-constrained\nenvironments. However, conventional distillation approaches often suffer from\ncomputational overhead and limited generalization. We propose a novel adaptive\ndistillation framework that dynamically augments training data in regions of\nhigh student model loss. Using UMAP-based dimensionality reduction and nearest\nneighbor sampling, our method identifies underperforming regions in the\nembedding space and generates targeted synthetic examples to guide student\nlearning. To further improve efficiency, we introduce a lightweight\nteacher-student interface that bypasses the teacher's input layer, enabling\ndirect distillation on vectorized representations. Experiments across standard\nNLP benchmarks demonstrate that our 66M-parameter student model consistently\nmatches or surpasses established baselines, achieving 91.2% on QNLI and 92.3%\non SST-2, while training with fewer epochs. These results highlight the promise\nof loss-aware data augmentation and vectorized distillation for efficient and\neffective model compression."
                },
                "authors": [
                    {
                        "name": "Suleyman Olcay Polat"
                    },
                    {
                        "name": "Poli A. Nemkova"
                    },
                    {
                        "name": "Mark V. Albert"
                    }
                ],
                "author_detail": {
                    "name": "Mark V. Albert"
                },
                "author": "Mark V. Albert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14782v1",
                "updated": "2025-08-20T15:27:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    27,
                    49,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:27:49Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    27,
                    49,
                    2,
                    232,
                    0
                ],
                "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban\n  Transportation via Learnable Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransLLM: A Unified Multi-Task Foundation Framework for Urban\n  Transportation via Learnable Prompting"
                },
                "summary": "Urban transportation systems encounter diverse challenges across multiple\ntasks, such as traffic forecasting, electric vehicle (EV) charging demand\nprediction, and taxi dispatch. Existing approaches suffer from two key\nlimitations: small-scale deep learning models are task-specific and\ndata-hungry, limiting their generalizability across diverse scenarios, while\nlarge language models (LLMs), despite offering flexibility through natural\nlanguage interfaces, struggle with structured spatiotemporal data and numerical\nreasoning in transportation domains. To address these limitations, we propose\nTransLLM, a unified foundation framework that integrates spatiotemporal\nmodeling with large language models through learnable prompt composition. Our\napproach features a lightweight spatiotemporal encoder that captures complex\ndependencies via dilated temporal convolutions and dual-adjacency graph\nattention networks, seamlessly interfacing with LLMs through structured\nembeddings. A novel instance-level prompt routing mechanism, trained via\nreinforcement learning, dynamically personalizes prompts based on input\ncharacteristics, moving beyond fixed task-specific templates. The framework\noperates by encoding spatiotemporal patterns into contextual representations,\ndynamically composing personalized prompts to guide LLM reasoning, and\nprojecting the resulting representations through specialized output layers to\ngenerate task-specific predictions. Experiments across seven datasets and three\ntasks demonstrate the exceptional effectiveness of TransLLM in both supervised\nand zero-shot settings. Compared to ten baseline models, it delivers\ncompetitive performance on both regression and planning problems, showing\nstrong generalization and cross-task adaptability. Our code is available at\nhttps://github.com/BiYunying/TransLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban transportation systems encounter diverse challenges across multiple\ntasks, such as traffic forecasting, electric vehicle (EV) charging demand\nprediction, and taxi dispatch. Existing approaches suffer from two key\nlimitations: small-scale deep learning models are task-specific and\ndata-hungry, limiting their generalizability across diverse scenarios, while\nlarge language models (LLMs), despite offering flexibility through natural\nlanguage interfaces, struggle with structured spatiotemporal data and numerical\nreasoning in transportation domains. To address these limitations, we propose\nTransLLM, a unified foundation framework that integrates spatiotemporal\nmodeling with large language models through learnable prompt composition. Our\napproach features a lightweight spatiotemporal encoder that captures complex\ndependencies via dilated temporal convolutions and dual-adjacency graph\nattention networks, seamlessly interfacing with LLMs through structured\nembeddings. A novel instance-level prompt routing mechanism, trained via\nreinforcement learning, dynamically personalizes prompts based on input\ncharacteristics, moving beyond fixed task-specific templates. The framework\noperates by encoding spatiotemporal patterns into contextual representations,\ndynamically composing personalized prompts to guide LLM reasoning, and\nprojecting the resulting representations through specialized output layers to\ngenerate task-specific predictions. Experiments across seven datasets and three\ntasks demonstrate the exceptional effectiveness of TransLLM in both supervised\nand zero-shot settings. Compared to ten baseline models, it delivers\ncompetitive performance on both regression and planning problems, showing\nstrong generalization and cross-task adaptability. Our code is available at\nhttps://github.com/BiYunying/TransLLM."
                },
                "authors": [
                    {
                        "name": "Jiaming Leng"
                    },
                    {
                        "name": "Yunying Bi"
                    },
                    {
                        "name": "Chuan Qin"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Chao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Wang"
                },
                "author": "Chao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14779v1",
                "updated": "2025-08-20T15:25:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    25,
                    16,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:25:16Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    25,
                    16,
                    2,
                    232,
                    0
                ],
                "title": "Adversarial Hospital-Invariant Feature Learning for WSI Patch\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Hospital-Invariant Feature Learning for WSI Patch\n  Classification"
                },
                "summary": "Pathology foundation models (PFMs) have demonstrated remarkable potential in\nwhole-slide image (WSI) diagnosis. However, pathology images from different\nhospitals often vary due to differences in scanning hardware and preprocessing\nstyles, which may lead PFMs to inadvertently learn hospital-specific features,\nposing risks for clinical deployment. In this work, we present the first\nsystematic study of domain bias in PFMs arising from hospital source\ncharacteristics. Specifically, we (1) construct a pipeline for quantifying\ndomain bias in PFMs, (2) evaluate and compare the performance of multiple\nmodels, and (3) propose a lightweight adversarial framework that removes latent\nhospital-specific features from frozen representations without modifying the\nencoder itself. By introducing a trainable adapter and a domain classifier\nconnected through a gradient reversal layer (GRL), our method learns\ntask-discriminative yet domain-invariant representations. Experiments on\nmulti-center histopathology datasets demonstrate that our approach\nsubstantially reduces domain predictability while maintaining or even improving\ndisease classification performance, particularly in out-of-domain (unseen\nhospital) scenarios. Further analyses, including hospital detection and feature\nspace visualization, confirm the effectiveness of our method in mitigating\nhospital bias. We will provide our code based on acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathology foundation models (PFMs) have demonstrated remarkable potential in\nwhole-slide image (WSI) diagnosis. However, pathology images from different\nhospitals often vary due to differences in scanning hardware and preprocessing\nstyles, which may lead PFMs to inadvertently learn hospital-specific features,\nposing risks for clinical deployment. In this work, we present the first\nsystematic study of domain bias in PFMs arising from hospital source\ncharacteristics. Specifically, we (1) construct a pipeline for quantifying\ndomain bias in PFMs, (2) evaluate and compare the performance of multiple\nmodels, and (3) propose a lightweight adversarial framework that removes latent\nhospital-specific features from frozen representations without modifying the\nencoder itself. By introducing a trainable adapter and a domain classifier\nconnected through a gradient reversal layer (GRL), our method learns\ntask-discriminative yet domain-invariant representations. Experiments on\nmulti-center histopathology datasets demonstrate that our approach\nsubstantially reduces domain predictability while maintaining or even improving\ndisease classification performance, particularly in out-of-domain (unseen\nhospital) scenarios. Further analyses, including hospital detection and feature\nspace visualization, confirm the effectiveness of our method in mitigating\nhospital bias. We will provide our code based on acceptance."
                },
                "authors": [
                    {
                        "name": "Mengliang Zhang"
                    },
                    {
                        "name": "Jacob M. Luber"
                    }
                ],
                "author_detail": {
                    "name": "Jacob M. Luber"
                },
                "author": "Jacob M. Luber",
                "arxiv_comment": "8 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14769v1",
                "updated": "2025-08-20T15:17:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    17,
                    59,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:17:59Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    17,
                    59,
                    2,
                    232,
                    0
                ],
                "title": "Federated Distillation on Edge Devices: Efficient Client-Side Filtering\n  for Non-IID Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Distillation on Edge Devices: Efficient Client-Side Filtering\n  for Non-IID Data"
                },
                "summary": "Federated distillation has emerged as a promising collaborative machine\nlearning approach, offering enhanced privacy protection and reduced\ncommunication compared to traditional federated learning by exchanging model\noutputs (soft logits) rather than full model parameters. However, existing\nmethods employ complex selective knowledge-sharing strategies that require\nclients to identify in-distribution proxy data through computationally\nexpensive statistical density ratio estimators. Additionally, server-side\nfiltering of ambiguous knowledge introduces latency to the process. To address\nthese challenges, we propose a robust, resource-efficient EdgeFD method that\nreduces the complexity of the client-side density ratio estimation and removes\nthe need for server-side filtering. EdgeFD introduces an efficient KMeans-based\ndensity ratio estimator for effectively filtering both in-distribution and\nout-of-distribution proxy data on clients, significantly improving the quality\nof knowledge sharing. We evaluate EdgeFD across diverse practical scenarios,\nincluding strong non-IID, weak non-IID, and IID data distributions on clients,\nwithout requiring a pre-trained teacher model on the server for knowledge\ndistillation. Experimental results demonstrate that EdgeFD outperforms\nstate-of-the-art methods, consistently achieving accuracy levels close to IID\nscenarios even under heterogeneous and challenging conditions. The\nsignificantly reduced computational overhead of the KMeans-based estimator is\nsuitable for deployment on resource-constrained edge devices, thereby enhancing\nthe scalability and real-world applicability of federated distillation. The\ncode is available online for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated distillation has emerged as a promising collaborative machine\nlearning approach, offering enhanced privacy protection and reduced\ncommunication compared to traditional federated learning by exchanging model\noutputs (soft logits) rather than full model parameters. However, existing\nmethods employ complex selective knowledge-sharing strategies that require\nclients to identify in-distribution proxy data through computationally\nexpensive statistical density ratio estimators. Additionally, server-side\nfiltering of ambiguous knowledge introduces latency to the process. To address\nthese challenges, we propose a robust, resource-efficient EdgeFD method that\nreduces the complexity of the client-side density ratio estimation and removes\nthe need for server-side filtering. EdgeFD introduces an efficient KMeans-based\ndensity ratio estimator for effectively filtering both in-distribution and\nout-of-distribution proxy data on clients, significantly improving the quality\nof knowledge sharing. We evaluate EdgeFD across diverse practical scenarios,\nincluding strong non-IID, weak non-IID, and IID data distributions on clients,\nwithout requiring a pre-trained teacher model on the server for knowledge\ndistillation. Experimental results demonstrate that EdgeFD outperforms\nstate-of-the-art methods, consistently achieving accuracy levels close to IID\nscenarios even under heterogeneous and challenging conditions. The\nsignificantly reduced computational overhead of the KMeans-based estimator is\nsuitable for deployment on resource-constrained edge devices, thereby enhancing\nthe scalability and real-world applicability of federated distillation. The\ncode is available online for reproducibility."
                },
                "authors": [
                    {
                        "name": "Ahmed Mujtaba"
                    },
                    {
                        "name": "Gleb Radchenko"
                    },
                    {
                        "name": "Radu Prodan"
                    },
                    {
                        "name": "Marc Masana"
                    }
                ],
                "author_detail": {
                    "name": "Marc Masana"
                },
                "author": "Marc Masana",
                "arxiv_comment": "This paper was accepted at the International Conference on Federated\n  Learning Technologies and Applications, 2025. The final version is available\n  at IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14765v1",
                "updated": "2025-08-20T15:13:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    13,
                    52,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:13:52Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    13,
                    52,
                    2,
                    232,
                    0
                ],
                "title": "PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT\n  SFT and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT\n  SFT and Reinforcement Learning"
                },
                "summary": "Designing therapeutic peptides with tailored properties is hindered by the\nvastness of sequence space, limited experimental data, and poor\ninterpretability of current generative models. To address these challenges, we\nintroduce PepThink-R1, a generative framework that integrates large language\nmodels (LLMs) with chain-of-thought (CoT) supervised fine-tuning and\nreinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly\nreasons about monomer-level modifications during sequence generation, enabling\ninterpretable design choices while optimizing for multiple pharmacological\nproperties. Guided by a tailored reward function balancing chemical validity\nand property improvements, the model autonomously explores diverse sequence\nvariants. We demonstrate that PepThink-R1 generates cyclic peptides with\nsignificantly enhanced lipophilicity, stability, and exposure, outperforming\nexisting general LLMs (e.g., GPT-5) and domain-specific baseline in both\noptimization success and interpretability. To our knowledge, this is the first\nLLM-based peptide design framework that combines explicit reasoning with\nRL-driven property control, marking a step toward reliable and transparent\npeptide optimization for therapeutic discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing therapeutic peptides with tailored properties is hindered by the\nvastness of sequence space, limited experimental data, and poor\ninterpretability of current generative models. To address these challenges, we\nintroduce PepThink-R1, a generative framework that integrates large language\nmodels (LLMs) with chain-of-thought (CoT) supervised fine-tuning and\nreinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly\nreasons about monomer-level modifications during sequence generation, enabling\ninterpretable design choices while optimizing for multiple pharmacological\nproperties. Guided by a tailored reward function balancing chemical validity\nand property improvements, the model autonomously explores diverse sequence\nvariants. We demonstrate that PepThink-R1 generates cyclic peptides with\nsignificantly enhanced lipophilicity, stability, and exposure, outperforming\nexisting general LLMs (e.g., GPT-5) and domain-specific baseline in both\noptimization success and interpretability. To our knowledge, this is the first\nLLM-based peptide design framework that combines explicit reasoning with\nRL-driven property control, marking a step toward reliable and transparent\npeptide optimization for therapeutic discovery."
                },
                "authors": [
                    {
                        "name": "Ruheng Wang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Trieu Nguyen"
                    },
                    {
                        "name": "Shasha Feng"
                    },
                    {
                        "name": "Hao-Wei Pang"
                    },
                    {
                        "name": "Xiang Yu"
                    },
                    {
                        "name": "Li Xiao"
                    },
                    {
                        "name": "Peter Zhiping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Peter Zhiping Zhang"
                },
                "author": "Peter Zhiping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14764v1",
                "updated": "2025-08-20T15:12:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    12,
                    52,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T15:12:52Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    12,
                    52,
                    2,
                    232,
                    0
                ],
                "title": "Investigation of the Inter-Rater Reliability between Large Language\n  Models and Human Raters in Qualitative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigation of the Inter-Rater Reliability between Large Language\n  Models and Human Raters in Qualitative Analysis"
                },
                "summary": "Qualitative analysis is typically limited to small datasets because it is\ntime-intensive. Moreover, a second human rater is required to ensure reliable\nfindings. Artificial intelligence tools may replace human raters if we\ndemonstrate high reliability compared to human ratings. We investigated the\ninter-rater reliability of state-of-the-art Large Language Models (LLMs),\nChatGPT-4o and ChatGPT-4.5-preview, in rating audio transcripts coded manually.\nWe explored prompts and hyperparameters to optimize model performance. The\nparticipants were 14 undergraduate student groups from a university in the\nmidwestern United States who discussed problem-solving strategies for a\nproject. We prompted an LLM to replicate manual coding, and calculated Cohen's\nKappa for inter-rater reliability. After optimizing model hyperparameters and\nprompts, the results showed substantial agreement (${\\kappa}>0.6$) for three\nthemes and moderate agreement on one. Our findings demonstrate the potential of\nGPT-4o and GPT-4.5 for efficient, scalable qualitative analysis in physics\neducation and identify their limitations in rating domain-general constructs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative analysis is typically limited to small datasets because it is\ntime-intensive. Moreover, a second human rater is required to ensure reliable\nfindings. Artificial intelligence tools may replace human raters if we\ndemonstrate high reliability compared to human ratings. We investigated the\ninter-rater reliability of state-of-the-art Large Language Models (LLMs),\nChatGPT-4o and ChatGPT-4.5-preview, in rating audio transcripts coded manually.\nWe explored prompts and hyperparameters to optimize model performance. The\nparticipants were 14 undergraduate student groups from a university in the\nmidwestern United States who discussed problem-solving strategies for a\nproject. We prompted an LLM to replicate manual coding, and calculated Cohen's\nKappa for inter-rater reliability. After optimizing model hyperparameters and\nprompts, the results showed substantial agreement (${\\kappa}>0.6$) for three\nthemes and moderate agreement on one. Our findings demonstrate the potential of\nGPT-4o and GPT-4.5 for efficient, scalable qualitative analysis in physics\neducation and identify their limitations in rating domain-general constructs."
                },
                "authors": [
                    {
                        "name": "Nikhil Sanjay Borse"
                    },
                    {
                        "name": "Ravishankar Chatta Subramaniam"
                    },
                    {
                        "name": "N. Sanjay Rebello"
                    }
                ],
                "author_detail": {
                    "name": "N. Sanjay Rebello"
                },
                "author": "N. Sanjay Rebello",
                "arxiv_comment": "7 pages, 4 figures, Physics Education Research Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02866v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02866v3",
                "updated": "2025-08-20T15:00:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    15,
                    0,
                    50,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-04T19:54:40Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    19,
                    54,
                    40,
                    0,
                    216,
                    0
                ],
                "title": "PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in\n  Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in\n  Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) and other foundation models are increasingly\nused as the core of AI agents. In agentic workflows, these agents plan tasks,\ninteract with humans and peers, and influence scientific outcomes across\nfederated and heterogeneous environments. However, agents can hallucinate or\nreason incorrectly, propagating errors when one agent's output becomes\nanother's input. Thus, assuring that agents' actions are transparent,\ntraceable, reproducible, and reliable is critical to assess hallucination risks\nand mitigate their workflow impacts. While provenance techniques have long\nsupported these principles, existing methods fail to capture and relate\nagent-centric metadata such as prompts, responses, and decisions with the\nbroader workflow context and downstream outcomes. In this paper, we introduce\nPROV-AGENT, a provenance model that extends W3C PROV and leverages the Model\nContext Protocol (MCP) and data observability to integrate agent interactions\ninto end-to-end workflow provenance. Our contributions include: (1) a\nprovenance model tailored for agentic workflows, (2) a near real-time,\nopen-source system for capturing agentic provenance, and (3) a cross-facility\nevaluation spanning edge, cloud, and HPC environments, demonstrating support\nfor critical provenance queries and agent reliability analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and other foundation models are increasingly\nused as the core of AI agents. In agentic workflows, these agents plan tasks,\ninteract with humans and peers, and influence scientific outcomes across\nfederated and heterogeneous environments. However, agents can hallucinate or\nreason incorrectly, propagating errors when one agent's output becomes\nanother's input. Thus, assuring that agents' actions are transparent,\ntraceable, reproducible, and reliable is critical to assess hallucination risks\nand mitigate their workflow impacts. While provenance techniques have long\nsupported these principles, existing methods fail to capture and relate\nagent-centric metadata such as prompts, responses, and decisions with the\nbroader workflow context and downstream outcomes. In this paper, we introduce\nPROV-AGENT, a provenance model that extends W3C PROV and leverages the Model\nContext Protocol (MCP) and data observability to integrate agent interactions\ninto end-to-end workflow provenance. Our contributions include: (1) a\nprovenance model tailored for agentic workflows, (2) a near real-time,\nopen-source system for capturing agentic provenance, and (3) a cross-facility\nevaluation spanning edge, cloud, and HPC environments, demonstrating support\nfor critical provenance queries and agent reliability analysis."
                },
                "authors": [
                    {
                        "name": "Renan Souza"
                    },
                    {
                        "name": "Amal Gueroudji"
                    },
                    {
                        "name": "Stephen DeWitt"
                    },
                    {
                        "name": "Daniel Rosendo"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    },
                    {
                        "name": "Robert Ross"
                    },
                    {
                        "name": "Prasanna Balaprakash"
                    },
                    {
                        "name": "Rafael Ferreira da Silva"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Ferreira da Silva"
                },
                "author": "Rafael Ferreira da Silva",
                "arxiv_comment": "Paper accepted for publication in the Proceedings of the 2025 IEEE\n  21st International Conference on e-Science. Cite it as: R. Souza, A.\n  Gueroudji, S. DeWitt, D. Rosendo, T. Ghosal, R. Ross, P. Balaprakash, R. F.\n  da Silva, \"PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions\n  in Agentic Workflows,\" IEEE International Conference on e-Science, Chicago,\n  IL, USA, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02866v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02866v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 68T30, 68P20, 68Q85, 68M14,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.12; H.2.4; I.2.11; C.2.4; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14755v1",
                "updated": "2025-08-20T14:58:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    58,
                    5,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:58:05Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    58,
                    5,
                    2,
                    232,
                    0
                ],
                "title": "Reliable generation of isomorphic physics problems using ChatGPT with\n  prompt-chaining and tool use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable generation of isomorphic physics problems using ChatGPT with\n  prompt-chaining and tool use"
                },
                "summary": "We present a method for generating large numbers of isomorphic physics\nproblems using ChatGPT through prompt chaining and tool use. This approach\nenables precise control over structural variations-such as numeric values and\nspatial relations-while supporting diverse contextual variations in the problem\nbody. By utilizing the Python code interpreter, the method supports automatic\nsolution validation and simple diagram generation, addressing key limitations\nin existing LLM-based methods. We generated two example isomorphic problem\nbanks and compared the outcome against simpler prompt-based approaches. Results\nshow that prompt-chaining produces significantly higher quality and more\nconsistent outputs than simpler, non-chaining prompts. This work demonstrates a\npromising method for efficient problem creation accessible to the average\ninstructor, which opens new possibilities for personalized adaptive testing and\nautomated content development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a method for generating large numbers of isomorphic physics\nproblems using ChatGPT through prompt chaining and tool use. This approach\nenables precise control over structural variations-such as numeric values and\nspatial relations-while supporting diverse contextual variations in the problem\nbody. By utilizing the Python code interpreter, the method supports automatic\nsolution validation and simple diagram generation, addressing key limitations\nin existing LLM-based methods. We generated two example isomorphic problem\nbanks and compared the outcome against simpler prompt-based approaches. Results\nshow that prompt-chaining produces significantly higher quality and more\nconsistent outputs than simpler, non-chaining prompts. This work demonstrates a\npromising method for efficient problem creation accessible to the average\ninstructor, which opens new possibilities for personalized adaptive testing and\nautomated content development."
                },
                "authors": [
                    {
                        "name": "Zhongzhou Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhongzhou Chen"
                },
                "author": "Zhongzhou Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14751v1",
                "updated": "2025-08-20T14:50:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    50,
                    28,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:50:28Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    50,
                    28,
                    2,
                    232,
                    0
                ],
                "title": "HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents"
                },
                "summary": "Open-ended AI agents need to be able to learn efficiently goals of increasing\ncomplexity, abstraction and heterogeneity over their lifetime. Beyond sampling\nefficiently their own goals, autotelic agents specifically need to be able to\nkeep the growing complexity of goals under control, limiting the associated\ngrowth in sample and computational complexity. To adress this challenge, recent\napproaches have leveraged hierarchical reinforcement learning (HRL) and\nlanguage, capitalizing on its compositional and combinatorial generalization\ncapabilities to acquire temporally extended reusable behaviours. Existing\napproaches use expert defined spaces of subgoals over which they instantiate a\nhierarchy, and often assume pre-trained associated low-level policies. Such\ndesigns are inadequate in open-ended scenarios, where goal spaces naturally\ndiversify across a broad spectrum of difficulties. We introduce HERAKLES, a\nframework that enables a two-level hierarchical autotelic agent to continuously\ncompile mastered goals into the low-level policy, executed by a small, fast\nneural network, dynamically expanding the set of subgoals available to the\nhigh-level policy. We train a Large Language Model (LLM) to serve as the\nhigh-level controller, exploiting its strengths in goal decomposition and\ngeneralization to operate effectively over this evolving subgoal space. We\nevaluate HERAKLES in the open-ended Crafter environment and show that it scales\neffectively with goal complexity, improves sample efficiency through skill\ncompilation, and enables the agent to adapt robustly to novel challenges over\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended AI agents need to be able to learn efficiently goals of increasing\ncomplexity, abstraction and heterogeneity over their lifetime. Beyond sampling\nefficiently their own goals, autotelic agents specifically need to be able to\nkeep the growing complexity of goals under control, limiting the associated\ngrowth in sample and computational complexity. To adress this challenge, recent\napproaches have leveraged hierarchical reinforcement learning (HRL) and\nlanguage, capitalizing on its compositional and combinatorial generalization\ncapabilities to acquire temporally extended reusable behaviours. Existing\napproaches use expert defined spaces of subgoals over which they instantiate a\nhierarchy, and often assume pre-trained associated low-level policies. Such\ndesigns are inadequate in open-ended scenarios, where goal spaces naturally\ndiversify across a broad spectrum of difficulties. We introduce HERAKLES, a\nframework that enables a two-level hierarchical autotelic agent to continuously\ncompile mastered goals into the low-level policy, executed by a small, fast\nneural network, dynamically expanding the set of subgoals available to the\nhigh-level policy. We train a Large Language Model (LLM) to serve as the\nhigh-level controller, exploiting its strengths in goal decomposition and\ngeneralization to operate effectively over this evolving subgoal space. We\nevaluate HERAKLES in the open-ended Crafter environment and show that it scales\neffectively with goal complexity, improves sample efficiency through skill\ncompilation, and enables the agent to adapt robustly to novel challenges over\ntime."
                },
                "authors": [
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Clément Romac"
                    },
                    {
                        "name": "Loris Gaven"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Lamprier"
                },
                "author": "Sylvain Lamprier",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14746v1",
                "updated": "2025-08-20T14:43:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    43,
                    4,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:43:04Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    43,
                    4,
                    2,
                    232,
                    0
                ],
                "title": "MissionHD: Data-Driven Refinement of Reasoning Graph Structure through\n  Hyperdimensional Causal Path Encoding and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MissionHD: Data-Driven Refinement of Reasoning Graph Structure through\n  Hyperdimensional Causal Path Encoding and Decoding"
                },
                "summary": "Reasoning graphs from Large Language Models (LLMs) are often misaligned with\ndownstream visual tasks such as video anomaly detection (VAD). Existing Graph\nStructure Refinement (GSR) methods are ill-suited for these novel, dataset-less\ngraphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly\noptimizes graph structure using downstream task data, and propose MissionHD, a\nhyperdimensional computing (HDC) framework to operationalize it. MissionHD uses\nan efficient encode-decode process to refine the graph, guided by the\ndownstream task signal. Experiments on challenging VAD and VAR benchmarks show\nsignificant performance improvements when using our refined graphs, validating\nour approach as an effective pre-processing step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning graphs from Large Language Models (LLMs) are often misaligned with\ndownstream visual tasks such as video anomaly detection (VAD). Existing Graph\nStructure Refinement (GSR) methods are ill-suited for these novel, dataset-less\ngraphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly\noptimizes graph structure using downstream task data, and propose MissionHD, a\nhyperdimensional computing (HDC) framework to operationalize it. MissionHD uses\nan efficient encode-decode process to refine the graph, guided by the\ndownstream task signal. Experiments on challenging VAD and VAR benchmarks show\nsignificant performance improvements when using our refined graphs, validating\nour approach as an effective pre-processing step."
                },
                "authors": [
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Raheeb Hassan"
                    },
                    {
                        "name": "Ryozo Masukawa"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14735v1",
                "updated": "2025-08-20T14:30:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    30,
                    34,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:30:34Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    30,
                    34,
                    2,
                    232,
                    0
                ],
                "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via\n  Synthetic Natural Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Multilingual and Code-Switched Alignment in LLMs via\n  Synthetic Natural Language Inference"
                },
                "summary": "Large language models (LLMs) are increasingly applied in multilingual\ncontexts, yet their capacity for consistent, logically grounded alignment\nacross languages remains underexplored. We present a controlled evaluation\nframework for multilingual natural language inference (NLI) that generates\nsynthetic, logic-based premise-hypothesis pairs and translates them into a\ntypologically diverse set of languages. This design enables precise control\nover semantic relations and allows testing in both monolingual and\nmixed-language (code-switched) conditions. Surprisingly, code-switching does\nnot degrade, and can even improve, performance, suggesting that\ntranslation-induced lexical variation may serve as a regularization signal. We\nvalidate semantic preservation through embedding-based similarity analyses and\ncross-lingual alignment visualizations, confirming the fidelity of translated\npairs. Our findings expose both the potential and the brittleness of current\nLLM cross-lingual reasoning, and identify code-switching as a promising lever\nfor improving multilingual robustness. Code available at:\nhttps://github.com/KurbanIntelligenceLab/nli-stress-testing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied in multilingual\ncontexts, yet their capacity for consistent, logically grounded alignment\nacross languages remains underexplored. We present a controlled evaluation\nframework for multilingual natural language inference (NLI) that generates\nsynthetic, logic-based premise-hypothesis pairs and translates them into a\ntypologically diverse set of languages. This design enables precise control\nover semantic relations and allows testing in both monolingual and\nmixed-language (code-switched) conditions. Surprisingly, code-switching does\nnot degrade, and can even improve, performance, suggesting that\ntranslation-induced lexical variation may serve as a regularization signal. We\nvalidate semantic preservation through embedding-based similarity analyses and\ncross-lingual alignment visualizations, confirming the fidelity of translated\npairs. Our findings expose both the potential and the brittleness of current\nLLM cross-lingual reasoning, and identify code-switching as a promising lever\nfor improving multilingual robustness. Code available at:\nhttps://github.com/KurbanIntelligenceLab/nli-stress-testing"
                },
                "authors": [
                    {
                        "name": "Samir Abdaljalil"
                    },
                    {
                        "name": "Erchin Serpedin"
                    },
                    {
                        "name": "Khalid Qaraqe"
                    },
                    {
                        "name": "Hasan Kurban"
                    }
                ],
                "author_detail": {
                    "name": "Hasan Kurban"
                },
                "author": "Hasan Kurban",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19254v3",
                "updated": "2025-08-20T14:26:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    26,
                    48,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-27T14:24:45Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    24,
                    45,
                    6,
                    117,
                    0
                ],
                "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box,\n  White-Box, LLM Judge, and Ensemble Scorers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification for Language Models: A Suite of Black-Box,\n  White-Box, LLM Judge, and Ensemble Scorers"
                },
                "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we outline a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we propose a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we outline a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we propose a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Dylan Bouchard"
                    },
                    {
                        "name": "Mohit Singh Chauhan"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Singh Chauhan"
                },
                "author": "Mohit Singh Chauhan",
                "arxiv_comment": "UQLM repository: https://github.com/cvs-health/uqlm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19061v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19061v3",
                "updated": "2025-08-20T14:24:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    24,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-27T00:39:12Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    0,
                    39,
                    12,
                    6,
                    117,
                    0
                ],
                "title": "Hallucinations and Key Information Extraction in Medical Texts: A\n  Comprehensive Assessment of Open-Source Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations and Key Information Extraction in Medical Texts: A\n  Comprehensive Assessment of Open-Source Large Language Models"
                },
                "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, including admission reasons, major in-hospital events, and\ncritical follow-up actions. In addition, we also assess the prevalence of\nvarious types of hallucinations in the summaries produced by these models.\nDetecting hallucinations is vital as it directly influences the reliability of\nthe information, potentially affecting patient care and treatment outcomes. We\nconduct comprehensive simulations to rigorously evaluate the performance of\nthese models, further probing the accuracy and fidelity of the extracted\ncontent in clinical summarization. Our results reveal that while the LLMs\n(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission\nreasons and hospitalization events, they are generally less consistent when it\ncomes to identifying follow-up recommendations, highlighting broader challenges\nin leveraging LLMs for comprehensive summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, including admission reasons, major in-hospital events, and\ncritical follow-up actions. In addition, we also assess the prevalence of\nvarious types of hallucinations in the summaries produced by these models.\nDetecting hallucinations is vital as it directly influences the reliability of\nthe information, potentially affecting patient care and treatment outcomes. We\nconduct comprehensive simulations to rigorously evaluate the performance of\nthese models, further probing the accuracy and fidelity of the extracted\ncontent in clinical summarization. Our results reveal that while the LLMs\n(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission\nreasons and hospitalization events, they are generally less consistent when it\ncomes to identifying follow-up recommendations, highlighting broader challenges\nin leveraging LLMs for comprehensive summarization."
                },
                "authors": [
                    {
                        "name": "Anindya Bijoy Das"
                    },
                    {
                        "name": "Shibbir Ahmed"
                    },
                    {
                        "name": "Shahnewaz Karim Sakib"
                    }
                ],
                "author_detail": {
                    "name": "Shahnewaz Karim Sakib"
                },
                "author": "Shahnewaz Karim Sakib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19061v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19061v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14727v1",
                "updated": "2025-08-20T14:16:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    16,
                    21,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:16:21Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    16,
                    21,
                    2,
                    232,
                    0
                ],
                "title": "Assessing the Quality and Security of AI-Generated Code: A Quantitative\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Quality and Security of AI-Generated Code: A Quantitative\n  Analysis"
                },
                "summary": "This study presents a quantitative evaluation of the code quality and\nsecurity of five prominent Large Language Models (LLMs): Claude Sonnet 4,\nClaude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior\nresearch has assessed the functional performance of LLM-generated code, this\nresearch tested LLM output from 4,442 Java coding assignments through\ncomprehensive static analysis using SonarQube. The findings suggest that\nalthough LLMs can generate functional code, they also introduce a range of\nsoftware defects, including bugs, security vulnerabilities, and code smells.\nThese defects do not appear to be isolated; rather, they may represent shared\nweaknesses stemming from systemic limitations within current LLM code\ngeneration methods. In particular, critically severe issues, such as hard-coded\npasswords and path traversal vulnerabilities, were observed across multiple\nmodels. These results indicate that LLM-generated code requires verification in\norder to be considered production-ready. This study found no direct correlation\nbetween a model's functional performance (measured by Pass@1 rate of unit\ntests) and the overall quality and security of its generated code, measured by\nthe number of SonarQube issues in benchmark solutions that passed the\nfunctional tests. This suggests that functional benchmark performance score is\nnot a good indicator of overall code quality and security. The goal of this\nstudy is not to rank LLM performance but to highlight that all evaluated models\nappear to share certain weaknesses. Consequently, these findings support the\nview that static analysis can be a valuable instrument for detecting latent\ndefects and an important safeguard for organizations that deploy AI in software\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a quantitative evaluation of the code quality and\nsecurity of five prominent Large Language Models (LLMs): Claude Sonnet 4,\nClaude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior\nresearch has assessed the functional performance of LLM-generated code, this\nresearch tested LLM output from 4,442 Java coding assignments through\ncomprehensive static analysis using SonarQube. The findings suggest that\nalthough LLMs can generate functional code, they also introduce a range of\nsoftware defects, including bugs, security vulnerabilities, and code smells.\nThese defects do not appear to be isolated; rather, they may represent shared\nweaknesses stemming from systemic limitations within current LLM code\ngeneration methods. In particular, critically severe issues, such as hard-coded\npasswords and path traversal vulnerabilities, were observed across multiple\nmodels. These results indicate that LLM-generated code requires verification in\norder to be considered production-ready. This study found no direct correlation\nbetween a model's functional performance (measured by Pass@1 rate of unit\ntests) and the overall quality and security of its generated code, measured by\nthe number of SonarQube issues in benchmark solutions that passed the\nfunctional tests. This suggests that functional benchmark performance score is\nnot a good indicator of overall code quality and security. The goal of this\nstudy is not to rank LLM performance but to highlight that all evaluated models\nappear to share certain weaknesses. Consequently, these findings support the\nview that static analysis can be a valuable instrument for detecting latent\ndefects and an important safeguard for organizations that deploy AI in software\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Abbas Sabra"
                    },
                    {
                        "name": "Olivier Schmitt"
                    },
                    {
                        "name": "Joseph Tyler"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Tyler"
                },
                "author": "Joseph Tyler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14723v1",
                "updated": "2025-08-20T14:05:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    5,
                    18,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T14:05:18Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    5,
                    18,
                    2,
                    232,
                    0
                ],
                "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation"
                },
                "summary": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows."
                },
                "authors": [
                    {
                        "name": "Guangzhan Wang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Beijun Shen"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13415v2",
                "updated": "2025-08-20T13:57:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    57,
                    38,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-19T00:26:07Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    0,
                    26,
                    7,
                    1,
                    231,
                    0
                ],
                "title": "MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed across diverse\napplications that demand balancing multiple, often conflicting, objectives --\nsuch as helpfulness, harmlessness, or humor. Aligning outputs to user-specific\npreferences in such multi-objective settings typically requires fine-tuning\nmodels for each objective or preference configuration, which is computationally\nexpensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via\nValue-Guided Inference-Time Search -- a lightweight inference-time alignment\nframework that enables dynamic control over LLM behavior without modifying the\nbase model's weights. MAVIS trains a set of small value models, each\ncorresponding to a distinct objective. At inference time, these value models\nare combined using user-specified weights to produce a tilting function that\nadjusts the base model's output distribution toward desired trade-offs. The\nvalue models are trained using a simple iterative algorithm that ensures\nmonotonic improvement of the KL-regularized policy. We show empirically that\nMAVIS outperforms baselines that fine-tune per-objective models and combine\nthem post hoc, and even approaches the performance of the idealized setting\nwhere models are fine-tuned for a user's exact preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed across diverse\napplications that demand balancing multiple, often conflicting, objectives --\nsuch as helpfulness, harmlessness, or humor. Aligning outputs to user-specific\npreferences in such multi-objective settings typically requires fine-tuning\nmodels for each objective or preference configuration, which is computationally\nexpensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via\nValue-Guided Inference-Time Search -- a lightweight inference-time alignment\nframework that enables dynamic control over LLM behavior without modifying the\nbase model's weights. MAVIS trains a set of small value models, each\ncorresponding to a distinct objective. At inference time, these value models\nare combined using user-specified weights to produce a tilting function that\nadjusts the base model's output distribution toward desired trade-offs. The\nvalue models are trained using a simple iterative algorithm that ensures\nmonotonic improvement of the KL-regularized policy. We show empirically that\nMAVIS outperforms baselines that fine-tune per-objective models and combine\nthem post hoc, and even approaches the performance of the idealized setting\nwhere models are fine-tuned for a user's exact preferences."
                },
                "authors": [
                    {
                        "name": "Jeremy Carleton"
                    },
                    {
                        "name": "Debajoy Mukherjee"
                    },
                    {
                        "name": "Srinivas Shakkottai"
                    },
                    {
                        "name": "Dileep Kalathil"
                    }
                ],
                "author_detail": {
                    "name": "Dileep Kalathil"
                },
                "author": "Dileep Kalathil",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04996v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04996v4",
                "updated": "2025-08-20T13:50:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    50,
                    10,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-07T13:34:49Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    34,
                    49,
                    0,
                    188,
                    0
                ],
                "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems"
                },
                "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems."
                },
                "authors": [
                    {
                        "name": "Jiangbo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangbo Yu"
                },
                "author": "Jiangbo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04996v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04996v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13790v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13790v3",
                "updated": "2025-08-20T13:47:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    47,
                    47,
                    2,
                    232,
                    0
                ],
                "published": "2025-06-11T11:40:11Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    40,
                    11,
                    2,
                    162,
                    0
                ],
                "title": "The NordDRG AI Benchmark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NordDRG AI Benchmark for Large Language Models"
                },
                "summary": "Large language models (LLMs) are being piloted for clinical coding and\ndecision support, yet no open benchmark targets the hospital-funding layer\nwhere Diagnosis-Related Groups (DRGs) determine reimbursement. In most OECD\nsystems, DRGs route a substantial share of multi-trillion-dollar health\nspending through governed grouper software, making transparency and\nauditability first-order concerns. We release NordDRG-AI-Benchmark, the first\npublic, rule-complete test bed for DRG reasoning. The package includes (i)\nmachine-readable approximately 20-sheet NordDRG definition tables and (ii)\nexpert manuals and change-log templates that capture governance workflows. It\nexposes two suites: a 13-task Logic benchmark (code lookup, cross-table\ninference, grouping features, multilingual terminology, and CC/MCC validity\nchecks) and a 13-task Grouper benchmark that requires full DRG grouper\nemulation with strict exact-match scoring on both the DRG and the triggering\ndrg_logic.id. Lightweight reference agents (LogicAgent, GrouperAgent) enable\nartefact-only evaluation. Under an artefact-only (no web) setting, on the 13\nLogic tasks GPT-5 Thinking and Opus 4.1 score 13/13, o3 scores 12/13; mid-tier\nmodels (GPT-5 Thinking Mini, o4-mini, GPT-5 Fast) achieve 6-8/13, and remaining\nmodels score 5/13 or below. On full grouper emulation across 13 tasks, GPT-5\nThinking solves 7/13, o3 6/13, o4-mini 3/13; GPT-5 Thinking Mini solves 1/13,\nand all other tested endpoints score 0/13. To our knowledge, this is the first\npublic report of an LLM partially emulating the complete NordDRG grouper logic\nwith governance-grade traceability. Coupling a rule-complete release with\nexact-match tasks and open scoring provides a reproducible yardstick for\nhead-to-head and longitudinal evaluation in hospital funding. Benchmark\nmaterials available in Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being piloted for clinical coding and\ndecision support, yet no open benchmark targets the hospital-funding layer\nwhere Diagnosis-Related Groups (DRGs) determine reimbursement. In most OECD\nsystems, DRGs route a substantial share of multi-trillion-dollar health\nspending through governed grouper software, making transparency and\nauditability first-order concerns. We release NordDRG-AI-Benchmark, the first\npublic, rule-complete test bed for DRG reasoning. The package includes (i)\nmachine-readable approximately 20-sheet NordDRG definition tables and (ii)\nexpert manuals and change-log templates that capture governance workflows. It\nexposes two suites: a 13-task Logic benchmark (code lookup, cross-table\ninference, grouping features, multilingual terminology, and CC/MCC validity\nchecks) and a 13-task Grouper benchmark that requires full DRG grouper\nemulation with strict exact-match scoring on both the DRG and the triggering\ndrg_logic.id. Lightweight reference agents (LogicAgent, GrouperAgent) enable\nartefact-only evaluation. Under an artefact-only (no web) setting, on the 13\nLogic tasks GPT-5 Thinking and Opus 4.1 score 13/13, o3 scores 12/13; mid-tier\nmodels (GPT-5 Thinking Mini, o4-mini, GPT-5 Fast) achieve 6-8/13, and remaining\nmodels score 5/13 or below. On full grouper emulation across 13 tasks, GPT-5\nThinking solves 7/13, o3 6/13, o4-mini 3/13; GPT-5 Thinking Mini solves 1/13,\nand all other tested endpoints score 0/13. To our knowledge, this is the first\npublic report of an LLM partially emulating the complete NordDRG grouper logic\nwith governance-grade traceability. Coupling a rule-complete release with\nexact-match tasks and open scoring provides a reproducible yardstick for\nhead-to-head and longitudinal evaluation in hospital funding. Benchmark\nmaterials available in Github."
                },
                "authors": [
                    {
                        "name": "Tapio Pitkäranta"
                    }
                ],
                "author_detail": {
                    "name": "Tapio Pitkäranta"
                },
                "author": "Tapio Pitkäranta",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13790v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13790v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14706v1",
                "updated": "2025-08-20T13:30:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    30,
                    20,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T13:30:20Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    30,
                    20,
                    2,
                    232,
                    0
                ],
                "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine"
                },
                "summary": "Despite the success of large language models (LLMs) in various domains, their\npotential in Traditional Chinese Medicine (TCM) remains largely underexplored\ndue to two critical barriers: (1) the scarcity of high-quality TCM data and (2)\nthe inherently multimodal nature of TCM diagnostics, which involve looking,\nlistening, smelling, and pulse-taking. These sensory-rich modalities are beyond\nthe scope of conventional LLMs. To address these challenges, we present\nShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data\nscarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text\nand 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and\nphysiological signals. ShizhenGPT is pretrained and instruction-tuned to\nachieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect\nrecent national TCM qualification exams and build a visual benchmark for\nMedicinal Recognition and Visual Diagnosis. Experiments demonstrate that\nShizhenGPT outperforms comparable-scale LLMs and competes with larger\nproprietary models. Moreover, it leads in TCM visual understanding among\nexisting multimodal LLMs and demonstrates unified perception across modalities\nlike sound, pulse, smell, and vision, paving the way toward holistic multimodal\nperception and diagnosis in TCM. Datasets, models, and code are publicly\navailable. We hope this work will inspire further exploration in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of large language models (LLMs) in various domains, their\npotential in Traditional Chinese Medicine (TCM) remains largely underexplored\ndue to two critical barriers: (1) the scarcity of high-quality TCM data and (2)\nthe inherently multimodal nature of TCM diagnostics, which involve looking,\nlistening, smelling, and pulse-taking. These sensory-rich modalities are beyond\nthe scope of conventional LLMs. To address these challenges, we present\nShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data\nscarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text\nand 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and\nphysiological signals. ShizhenGPT is pretrained and instruction-tuned to\nachieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect\nrecent national TCM qualification exams and build a visual benchmark for\nMedicinal Recognition and Visual Diagnosis. Experiments demonstrate that\nShizhenGPT outperforms comparable-scale LLMs and competes with larger\nproprietary models. Moreover, it leads in TCM visual understanding among\nexisting multimodal LLMs and demonstrates unified perception across modalities\nlike sound, pulse, smell, and vision, paving the way toward holistic multimodal\nperception and diagnosis in TCM. Datasets, models, and code are publicly\navailable. We hope this work will inspire further exploration in this field."
                },
                "authors": [
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Zhenyang Cai"
                    },
                    {
                        "name": "Zhiheng Liu"
                    },
                    {
                        "name": "Yunjin Yang"
                    },
                    {
                        "name": "Rongsheng Wang"
                    },
                    {
                        "name": "Qingying Xiao"
                    },
                    {
                        "name": "Xiangyi Feng"
                    },
                    {
                        "name": "Zhan Su"
                    },
                    {
                        "name": "Jing Guo"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14704v1",
                "updated": "2025-08-20T13:28:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    28,
                    58,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T13:28:58Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    28,
                    58,
                    2,
                    232,
                    0
                ],
                "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model\n  Context Protocol Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP-Universe: Benchmarking Large Language Models with Real-World Model\n  Context Protocol Servers"
                },
                "summary": "The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem."
                },
                "authors": [
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Zhiqi Shen"
                    },
                    {
                        "name": "Wenzhuo Yang"
                    },
                    {
                        "name": "Zirui Zhao"
                    },
                    {
                        "name": "Prathyusha Jwalapuram"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Doyen Sahoo"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Junnan Li"
                    }
                ],
                "author_detail": {
                    "name": "Junnan Li"
                },
                "author": "Junnan Li",
                "arxiv_comment": "Website: https://mcp-universe.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10207v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10207v3",
                "updated": "2025-08-20T13:15:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    15,
                    18,
                    2,
                    232,
                    0
                ],
                "published": "2024-12-13T15:30:20Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    30,
                    20,
                    4,
                    348,
                    0
                ],
                "title": "Retrieval-Augmented Semantic Parsing: Improving Generalization with\n  Lexical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Semantic Parsing: Improving Generalization with\n  Lexical Knowledge"
                },
                "summary": "Open-domain semantic parsing remains a challenging task, as neural models\noften rely on heuristics and struggle to handle unseen concepts. In this paper,\nwe investigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external symbolic knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-domain semantic parsing remains a challenging task, as neural models\noften rely on heuristics and struggle to handle unseen concepts. In this paper,\nwe investigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external symbolic knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing."
                },
                "authors": [
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Qianru Meng"
                    },
                    {
                        "name": "Johan Bos"
                    }
                ],
                "author_detail": {
                    "name": "Johan Bos"
                },
                "author": "Johan Bos",
                "arxiv_comment": "Accpted by 16th IWCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10207v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10207v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13953v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13953v2",
                "updated": "2025-08-20T13:10:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    10,
                    14,
                    2,
                    232,
                    0
                ],
                "published": "2025-02-19T18:53:16Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    53,
                    16,
                    2,
                    50,
                    0
                ],
                "title": "Benchmarking graph construction by large language models for\n  coherence-driven inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking graph construction by large language models for\n  coherence-driven inference"
                },
                "summary": "We devise an algorithm to generate propositions that objectively instantiate\ngraphs supporting coherence-driven inference. We also benchmark the ability of\nlarge language models (LLMs) to reconstruct coherence graphs from (a simple\ntransformation of) propositions expressed in natural language, with promising\nresults from a single prompt to reasoning-optimized LLMs. For example,\no1/3/4-mini achieve perfect reconstruction half of the time on sparse graphs.\nCoherence-driven inference on consistency evaluations by LLMs may advance\nmachine cognition capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We devise an algorithm to generate propositions that objectively instantiate\ngraphs supporting coherence-driven inference. We also benchmark the ability of\nlarge language models (LLMs) to reconstruct coherence graphs from (a simple\ntransformation of) propositions expressed in natural language, with promising\nresults from a single prompt to reasoning-optimized LLMs. For example,\no1/3/4-mini achieve perfect reconstruction half of the time on sparse graphs.\nCoherence-driven inference on consistency evaluations by LLMs may advance\nmachine cognition capabilities."
                },
                "authors": [
                    {
                        "name": "Steve Huntsman"
                    },
                    {
                        "name": "Jewell Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Jewell Thomas"
                },
                "author": "Jewell Thomas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13953v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14685v1",
                "updated": "2025-08-20T13:01:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    1,
                    34,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T13:01:34Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    1,
                    34,
                    2,
                    232,
                    0
                ],
                "title": "Improving in-context learning with a better scoring function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving in-context learning with a better scoring function"
                },
                "summary": "Large language models (LLMs) exhibit a remarkable capacity to learn by\nanalogy, known as in-context learning (ICL). However, recent studies have\nrevealed limitations in this ability. In this paper, we examine these\nlimitations on tasks involving first-order quantifiers such as {\\em all} and\n{\\em some}, as well as on ICL with linear functions. We identify Softmax, the\nscoring function in attention mechanism, as a contributing factor to these\nconstraints. To address this, we propose \\textbf{scaled signed averaging\n(SSA)}, a novel alternative to Softmax. Empirical results show that SSA\ndramatically improves performance on our target tasks. Furthermore, we evaluate\nboth encoder-only and decoder-only transformers models with SSA, demonstrating\nthat they match or exceed their Softmax-based counterparts across a variety of\nlinguistic probing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit a remarkable capacity to learn by\nanalogy, known as in-context learning (ICL). However, recent studies have\nrevealed limitations in this ability. In this paper, we examine these\nlimitations on tasks involving first-order quantifiers such as {\\em all} and\n{\\em some}, as well as on ICL with linear functions. We identify Softmax, the\nscoring function in attention mechanism, as a contributing factor to these\nconstraints. To address this, we propose \\textbf{scaled signed averaging\n(SSA)}, a novel alternative to Softmax. Empirical results show that SSA\ndramatically improves performance on our target tasks. Furthermore, we evaluate\nboth encoder-only and decoder-only transformers models with SSA, demonstrating\nthat they match or exceed their Softmax-based counterparts across a variety of\nlinguistic probing tasks."
                },
                "authors": [
                    {
                        "name": "Omar Naim"
                    },
                    {
                        "name": "Swarnadeep Bhar"
                    },
                    {
                        "name": "Jérôme Bolte"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14679v1",
                "updated": "2025-08-20T12:52:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    52,
                    52,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T12:52:52Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    52,
                    52,
                    2,
                    232,
                    0
                ],
                "title": "Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A\n  Multi-Agent Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A\n  Multi-Agent Reinforcement Learning Approach"
                },
                "summary": "Efficient energy management is essential in Wireless Sensor Networks (WSNs)\nto extend network lifetime and ensure reliable data transmission. This paper\npresents a novel method using reinforcement learning-based cluster-head\nselection and a hybrid multi-hop routing algorithm, which leverages Q-learning\nwithin a multi-agent system to dynamically adapt transmission paths based on\nthe energy distribution across sensor nodes. Each sensor node is modeled as an\nautonomous agent that observes local state parameters, such as residual energy,\ndistance to sink, hop count, and hotspot proximity, and selects routing actions\nthat maximize long-term energy efficiency. After computing the optimal paths,\neach sensor aggregates sensed data and forwards it through intermediate nodes\nto a selected transmitter node, chosen based on the highest remaining State of\nCharge (SoC), thereby avoiding premature node depletion. To promote efficient\nlearning, a carefully designed reward function incentivizes balanced load\ndistribution, hotspot avoidance, and energy-aware forwarding while maintaining\nsignal quality. The learning process occurs either in a decentralized manner or\nvia a cloud-based controller that offloads computation in large-scale\ndeployments. Moreover, the RL-driven routing decisions are fused with classical\ngraph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum\nSpanning Tree (MST), to optimize energy consumption and load balancing.\nSimulations confirm that the proposed approach significantly improves node\nsurvival rate, reduces SoC variance, and enhances network resilience, making it\na scalable and adaptive solution for energy-constrained WSNs in dynamic sensor\ndeployments and IoT applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient energy management is essential in Wireless Sensor Networks (WSNs)\nto extend network lifetime and ensure reliable data transmission. This paper\npresents a novel method using reinforcement learning-based cluster-head\nselection and a hybrid multi-hop routing algorithm, which leverages Q-learning\nwithin a multi-agent system to dynamically adapt transmission paths based on\nthe energy distribution across sensor nodes. Each sensor node is modeled as an\nautonomous agent that observes local state parameters, such as residual energy,\ndistance to sink, hop count, and hotspot proximity, and selects routing actions\nthat maximize long-term energy efficiency. After computing the optimal paths,\neach sensor aggregates sensed data and forwards it through intermediate nodes\nto a selected transmitter node, chosen based on the highest remaining State of\nCharge (SoC), thereby avoiding premature node depletion. To promote efficient\nlearning, a carefully designed reward function incentivizes balanced load\ndistribution, hotspot avoidance, and energy-aware forwarding while maintaining\nsignal quality. The learning process occurs either in a decentralized manner or\nvia a cloud-based controller that offloads computation in large-scale\ndeployments. Moreover, the RL-driven routing decisions are fused with classical\ngraph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum\nSpanning Tree (MST), to optimize energy consumption and load balancing.\nSimulations confirm that the proposed approach significantly improves node\nsurvival rate, reduces SoC variance, and enhances network resilience, making it\na scalable and adaptive solution for energy-constrained WSNs in dynamic sensor\ndeployments and IoT applications."
                },
                "authors": [
                    {
                        "name": "Parham Soltani"
                    },
                    {
                        "name": "Mehrshad Eskandarpour"
                    },
                    {
                        "name": "Amir Ahmadizad"
                    },
                    {
                        "name": "Hossein Soleimani"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Soleimani"
                },
                "author": "Hossein Soleimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14676v1",
                "updated": "2025-08-20T12:48:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    48,
                    21,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T12:48:21Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    48,
                    21,
                    2,
                    232,
                    0
                ],
                "title": "Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor\n  Networks: A Multi-Agent Deep Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor\n  Networks: A Multi-Agent Deep Reinforcement Learning Approach"
                },
                "summary": "Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of\nthe target area, network size, and sensor coverage to determine initial\ndeployment. This often results in significant overlap to ensure continued\nnetwork operation despite sensor energy depletion. With the emergence of Mobile\nWireless Sensor Networks (MWSNs), issues such as sensor failure and static\ncoverage limitations can be more effectively addressed through mobility. This\npaper proposes a novel deployment strategy in which mobile sensors autonomously\nposition themselves to maximize area coverage, eliminating the need for\npredefined policies. A live camera system, combined with deep reinforcement\nlearning (DRL), monitors the network by detecting sensor LED indicators and\nevaluating real-time coverage. Rewards based on coverage efficiency and sensor\nmovement are computed at each learning step and shared across the network\nthrough a Multi-Agent Reinforcement Learning (MARL) framework, enabling\ndecentralized, cooperative sensor control. Key contributions include a\nvision-based, low-cost coverage evaluation method; a scalable MARL-DRL\nframework for autonomous deployment; and a self-reconfigurable system that\nadjusts sensor positioning in response to energy depletion. Compared to\ntraditional distance-based localization, the proposed method achieves a 26.5%\nimprovement in coverage, a 32% reduction in energy consumption, and a 22%\ndecrease in redundancy, extending network lifetime by 45%. This approach\nsignificantly enhances adaptability, energy efficiency, and robustness in\nMWSNs, offering a practical deployment solution within the IoT framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of\nthe target area, network size, and sensor coverage to determine initial\ndeployment. This often results in significant overlap to ensure continued\nnetwork operation despite sensor energy depletion. With the emergence of Mobile\nWireless Sensor Networks (MWSNs), issues such as sensor failure and static\ncoverage limitations can be more effectively addressed through mobility. This\npaper proposes a novel deployment strategy in which mobile sensors autonomously\nposition themselves to maximize area coverage, eliminating the need for\npredefined policies. A live camera system, combined with deep reinforcement\nlearning (DRL), monitors the network by detecting sensor LED indicators and\nevaluating real-time coverage. Rewards based on coverage efficiency and sensor\nmovement are computed at each learning step and shared across the network\nthrough a Multi-Agent Reinforcement Learning (MARL) framework, enabling\ndecentralized, cooperative sensor control. Key contributions include a\nvision-based, low-cost coverage evaluation method; a scalable MARL-DRL\nframework for autonomous deployment; and a self-reconfigurable system that\nadjusts sensor positioning in response to energy depletion. Compared to\ntraditional distance-based localization, the proposed method achieves a 26.5%\nimprovement in coverage, a 32% reduction in energy consumption, and a 22%\ndecrease in redundancy, extending network lifetime by 45%. This approach\nsignificantly enhances adaptability, energy efficiency, and robustness in\nMWSNs, offering a practical deployment solution within the IoT framework."
                },
                "authors": [
                    {
                        "name": "Parham Soltani"
                    },
                    {
                        "name": "Mehrshad Eskandarpour"
                    },
                    {
                        "name": "Sina Heidari"
                    },
                    {
                        "name": "Farnaz Alizadeh"
                    },
                    {
                        "name": "Hossein Soleimani"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Soleimani"
                },
                "author": "Hossein Soleimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07773v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07773v4",
                "updated": "2025-08-20T12:20:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    20,
                    55,
                    2,
                    232,
                    0
                ],
                "published": "2025-05-12T17:23:34Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    23,
                    34,
                    0,
                    132,
                    0
                ],
                "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving"
                },
                "summary": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}."
                },
                "authors": [
                    {
                        "name": "Xinji Mai"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Xing W"
                    },
                    {
                        "name": "Weinong Wang"
                    },
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Wenqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Zhang"
                },
                "author": "Wenqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07773v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07773v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14654v1",
                "updated": "2025-08-20T12:13:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    13,
                    3,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T12:13:03Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    13,
                    3,
                    2,
                    232,
                    0
                ],
                "title": "Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent\n  Framework with LLM and Knowledge Graph Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent\n  Framework with LLM and Knowledge Graph Integration"
                },
                "summary": "In recent years, the increasing frequency of extreme urban rainfall events\nhas posed significant challenges to emergency scheduling systems. Urban\nflooding often leads to severe traffic congestion and service disruptions,\nthreatening public safety and mobility. However, effective decision making\nremains hindered by three key challenges: (1) managing trade-offs among\ncompeting goals (e.g., traffic flow, task completion, and risk mitigation)\nrequires dynamic, context-aware strategies; (2) rapidly evolving environmental\nconditions render static rules inadequate; and (3) LLM-generated strategies\nfrequently suffer from semantic instability and execution inconsistency.\nExisting methods fail to align perception, global optimization, and multi-agent\ncoordination within a unified framework. To tackle these challenges, we\nintroduce H-J, a hierarchical multi-agent framework that integrates\nknowledge-guided prompting, entropy-constrained generation, and feedback-driven\noptimization. The framework establishes a closed-loop pipeline spanning from\nmulti-source perception to strategic execution and continuous refinement. We\nevaluate H-J on real-world urban topology and rainfall data under three\nrepresentative conditions: extreme rainfall, intermittent bursts, and daily\nlight rain. Experiments show that H-J outperforms rule-based and\nreinforcement-learning baselines in traffic smoothness, task success rate, and\nsystem robustness. These findings highlight the promise of uncertainty-aware,\nknowledge-constrained LLM-based approaches for enhancing resilience in urban\nflood response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the increasing frequency of extreme urban rainfall events\nhas posed significant challenges to emergency scheduling systems. Urban\nflooding often leads to severe traffic congestion and service disruptions,\nthreatening public safety and mobility. However, effective decision making\nremains hindered by three key challenges: (1) managing trade-offs among\ncompeting goals (e.g., traffic flow, task completion, and risk mitigation)\nrequires dynamic, context-aware strategies; (2) rapidly evolving environmental\nconditions render static rules inadequate; and (3) LLM-generated strategies\nfrequently suffer from semantic instability and execution inconsistency.\nExisting methods fail to align perception, global optimization, and multi-agent\ncoordination within a unified framework. To tackle these challenges, we\nintroduce H-J, a hierarchical multi-agent framework that integrates\nknowledge-guided prompting, entropy-constrained generation, and feedback-driven\noptimization. The framework establishes a closed-loop pipeline spanning from\nmulti-source perception to strategic execution and continuous refinement. We\nevaluate H-J on real-world urban topology and rainfall data under three\nrepresentative conditions: extreme rainfall, intermittent bursts, and daily\nlight rain. Experiments show that H-J outperforms rule-based and\nreinforcement-learning baselines in traffic smoothness, task success rate, and\nsystem robustness. These findings highlight the promise of uncertainty-aware,\nknowledge-constrained LLM-based approaches for enhancing resilience in urban\nflood response."
                },
                "authors": [
                    {
                        "name": "Peilin Ji"
                    },
                    {
                        "name": "Xiao Xue"
                    },
                    {
                        "name": "Simeng Wang"
                    },
                    {
                        "name": "Wenhao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Yan"
                },
                "author": "Wenhao Yan",
                "arxiv_comment": "17 pages including appendix, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14635v1",
                "updated": "2025-08-20T11:44:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    11,
                    44,
                    10,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T11:44:10Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    11,
                    44,
                    10,
                    2,
                    232,
                    0
                ],
                "title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware\n  Planning and Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware\n  Planning and Coordination"
                },
                "summary": "The ability to coordinate actions across multiple agents is critical for\nsolving complex, real-world problems. Large Language Models (LLMs) have shown\nstrong capabilities in communication, planning, and reasoning, raising the\nquestion of whether they can also support effective collaboration in\nmulti-agent settings. In this work, we investigate the use of LLM agents to\nsolve a structured victim rescue task that requires division of labor,\nprioritization, and cooperative planning. Agents operate in a fully known\ngraph-based environment and must allocate resources to victims with varying\nneeds and urgency levels. We systematically evaluate their performance using a\nsuite of coordination-sensitive metrics, including task success rate, redundant\nactions, room conflicts, and urgency-weighted efficiency. This study offers new\ninsights into the strengths and failure modes of LLMs in physically grounded\nmulti-agent collaboration tasks, contributing to future benchmarks and\narchitectural improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to coordinate actions across multiple agents is critical for\nsolving complex, real-world problems. Large Language Models (LLMs) have shown\nstrong capabilities in communication, planning, and reasoning, raising the\nquestion of whether they can also support effective collaboration in\nmulti-agent settings. In this work, we investigate the use of LLM agents to\nsolve a structured victim rescue task that requires division of labor,\nprioritization, and cooperative planning. Agents operate in a fully known\ngraph-based environment and must allocate resources to victims with varying\nneeds and urgency levels. We systematically evaluate their performance using a\nsuite of coordination-sensitive metrics, including task success rate, redundant\nactions, room conflicts, and urgency-weighted efficiency. This study offers new\ninsights into the strengths and failure modes of LLMs in physically grounded\nmulti-agent collaboration tasks, contributing to future benchmarks and\narchitectural improvements."
                },
                "authors": [
                    {
                        "name": "João Vitor de Carvalho Silva"
                    },
                    {
                        "name": "Douglas G. Macharet"
                    }
                ],
                "author_detail": {
                    "name": "Douglas G. Macharet"
                },
                "author": "Douglas G. Macharet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23537v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23537v3",
                "updated": "2025-08-21T01:08:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    1,
                    8,
                    55,
                    3,
                    233,
                    0
                ],
                "published": "2025-03-30T17:44:12Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    17,
                    44,
                    12,
                    6,
                    89,
                    0
                ],
                "title": "Redundant feature screening method for human activity recognition based\n  on attention purification mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redundant feature screening method for human activity recognition based\n  on attention purification mechanism"
                },
                "summary": "In the field of sensor-based Human Activity Recognition (HAR), deep neural\nnetworks provide advanced technical support. Many studies have proven that\nrecognition accuracy can be improved by increasing the depth or width of the\nnetwork. However, for wearable devices, the balance between network performance\nand resource consumption is crucial. With minimum resource consumption as the\nbasic principle, we propose a universal attention feature purification\nmechanism, called MSAP, which is suitable for multi-scale networks. The\nmechanism effectively solves the feature redundancy caused by the superposition\nof multi-scale features by means of inter-scale attention screening and\nconnection method. In addition, we have designed a network correction module\nthat integrates seamlessly between layers of individual network modules to\nmitigate inherent problems in deep networks. We also built an embedded\ndeployment system that is in line with the current level of wearable technology\nto test the practical feasibility of the HAR model, and further prove the\nefficiency of the method. Extensive experiments on four public datasets show\nthat the proposed method model effectively reduces redundant features in\nfiltered data and provides excellent performance with little resource\nconsumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of sensor-based Human Activity Recognition (HAR), deep neural\nnetworks provide advanced technical support. Many studies have proven that\nrecognition accuracy can be improved by increasing the depth or width of the\nnetwork. However, for wearable devices, the balance between network performance\nand resource consumption is crucial. With minimum resource consumption as the\nbasic principle, we propose a universal attention feature purification\nmechanism, called MSAP, which is suitable for multi-scale networks. The\nmechanism effectively solves the feature redundancy caused by the superposition\nof multi-scale features by means of inter-scale attention screening and\nconnection method. In addition, we have designed a network correction module\nthat integrates seamlessly between layers of individual network modules to\nmitigate inherent problems in deep networks. We also built an embedded\ndeployment system that is in line with the current level of wearable technology\nto test the practical feasibility of the HAR model, and further prove the\nefficiency of the method. Extensive experiments on four public datasets show\nthat the proposed method model effectively reduces redundant features in\nfiltered data and provides excellent performance with little resource\nconsumption."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Yixuan Jiang"
                    },
                    {
                        "name": "Junze Zhu"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Dongchen Wu"
                    },
                    {
                        "name": "Hanyu Liu"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "arxiv_comment": "12 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23537v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23537v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14613v1",
                "updated": "2025-08-20T10:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    58,
                    17,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T10:58:17Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    58,
                    17,
                    2,
                    232,
                    0
                ],
                "title": "A Simple and Scalable Kernel Density Approach for Reliable Uncertainty\n  Quantification in Atomistic Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Scalable Kernel Density Approach for Reliable Uncertainty\n  Quantification in Atomistic Machine Learning"
                },
                "summary": "Machine learning models are increasingly used to predict material properties\nand accelerate atomistic simulations, but the reliability of their predictions\ndepends on the representativeness of the training data. We present a scalable,\nGPU-accelerated uncertainty quantification framework based on\n$k$-nearest-neighbor kernel density estimation (KDE) in a PCA-reduced\ndescriptor space. This method efficiently detects sparsely sampled regions in\nlarge, high-dimensional datasets and provides a transferable, model-agnostic\nuncertainty metric without requiring retraining costly model ensembles. The\nframework is validated across diverse case studies varying in: i) chemistry,\nii) prediction models (including foundational neural network), iii) descriptors\nused for KDE estimation, and iv) properties whose uncertainty is sought. In all\ncases, the KDE-based score reliably flags extrapolative configurations,\ncorrelates well with conventional ensemble-based uncertainties, and highlights\nregions of reduced prediction trustworthiness. The approach offers a practical\nroute for improving the interpretability, robustness, and deployment readiness\nof ML models in materials science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models are increasingly used to predict material properties\nand accelerate atomistic simulations, but the reliability of their predictions\ndepends on the representativeness of the training data. We present a scalable,\nGPU-accelerated uncertainty quantification framework based on\n$k$-nearest-neighbor kernel density estimation (KDE) in a PCA-reduced\ndescriptor space. This method efficiently detects sparsely sampled regions in\nlarge, high-dimensional datasets and provides a transferable, model-agnostic\nuncertainty metric without requiring retraining costly model ensembles. The\nframework is validated across diverse case studies varying in: i) chemistry,\nii) prediction models (including foundational neural network), iii) descriptors\nused for KDE estimation, and iv) properties whose uncertainty is sought. In all\ncases, the KDE-based score reliably flags extrapolative configurations,\ncorrelates well with conventional ensemble-based uncertainties, and highlights\nregions of reduced prediction trustworthiness. The approach offers a practical\nroute for improving the interpretability, robustness, and deployment readiness\nof ML models in materials science."
                },
                "authors": [
                    {
                        "name": "Daniel Willimetz"
                    },
                    {
                        "name": "Lukáš Grajciar"
                    }
                ],
                "author_detail": {
                    "name": "Lukáš Grajciar"
                },
                "author": "Lukáš Grajciar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03082v2",
                "updated": "2025-08-20T10:33:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    33,
                    40,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-05T04:55:03Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    4,
                    55,
                    3,
                    1,
                    217,
                    0
                ],
                "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic\n  Design"
                },
                "summary": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yilu Liu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03984v2",
                "updated": "2025-08-20T10:14:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    14,
                    24,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-05T10:23:40Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    23,
                    40,
                    5,
                    186,
                    0
                ],
                "title": "CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via\n  Chain-of-Thought Reasoning"
                },
                "summary": "Effective Out-of-Distribution (OOD) detection is criti-cal for ensuring the\nreliability of semantic segmentation models, particularly in complex road\nenvironments where safety and accuracy are paramount. Despite recent\nadvancements in large language models (LLMs), notably GPT-4, which\nsignificantly enhanced multimodal reasoning through Chain-of-Thought (CoT)\nprompting, the application of CoT-based visual reasoning for OOD semantic\nsegmentation remains largely unexplored. In this paper, through extensive\nanalyses of the road scene anomalies, we identify three challenging scenarios\nwhere current state-of-the-art OOD segmentation methods consistently struggle:\n(1) densely packed and overlapping objects, (2) distant scenes with small\nobjects, and (3) large foreground-dominant objects. To address the presented\nchallenges, we propose a novel CoT-based framework targeting OOD detection in\nroad anomaly scenes. Our method leverages the extensive knowledge and reasoning\ncapabilities of foundation models, such as GPT-4, to enhance OOD detection\nthrough improved image understanding and prompt-based reasoning aligned with\nobserved problematic scene attributes. Extensive experiments show that our\nframework consistently outperforms state-of-the-art methods on both standard\nbenchmarks and our newly defined challenging subset of the RoadAnomaly dataset,\noffering a robust and interpretable solution for OOD semantic segmentation in\ncomplex driving environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Out-of-Distribution (OOD) detection is criti-cal for ensuring the\nreliability of semantic segmentation models, particularly in complex road\nenvironments where safety and accuracy are paramount. Despite recent\nadvancements in large language models (LLMs), notably GPT-4, which\nsignificantly enhanced multimodal reasoning through Chain-of-Thought (CoT)\nprompting, the application of CoT-based visual reasoning for OOD semantic\nsegmentation remains largely unexplored. In this paper, through extensive\nanalyses of the road scene anomalies, we identify three challenging scenarios\nwhere current state-of-the-art OOD segmentation methods consistently struggle:\n(1) densely packed and overlapping objects, (2) distant scenes with small\nobjects, and (3) large foreground-dominant objects. To address the presented\nchallenges, we propose a novel CoT-based framework targeting OOD detection in\nroad anomaly scenes. Our method leverages the extensive knowledge and reasoning\ncapabilities of foundation models, such as GPT-4, to enhance OOD detection\nthrough improved image understanding and prompt-based reasoning aligned with\nobserved problematic scene attributes. Extensive experiments show that our\nframework consistently outperforms state-of-the-art methods on both standard\nbenchmarks and our newly defined challenging subset of the RoadAnomaly dataset,\noffering a robust and interpretable solution for OOD semantic segmentation in\ncomplex driving environments."
                },
                "authors": [
                    {
                        "name": "Jeonghyo Song"
                    },
                    {
                        "name": "Kimin Yun"
                    },
                    {
                        "name": "DaeUng Jo"
                    },
                    {
                        "name": "Jinyoung Kim"
                    },
                    {
                        "name": "Youngjoon Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoon Yoo"
                },
                "author": "Youngjoon Yoo",
                "arxiv_comment": "6 pages, 3 figures. Accepted at IEEE International Conference on\n  Advanced Visual and Signal-Based Systems 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14582v1",
                "updated": "2025-08-20T10:04:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    4,
                    21,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T10:04:21Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    4,
                    21,
                    2,
                    232,
                    0
                ],
                "title": "An Open-Source HW-SW Co-Development Framework Enabling Efficient\n  Multi-Accelerator Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Open-Source HW-SW Co-Development Framework Enabling Efficient\n  Multi-Accelerator Systems"
                },
                "summary": "Heterogeneous accelerator-centric compute clusters are emerging as efficient\nsolutions for diverse AI workloads. However, current integration strategies\noften compromise data movement efficiency and encounter compatibility issues in\nhardware and software. This prevents a unified approach that balances\nperformance and ease of use. To this end, we present SNAX, an open-source\nintegrated HW-SW framework enabling efficient multi-accelerator platforms\nthrough a novel hybrid-coupling scheme, consisting of loosely coupled\nasynchronous control and tightly coupled data access. SNAX brings reusable\nhardware modules designed to enhance compute accelerator utilization, and its\ncustomizable MLIR-based compiler to automate key system management tasks,\njointly enabling rapid development and deployment of customized\nmulti-accelerator compute clusters. Through extensive experimentation, we\ndemonstrate SNAX's efficiency and flexibility in a low-power heterogeneous SoC.\nAccelerators can easily be integrated and programmed to achieve > 10x\nimprovement in neural network performance compared to other accelerator systems\nwhile maintaining accelerator utilization of > 90% in full system operation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous accelerator-centric compute clusters are emerging as efficient\nsolutions for diverse AI workloads. However, current integration strategies\noften compromise data movement efficiency and encounter compatibility issues in\nhardware and software. This prevents a unified approach that balances\nperformance and ease of use. To this end, we present SNAX, an open-source\nintegrated HW-SW framework enabling efficient multi-accelerator platforms\nthrough a novel hybrid-coupling scheme, consisting of loosely coupled\nasynchronous control and tightly coupled data access. SNAX brings reusable\nhardware modules designed to enhance compute accelerator utilization, and its\ncustomizable MLIR-based compiler to automate key system management tasks,\njointly enabling rapid development and deployment of customized\nmulti-accelerator compute clusters. Through extensive experimentation, we\ndemonstrate SNAX's efficiency and flexibility in a low-power heterogeneous SoC.\nAccelerators can easily be integrated and programmed to achieve > 10x\nimprovement in neural network performance compared to other accelerator systems\nwhile maintaining accelerator utilization of > 90% in full system operation."
                },
                "authors": [
                    {
                        "name": "Ryan Albert Antonio"
                    },
                    {
                        "name": "Joren Dumoulin"
                    },
                    {
                        "name": "Xiaoling Yi"
                    },
                    {
                        "name": "Josse Van Delm"
                    },
                    {
                        "name": "Yunhao Deng"
                    },
                    {
                        "name": "Guilherme Paim"
                    },
                    {
                        "name": "Marian Verhelst"
                    }
                ],
                "author_detail": {
                    "name": "Marian Verhelst"
                },
                "author": "Marian Verhelst",
                "arxiv_comment": "7 pages, 10 figures, 1 table, to be published in ISLPED 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14581v1",
                "updated": "2025-08-20T10:03:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    3,
                    31,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T10:03:31Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    10,
                    3,
                    31,
                    2,
                    232,
                    0
                ],
                "title": "FakeHunter: Multimodal Step-by-Step Reasoning for Explainable Video\n  Forensics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FakeHunter: Multimodal Step-by-Step Reasoning for Explainable Video\n  Forensics"
                },
                "summary": "FakeHunter is a multimodal deepfake detection framework that combines\nmemory-guided retrieval, chain-of-thought (Observation-Thought-Action)\nreasoning, and tool-augmented verification to provide accurate and\ninterpretable video forensics. FakeHunter encodes visual content using CLIP and\naudio using CLAP, generating joint audio-visual embeddings that retrieve\nsemantically similar real exemplars from a FAISS-indexed memory bank for\ncontextual grounding. Guided by the retrieved context, the system iteratively\nreasons over evidence to localize manipulations and explain them. When\nconfidence is low, it automatically invokes specialized tools-such as zoom-in\nimage forensics or mel-spectrogram inspection-for fine-grained verification.\nBuilt on Qwen2.5-Omni-7B, FakeHunter produces structured JSON verdicts that\nspecify what was modified, where it occurs, and why it is judged fake. We also\nintroduce X-AVFake, a benchmark comprising 5.7k+ manipulated and real videos\n(950+ min) annotated with manipulation type, region/entity, violated reasoning\ncategory, and free-form justification. On X-AVFake, FakeHunter achieves an\naccuracy of 34.75%, outperforming the vanilla Qwen2.5-Omni-7B by 16.87\npercentage points and MiniCPM-2.6 by 25.56 percentage points. Ablation studies\nreveal that memory retrieval contributes a 7.75 percentage point gain, and\ntool-based inspection improves low-confidence cases to 46.50%. Despite its\nmulti-stage design, the pipeline processes a 10-minute clip in 8 minutes on a\nsingle NVIDIA A800 (0.8x real-time) or 2 minutes on four GPUs (0.2x),\ndemonstrating practical deployability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FakeHunter is a multimodal deepfake detection framework that combines\nmemory-guided retrieval, chain-of-thought (Observation-Thought-Action)\nreasoning, and tool-augmented verification to provide accurate and\ninterpretable video forensics. FakeHunter encodes visual content using CLIP and\naudio using CLAP, generating joint audio-visual embeddings that retrieve\nsemantically similar real exemplars from a FAISS-indexed memory bank for\ncontextual grounding. Guided by the retrieved context, the system iteratively\nreasons over evidence to localize manipulations and explain them. When\nconfidence is low, it automatically invokes specialized tools-such as zoom-in\nimage forensics or mel-spectrogram inspection-for fine-grained verification.\nBuilt on Qwen2.5-Omni-7B, FakeHunter produces structured JSON verdicts that\nspecify what was modified, where it occurs, and why it is judged fake. We also\nintroduce X-AVFake, a benchmark comprising 5.7k+ manipulated and real videos\n(950+ min) annotated with manipulation type, region/entity, violated reasoning\ncategory, and free-form justification. On X-AVFake, FakeHunter achieves an\naccuracy of 34.75%, outperforming the vanilla Qwen2.5-Omni-7B by 16.87\npercentage points and MiniCPM-2.6 by 25.56 percentage points. Ablation studies\nreveal that memory retrieval contributes a 7.75 percentage point gain, and\ntool-based inspection improves low-confidence cases to 46.50%. Despite its\nmulti-stage design, the pipeline processes a 10-minute clip in 8 minutes on a\nsingle NVIDIA A800 (0.8x real-time) or 2 minutes on four GPUs (0.2x),\ndemonstrating practical deployability."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Runze Li"
                    },
                    {
                        "name": "Zejun Zhang"
                    },
                    {
                        "name": "Pukun Zhao"
                    },
                    {
                        "name": "Fanqing Zhou"
                    },
                    {
                        "name": "Longxiang Wang"
                    },
                    {
                        "name": "Haojian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Haojian Huang"
                },
                "author": "Haojian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12096v2",
                "updated": "2025-08-20T09:52:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    52,
                    0,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-16T16:36:43Z",
                "published_parsed": [
                    2025,
                    8,
                    16,
                    16,
                    36,
                    43,
                    5,
                    228,
                    0
                ],
                "title": "STEM: Efficient Relative Capability Evaluation of LLMs through\n  Structured Transition Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STEM: Efficient Relative Capability Evaluation of LLMs through\n  Structured Transition Samples"
                },
                "summary": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Haiquan Hu"
                    },
                    {
                        "name": "Jiazhi Jiang"
                    },
                    {
                        "name": "Shiyou Xu"
                    },
                    {
                        "name": "Ruhan Zeng"
                    },
                    {
                        "name": "Tian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tian Wang"
                },
                "author": "Tian Wang",
                "arxiv_comment": "Submit to AAAI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14496v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14496v3",
                "updated": "2025-08-20T09:42:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    42,
                    24,
                    2,
                    232,
                    0
                ],
                "published": "2025-02-20T12:26:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    26,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for\n  Interactive Environment Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for\n  Interactive Environment Generalization"
                },
                "summary": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Published in COLM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14496v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14496v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14564v1",
                "updated": "2025-08-20T09:36:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    36,
                    53,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T09:36:53Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    36,
                    53,
                    2,
                    232,
                    0
                ],
                "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic\n  Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Sees What? Structured Thought-Action Sequences for Epistemic\n  Reasoning in LLMs"
                },
                "summary": "Recent advances in large language models (LLMs) and reasoning frameworks have\nopened new possibilities for improving the perspective -taking capabilities of\nautonomous agents. However, tasks that involve active perception, collaborative\nreasoning, and perspective taking (understanding what another agent can see or\nknows) pose persistent challenges for current LLM-based systems. This study\ninvestigates the potential of structured examples derived from transformed\nsolution graphs generated by the Fast Downward planner to improve the\nperformance of LLM-based agents within a ReAct framework. We propose a\nstructured solution-processing pipeline that generates three distinct\ncategories of examples: optimal goal paths (G-type), informative node paths\n(E-type), and step-by-step optimal decision sequences contrasting alternative\nactions (L-type). These solutions are further converted into ``thought-action''\nexamples by prompting an LLM to explicitly articulate the reasoning behind each\ndecision. While L-type examples slightly reduce clarification requests and\noverall action steps, they do not yield consistent improvements. Agents are\nsuccessful in tasks requiring basic attentional filtering but struggle in\nscenarios that required mentalising about occluded spaces or weighing the costs\nof epistemic actions. These findings suggest that structured examples alone are\ninsufficient for robust perspective-taking, underscoring the need for explicit\nbelief tracking, cost modelling, and richer environments to enable socially\ngrounded collaboration in LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and reasoning frameworks have\nopened new possibilities for improving the perspective -taking capabilities of\nautonomous agents. However, tasks that involve active perception, collaborative\nreasoning, and perspective taking (understanding what another agent can see or\nknows) pose persistent challenges for current LLM-based systems. This study\ninvestigates the potential of structured examples derived from transformed\nsolution graphs generated by the Fast Downward planner to improve the\nperformance of LLM-based agents within a ReAct framework. We propose a\nstructured solution-processing pipeline that generates three distinct\ncategories of examples: optimal goal paths (G-type), informative node paths\n(E-type), and step-by-step optimal decision sequences contrasting alternative\nactions (L-type). These solutions are further converted into ``thought-action''\nexamples by prompting an LLM to explicitly articulate the reasoning behind each\ndecision. While L-type examples slightly reduce clarification requests and\noverall action steps, they do not yield consistent improvements. Agents are\nsuccessful in tasks requiring basic attentional filtering but struggle in\nscenarios that required mentalising about occluded spaces or weighing the costs\nof epistemic actions. These findings suggest that structured examples alone are\ninsufficient for robust perspective-taking, underscoring the need for explicit\nbelief tracking, cost modelling, and richer environments to enable socially\ngrounded collaboration in LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Luca Annese"
                    },
                    {
                        "name": "Sabrina Patania"
                    },
                    {
                        "name": "Silvia Serino"
                    },
                    {
                        "name": "Tom Foulsham"
                    },
                    {
                        "name": "Silvia Rossi"
                    },
                    {
                        "name": "Azzurra Ruggeri"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    }
                ],
                "author_detail": {
                    "name": "Dimitri Ognibene"
                },
                "author": "Dimitri Ognibene",
                "arxiv_comment": "Accepted at ICSR25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.10; I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14553v1",
                "updated": "2025-08-20T09:14:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    14,
                    48,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T09:14:48Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    14,
                    48,
                    2,
                    232,
                    0
                ],
                "title": "Towards LLM-generated explanations for Component-based Knowledge Graph\n  Question Answering Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM-generated explanations for Component-based Knowledge Graph\n  Question Answering Systems"
                },
                "summary": "Over time, software systems have reached a level of complexity that makes it\ndifficult for their developers and users to explain particular decisions made\nby them. In this paper, we focus on the explainability of component-based\nsystems for Question Answering (QA). These components often conduct processes\ndriven by AI methods, in which behavior and decisions cannot be clearly\nexplained or justified, s.t., even for QA experts interpreting the executed\nprocess and its results is hard. To address this challenge, we present an\napproach that considers the components' input and output data flows as a source\nfor representing the behavior and provide explanations for the components,\nenabling users to comprehend what happened. In the QA framework used here, the\ndata flows of the components are represented as SPARQL queries (inputs) and RDF\ntriples (outputs). Hence, we are also providing valuable insights on\nverbalization regarding these data types. In our experiments, the approach\ngenerates explanations while following template-based settings (baseline) or\nvia the use of Large Language Models (LLMs) with different configurations\n(automatic generation). Our evaluation shows that the explanations generated\nvia LLMs achieve high quality and mostly outperform template-based approaches\naccording to the users' ratings. Therefore, it enables us to automatically\nexplain the behavior and decisions of QA components to humans while using RDF\nand SPARQL as a context for explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over time, software systems have reached a level of complexity that makes it\ndifficult for their developers and users to explain particular decisions made\nby them. In this paper, we focus on the explainability of component-based\nsystems for Question Answering (QA). These components often conduct processes\ndriven by AI methods, in which behavior and decisions cannot be clearly\nexplained or justified, s.t., even for QA experts interpreting the executed\nprocess and its results is hard. To address this challenge, we present an\napproach that considers the components' input and output data flows as a source\nfor representing the behavior and provide explanations for the components,\nenabling users to comprehend what happened. In the QA framework used here, the\ndata flows of the components are represented as SPARQL queries (inputs) and RDF\ntriples (outputs). Hence, we are also providing valuable insights on\nverbalization regarding these data types. In our experiments, the approach\ngenerates explanations while following template-based settings (baseline) or\nvia the use of Large Language Models (LLMs) with different configurations\n(automatic generation). Our evaluation shows that the explanations generated\nvia LLMs achieve high quality and mostly outperform template-based approaches\naccording to the users' ratings. Therefore, it enables us to automatically\nexplain the behavior and decisions of QA components to humans while using RDF\nand SPARQL as a context for explanations."
                },
                "authors": [
                    {
                        "name": "Dennis Schiese"
                    },
                    {
                        "name": "Aleksandr Perevalov"
                    },
                    {
                        "name": "Andreas Both"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Both"
                },
                "author": "Andreas Both",
                "arxiv_comment": "Presented at ICWI 2024, Zagreb. Released with ISBN:\n  978-989-8704-62-7. Data source:\n  https://figshare.com/articles/dataset/Towards_LLM-generated_explanations_for_component-based_knowledge_graph_question_answering_systems/27079687",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22272v2",
                "updated": "2025-08-20T09:11:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    11,
                    58,
                    2,
                    232,
                    0
                ],
                "published": "2025-03-28T09:41:05Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    41,
                    5,
                    4,
                    87,
                    0
                ],
                "title": "Robust simultaneous UWB-anchor calibration and robot localization for\n  emergency situations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust simultaneous UWB-anchor calibration and robot localization for\n  emergency situations"
                },
                "summary": "In this work, we propose a factor graph optimization (FGO) framework to\nsimultaneously solve the calibration problem for Ultra-WideBand (UWB) anchors\nand the robot localization problem. Calibrating UWB anchors manually can be\ntime-consuming and even impossible in emergencies or those situations without\nspecial calibration tools. Therefore, automatic estimation of the anchor\npositions becomes a necessity. The proposed method enables the creation of a\nsoft sensor providing the position information of the anchors in a UWB network.\nThis soft sensor requires only UWB and LiDAR measurements measured from a\nmoving robot. The proposed FGO framework is suitable for the calibration of an\nextendable large UWB network. Moreover, the anchor calibration problem and\nrobot localization problem can be solved simultaneously, which saves time for\nUWB network deployment. The proposed framework also helps to avoid artificial\nerrors in the UWB-anchor position estimation and improves the accuracy and\nrobustness of the robot-pose. The experimental results of the robot\nlocalization using LiDAR and a UWB network in a 3D environment are discussed,\ndemonstrating the performance of the proposed method. More specifically, the\nanchor calibration problem with four anchors and the robot localization problem\ncan be solved simultaneously and automatically within 30 seconds by the\nproposed framework. The supplementary video and codes can be accessed via\nhttps://github.com/LiuxhRobotAI/Simultaneous_calibration_localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a factor graph optimization (FGO) framework to\nsimultaneously solve the calibration problem for Ultra-WideBand (UWB) anchors\nand the robot localization problem. Calibrating UWB anchors manually can be\ntime-consuming and even impossible in emergencies or those situations without\nspecial calibration tools. Therefore, automatic estimation of the anchor\npositions becomes a necessity. The proposed method enables the creation of a\nsoft sensor providing the position information of the anchors in a UWB network.\nThis soft sensor requires only UWB and LiDAR measurements measured from a\nmoving robot. The proposed FGO framework is suitable for the calibration of an\nextendable large UWB network. Moreover, the anchor calibration problem and\nrobot localization problem can be solved simultaneously, which saves time for\nUWB network deployment. The proposed framework also helps to avoid artificial\nerrors in the UWB-anchor position estimation and improves the accuracy and\nrobustness of the robot-pose. The experimental results of the robot\nlocalization using LiDAR and a UWB network in a 3D environment are discussed,\ndemonstrating the performance of the proposed method. More specifically, the\nanchor calibration problem with four anchors and the robot localization problem\ncan be solved simultaneously and automatically within 30 seconds by the\nproposed framework. The supplementary video and codes can be accessed via\nhttps://github.com/LiuxhRobotAI/Simultaneous_calibration_localization."
                },
                "authors": [
                    {
                        "name": "Xinghua Liu"
                    },
                    {
                        "name": "Ming Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ming Cao"
                },
                "author": "Ming Cao",
                "arxiv_comment": "Submit to IEEE SMC 2025. This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03106v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03106v5",
                "updated": "2025-08-20T09:10:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    10,
                    5,
                    2,
                    232,
                    0
                ],
                "published": "2025-06-03T17:39:02Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    39,
                    2,
                    1,
                    154,
                    0
                ],
                "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback"
                },
                "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of spontaneous self-reflection, and\npersistent failures. We then demonstrate that RL-finetuned models, even after\nexhibiting performance plateaus, can generate correct refinements on\npersistently failed problems by leveraging natural language feedback in the\nform of critiques. Building on this insight, we propose Critique-GRPO, an\nonline RL framework that integrates both natural language and numerical\nfeedback for effective policy optimization. Critique-GRPO enables LLMs to learn\nfrom initial responses and critique-guided self-refinements simultaneously\nwhile maintaining exploration. Additionally, we employ a shaping function to\namplify learning from correct, especially unfamiliar, refinements and penalize\nincorrect ones. Extensive experiments with Qwen2.5-7B-Base,\nQwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently\noutperforms supervised learning and RL-based fine-tuning methods across eight\nchallenging mathematical, STEM, and general reasoning tasks. Specifically,\nCritique-GRPO improves average pass@1 scores across all compared methods by\napproximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably,\nCritique-GRPO enables effective self-improvement through self-critiquing,\nachieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME\n2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of spontaneous self-reflection, and\npersistent failures. We then demonstrate that RL-finetuned models, even after\nexhibiting performance plateaus, can generate correct refinements on\npersistently failed problems by leveraging natural language feedback in the\nform of critiques. Building on this insight, we propose Critique-GRPO, an\nonline RL framework that integrates both natural language and numerical\nfeedback for effective policy optimization. Critique-GRPO enables LLMs to learn\nfrom initial responses and critique-guided self-refinements simultaneously\nwhile maintaining exploration. Additionally, we employ a shaping function to\namplify learning from correct, especially unfamiliar, refinements and penalize\nincorrect ones. Extensive experiments with Qwen2.5-7B-Base,\nQwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently\noutperforms supervised learning and RL-based fine-tuning methods across eight\nchallenging mathematical, STEM, and general reasoning tasks. Specifically,\nCritique-GRPO improves average pass@1 scores across all compared methods by\napproximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably,\nCritique-GRPO enables effective self-improvement through self-critiquing,\nachieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME\n2024."
                },
                "authors": [
                    {
                        "name": "Xiaoying Zhang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "52 pages, updated with new experimental results and implementation\n  details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03106v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03106v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03047v2",
                "updated": "2025-08-20T09:09:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    9,
                    9,
                    56,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-03T10:11:35Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    11,
                    35,
                    3,
                    184,
                    0
                ],
                "title": "Enhancing Temporal Sensitivity of Large Language Model for\n  Recommendation with Counterfactual Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Temporal Sensitivity of Large Language Model for\n  Recommendation with Counterfactual Tuning"
                },
                "summary": "Recent advances have applied large language models (LLMs) to sequential\nrecommendation, leveraging their pre-training knowledge and reasoning\ncapabilities to provide more personalized user experiences. However, existing\nLLM-based methods fail to sufficiently leverage the rich temporal information\ninherent in users' historical interaction sequences, stemming from fundamental\narchitectural constraints: LLMs process information through self-attention\nmechanisms that lack inherent sequence ordering and rely on position embeddings\ndesigned primarily for natural language rather than user interaction sequences.\nThis limitation significantly impairs their ability to capture the evolution of\nuser preferences over time and predict future interests accurately.\n  To address this critical gap, we propose \\underline{C}ounterfactual\n\\underline{E}nhanced \\underline{T}emporal Framework for LLM-Based\n\\underline{Rec}ommendation (CETRec). CETRec is grounded in causal inference\nprinciples, which allow it to isolate and measure the specific impact of\ntemporal information on recommendation outcomes. Combined with our\ncounterfactual tuning task derived from causal analysis, CETRec effectively\nenhances LLMs' awareness of both absolute order (how recently items were\ninteracted with) and relative order (the sequential relationships between\nitems). Extensive experiments on real-world datasets demonstrate the\neffectiveness of our CETRec. Our code is available at\nhttps://anonymous.4open.science/r/CETRec-B9CE/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have applied large language models (LLMs) to sequential\nrecommendation, leveraging their pre-training knowledge and reasoning\ncapabilities to provide more personalized user experiences. However, existing\nLLM-based methods fail to sufficiently leverage the rich temporal information\ninherent in users' historical interaction sequences, stemming from fundamental\narchitectural constraints: LLMs process information through self-attention\nmechanisms that lack inherent sequence ordering and rely on position embeddings\ndesigned primarily for natural language rather than user interaction sequences.\nThis limitation significantly impairs their ability to capture the evolution of\nuser preferences over time and predict future interests accurately.\n  To address this critical gap, we propose \\underline{C}ounterfactual\n\\underline{E}nhanced \\underline{T}emporal Framework for LLM-Based\n\\underline{Rec}ommendation (CETRec). CETRec is grounded in causal inference\nprinciples, which allow it to isolate and measure the specific impact of\ntemporal information on recommendation outcomes. Combined with our\ncounterfactual tuning task derived from causal analysis, CETRec effectively\nenhances LLMs' awareness of both absolute order (how recently items were\ninteracted with) and relative order (the sequential relationships between\nitems). Extensive experiments on real-world datasets demonstrate the\neffectiveness of our CETRec. Our code is available at\nhttps://anonymous.4open.science/r/CETRec-B9CE/."
                },
                "authors": [
                    {
                        "name": "Yutian Liu"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14544v1",
                "updated": "2025-08-20T08:55:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    55,
                    26,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:55:26Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    55,
                    26,
                    2,
                    232,
                    0
                ],
                "title": "Adaptively Robust LLM Inference Optimization under Prediction\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptively Robust LLM Inference Optimization under Prediction\n  Uncertainty"
                },
                "summary": "We study the problem of optimizing Large Language Model (LLM) inference\nscheduling to minimize total latency. LLM inference is an online and multi-task\nservice process and also heavily energy consuming by which a pre-trained LLM\nprocesses input requests and generates output tokens sequentially. Therefore,\nit is vital to improve its scheduling efficiency and reduce the power\nconsumption while a great amount of prompt requests are arriving. A key\nchallenge in LLM inference scheduling is that while the prompt length is known\nupon arrival, the output length, which critically impacts memory usage and\nprocessing time, is unknown. To address this uncertainty, we propose algorithms\nthat leverage machine learning to predict output lengths, assuming the\nprediction provides an interval classification (min-max range) for each\nrequest.\n  We first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which\nschedules requests based on the upper bound of predicted output lengths to\nprevent memory overflow. However, this approach is overly conservative: as\nprediction accuracy decreases, performance degrades significantly due to\npotential overestimation. To overcome this limitation, we propose\n$\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted\nlower bound as the output length and dynamically refines this estimate during\ninferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale\ncompetitive ratio. Through numerical simulations, we demonstrate that\n$\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler,\nhighlighting both its efficiency and robustness in practical scenarios.\nMoreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the\nprediction interval--an advantageous design choice since upper bounds on output\nlength are typically more challenging to predict accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of optimizing Large Language Model (LLM) inference\nscheduling to minimize total latency. LLM inference is an online and multi-task\nservice process and also heavily energy consuming by which a pre-trained LLM\nprocesses input requests and generates output tokens sequentially. Therefore,\nit is vital to improve its scheduling efficiency and reduce the power\nconsumption while a great amount of prompt requests are arriving. A key\nchallenge in LLM inference scheduling is that while the prompt length is known\nupon arrival, the output length, which critically impacts memory usage and\nprocessing time, is unknown. To address this uncertainty, we propose algorithms\nthat leverage machine learning to predict output lengths, assuming the\nprediction provides an interval classification (min-max range) for each\nrequest.\n  We first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which\nschedules requests based on the upper bound of predicted output lengths to\nprevent memory overflow. However, this approach is overly conservative: as\nprediction accuracy decreases, performance degrades significantly due to\npotential overestimation. To overcome this limitation, we propose\n$\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted\nlower bound as the output length and dynamically refines this estimate during\ninferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale\ncompetitive ratio. Through numerical simulations, we demonstrate that\n$\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler,\nhighlighting both its efficiency and robustness in practical scenarios.\nMoreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the\nprediction interval--an advantageous design choice since upper bounds on output\nlength are typically more challenging to predict accurately."
                },
                "authors": [
                    {
                        "name": "Zixi Chen"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14540v1",
                "updated": "2025-08-20T08:45:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    45,
                    53,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:45:53Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    45,
                    53,
                    2,
                    232,
                    0
                ],
                "title": "Post-hoc LLM-Supported Debugging of Distributed Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-hoc LLM-Supported Debugging of Distributed Processes"
                },
                "summary": "In this paper, we address the problem of manual debugging, which nowadays\nremains resource-intensive and in some parts archaic. This problem is\nespecially evident in increasingly complex and distributed software systems.\nTherefore, our objective of this work is to introduce an approach that can\npossibly be applied to any system, at both the macro- and micro-level, to ease\nthis debugging process. This approach utilizes a system's process data, in\nconjunction with generative AI, to generate natural-language explanations.\nThese explanations are generated from the actual process data, interface\ninformation, and documentation to guide the developers more efficiently to\nunderstand the behavior and possible errors of a process and its sub-processes.\nHere, we present a demonstrator that employs this approach on a component-based\nJava system. However, our approach is language-agnostic. Ideally, the generated\nexplanations will provide a good understanding of the process, even if\ndevelopers are not familiar with all the details of the considered system. Our\ndemonstrator is provided as an open-source web application that is freely\naccessible to all users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the problem of manual debugging, which nowadays\nremains resource-intensive and in some parts archaic. This problem is\nespecially evident in increasingly complex and distributed software systems.\nTherefore, our objective of this work is to introduce an approach that can\npossibly be applied to any system, at both the macro- and micro-level, to ease\nthis debugging process. This approach utilizes a system's process data, in\nconjunction with generative AI, to generate natural-language explanations.\nThese explanations are generated from the actual process data, interface\ninformation, and documentation to guide the developers more efficiently to\nunderstand the behavior and possible errors of a process and its sub-processes.\nHere, we present a demonstrator that employs this approach on a component-based\nJava system. However, our approach is language-agnostic. Ideally, the generated\nexplanations will provide a good understanding of the process, even if\ndevelopers are not familiar with all the details of the considered system. Our\ndemonstrator is provided as an open-source web application that is freely\naccessible to all users."
                },
                "authors": [
                    {
                        "name": "Dennis Schiese"
                    },
                    {
                        "name": "Andreas Both"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Both"
                },
                "author": "Andreas Both",
                "arxiv_comment": "Presented at ICWE 2025, Delft (30 June - 03 July 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14537v1",
                "updated": "2025-08-20T08:41:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    41,
                    19,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:41:19Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    41,
                    19,
                    2,
                    232,
                    0
                ],
                "title": "WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch\n  Selection with VLM and LLM Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch\n  Selection with VLM and LLM Knowledge Fusion"
                },
                "summary": "Whole slide images (WSIs) in computational pathology (CPath) pose a major\ncomputational challenge due to their gigapixel scale, often requiring the\nprocessing of tens to hundreds of thousands of high-resolution patches per\nslide. This results in prohibitive encoding costs, with preprocessing and\ntraining times extending to days or even weeks-making WSI encoding the most\nsignificant bottleneck in real-world deployment. In this work, we propose\nWISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain\nvision-language models and large language models to address this challenge by\nselectively processing diagnostically relevant regions. WISE-FUSE first\ncomputes similarity scores between low-resolution patches and class-specific\ntextual descriptions using a knowledge distillation mechanism that preserves\nfine-grained diagnostic features. Based on these similarity scores, we select a\nsmall subset of informative regions for the target task, which quickly\neliminates irrelevant patches at the coarse level. The corresponding\nhigh-resolution patches are then selectively encoded and fused with textual\nembeddings to reinforce diagnostic context. Extensive experiments demonstrate\nthat WISE-FUSE reduces WSI encoding time by over threefold while achieving\ndiagnostic performance comparable to or surpassing that of exhaustive patch\nprocessing, offering a scalable and practical solution for CPath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole slide images (WSIs) in computational pathology (CPath) pose a major\ncomputational challenge due to their gigapixel scale, often requiring the\nprocessing of tens to hundreds of thousands of high-resolution patches per\nslide. This results in prohibitive encoding costs, with preprocessing and\ntraining times extending to days or even weeks-making WSI encoding the most\nsignificant bottleneck in real-world deployment. In this work, we propose\nWISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain\nvision-language models and large language models to address this challenge by\nselectively processing diagnostically relevant regions. WISE-FUSE first\ncomputes similarity scores between low-resolution patches and class-specific\ntextual descriptions using a knowledge distillation mechanism that preserves\nfine-grained diagnostic features. Based on these similarity scores, we select a\nsmall subset of informative regions for the target task, which quickly\neliminates irrelevant patches at the coarse level. The corresponding\nhigh-resolution patches are then selectively encoded and fused with textual\nembeddings to reinforce diagnostic context. Extensive experiments demonstrate\nthat WISE-FUSE reduces WSI encoding time by over threefold while achieving\ndiagnostic performance comparable to or surpassing that of exhaustive patch\nprocessing, offering a scalable and practical solution for CPath."
                },
                "authors": [
                    {
                        "name": "Yonghan Shin"
                    },
                    {
                        "name": "SeungKyu Kim"
                    },
                    {
                        "name": "Won-Ki Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Won-Ki Jeong"
                },
                "author": "Won-Ki Jeong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14532v1",
                "updated": "2025-08-20T08:40:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    40,
                    22,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:40:22Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    40,
                    22,
                    2,
                    232,
                    0
                ],
                "title": "Preguss: It Analyzes, It Specifies, It Verifies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preguss: It Analyzes, It Specifies, It Verifies"
                },
                "summary": "Fully automated verification of large-scale software and hardware systems is\narguably the holy grail of formal methods. Large language models (LLMs) have\nrecently demonstrated their potential for enhancing the degree of automation in\nformal verification by, e.g., generating formal specifications as essential to\ndeductive verification, yet exhibit poor scalability due to context-length\nlimitations and, more importantly, the difficulty of inferring complex,\ninterprocedural specifications. This paper outlines Preguss - a modular,\nfine-grained framework for automating the generation and refinement of formal\nspecifications. Preguss synergizes between static analysis and deductive\nverification by orchestrating two components: (i) potential runtime error\n(RTE)-guided construction and prioritization of verification units, and (ii)\nLLM-aided synthesis of interprocedural specifications at the unit level. We\nenvisage that Preguss paves a compelling path towards the automated\nverification of large-scale programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully automated verification of large-scale software and hardware systems is\narguably the holy grail of formal methods. Large language models (LLMs) have\nrecently demonstrated their potential for enhancing the degree of automation in\nformal verification by, e.g., generating formal specifications as essential to\ndeductive verification, yet exhibit poor scalability due to context-length\nlimitations and, more importantly, the difficulty of inferring complex,\ninterprocedural specifications. This paper outlines Preguss - a modular,\nfine-grained framework for automating the generation and refinement of formal\nspecifications. Preguss synergizes between static analysis and deductive\nverification by orchestrating two components: (i) potential runtime error\n(RTE)-guided construction and prioritization of verification units, and (ii)\nLLM-aided synthesis of interprocedural specifications at the unit level. We\nenvisage that Preguss paves a compelling path towards the automated\nverification of large-scale programs."
                },
                "authors": [
                    {
                        "name": "Zhongyi Wang"
                    },
                    {
                        "name": "Tengjie Lin"
                    },
                    {
                        "name": "Mingshuai Chen"
                    },
                    {
                        "name": "Mingqi Yang"
                    },
                    {
                        "name": "Haokun Li"
                    },
                    {
                        "name": "Xiao Yi"
                    },
                    {
                        "name": "Shengchao Qin"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "arxiv_comment": "Position paper to appear in the 1st International Workshop on\n  Language Models and Programming Languages (LMPL '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03608v2",
                "updated": "2025-08-20T08:37:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    37,
                    28,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-04T14:31:30Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    14,
                    31,
                    30,
                    4,
                    185,
                    0
                ],
                "title": "Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation\n  (RAG) Pipelines for Open Radio Access Networks (ORAN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation\n  (RAG) Pipelines for Open Radio Access Networks (ORAN)"
                },
                "summary": "Generative AI (GenAI) is expected to play a pivotal role in enabling\nautonomous optimization in future wireless networks. Within the ORAN\narchitecture, Large Language Models (LLMs) can be specialized to generate xApps\nand rApps by leveraging specifications and API definitions from the RAN\nIntelligent Controller (RIC) platform. However, fine-tuning base LLMs for\ntelecom-specific tasks remains expensive and resource-intensive.\nRetrieval-Augmented Generation (RAG) offers a practical alternative through\nin-context learning, enabling domain adaptation without full retraining. While\ntraditional RAG systems rely on vector-based retrieval, emerging variants such\nas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval\nstrategies to support multi-hop reasoning and improve factual grounding.\nDespite their promise, these methods lack systematic, metric-driven\nevaluations, particularly in high-stakes domains such as ORAN. In this study,\nwe conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid\nGraphRAG using ORAN specifications. We assess performance across varying\nquestion complexities using established generation metrics: faithfulness,\nanswer relevance, context relevance, and factual correctness. Results show that\nboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG\nimproves factual correctness by 8%, while GraphRAG improves context relevance\nby 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) is expected to play a pivotal role in enabling\nautonomous optimization in future wireless networks. Within the ORAN\narchitecture, Large Language Models (LLMs) can be specialized to generate xApps\nand rApps by leveraging specifications and API definitions from the RAN\nIntelligent Controller (RIC) platform. However, fine-tuning base LLMs for\ntelecom-specific tasks remains expensive and resource-intensive.\nRetrieval-Augmented Generation (RAG) offers a practical alternative through\nin-context learning, enabling domain adaptation without full retraining. While\ntraditional RAG systems rely on vector-based retrieval, emerging variants such\nas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval\nstrategies to support multi-hop reasoning and improve factual grounding.\nDespite their promise, these methods lack systematic, metric-driven\nevaluations, particularly in high-stakes domains such as ORAN. In this study,\nwe conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid\nGraphRAG using ORAN specifications. We assess performance across varying\nquestion complexities using established generation metrics: faithfulness,\nanswer relevance, context relevance, and factual correctness. Results show that\nboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG\nimproves factual correctness by 8%, while GraphRAG improves context relevance\nby 11%."
                },
                "authors": [
                    {
                        "name": "Sarat Ahmad"
                    },
                    {
                        "name": "Zeinab Nezami"
                    },
                    {
                        "name": "Maryam Hafeez"
                    },
                    {
                        "name": "Syed Ali Raza Zaidi"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ali Raza Zaidi"
                },
                "author": "Syed Ali Raza Zaidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14527v1",
                "updated": "2025-08-20T08:36:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    36,
                    57,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:36:57Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    36,
                    57,
                    2,
                    232,
                    0
                ],
                "title": "Adversarial Generation and Collaborative Evolution of Safety-Critical\n  Scenarios for Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Generation and Collaborative Evolution of Safety-Critical\n  Scenarios for Autonomous Vehicles"
                },
                "summary": "The generation of safety-critical scenarios in simulation has become\nincreasingly crucial for safety evaluation in autonomous vehicles prior to road\ndeployment in society. However, current approaches largely rely on predefined\nthreat patterns or rule-based strategies, which limit their ability to expose\ndiverse and unforeseen failure modes. To overcome these, we propose ScenGE, a\nframework that can generate plentiful safety-critical scenarios by reasoning\nnovel adversarial cases and then amplifying them with complex traffic flows.\nGiven a simple prompt of a benign scene, it first performs Meta-Scenario\nGeneration, where a large language model, grounded in structured driving\nknowledge, infers an adversarial agent whose behavior poses a threat that is\nboth plausible and deliberately challenging. This meta-scenario is then\nspecified in executable code for precise in-simulator control. Subsequently,\nComplex Scenario Evolution uses background vehicles to amplify the core threat\nintroduced by Meta-Scenario. It builds an adversarial collaborator graph to\nidentify key agent trajectories for optimization. These perturbations are\ndesigned to simultaneously reduce the ego vehicle's maneuvering space and\ncreate critical occlusions. Extensive experiments conducted on multiple\nreinforcement learning based AV models show that ScenGE uncovers more severe\ncollision cases (+31.96%) on average than SoTA baselines. Additionally, our\nScenGE can be applied to large model based AV systems and deployed on different\nsimulators; we further observe that adversarial training on our scenarios\nimproves the model robustness. Finally, we validate our framework through\nreal-world vehicle tests and human evaluation, confirming that the generated\nscenarios are both plausible and critical. We hope our paper can build up a\ncritical step towards building public trust and ensuring their safe deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation of safety-critical scenarios in simulation has become\nincreasingly crucial for safety evaluation in autonomous vehicles prior to road\ndeployment in society. However, current approaches largely rely on predefined\nthreat patterns or rule-based strategies, which limit their ability to expose\ndiverse and unforeseen failure modes. To overcome these, we propose ScenGE, a\nframework that can generate plentiful safety-critical scenarios by reasoning\nnovel adversarial cases and then amplifying them with complex traffic flows.\nGiven a simple prompt of a benign scene, it first performs Meta-Scenario\nGeneration, where a large language model, grounded in structured driving\nknowledge, infers an adversarial agent whose behavior poses a threat that is\nboth plausible and deliberately challenging. This meta-scenario is then\nspecified in executable code for precise in-simulator control. Subsequently,\nComplex Scenario Evolution uses background vehicles to amplify the core threat\nintroduced by Meta-Scenario. It builds an adversarial collaborator graph to\nidentify key agent trajectories for optimization. These perturbations are\ndesigned to simultaneously reduce the ego vehicle's maneuvering space and\ncreate critical occlusions. Extensive experiments conducted on multiple\nreinforcement learning based AV models show that ScenGE uncovers more severe\ncollision cases (+31.96%) on average than SoTA baselines. Additionally, our\nScenGE can be applied to large model based AV systems and deployed on different\nsimulators; we further observe that adversarial training on our scenarios\nimproves the model robustness. Finally, we validate our framework through\nreal-world vehicle tests and human evaluation, confirming that the generated\nscenarios are both plausible and critical. We hope our paper can build up a\ncritical step towards building public trust and ensuring their safe deployment."
                },
                "authors": [
                    {
                        "name": "Jiangfan Liu"
                    },
                    {
                        "name": "Yongkang Guo"
                    },
                    {
                        "name": "Fangzhi Zhong"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Zonglei Jing"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Jiakai Wang"
                    },
                    {
                        "name": "Mingchuan Zhang"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12769v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12769v3",
                "updated": "2025-08-20T08:11:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    11,
                    10,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-18T09:43:07Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    43,
                    7,
                    0,
                    230,
                    0
                ],
                "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing\n  through Cluster Retrieval and Execution Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing\n  through Cluster Retrieval and Execution Description"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git"
                },
                "authors": [
                    {
                        "name": "Shaoming Duan"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Chuanyi Liu"
                    },
                    {
                        "name": "Zhibin Zhu"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Peiyi Han"
                    },
                    {
                        "name": "Liang Yan"
                    },
                    {
                        "name": "Zewu Peng"
                    }
                ],
                "author_detail": {
                    "name": "Zewu Peng"
                },
                "author": "Zewu Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12769v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12769v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24773v2",
                "updated": "2025-08-20T08:08:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    8,
                    3,
                    2,
                    232,
                    0
                ],
                "published": "2025-05-30T16:35:32Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    35,
                    32,
                    4,
                    150,
                    0
                ],
                "title": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with\n  Resource-Aware Low-Rank Adaption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with\n  Resource-Aware Low-Rank Adaption"
                },
                "summary": "Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world."
                },
                "authors": [
                    {
                        "name": "Yajie Zhou"
                    },
                    {
                        "name": "Xiaoyi Pang"
                    },
                    {
                        "name": "Zhibo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Wang"
                },
                "author": "Zhibo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14511v1",
                "updated": "2025-08-20T08:03:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    3,
                    0,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T08:03:00Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    3,
                    0,
                    2,
                    232,
                    0
                ],
                "title": "What You See Is What It Does: A Structural Pattern for Legible Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What You See Is What It Does: A Structural Pattern for Legible Software"
                },
                "summary": "The opportunities offered by LLM coders (and their current limitations)\ndemand a reevaluation of how software is structured. Software today is often\n\"illegible\" - lacking a direct correspondence between code and observed\nbehavior - and insufficiently modular, leading to a failure of three key\nrequirements of robust coding: incrementality (the ability to deliver small\nincrements by making localized changes), integrity (avoiding breaking prior\nincrements) and transparency (making clear what has changed at build time, and\nwhat actions have happened at runtime).\n  A new structural pattern offers improved legibility and modularity. Its\nelements are concepts and synchronizations: fully independent services and\nevent-based rules that mediate between them. A domain-specific language for\nsynchronizations allows behavioral features to be expressed in a granular and\ndeclarative way (and thus readily generated by an LLM). A case study of the\nRealWorld benchmark is used to illustrate and evaluate the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The opportunities offered by LLM coders (and their current limitations)\ndemand a reevaluation of how software is structured. Software today is often\n\"illegible\" - lacking a direct correspondence between code and observed\nbehavior - and insufficiently modular, leading to a failure of three key\nrequirements of robust coding: incrementality (the ability to deliver small\nincrements by making localized changes), integrity (avoiding breaking prior\nincrements) and transparency (making clear what has changed at build time, and\nwhat actions have happened at runtime).\n  A new structural pattern offers improved legibility and modularity. Its\nelements are concepts and synchronizations: fully independent services and\nevent-based rules that mediate between them. A domain-specific language for\nsynchronizations allows behavioral features to be expressed in a granular and\ndeclarative way (and thus readily generated by an LLM). A case study of the\nRealWorld benchmark is used to illustrate and evaluate the approach."
                },
                "authors": [
                    {
                        "name": "Eagon Meng"
                    },
                    {
                        "name": "Daniel Jackson"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Jackson"
                },
                "author": "Daniel Jackson",
                "arxiv_comment": "16 pages. Appearing in Onward! at SPLASH 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08113v2",
                "updated": "2025-08-20T07:59:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    59,
                    8,
                    2,
                    232,
                    0
                ],
                "published": "2025-06-09T18:10:00Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    18,
                    10,
                    0,
                    0,
                    160,
                    0
                ],
                "title": "Benchmarking Pre-Trained Time Series Models for Electricity Price\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Pre-Trained Time Series Models for Electricity Price\n  Forecasting"
                },
                "summary": "Accurate electricity price forecasting (EPF) is crucial for effective\ndecision-making in power trading on the spot market. While recent advances in\ngenerative artificial intelligence (GenAI) and pre-trained large language\nmodels (LLMs) have inspired the development of numerous time series foundation\nmodels (TSFMs) for time series forecasting, their effectiveness in EPF remains\nuncertain. To address this gap, we benchmark several state-of-the-art\npretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and\nTimeGPT--against established statistical and machine learning (ML) methods for\nEPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,\nFrance, the Netherlands, Austria, and Belgium, we generate daily forecasts with\na one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the\nTSFMs, performing on par with traditional models. However, the biseasonal MSTL\nmodel, which captures daily and weekly seasonality, stands out for its\nconsistent performance across countries and evaluation metrics, with no TSFM\nstatistically outperforming it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate electricity price forecasting (EPF) is crucial for effective\ndecision-making in power trading on the spot market. While recent advances in\ngenerative artificial intelligence (GenAI) and pre-trained large language\nmodels (LLMs) have inspired the development of numerous time series foundation\nmodels (TSFMs) for time series forecasting, their effectiveness in EPF remains\nuncertain. To address this gap, we benchmark several state-of-the-art\npretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and\nTimeGPT--against established statistical and machine learning (ML) methods for\nEPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,\nFrance, the Netherlands, Austria, and Belgium, we generate daily forecasts with\na one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the\nTSFMs, performing on par with traditional models. However, the biseasonal MSTL\nmodel, which captures daily and weekly seasonality, stands out for its\nconsistent performance across countries and evaluation metrics, with no TSFM\nstatistically outperforming it."
                },
                "authors": [
                    {
                        "name": "Timothée Hornek Amir Sartipi"
                    },
                    {
                        "name": "Igor Tchappi"
                    },
                    {
                        "name": "Gilbert Fridgen"
                    }
                ],
                "author_detail": {
                    "name": "Gilbert Fridgen"
                },
                "author": "Gilbert Fridgen",
                "arxiv_doi": "10.1109/eem64765.2025.11050326",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/eem64765.2025.11050326",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.08113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14507v1",
                "updated": "2025-08-20T07:56:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    56,
                    13,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T07:56:13Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    56,
                    13,
                    2,
                    232,
                    0
                ],
                "title": "DeepTelecom: A Digital-Twin Deep Learning Dataset for Channel and MIMO\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTelecom: A Digital-Twin Deep Learning Dataset for Channel and MIMO\n  Applications"
                },
                "summary": "Domain-specific datasets are the foundation for unleashing artificial\nintelligence (AI)-driven wireless innovation. Yet existing wireless AI corpora\nare slow to produce, offer limited modeling fidelity, and cover only narrow\nscenario types. To address the challenges, we create DeepTelecom, a\nthree-dimension (3D) digital-twin channel dataset. Specifically, a large\nlanguage model (LLM)-assisted pipeline first builds the third level of details\n(LoD3) outdoor and indoor scenes with segmentable material-parameterizable\nsurfaces. Then, DeepTelecom simulates full radio-wave propagation effects based\non Sionna's ray-tracing engine. Leveraging GPU acceleration, DeepTelecom\nstreams ray-path trajectories and real-time signal-strength heat maps, compiles\nthem into high-frame-rate videos, and simultaneously outputs synchronized\nmulti-view images, channel tensors, and multi-scale fading traces. By\nefficiently streaming large-scale, high-fidelity, and multimodal channel data,\nDeepTelecom not only furnishes a unified benchmark for wireless AI research but\nalso supplies the domain-rich training substrate that enables foundation models\nto tightly fuse large model intelligence with future communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-specific datasets are the foundation for unleashing artificial\nintelligence (AI)-driven wireless innovation. Yet existing wireless AI corpora\nare slow to produce, offer limited modeling fidelity, and cover only narrow\nscenario types. To address the challenges, we create DeepTelecom, a\nthree-dimension (3D) digital-twin channel dataset. Specifically, a large\nlanguage model (LLM)-assisted pipeline first builds the third level of details\n(LoD3) outdoor and indoor scenes with segmentable material-parameterizable\nsurfaces. Then, DeepTelecom simulates full radio-wave propagation effects based\non Sionna's ray-tracing engine. Leveraging GPU acceleration, DeepTelecom\nstreams ray-path trajectories and real-time signal-strength heat maps, compiles\nthem into high-frame-rate videos, and simultaneously outputs synchronized\nmulti-view images, channel tensors, and multi-scale fading traces. By\nefficiently streaming large-scale, high-fidelity, and multimodal channel data,\nDeepTelecom not only furnishes a unified benchmark for wireless AI research but\nalso supplies the domain-rich training substrate that enables foundation models\nto tightly fuse large model intelligence with future communication systems."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Zehua Jiang"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Chongwen Huang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Siming Jiang"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Richeng Jin"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Sami Muhaidat"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09586v2",
                "updated": "2025-08-20T07:50:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    50,
                    49,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-13T07:59:29Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    59,
                    29,
                    2,
                    225,
                    0
                ],
                "title": "EvoCurr: Self-evolving Curriculum with Behavior Code Generation for\n  Complex Decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCurr: Self-evolving Curriculum with Behavior Code Generation for\n  Complex Decision-making"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse domains, including programming, planning, and decision-making. However,\ntheir performance often degrades when faced with highly complex problem\ninstances that require deep reasoning over long horizons. In such cases, direct\nproblem-solving approaches can lead to inefficiency or failure due to the lack\nof structured intermediate guidance. To address this, we propose a novel\nself-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM\nconstructs a sequence of problem instances with gradually increasing\ndifficulty, tailored to the solver LLM's learning progress. The curriculum\ndynamically adapts easing challenges when the solver struggles and escalating\nthem when success is consistent, thus maintaining an optimal learning\ntrajectory. This approach enables the solver LLM, implemented as a\ncode-generation model producing Python decision-tree scripts, to progressively\nacquire the skills needed for complex decision-making tasks. Experimental\nresults on challenging decision-making benchmarks show that our method\nsignificantly improves task success rates and solution efficiency compared to\ndirect-solving baselines. These findings suggest that LLM-driven curriculum\nlearning holds strong potential for enhancing automated reasoning in\nreal-world, high-complexity domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse domains, including programming, planning, and decision-making. However,\ntheir performance often degrades when faced with highly complex problem\ninstances that require deep reasoning over long horizons. In such cases, direct\nproblem-solving approaches can lead to inefficiency or failure due to the lack\nof structured intermediate guidance. To address this, we propose a novel\nself-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM\nconstructs a sequence of problem instances with gradually increasing\ndifficulty, tailored to the solver LLM's learning progress. The curriculum\ndynamically adapts easing challenges when the solver struggles and escalating\nthem when success is consistent, thus maintaining an optimal learning\ntrajectory. This approach enables the solver LLM, implemented as a\ncode-generation model producing Python decision-tree scripts, to progressively\nacquire the skills needed for complex decision-making tasks. Experimental\nresults on challenging decision-making benchmarks show that our method\nsignificantly improves task success rates and solution efficiency compared to\ndirect-solving baselines. These findings suggest that LLM-driven curriculum\nlearning holds strong potential for enhancing automated reasoning in\nreal-world, high-complexity domains."
                },
                "authors": [
                    {
                        "name": "Yang Cheng"
                    },
                    {
                        "name": "Zilai Wang"
                    },
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Wenhui Zhu"
                    },
                    {
                        "name": "Yue Deng"
                    },
                    {
                        "name": "Jian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhao"
                },
                "author": "Jian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02329v2",
                "updated": "2025-08-20T07:44:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    44,
                    52,
                    2,
                    232,
                    0
                ],
                "published": "2025-02-04T14:00:32Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    0,
                    32,
                    1,
                    35,
                    0
                ],
                "title": "ReSpark: Leveraging Previous Data Reports as References to Generate New\n  Reports with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSpark: Leveraging Previous Data Reports as References to Generate New\n  Reports with LLMs"
                },
                "summary": "Creating data reports is a labor-intensive task involving iterative data\nexploration, insight extraction, and narrative construction. A key challenge\nlies in composing the analysis logic-from defining objectives and transforming\ndata to identifying and communicating insights. Manually crafting this logic\ncan be cognitively demanding. While experienced analysts often reuse scripts\nfrom past projects, finding a perfect match for a new dataset is rare. Even\nwhen similar analyses are available online, they usually share only results or\nvisualizations, not the underlying code, making reuse difficult. To address\nthis, we present ReSpark, a system that leverages large language models (LLMs)\nto reverse-engineer analysis logic from existing reports and adapt it to new\ndatasets. By generating draft analysis steps, ReSpark provides a warm start for\nusers. It also supports interactive refinement, allowing users to inspect\nintermediate outputs, insert objectives, and revise content. We evaluate\nReSpark through comparative and user studies, demonstrating its effectiveness\nin lowering the barrier to generating data reports without relying on existing\nanalysis code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating data reports is a labor-intensive task involving iterative data\nexploration, insight extraction, and narrative construction. A key challenge\nlies in composing the analysis logic-from defining objectives and transforming\ndata to identifying and communicating insights. Manually crafting this logic\ncan be cognitively demanding. While experienced analysts often reuse scripts\nfrom past projects, finding a perfect match for a new dataset is rare. Even\nwhen similar analyses are available online, they usually share only results or\nvisualizations, not the underlying code, making reuse difficult. To address\nthis, we present ReSpark, a system that leverages large language models (LLMs)\nto reverse-engineer analysis logic from existing reports and adapt it to new\ndatasets. By generating draft analysis steps, ReSpark provides a warm start for\nusers. It also supports interactive refinement, allowing users to inspect\nintermediate outputs, insert objectives, and revise content. We evaluate\nReSpark through comparative and user studies, demonstrating its effectiveness\nin lowering the barrier to generating data reports without relying on existing\nanalysis code."
                },
                "authors": [
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Chuhan Zhang"
                    },
                    {
                        "name": "Xiaotong Wang"
                    },
                    {
                        "name": "Sitong Pan"
                    },
                    {
                        "name": "Weiwei Cui"
                    },
                    {
                        "name": "Haidong Zhang"
                    },
                    {
                        "name": "Dazhen Deng"
                    },
                    {
                        "name": "Yingcai Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yingcai Wu"
                },
                "author": "Yingcai Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24157v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24157v3",
                "updated": "2025-08-20T07:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    35,
                    22,
                    2,
                    232,
                    0
                ],
                "published": "2025-03-31T14:40:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    40,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "LLM4FS: Leveraging Large Language Models for Feature Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4FS: Leveraging Large Language Models for Feature Selection"
                },
                "summary": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a new hybrid strategy called LLM4FS\nthat integrates LLMs with traditional data-driven methods. Specifically, input\ndata samples into LLMs, and directly call traditional data-driven techniques\nsuch as random forest and forward sequential selection. Notably, our analysis\nreveals that the hybrid strategy leverages the contextual understanding of LLMs\nand the high statistical reliability of traditional data-driven methods to\nachieve excellent feature selection performance, even surpassing LLMs and\ntraditional data-driven methods. Finally, we point out the limitations of its\napplication in decision-making. Our code is available at\nhttps://github.com/xianchaoxiu/LLM4FS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a new hybrid strategy called LLM4FS\nthat integrates LLMs with traditional data-driven methods. Specifically, input\ndata samples into LLMs, and directly call traditional data-driven techniques\nsuch as random forest and forward sequential selection. Notably, our analysis\nreveals that the hybrid strategy leverages the contextual understanding of LLMs\nand the high statistical reliability of traditional data-driven methods to\nachieve excellent feature selection performance, even surpassing LLMs and\ntraditional data-driven methods. Finally, we point out the limitations of its\napplication in decision-making. Our code is available at\nhttps://github.com/xianchaoxiu/LLM4FS."
                },
                "authors": [
                    {
                        "name": "Jianhao Li"
                    },
                    {
                        "name": "Xianchao Xiu"
                    }
                ],
                "author_detail": {
                    "name": "Xianchao Xiu"
                },
                "author": "Xianchao Xiu",
                "arxiv_comment": "CAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24157v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24157v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14496v1",
                "updated": "2025-08-20T07:33:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    33,
                    50,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T07:33:50Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    33,
                    50,
                    2,
                    232,
                    0
                ],
                "title": "Semantic Energy: Detecting LLM Hallucination Beyond Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Energy: Detecting LLM Hallucination Beyond Entropy"
                },
                "summary": "Large Language Models (LLMs) are being increasingly deployed in real-world\napplications, but they remain susceptible to hallucinations, which produce\nfluent yet incorrect responses and lead to erroneous decision-making.\nUncertainty estimation is a feasible approach to detect such hallucinations.\nFor example, semantic entropy estimates uncertainty by considering the semantic\ndiversity across multiple sampled responses, thus identifying hallucinations.\nHowever, semantic entropy relies on post-softmax probabilities and fails to\ncapture the model's inherent uncertainty, causing it to be ineffective in\ncertain scenarios. To address this issue, we introduce Semantic Energy, a novel\nuncertainty estimation framework that leverages the inherent confidence of LLMs\nby operating directly on logits of penultimate layer. By combining semantic\nclustering with a Boltzmann-inspired energy distribution, our method better\ncaptures uncertainty in cases where semantic entropy fails. Experiments across\nmultiple benchmarks show that Semantic Energy significantly improves\nhallucination detection and uncertainty estimation, offering more reliable\nsignals for downstream applications such as hallucination detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being increasingly deployed in real-world\napplications, but they remain susceptible to hallucinations, which produce\nfluent yet incorrect responses and lead to erroneous decision-making.\nUncertainty estimation is a feasible approach to detect such hallucinations.\nFor example, semantic entropy estimates uncertainty by considering the semantic\ndiversity across multiple sampled responses, thus identifying hallucinations.\nHowever, semantic entropy relies on post-softmax probabilities and fails to\ncapture the model's inherent uncertainty, causing it to be ineffective in\ncertain scenarios. To address this issue, we introduce Semantic Energy, a novel\nuncertainty estimation framework that leverages the inherent confidence of LLMs\nby operating directly on logits of penultimate layer. By combining semantic\nclustering with a Boltzmann-inspired energy distribution, our method better\ncaptures uncertainty in cases where semantic entropy fails. Experiments across\nmultiple benchmarks show that Semantic Energy significantly improves\nhallucination detection and uncertainty estimation, offering more reliable\nsignals for downstream applications such as hallucination detection."
                },
                "authors": [
                    {
                        "name": "Huan Ma"
                    },
                    {
                        "name": "Jiadong Pan"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Changqing Zhang"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06569v2",
                "updated": "2025-08-20T07:24:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    24,
                    46,
                    2,
                    232,
                    0
                ],
                "published": "2024-08-13T02:08:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    8,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "Social Debiasing for Fair Multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Debiasing for Fair Multi-modal LLMs"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) have dramatically advanced the\nresearch field and delivered powerful vision-language understanding\ncapabilities. However, these models often inherit deep-rooted social biases\nfrom their training data, leading to uncomfortable responses with respect to\nattributes such as race and gender. This paper addresses the issue of social\nbiases in MLLMs by i) introducing a comprehensive counterfactual dataset with\nmultiple social concepts (CMSC), which complements existing datasets by\nproviding 18 diverse and balanced social concepts; and ii) proposing a\ncounter-stereotype debiasing (CSD) strategy that mitigates social biases in\nMLLMs by leveraging the opposites of prevalent stereotypes. CSD incorporates\nboth a novel bias-aware data sampling method and a loss rescaling method,\nenabling the model to effectively reduce biases. We conduct extensive\nexperiments with four prevalent MLLM architectures. The results demonstrate the\nadvantage of the CMSC dataset and the edge of CSD strategy in reducing social\nbiases compared to existing competing methods, without compromising the overall\nperformance on general multi-modal reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) have dramatically advanced the\nresearch field and delivered powerful vision-language understanding\ncapabilities. However, these models often inherit deep-rooted social biases\nfrom their training data, leading to uncomfortable responses with respect to\nattributes such as race and gender. This paper addresses the issue of social\nbiases in MLLMs by i) introducing a comprehensive counterfactual dataset with\nmultiple social concepts (CMSC), which complements existing datasets by\nproviding 18 diverse and balanced social concepts; and ii) proposing a\ncounter-stereotype debiasing (CSD) strategy that mitigates social biases in\nMLLMs by leveraging the opposites of prevalent stereotypes. CSD incorporates\nboth a novel bias-aware data sampling method and a loss rescaling method,\nenabling the model to effectively reduce biases. We conduct extensive\nexperiments with four prevalent MLLM architectures. The results demonstrate the\nadvantage of the CMSC dataset and the edge of CSD strategy in reducing social\nbiases compared to existing competing methods, without compromising the overall\nperformance on general multi-modal reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Harry Cheng"
                    },
                    {
                        "name": "Yangyang Guo"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Ming Yang"
                    },
                    {
                        "name": "Tian Gan"
                    },
                    {
                        "name": "Weili Guan"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Project page:\n  https://github.com/xaCheng1996/Social_Debiasing_For_Fair_MLLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09675v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09675v3",
                "updated": "2025-08-20T07:15:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    15,
                    59,
                    2,
                    232,
                    0
                ],
                "published": "2024-06-14T02:56:57Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    2,
                    56,
                    57,
                    4,
                    166,
                    0
                ],
                "title": "A Comprehensive Benchmark on Spectral GNNs: The Impact on Efficiency,\n  Memory, and Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Benchmark on Spectral GNNs: The Impact on Efficiency,\n  Memory, and Effectiveness"
                },
                "summary": "With recent advancements in graph neural networks (GNNs), spectral GNNs have\nreceived increasing popularity by virtue of their ability to retrieve graph\nsignals in the spectral domain. These models feature uniqueness in efficient\ncomputation as well as rich expressiveness, which stems from advanced\nmanagement and profound understanding of graph data. However, few systematic\nstudies have been conducted to assess spectral GNNs, particularly in\nbenchmarking their efficiency, memory consumption, and effectiveness in a\nunified and fair manner. There is also a pressing need to select spectral\nmodels suitable for learning specific graph data and deploying them to massive\nweb-scale graphs, which is currently constrained by the varied model designs\nand training settings.\n  In this work, we extensively benchmark spectral GNNs with a focus on the\nspectral perspective, demystifying them as spectral graph filters. We analyze\nand categorize 35 GNNs with 27 corresponding filters, spanning diverse\nformulations and utilizations of the graph data. Then, we implement the filters\nwithin a unified spectral-oriented framework with dedicated graph computations\nand efficient training schemes. In particular, our implementation enables the\ndeployment of spectral GNNs over million-scale graphs and various tasks with\ncomparable performance and less overhead. Thorough experiments are conducted on\nthe graph filters with comprehensive metrics on effectiveness and efficiency,\noffering novel observations and practical guidelines that are only available\nfrom our evaluations across graph scales. Different from the prevailing belief,\nour benchmark reveals an intricate landscape regarding the effectiveness and\nefficiency of spectral graph filters, demonstrating the potential to achieve\ndesirable performance through tailored spectral manipulation of graph data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With recent advancements in graph neural networks (GNNs), spectral GNNs have\nreceived increasing popularity by virtue of their ability to retrieve graph\nsignals in the spectral domain. These models feature uniqueness in efficient\ncomputation as well as rich expressiveness, which stems from advanced\nmanagement and profound understanding of graph data. However, few systematic\nstudies have been conducted to assess spectral GNNs, particularly in\nbenchmarking their efficiency, memory consumption, and effectiveness in a\nunified and fair manner. There is also a pressing need to select spectral\nmodels suitable for learning specific graph data and deploying them to massive\nweb-scale graphs, which is currently constrained by the varied model designs\nand training settings.\n  In this work, we extensively benchmark spectral GNNs with a focus on the\nspectral perspective, demystifying them as spectral graph filters. We analyze\nand categorize 35 GNNs with 27 corresponding filters, spanning diverse\nformulations and utilizations of the graph data. Then, we implement the filters\nwithin a unified spectral-oriented framework with dedicated graph computations\nand efficient training schemes. In particular, our implementation enables the\ndeployment of spectral GNNs over million-scale graphs and various tasks with\ncomparable performance and less overhead. Thorough experiments are conducted on\nthe graph filters with comprehensive metrics on effectiveness and efficiency,\noffering novel observations and practical guidelines that are only available\nfrom our evaluations across graph scales. Different from the prevailing belief,\nour benchmark reveals an intricate landscape regarding the effectiveness and\nefficiency of spectral graph filters, demonstrating the potential to achieve\ndesirable performance through tailored spectral manipulation of graph data."
                },
                "authors": [
                    {
                        "name": "Ningyi Liao"
                    },
                    {
                        "name": "Haoyu Liu"
                    },
                    {
                        "name": "Zulun Zhu"
                    },
                    {
                        "name": "Siqiang Luo"
                    },
                    {
                        "name": "Laks V. S. Lakshmanan"
                    }
                ],
                "author_detail": {
                    "name": "Laks V. S. Lakshmanan"
                },
                "author": "Laks V. S. Lakshmanan",
                "arxiv_doi": "10.1145/3749156",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3749156",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.09675v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09675v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Technical Report with Appendix. Our code and evaluation is available\n  at: https://github.com/gdmnl/Spectral-GNN-Benchmark",
                "arxiv_journal_ref": "Proceedings of the ACM on Management of Data 3 (SIGMOD 2025),\n  Article 238",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10016v2",
                "updated": "2025-08-20T07:04:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    7,
                    4,
                    41,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-14T07:51:56Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    51,
                    56,
                    0,
                    195,
                    0
                ],
                "title": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling\n  via Multimodal Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling\n  via Multimodal Large Language Model Agents"
                },
                "summary": "Our research uncovers a novel privacy risk associated with multimodal large\nlanguage models (MLLMs): the ability to infer sensitive personal attributes\nfrom audio data -- a technique we term audio private attribute profiling. This\ncapability poses a significant threat, as audio can be covertly captured\nwithout direct interaction or visibility. Moreover, compared to images and\ntext, audio carries unique characteristics, such as tone and pitch, which can\nbe exploited for more detailed profiling. However, two key challenges exist in\nunderstanding MLLM-employed private attribute profiling from audio: (1) the\nlack of audio benchmark datasets with sensitive attribute annotations and (2)\nthe limited ability of current MLLMs to infer such attributes directly from\naudio. To address these challenges, we introduce AP^2, an audio benchmark\ndataset that consists of two subsets collected and composed from real-world\ndata, and both are annotated with sensitive attribute labels. Additionally, we\npropose Gifts, a hybrid multi-agent framework that leverages the complementary\nstrengths of audio-language models (ALMs) and large language models (LLMs) to\nenhance inference capabilities. Gifts employs an LLM to guide the ALM in\ninferring sensitive attributes, then forensically analyzes and consolidates the\nALM's inferences, overcoming severe hallucinations of existing ALMs in\ngenerating long-context responses. Our evaluations demonstrate that Gifts\nsignificantly outperforms baseline approaches in inferring sensitive\nattributes. Finally, we investigate model-level and data-level defense\nstrategies to mitigate the risks of audio private attribute profiling. Our work\nvalidates the feasibility of audio-based privacy attacks using MLLMs,\nhighlighting the need for robust defenses, and provides a dataset and framework\nto facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our research uncovers a novel privacy risk associated with multimodal large\nlanguage models (MLLMs): the ability to infer sensitive personal attributes\nfrom audio data -- a technique we term audio private attribute profiling. This\ncapability poses a significant threat, as audio can be covertly captured\nwithout direct interaction or visibility. Moreover, compared to images and\ntext, audio carries unique characteristics, such as tone and pitch, which can\nbe exploited for more detailed profiling. However, two key challenges exist in\nunderstanding MLLM-employed private attribute profiling from audio: (1) the\nlack of audio benchmark datasets with sensitive attribute annotations and (2)\nthe limited ability of current MLLMs to infer such attributes directly from\naudio. To address these challenges, we introduce AP^2, an audio benchmark\ndataset that consists of two subsets collected and composed from real-world\ndata, and both are annotated with sensitive attribute labels. Additionally, we\npropose Gifts, a hybrid multi-agent framework that leverages the complementary\nstrengths of audio-language models (ALMs) and large language models (LLMs) to\nenhance inference capabilities. Gifts employs an LLM to guide the ALM in\ninferring sensitive attributes, then forensically analyzes and consolidates the\nALM's inferences, overcoming severe hallucinations of existing ALMs in\ngenerating long-context responses. Our evaluations demonstrate that Gifts\nsignificantly outperforms baseline approaches in inferring sensitive\nattributes. Finally, we investigate model-level and data-level defense\nstrategies to mitigate the risks of audio private attribute profiling. Our work\nvalidates the feasibility of audio-based privacy attacks using MLLMs,\nhighlighting the need for robust defenses, and provides a dataset and framework\nto facilitate future research."
                },
                "authors": [
                    {
                        "name": "Lixu Wang"
                    },
                    {
                        "name": "Kaixiang Yao"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Dong Yang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Wei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Dong"
                },
                "author": "Wei Dong",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14472v1",
                "updated": "2025-08-20T06:52:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    52,
                    42,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:52:42Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    52,
                    42,
                    2,
                    232,
                    0
                ],
                "title": "In2x at WMT25 Translation Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In2x at WMT25 Translation Task"
                },
                "summary": "This paper presents the open-system submission by the In2x research team for\nthe WMT25 General Machine Translation Shared Task. Our submission focuses on\nJapanese-related translation tasks, aiming to explore a generalizable paradigm\nfor extending large language models (LLMs) to other languages. This paradigm\nencompasses aspects such as data construction methods and reward model design.\nThe ultimate goal is to enable large language model systems to achieve\nexceptional performance in low-resource or less commonly spoken languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the open-system submission by the In2x research team for\nthe WMT25 General Machine Translation Shared Task. Our submission focuses on\nJapanese-related translation tasks, aiming to explore a generalizable paradigm\nfor extending large language models (LLMs) to other languages. This paradigm\nencompasses aspects such as data construction methods and reward model design.\nThe ultimate goal is to enable large language model systems to achieve\nexceptional performance in low-resource or less commonly spoken languages."
                },
                "authors": [
                    {
                        "name": "Lei Pang"
                    },
                    {
                        "name": "Hanyi Mao"
                    },
                    {
                        "name": "Quanjia Xiao"
                    },
                    {
                        "name": "HaiXiao Liu"
                    },
                    {
                        "name": "Xiangyi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyi Li"
                },
                "author": "Xiangyi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14471v1",
                "updated": "2025-08-20T06:52:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    52,
                    33,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:52:33Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    52,
                    33,
                    2,
                    232,
                    0
                ],
                "title": "Adaptive Network Selection for Latency-Aware V2X Systems under Varying\n  Network and Vehicle Densities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Network Selection for Latency-Aware V2X Systems under Varying\n  Network and Vehicle Densities"
                },
                "summary": "This paper presents ANS-V2X, an Adaptive Network Selection framework tailored\nfor latency-aware V2X systems operating under varying vehicle densities and\nheterogeneous network conditions. Modern vehicular environments demand\nlow-latency and high-throughput communication, yet real-time network selection\nis hindered by diverse application requirements and the coexistence of multiple\nRadio Access Technologies (RATs) such as 4G, 5G, and ad hoc links. ANS-V2X\nemploys a heuristic-driven approach to assign vehicles to networks by\nconsidering application sensitivity, latency, computational load, and\ndirectionality constraints. The framework is benchmarked against a\nMixed-Integer Linear Programming (MILP) formulation for optimal solutions and a\nQ-learning-based method representing reinforcement learning. Simulation results\ndemonstrate that ANS-V2X achieves near-optimal performance, typically within 5\nto 10% of the utility achieved by MILP-V2X, while reducing execution time by\nmore than 85%. Although MILP-V2X offers globally optimal results, its\ncomputation time often exceeds 100 milliseconds, making it unsuitable for\nreal-time applications. The Q-learning-based method is more adaptable but\nrequires extensive training and converges slowly in dynamic scenarios. In\ncontrast, ANS-V2X completes decisions in under 15 milliseconds and consistently\ndelivers lower latency than both alternatives. This confirms its suitability\nfor real-time, edge-level deployment in latency-critical V2X systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ANS-V2X, an Adaptive Network Selection framework tailored\nfor latency-aware V2X systems operating under varying vehicle densities and\nheterogeneous network conditions. Modern vehicular environments demand\nlow-latency and high-throughput communication, yet real-time network selection\nis hindered by diverse application requirements and the coexistence of multiple\nRadio Access Technologies (RATs) such as 4G, 5G, and ad hoc links. ANS-V2X\nemploys a heuristic-driven approach to assign vehicles to networks by\nconsidering application sensitivity, latency, computational load, and\ndirectionality constraints. The framework is benchmarked against a\nMixed-Integer Linear Programming (MILP) formulation for optimal solutions and a\nQ-learning-based method representing reinforcement learning. Simulation results\ndemonstrate that ANS-V2X achieves near-optimal performance, typically within 5\nto 10% of the utility achieved by MILP-V2X, while reducing execution time by\nmore than 85%. Although MILP-V2X offers globally optimal results, its\ncomputation time often exceeds 100 milliseconds, making it unsuitable for\nreal-time applications. The Q-learning-based method is more adaptable but\nrequires extensive training and converges slowly in dynamic scenarios. In\ncontrast, ANS-V2X completes decisions in under 15 milliseconds and consistently\ndelivers lower latency than both alternatives. This confirms its suitability\nfor real-time, edge-level deployment in latency-critical V2X systems"
                },
                "authors": [
                    {
                        "name": "Muhammad Z. Haq"
                    },
                    {
                        "name": "Nadia N. Qadri"
                    },
                    {
                        "name": "Omer Chughtai"
                    },
                    {
                        "name": "Sadiq A. Ahmad"
                    },
                    {
                        "name": "Waqas Khalid"
                    },
                    {
                        "name": "Heejung Yu"
                    }
                ],
                "author_detail": {
                    "name": "Heejung Yu"
                },
                "author": "Heejung Yu",
                "arxiv_comment": "Accepted for IEEE Access",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17442v2",
                "updated": "2025-08-20T06:44:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    44,
                    38,
                    2,
                    232,
                    0
                ],
                "published": "2025-07-23T12:03:54Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    12,
                    3,
                    54,
                    2,
                    204,
                    0
                ],
                "title": "Each to Their Own: Exploring the Optimal Embedding in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Each to Their Own: Exploring the Optimal Embedding in RAG"
                },
                "summary": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication."
                },
                "authors": [
                    {
                        "name": "Shiting Chen"
                    },
                    {
                        "name": "Zijian Zhao"
                    },
                    {
                        "name": "Jinsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Chen"
                },
                "author": "Jinsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13654v2",
                "updated": "2025-08-20T06:41:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    41,
                    59,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-19T09:04:13Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    4,
                    13,
                    1,
                    231,
                    0
                ],
                "title": "Input Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Input Time Scaling"
                },
                "summary": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we combine\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\nfind a new phenomenon, training-testing co-design there. We need to apply query\nstrategies during both training and testing. Only applying strategies on\ntraining or testing would seriously degrade the performance. We are also\nsurprised to find that seemingly low data quality datasets can gain high\nperformance. Adding irrelevant information to the queries, randomly selecting\nexamples from a minimally filtered dataset, can even perform the best. These\nfindings contradict the widely held inductive bias, \"garbage in, garbage out\".\nCurating datasets with seemingly high-quality data can even potentially limit\nthe performance ceiling. In addition, models trained on more data with similar\nquality (15k VS 1k) perform worse, simple dataset size scaling should also be\ncarefully inspected. The good news is that our findings are compatible with the\nLess is More phenomenon. A small set of examples is enough to evoke high-level\nreasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,\nwe are able to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we combine\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\nfind a new phenomenon, training-testing co-design there. We need to apply query\nstrategies during both training and testing. Only applying strategies on\ntraining or testing would seriously degrade the performance. We are also\nsurprised to find that seemingly low data quality datasets can gain high\nperformance. Adding irrelevant information to the queries, randomly selecting\nexamples from a minimally filtered dataset, can even perform the best. These\nfindings contradict the widely held inductive bias, \"garbage in, garbage out\".\nCurating datasets with seemingly high-quality data can even potentially limit\nthe performance ceiling. In addition, models trained on more data with similar\nquality (15k VS 1k) perform worse, simple dataset size scaling should also be\ncarefully inspected. The good news is that our findings are compatible with the\nLess is More phenomenon. A small set of examples is enough to evoke high-level\nreasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,\nwe are able to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints."
                },
                "authors": [
                    {
                        "name": "Rapheal Huang"
                    },
                    {
                        "name": "Weilong Guo"
                    }
                ],
                "author_detail": {
                    "name": "Weilong Guo"
                },
                "arxiv_affiliation": "Yuming",
                "author": "Weilong Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05846v2",
                "updated": "2025-08-20T06:37:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    37,
                    23,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-08T09:25:21Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    25,
                    21,
                    1,
                    98,
                    0
                ],
                "title": "PathGPT: Reframing Path Recommendation as a Natural Language Generation\n  Task with Retrieval-Augmented Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PathGPT: Reframing Path Recommendation as a Natural Language Generation\n  Task with Retrieval-Augmented Language Models"
                },
                "summary": "Path recommendation (PR) aims to generate travel paths that are customized to\na user's specific preferences and constraints. Conventional approaches often\nemploy explicit optimization objectives or specialized machine learning\narchitectures; however, these methods typically exhibit limited flexibility and\ngeneralizability, necessitating costly retraining to accommodate new scenarios.\nThis paper introduces an alternative paradigm that conceptualizes PR as a\nnatural language generation task. We present PathGPT, a retrieval-augmented\nlarge language model (LLM) system that leverages historical trajectory data and\nnatural language user constraints to generate plausible paths. The proposed\nmethodology first converts raw trajectory data into a human-interpretable\ntextual format, which is then stored in a database. Subsequently, a hybrid\nretrieval system extracts path-specific context from this database to inform a\npretrained LLM. The primary contribution of this work is a novel framework that\ndemonstrates how integrating established information retrieval and generative\nmodel components can enable adaptive, zero-shot path generation across diverse\nscenarios. Extensive experiments on large-scale trajectory datasets indicate\nthat PathGPT's performance is competitive with specialized, learning-based\nmethods, underscoring its potential as a flexible and generalizable path\ngeneration system that avoids the need for retraining inherent in previous\ndata-driven models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path recommendation (PR) aims to generate travel paths that are customized to\na user's specific preferences and constraints. Conventional approaches often\nemploy explicit optimization objectives or specialized machine learning\narchitectures; however, these methods typically exhibit limited flexibility and\ngeneralizability, necessitating costly retraining to accommodate new scenarios.\nThis paper introduces an alternative paradigm that conceptualizes PR as a\nnatural language generation task. We present PathGPT, a retrieval-augmented\nlarge language model (LLM) system that leverages historical trajectory data and\nnatural language user constraints to generate plausible paths. The proposed\nmethodology first converts raw trajectory data into a human-interpretable\ntextual format, which is then stored in a database. Subsequently, a hybrid\nretrieval system extracts path-specific context from this database to inform a\npretrained LLM. The primary contribution of this work is a novel framework that\ndemonstrates how integrating established information retrieval and generative\nmodel components can enable adaptive, zero-shot path generation across diverse\nscenarios. Extensive experiments on large-scale trajectory datasets indicate\nthat PathGPT's performance is competitive with specialized, learning-based\nmethods, underscoring its potential as a flexible and generalizable path\ngeneration system that avoids the need for retraining inherent in previous\ndata-driven models."
                },
                "authors": [
                    {
                        "name": "Steeve Cuthbert Marcelyn"
                    },
                    {
                        "name": "Yucen Gao"
                    },
                    {
                        "name": "Yuzhe Zhang"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Gao"
                },
                "author": "Xiaofeng Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14460v1",
                "updated": "2025-08-20T06:31:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    31,
                    18,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:31:18Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    31,
                    18,
                    2,
                    232,
                    0
                ],
                "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference\n  Optimization"
                },
                "summary": "We present DuPO, a dual learning-based preference optimization framework that\ngenerates annotation-free feedback via a generalized duality. DuPO addresses\ntwo key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s\nreliance on costly labels and applicability restricted to verifiable tasks, and\ntraditional dual learning's restriction to strictly dual task pairs (e.g.,\ntranslation and back-translation). Specifically, DuPO decomposes a primal\ntask's input into known and unknown components, then constructs its dual task\nto reconstruct the unknown part using the primal output and known information\n(e.g., reversing math solutions to recover hidden variables), broadening\napplicability to non-invertible tasks. The quality of this reconstruction\nserves as a self-supervised reward to optimize the primal task, synergizing\nwith LLMs' ability to instantiate both tasks via a single model. Empirically,\nDuPO achieves substantial gains across diverse tasks: it enhances the average\ntranslation quality by 2.13 COMET over 756 directions, boosts the mathematical\nreasoning accuracy by an average of 6.4 points on three challenge benchmarks,\nand enhances performance by 9.3 points as an inference-time reranker (trading\ncomputation for accuracy). These results position DuPO as a scalable, general,\nand annotation-free paradigm for LLM optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DuPO, a dual learning-based preference optimization framework that\ngenerates annotation-free feedback via a generalized duality. DuPO addresses\ntwo key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s\nreliance on costly labels and applicability restricted to verifiable tasks, and\ntraditional dual learning's restriction to strictly dual task pairs (e.g.,\ntranslation and back-translation). Specifically, DuPO decomposes a primal\ntask's input into known and unknown components, then constructs its dual task\nto reconstruct the unknown part using the primal output and known information\n(e.g., reversing math solutions to recover hidden variables), broadening\napplicability to non-invertible tasks. The quality of this reconstruction\nserves as a self-supervised reward to optimize the primal task, synergizing\nwith LLMs' ability to instantiate both tasks via a single model. Empirically,\nDuPO achieves substantial gains across diverse tasks: it enhances the average\ntranslation quality by 2.13 COMET over 756 directions, boosts the mathematical\nreasoning accuracy by an average of 6.4 points on three challenge benchmarks,\nand enhances performance by 9.3 points as an inference-time reranker (trading\ncomputation for accuracy). These results position DuPO as a scalable, general,\nand annotation-free paradigm for LLM optimization."
                },
                "authors": [
                    {
                        "name": "Shuaijie She"
                    },
                    {
                        "name": "Yu Bao"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Wenhao Zhu"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Shanbo Cheng"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Wang"
                },
                "author": "Yuxuan Wang",
                "arxiv_comment": "18 pages, 4 figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14451v1",
                "updated": "2025-08-20T06:19:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    19,
                    27,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:19:27Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    19,
                    27,
                    2,
                    232,
                    0
                ],
                "title": "Design and Evaluation of a Scalable Data Pipeline for AI-Driven Air\n  Quality Monitoring in Low-Resource Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Evaluation of a Scalable Data Pipeline for AI-Driven Air\n  Quality Monitoring in Low-Resource Settings"
                },
                "summary": "The increasing adoption of low-cost environmental sensors and AI-enabled\napplications has accelerated the demand for scalable and resilient data\ninfrastructures, particularly in data-scarce and resource-constrained regions.\nThis paper presents the design, implementation, and evaluation of the AirQo\ndata pipeline: a modular, cloud-native Extract-Transform-Load (ETL) system\nengineered to support both real-time and batch processing of heterogeneous air\nquality data across urban deployments in Africa. It is Built using open-source\ntechnologies such as Apache Airflow, Apache Kafka, and Google BigQuery. The\npipeline integrates diverse data streams from low-cost sensors, third-party\nweather APIs, and reference-grade monitors to enable automated calibration,\nforecasting, and accessible analytics. We demonstrate the pipeline's ability to\ningest, transform, and distribute millions of air quality measurements monthly\nfrom over 400 monitoring devices while achieving low latency, high throughput,\nand robust data availability, even under constrained power and connectivity\nconditions. The paper details key architectural features, including workflow\norchestration, decoupled ingestion layers, machine learning-driven sensor\ncalibration, and observability frameworks. Performance is evaluated across\noperational metrics such as resource utilization, ingestion throughput,\ncalibration accuracy, and data availability, offering practical insights into\nbuilding sustainable environmental data platforms. By open-sourcing the\nplatform and documenting deployment experiences, this work contributes a\nreusable blueprint for similar initiatives seeking to advance environmental\nintelligence through data engineering in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of low-cost environmental sensors and AI-enabled\napplications has accelerated the demand for scalable and resilient data\ninfrastructures, particularly in data-scarce and resource-constrained regions.\nThis paper presents the design, implementation, and evaluation of the AirQo\ndata pipeline: a modular, cloud-native Extract-Transform-Load (ETL) system\nengineered to support both real-time and batch processing of heterogeneous air\nquality data across urban deployments in Africa. It is Built using open-source\ntechnologies such as Apache Airflow, Apache Kafka, and Google BigQuery. The\npipeline integrates diverse data streams from low-cost sensors, third-party\nweather APIs, and reference-grade monitors to enable automated calibration,\nforecasting, and accessible analytics. We demonstrate the pipeline's ability to\ningest, transform, and distribute millions of air quality measurements monthly\nfrom over 400 monitoring devices while achieving low latency, high throughput,\nand robust data availability, even under constrained power and connectivity\nconditions. The paper details key architectural features, including workflow\norchestration, decoupled ingestion layers, machine learning-driven sensor\ncalibration, and observability frameworks. Performance is evaluated across\noperational metrics such as resource utilization, ingestion throughput,\ncalibration accuracy, and data availability, offering practical insights into\nbuilding sustainable environmental data platforms. By open-sourcing the\nplatform and documenting deployment experiences, this work contributes a\nreusable blueprint for similar initiatives seeking to advance environmental\nintelligence through data engineering in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Richard Sserujongi"
                    },
                    {
                        "name": "Daniel Ogenrwot"
                    },
                    {
                        "name": "Nicholas Niwamanya"
                    },
                    {
                        "name": "Noah Nsimbe"
                    },
                    {
                        "name": "Martin Bbaale"
                    },
                    {
                        "name": "Benjamin Ssempala"
                    },
                    {
                        "name": "Noble Mutabazi"
                    },
                    {
                        "name": "Raja Fidel Wabinyai"
                    },
                    {
                        "name": "Deo Okure"
                    },
                    {
                        "name": "Engineer Bainomugisha"
                    }
                ],
                "author_detail": {
                    "name": "Engineer Bainomugisha"
                },
                "author": "Engineer Bainomugisha",
                "arxiv_comment": "15 pages, 11 figures, 34th International Conference on Software\n  Engineering and Data Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.3; E.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20654v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20654v3",
                "updated": "2025-08-20T06:18:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    18,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-02-28T02:17:31Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    2,
                    17,
                    31,
                    4,
                    59,
                    0
                ],
                "title": "Deployment and validation of predictive 6-dimensional beam diagnostics\n  through generative reconstruction with standard accelerator elements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment and validation of predictive 6-dimensional beam diagnostics\n  through generative reconstruction with standard accelerator elements"
                },
                "summary": "Understanding the 6-dimensional phase space distribution of particle beams is\nessential for optimizing accelerator performance. Conventional diagnostics such\nas use of transverse deflecting cavities offer detailed characterization but\nrequire dedicated hardware and space. Generative phase space reconstruction\n(GPSR) methods have shown promise in beam diagnostics, yet prior\nimplementations still rely on such components. Here we present the first\nexperimental implementation and validation of the GPSR methodology, realized by\nthe use of standard accelerator elements including accelerating cavities and\ndipole magnets, to achieve complete 6-dimensional phase space reconstruction.\nThrough simulations and experiments at the Pohang Accelerator Laboratory X-ray\nFree Electron Laser facility, we successfully reconstruct complex, nonlinear\nbeam structures. Furthermore, we validate the methodology by predicting\nindependent downstream measurements excluded from training, revealing\nnear-unique reconstruction closely resembling ground truth. This advancement\nestablishes a pathway for predictive diagnostics across beamline segments while\nreducing hardware requirements and expanding applicability to various\naccelerator facilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the 6-dimensional phase space distribution of particle beams is\nessential for optimizing accelerator performance. Conventional diagnostics such\nas use of transverse deflecting cavities offer detailed characterization but\nrequire dedicated hardware and space. Generative phase space reconstruction\n(GPSR) methods have shown promise in beam diagnostics, yet prior\nimplementations still rely on such components. Here we present the first\nexperimental implementation and validation of the GPSR methodology, realized by\nthe use of standard accelerator elements including accelerating cavities and\ndipole magnets, to achieve complete 6-dimensional phase space reconstruction.\nThrough simulations and experiments at the Pohang Accelerator Laboratory X-ray\nFree Electron Laser facility, we successfully reconstruct complex, nonlinear\nbeam structures. Furthermore, we validate the methodology by predicting\nindependent downstream measurements excluded from training, revealing\nnear-unique reconstruction closely resembling ground truth. This advancement\nestablishes a pathway for predictive diagnostics across beamline segments while\nreducing hardware requirements and expanding applicability to various\naccelerator facilities."
                },
                "authors": [
                    {
                        "name": "Seongyeol Kim"
                    },
                    {
                        "name": "Juan Pablo Gonzalez-Aguilera"
                    },
                    {
                        "name": "Ryan Roussel"
                    },
                    {
                        "name": "Gyujin Kim"
                    },
                    {
                        "name": "Auralee Edelen"
                    },
                    {
                        "name": "Myung-Hoon Cho"
                    },
                    {
                        "name": "Young-Kee Kim"
                    },
                    {
                        "name": "Chi Hyun Shim"
                    },
                    {
                        "name": "Hoon Heo"
                    },
                    {
                        "name": "Haeryong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Haeryong Yang"
                },
                "author": "Haeryong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20654v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v2",
                "updated": "2025-08-20T06:16:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    16,
                    28,
                    2,
                    232,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "UpANNS: Enhancing Billion-Scale ANNS Efficiency with Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpANNS: Enhancing Billion-Scale ANNS Efficiency with Real-World PIM\n  Architecture"
                },
                "summary": "Approximate Nearest Neighbor Search (ANNS) is a critical component of modern\nAI systems, such as recommendation engines and retrieval-augmented large\nlanguage models (RAG-LLMs). However, scaling ANNS to billion-entry datasets\nexposes critical inefficiencies: CPU-based solutions are bottlenecked by memory\nbandwidth limitations, while GPU implementations underutilize hardware\nresources, leading to suboptimal performance and energy consumption. To address\nthese challenges, we introduce \\emph{UpANNS}, a novel framework leveraging\nProcessing-in-Memory (PIM) architecture to accelerate billion-scale ANNS.\nUpANNS integrates four key innovations, including 1) architecture-aware data\nplacement to minimize latency through workload balancing, 2) dynamic resource\nmanagement for optimal PIM utilization, 3) co-occurrence optimized encoding to\nreduce redundant computations, and 4) an early-pruning strategy for efficient\ntop-k selection. Evaluation on commercial UPMEM hardware demonstrates that\nUpANNS achieves 4.3x higher QPS than CPU-based Faiss, while matching GPU\nperformance with 2.3x greater energy efficiency. Its near-linear scalability\nensures practicality for growing datasets, making it ideal for applications\nlike real-time LLM serving and large-scale retrieval systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest Neighbor Search (ANNS) is a critical component of modern\nAI systems, such as recommendation engines and retrieval-augmented large\nlanguage models (RAG-LLMs). However, scaling ANNS to billion-entry datasets\nexposes critical inefficiencies: CPU-based solutions are bottlenecked by memory\nbandwidth limitations, while GPU implementations underutilize hardware\nresources, leading to suboptimal performance and energy consumption. To address\nthese challenges, we introduce \\emph{UpANNS}, a novel framework leveraging\nProcessing-in-Memory (PIM) architecture to accelerate billion-scale ANNS.\nUpANNS integrates four key innovations, including 1) architecture-aware data\nplacement to minimize latency through workload balancing, 2) dynamic resource\nmanagement for optimal PIM utilization, 3) co-occurrence optimized encoding to\nreduce redundant computations, and 4) an early-pruning strategy for efficient\ntop-k selection. Evaluation on commercial UPMEM hardware demonstrates that\nUpANNS achieves 4.3x higher QPS than CPU-based Faiss, while matching GPU\nperformance with 2.3x greater energy efficiency. Its near-linear scalability\nensures practicality for growing datasets, making it ideal for applications\nlike real-time LLM serving and large-scale retrieval systems."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "arxiv_comment": "Accepted by SC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14448v1",
                "updated": "2025-08-20T06:10:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    10,
                    3,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:10:03Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    10,
                    3,
                    2,
                    232,
                    0
                ],
                "title": "Generalizable Engagement Estimation in Conversation via Domain Prompting\n  and Parallel Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizable Engagement Estimation in Conversation via Domain Prompting\n  and Parallel Attention"
                },
                "summary": "Accurate engagement estimation is essential for adaptive human-computer\ninteraction systems, yet robust deployment is hindered by poor generalizability\nacross diverse domains and challenges in modeling complex interaction\ndynamics.To tackle these issues, we propose DAPA (Domain-Adaptive Parallel\nAttention), a novel framework for generalizable conversational engagement\nmodeling. DAPA introduces a Domain Prompting mechanism by prepending learnable\ndomain-specific vectors to the input, explicitly conditioning the model on the\ndata's origin to facilitate domain-aware adaptation while preserving\ngeneralizable engagement representations. To capture interactional synchrony,\nthe framework also incorporates a Parallel Cross-Attention module that\nexplicitly aligns reactive (forward BiLSTM) and anticipatory (backward BiLSTM)\nstates between participants.Extensive experiments demonstrate that DAPA\nestablishes a new state-of-the-art performance on several cross-cultural and\ncross-linguistic benchmarks, notably achieving an absolute improvement of 0.45\nin Concordance Correlation Coefficient (CCC) over a strong baseline on the\nNoXi-J test set. The superiority of our method was also confirmed by winning\nthe first place in the Multi-Domain Engagement Estimation Challenge at\nMultiMediate'25.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate engagement estimation is essential for adaptive human-computer\ninteraction systems, yet robust deployment is hindered by poor generalizability\nacross diverse domains and challenges in modeling complex interaction\ndynamics.To tackle these issues, we propose DAPA (Domain-Adaptive Parallel\nAttention), a novel framework for generalizable conversational engagement\nmodeling. DAPA introduces a Domain Prompting mechanism by prepending learnable\ndomain-specific vectors to the input, explicitly conditioning the model on the\ndata's origin to facilitate domain-aware adaptation while preserving\ngeneralizable engagement representations. To capture interactional synchrony,\nthe framework also incorporates a Parallel Cross-Attention module that\nexplicitly aligns reactive (forward BiLSTM) and anticipatory (backward BiLSTM)\nstates between participants.Extensive experiments demonstrate that DAPA\nestablishes a new state-of-the-art performance on several cross-cultural and\ncross-linguistic benchmarks, notably achieving an absolute improvement of 0.45\nin Concordance Correlation Coefficient (CCC) over a strong baseline on the\nNoXi-J test set. The superiority of our method was also confirmed by winning\nthe first place in the Multi-Domain Engagement Estimation Challenge at\nMultiMediate'25."
                },
                "authors": [
                    {
                        "name": "Yangche Yu"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Peng Jia"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Li Dai"
                    },
                    {
                        "name": "Zhenzhen Hu"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Richang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Richang Hong"
                },
                "author": "Richang Hong",
                "arxiv_comment": "1st Place in the Engagement Estimation Task held by MultiMediate 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14445v1",
                "updated": "2025-08-20T06:02:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    2,
                    4,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:02:04Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    2,
                    4,
                    2,
                    232,
                    0
                ],
                "title": "Transforming Next-generation Network Planning assisted by Data\n  Acquisition of Top Three Spanish MNOs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Next-generation Network Planning assisted by Data\n  Acquisition of Top Three Spanish MNOs"
                },
                "summary": "In this paper, we address the necessity of data related to mobile traffic of\nthe legacy infrastructure to extract useful information and perform network\ndimensioning for 5G. These data can help us achieve a more efficient network\nplanning design, especially in terms of topology and cost. To that end, a real\nopen database of top three Spanish mobile network operators (MNOs) is used to\nestimate the traffic and to identify the area of highest user density for the\ndeployment of new services. We propose the data acquisition procedure described\nto clean the database, to extract meaningful traffic information and to\nvisualize traffic density patterns for new gNB deployments. We present the\nstate of the art in Network Data. We describe the considered network database\nin detail. The Network Data Acquisition entity along with the proposed\nprocedure is explained. The corresponding results are discussed, following the\nconclusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the necessity of data related to mobile traffic of\nthe legacy infrastructure to extract useful information and perform network\ndimensioning for 5G. These data can help us achieve a more efficient network\nplanning design, especially in terms of topology and cost. To that end, a real\nopen database of top three Spanish mobile network operators (MNOs) is used to\nestimate the traffic and to identify the area of highest user density for the\ndeployment of new services. We propose the data acquisition procedure described\nto clean the database, to extract meaningful traffic information and to\nvisualize traffic density patterns for new gNB deployments. We present the\nstate of the art in Network Data. We describe the considered network database\nin detail. The Network Data Acquisition entity along with the proposed\nprocedure is explained. The corresponding results are discussed, following the\nconclusions."
                },
                "authors": [
                    {
                        "name": "M. Umar Khan"
                    }
                ],
                "author_detail": {
                    "name": "M. Umar Khan"
                },
                "author": "M. Umar Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19391v2",
                "updated": "2025-08-20T04:48:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    48,
                    9,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-27T23:48:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    23,
                    48,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "Bi-directional Model Cascading with Proxy Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-directional Model Cascading with Proxy Confidence"
                },
                "summary": "Model Cascading, recently applied successfully to LLMs, is a simple but\npowerful technique that improves the efficiency of inference by selectively\napplying models of varying sizes. Models are used in sequence from smallest to\nlargest, only deferring samples to large, costly models when smaller models are\nnot sufficiently confident. Existing approaches to deferral use only limited\nsmall model confidence estimates because of the inaccessibility of the large\nmodel, although large model confidence is known to be important. We therefore\npropose a bi-directional approach to deferral that considers the confidence of\nsmall and large models in the cascade simultaneously through the use of a proxy\nfor the large model. This requires a richer representation of model confidence\nto enable comparative calibration: we use an analysis of hidden states to\nimprove post-invocation confidence of the small model, which in itself improves\ncascading results over prior approaches. We then combine this with a tiny proxy\nmodel to estimate pre-invocation confidence of the large model. We examine the\nproposed cascading system over challenging, multiple-choice datasets, finding\nimprovements over standard cascading baselines reflected in reductions in\ndeferrals to more costly models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Cascading, recently applied successfully to LLMs, is a simple but\npowerful technique that improves the efficiency of inference by selectively\napplying models of varying sizes. Models are used in sequence from smallest to\nlargest, only deferring samples to large, costly models when smaller models are\nnot sufficiently confident. Existing approaches to deferral use only limited\nsmall model confidence estimates because of the inaccessibility of the large\nmodel, although large model confidence is known to be important. We therefore\npropose a bi-directional approach to deferral that considers the confidence of\nsmall and large models in the cascade simultaneously through the use of a proxy\nfor the large model. This requires a richer representation of model confidence\nto enable comparative calibration: we use an analysis of hidden states to\nimprove post-invocation confidence of the small model, which in itself improves\ncascading results over prior approaches. We then combine this with a tiny proxy\nmodel to estimate pre-invocation confidence of the large model. We examine the\nproposed cascading system over challenging, multiple-choice datasets, finding\nimprovements over standard cascading baselines reflected in reductions in\ndeferrals to more costly models."
                },
                "authors": [
                    {
                        "name": "David Warren"
                    },
                    {
                        "name": "Mark Dras"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dras"
                },
                "author": "Mark Dras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10287v2",
                "updated": "2025-08-20T04:35:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    35,
                    1,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-14T02:31:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    2,
                    31,
                    22,
                    3,
                    226,
                    0
                ],
                "title": "JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in\n  Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in\n  Robotics"
                },
                "summary": "Recent advances in Vision-Language Models (VLMs) and large language models\n(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI\nagents like robots. However, existing visual reasoning benchmarks often suffer\nfrom several limitations: they lack a clear definition of reasoning complexity,\noffer have no control to generate questions over varying difficulty and task\ncustomization, and fail to provide structured, step-by-step reasoning\nannotations (workflows). To bridge these gaps, we formalize reasoning\ncomplexity, introduce an adaptive query engine that generates customizable\nquestions of varying complexity with detailed intermediate annotations, and\nextend the JRDB dataset with human-object interaction and geometric\nrelationship annotations to create JRDB-Reasoning, a benchmark tailored for\nvisual reasoning in human-crowded environments. Our engine and benchmark enable\nfine-grained evaluation of visual reasoning frameworks and dynamic assessment\nof visual-language models across reasoning levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Vision-Language Models (VLMs) and large language models\n(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI\nagents like robots. However, existing visual reasoning benchmarks often suffer\nfrom several limitations: they lack a clear definition of reasoning complexity,\noffer have no control to generate questions over varying difficulty and task\ncustomization, and fail to provide structured, step-by-step reasoning\nannotations (workflows). To bridge these gaps, we formalize reasoning\ncomplexity, introduce an adaptive query engine that generates customizable\nquestions of varying complexity with detailed intermediate annotations, and\nextend the JRDB dataset with human-object interaction and geometric\nrelationship annotations to create JRDB-Reasoning, a benchmark tailored for\nvisual reasoning in human-crowded environments. Our engine and benchmark enable\nfine-grained evaluation of visual reasoning frameworks and dynamic assessment\nof visual-language models across reasoning levels."
                },
                "authors": [
                    {
                        "name": "Simindokht Jahangard"
                    },
                    {
                        "name": "Mehrzad Mohammadi"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Zhixi Cai"
                    },
                    {
                        "name": "Hamid Rezatofighi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Rezatofighi"
                },
                "author": "Hamid Rezatofighi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14419v1",
                "updated": "2025-08-20T04:31:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    31,
                    31,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T04:31:31Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    31,
                    31,
                    2,
                    232,
                    0
                ],
                "title": "Static Analysis as a Feedback Loop: Enhancing LLM-Generated Code Beyond\n  Correctness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Analysis as a Feedback Loop: Enhancing LLM-Generated Code Beyond\n  Correctness"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ncode generation, achieving high scores on benchmarks such as HumanEval and\nMBPP. However, these benchmarks primarily assess functional correctness and\nneglect broader dimensions of code quality, including security, reliability,\nreadability, and maintainability. In this work, we systematically evaluate the\nability of LLMs to generate high-quality code across multiple dimensions using\nthe PythonSecurityEval benchmark. We introduce an iterative static\nanalysis-driven prompting algorithm that leverages Bandit and Pylint to\nidentify and resolve code quality issues. Our experiments with GPT-4o show\nsubstantial improvements: security issues reduced from >40% to 13%, readability\nviolations from >80% to 11%, and reliability warnings from >50% to 11% within\nten iterations. These results demonstrate that LLMs, when guided by static\nanalysis feedback, can significantly enhance code quality beyond functional\ncorrectness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\ncode generation, achieving high scores on benchmarks such as HumanEval and\nMBPP. However, these benchmarks primarily assess functional correctness and\nneglect broader dimensions of code quality, including security, reliability,\nreadability, and maintainability. In this work, we systematically evaluate the\nability of LLMs to generate high-quality code across multiple dimensions using\nthe PythonSecurityEval benchmark. We introduce an iterative static\nanalysis-driven prompting algorithm that leverages Bandit and Pylint to\nidentify and resolve code quality issues. Our experiments with GPT-4o show\nsubstantial improvements: security issues reduced from >40% to 13%, readability\nviolations from >80% to 11%, and reliability warnings from >50% to 11% within\nten iterations. These results demonstrate that LLMs, when guided by static\nanalysis feedback, can significantly enhance code quality beyond functional\ncorrectness."
                },
                "authors": [
                    {
                        "name": "Scott Blyth"
                    },
                    {
                        "name": "Sherlock A. Licorish"
                    },
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "Markus Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Markus Wagner"
                },
                "author": "Markus Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14410v1",
                "updated": "2025-08-20T04:14:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    14,
                    54,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T04:14:54Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    14,
                    54,
                    2,
                    232,
                    0
                ],
                "title": "Automated Optimization Modeling through Expert-Guided Large Language\n  Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Optimization Modeling through Expert-Guided Large Language\n  Model Reasoning"
                },
                "summary": "Optimization Modeling (OM) is essential for solving complex decision-making\nproblems. However, the process remains time-consuming and error-prone, heavily\nrelying on domain experts. While Large Language Models (LLMs) show promise in\naddressing these challenges through their natural language understanding and\nreasoning capabilities, current approaches face three critical limitations:\nhigh benchmark labeling error rates reaching up to 42\\%, narrow evaluation\nscope that only considers optimal values, and computational inefficiency due to\nheavy reliance on multi-agent systems or model fine-tuning. In this work, we\nfirst enhance existing datasets through systematic error correction and more\ncomprehensive annotation. Additionally, we introduce LogiOR, a new optimization\nmodeling benchmark from the logistics domain, containing more complex problems\nwith standardized annotations. Furthermore, we present ORThought, a novel\nframework that leverages expert-level optimization modeling principles through\nchain-of-thought reasoning to automate the OM process. Through extensive\nempirical evaluation, we demonstrate that ORThought outperforms existing\napproaches, including multi-agent frameworks, with particularly significant\nadvantages on complex optimization problems. Finally, we provide a systematic\nanalysis of our method, identifying critical success factors and failure modes,\nproviding valuable insights for future research on LLM-based optimization\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization Modeling (OM) is essential for solving complex decision-making\nproblems. However, the process remains time-consuming and error-prone, heavily\nrelying on domain experts. While Large Language Models (LLMs) show promise in\naddressing these challenges through their natural language understanding and\nreasoning capabilities, current approaches face three critical limitations:\nhigh benchmark labeling error rates reaching up to 42\\%, narrow evaluation\nscope that only considers optimal values, and computational inefficiency due to\nheavy reliance on multi-agent systems or model fine-tuning. In this work, we\nfirst enhance existing datasets through systematic error correction and more\ncomprehensive annotation. Additionally, we introduce LogiOR, a new optimization\nmodeling benchmark from the logistics domain, containing more complex problems\nwith standardized annotations. Furthermore, we present ORThought, a novel\nframework that leverages expert-level optimization modeling principles through\nchain-of-thought reasoning to automate the OM process. Through extensive\nempirical evaluation, we demonstrate that ORThought outperforms existing\napproaches, including multi-agent frameworks, with particularly significant\nadvantages on complex optimization problems. Finally, we provide a systematic\nanalysis of our method, identifying critical success factors and failure modes,\nproviding valuable insights for future research on LLM-based optimization\nmodeling."
                },
                "authors": [
                    {
                        "name": "Beinuo Yang"
                    },
                    {
                        "name": "Qishen Zhou"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Xingchen Su"
                    },
                    {
                        "name": "Simon Hu"
                    }
                ],
                "author_detail": {
                    "name": "Simon Hu"
                },
                "author": "Simon Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14408v1",
                "updated": "2025-08-20T04:08:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    8,
                    18,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T04:08:18Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    8,
                    18,
                    2,
                    232,
                    0
                ],
                "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in\n  LLMs"
                },
                "summary": "Large language models (LLMs) have been shown to possess a degree of\nself-recognition capability-the ability to identify whether a given text was\ngenerated by themselves. Prior work has demonstrated that this capability is\nreliably expressed under the Pair Presentation Paradigm (PPP), where the model\nis presented with two texts and asked to choose which one it authored. However,\nperformance deteriorates sharply under the Individual Presentation Paradigm\n(IPP), where the model is given a single text to judge authorship. Although\nthis phenomenon has been observed, its underlying causes have not been\nsystematically analyzed. In this paper, we first replicate existing findings to\nconfirm that LLMs struggle to distinguish self- from other-generated text under\nIPP. We then investigate the reasons for this failure and attribute it to a\nphenomenon we term Implicit Territorial Awareness (ITA)-the model's latent\nability to distinguish self- and other-texts in representational space, which\nremains unexpressed in its output behavior. To awaken the ITA of LLMs, we\npropose Cognitive Surgery (CoSur), a novel framework comprising four main\nmodules: representation extraction, territory construction, authorship\ndiscrimination and cognitive editing. Experimental results demonstrate that our\nproposed method improves the performance of three different LLMs in the IPP\nscenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to possess a degree of\nself-recognition capability-the ability to identify whether a given text was\ngenerated by themselves. Prior work has demonstrated that this capability is\nreliably expressed under the Pair Presentation Paradigm (PPP), where the model\nis presented with two texts and asked to choose which one it authored. However,\nperformance deteriorates sharply under the Individual Presentation Paradigm\n(IPP), where the model is given a single text to judge authorship. Although\nthis phenomenon has been observed, its underlying causes have not been\nsystematically analyzed. In this paper, we first replicate existing findings to\nconfirm that LLMs struggle to distinguish self- from other-generated text under\nIPP. We then investigate the reasons for this failure and attribute it to a\nphenomenon we term Implicit Territorial Awareness (ITA)-the model's latent\nability to distinguish self- and other-texts in representational space, which\nremains unexpressed in its output behavior. To awaken the ITA of LLMs, we\npropose Cognitive Surgery (CoSur), a novel framework comprising four main\nmodules: representation extraction, territory construction, authorship\ndiscrimination and cognitive editing. Experimental results demonstrate that our\nproposed method improves the performance of three different LLMs in the IPP\nscenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yinghan Zhou"
                    },
                    {
                        "name": "Weifeng Zhu"
                    },
                    {
                        "name": "Juan Wen"
                    },
                    {
                        "name": "Wanli Peng"
                    },
                    {
                        "name": "Zhengxian Wu"
                    },
                    {
                        "name": "Yiming Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Xue"
                },
                "author": "Yiming Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12645v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12645v3",
                "updated": "2025-08-20T04:07:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    7,
                    7,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-18T06:17:59Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    6,
                    17,
                    59,
                    0,
                    230,
                    0
                ],
                "title": "Diagnostic-Guided Dynamic Profile Optimization for LLM-based User\n  Simulators in Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic-Guided Dynamic Profile Optimization for LLM-based User\n  Simulators in Sequential Recommendation"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled realistic user\nsimulators for developing and evaluating recommender systems (RSs). However,\nexisting LLM-based simulators for RSs face two major limitations: (1) static\nand single-step prompt-based inference that leads to inaccurate and incomplete\nuser profile construction; (2) unrealistic and single-round\nrecommendation-feedback interaction pattern that fails to capture real-world\nscenarios. To address these limitations, we propose DGDPO (Diagnostic-Guided\nDynamic Profile Optimization), a novel framework that constructs user profile\nthrough a dynamic and iterative optimization process to enhance the simulation\nfidelity. Specifically, DGDPO incorporates two core modules within each\noptimization loop: firstly, a specialized LLM-based diagnostic module,\ncalibrated through our novel training strategy, accurately identifies specific\ndefects in the user profile. Subsequently, a generalized LLM-based treatment\nmodule analyzes the diagnosed defect and generates targeted suggestions to\nrefine the profile. Furthermore, unlike existing LLM-based user simulators that\nare limited to single-round interactions, we are the first to integrate DGDPO\nwith sequential recommenders, enabling a bidirectional evolution where user\nprofiles and recommendation strategies adapt to each other over multi-round\ninteractions. Extensive experiments conducted on three real-world datasets\ndemonstrate the effectiveness of our proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled realistic user\nsimulators for developing and evaluating recommender systems (RSs). However,\nexisting LLM-based simulators for RSs face two major limitations: (1) static\nand single-step prompt-based inference that leads to inaccurate and incomplete\nuser profile construction; (2) unrealistic and single-round\nrecommendation-feedback interaction pattern that fails to capture real-world\nscenarios. To address these limitations, we propose DGDPO (Diagnostic-Guided\nDynamic Profile Optimization), a novel framework that constructs user profile\nthrough a dynamic and iterative optimization process to enhance the simulation\nfidelity. Specifically, DGDPO incorporates two core modules within each\noptimization loop: firstly, a specialized LLM-based diagnostic module,\ncalibrated through our novel training strategy, accurately identifies specific\ndefects in the user profile. Subsequently, a generalized LLM-based treatment\nmodule analyzes the diagnosed defect and generates targeted suggestions to\nrefine the profile. Furthermore, unlike existing LLM-based user simulators that\nare limited to single-round interactions, we are the first to integrate DGDPO\nwith sequential recommenders, enabling a bidirectional evolution where user\nprofiles and recommendation strategies adapt to each other over multi-round\ninteractions. Extensive experiments conducted on three real-world datasets\ndemonstrate the effectiveness of our proposed framework."
                },
                "authors": [
                    {
                        "name": "Hongyang Liu"
                    },
                    {
                        "name": "Zhu Sun"
                    },
                    {
                        "name": "Tianjun Wei"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Jiajie Zhu"
                    },
                    {
                        "name": "Xinghua Qu"
                    }
                ],
                "author_detail": {
                    "name": "Xinghua Qu"
                },
                "author": "Xinghua Qu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12645v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12645v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14391v1",
                "updated": "2025-08-20T03:35:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    35,
                    24,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T03:35:24Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    35,
                    24,
                    2,
                    232,
                    0
                ],
                "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware\n  Sentence Simplification and Two-tiered Hierarchical Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware\n  Sentence Simplification and Two-tiered Hierarchical Refinement"
                },
                "summary": "Relation extraction enables the construction of structured knowledge for many\ndownstream applications. While large language models (LLMs) have shown great\npromise in this domain, most existing methods concentrate on relation\nclassification, which predicts the semantic relation type between a related\nentity pair. However, we observe that LLMs often struggle to reliably determine\nwhether a relation exists, especially in cases involving complex sentence\nstructures or intricate semantics, which leads to spurious predictions. Such\nhallucinations can introduce noisy edges in knowledge graphs, compromising the\nintegrity of structured knowledge and downstream reliability. To address these\nchallenges, we propose DEPTH, a framework that integrates Dependency-aware\nsEntence simPlification and Two-tiered Hierarchical refinement into the\nrelation extraction pipeline. Given a sentence and its candidate entity pairs,\nDEPTH operates in two stages: (1) the Grounding module extracts relations for\neach pair by leveraging their shortest dependency path, distilling the sentence\ninto a minimal yet coherent relational context that reduces syntactic noise\nwhile preserving key semantics; (2) the Refinement module aggregates all local\npredictions and revises them based on a holistic understanding of the sentence,\ncorrecting omissions and inconsistencies. We further introduce a\ncausality-driven reward model that mitigates reward hacking by disentangling\nspurious correlations, enabling robust fine-tuning via reinforcement learning\nwith human feedback. Experiments on six benchmarks demonstrate that DEPTH\nreduces the average hallucination rate to 7.0\\% while achieving a 17.2\\%\nimprovement in average F1 score over state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relation extraction enables the construction of structured knowledge for many\ndownstream applications. While large language models (LLMs) have shown great\npromise in this domain, most existing methods concentrate on relation\nclassification, which predicts the semantic relation type between a related\nentity pair. However, we observe that LLMs often struggle to reliably determine\nwhether a relation exists, especially in cases involving complex sentence\nstructures or intricate semantics, which leads to spurious predictions. Such\nhallucinations can introduce noisy edges in knowledge graphs, compromising the\nintegrity of structured knowledge and downstream reliability. To address these\nchallenges, we propose DEPTH, a framework that integrates Dependency-aware\nsEntence simPlification and Two-tiered Hierarchical refinement into the\nrelation extraction pipeline. Given a sentence and its candidate entity pairs,\nDEPTH operates in two stages: (1) the Grounding module extracts relations for\neach pair by leveraging their shortest dependency path, distilling the sentence\ninto a minimal yet coherent relational context that reduces syntactic noise\nwhile preserving key semantics; (2) the Refinement module aggregates all local\npredictions and revises them based on a holistic understanding of the sentence,\ncorrecting omissions and inconsistencies. We further introduce a\ncausality-driven reward model that mitigates reward hacking by disentangling\nspurious correlations, enabling robust fine-tuning via reinforcement learning\nwith human feedback. Experiments on six benchmarks demonstrate that DEPTH\nreduces the average hallucination rate to 7.0\\% while achieving a 17.2\\%\nimprovement in average F1 score over state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Yupei Yang"
                    },
                    {
                        "name": "Fan Feng"
                    },
                    {
                        "name": "Lin Yang"
                    },
                    {
                        "name": "Wanxi Deng"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Biwei Huang"
                    },
                    {
                        "name": "Shikui Tu"
                    },
                    {
                        "name": "Lei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xu"
                },
                "author": "Lei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14390v1",
                "updated": "2025-08-20T03:33:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    33,
                    38,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T03:33:38Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    33,
                    38,
                    2,
                    232,
                    0
                ],
                "title": "Credence Calibration Game? Calibrating Large Language Models through\n  Structured Play",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Credence Calibration Game? Calibrating Large Language Models through\n  Structured Play"
                },
                "summary": "As Large Language Models (LLMs) are increasingly deployed in\ndecision-critical domains, it becomes essential to ensure that their confidence\nestimates faithfully correspond to their actual correctness. Existing\ncalibration methods have primarily focused on post-hoc adjustments or auxiliary\nmodel training; however, many of these approaches necessitate additional\nsupervision or parameter updates. In this work, we propose a novel prompt-based\ncalibration framework inspired by the Credence Calibration Game. Our method\nestablishes a structured interaction loop wherein LLMs receive feedback based\non the alignment of their predicted confidence with correctness. Through\nfeedback-driven prompting and natural language summaries of prior performance,\nour framework dynamically improves model calibration. Extensive experiments\nacross models and game configurations demonstrate consistent improvements in\nevaluation metrics. Our results highlight the potential of game-based prompting\nas an effective strategy for LLM calibration. Code and data are available at\nhttps://anonymous.4open.science/r/LLM-Calibration/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly deployed in\ndecision-critical domains, it becomes essential to ensure that their confidence\nestimates faithfully correspond to their actual correctness. Existing\ncalibration methods have primarily focused on post-hoc adjustments or auxiliary\nmodel training; however, many of these approaches necessitate additional\nsupervision or parameter updates. In this work, we propose a novel prompt-based\ncalibration framework inspired by the Credence Calibration Game. Our method\nestablishes a structured interaction loop wherein LLMs receive feedback based\non the alignment of their predicted confidence with correctness. Through\nfeedback-driven prompting and natural language summaries of prior performance,\nour framework dynamically improves model calibration. Extensive experiments\nacross models and game configurations demonstrate consistent improvements in\nevaluation metrics. Our results highlight the potential of game-based prompting\nas an effective strategy for LLM calibration. Code and data are available at\nhttps://anonymous.4open.science/r/LLM-Calibration/."
                },
                "authors": [
                    {
                        "name": "Ke Fang"
                    },
                    {
                        "name": "Tianyi Zhao"
                    },
                    {
                        "name": "Lu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Cheng"
                },
                "author": "Lu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14387v1",
                "updated": "2025-08-20T03:27:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    27,
                    23,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T03:27:23Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    27,
                    23,
                    2,
                    232,
                    0
                ],
                "title": "DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems\n  in Unknown Environments via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems\n  in Unknown Environments via Large Language Models"
                },
                "summary": "Online coordination of multi-robot systems in open and unknown environments\nfaces significant challenges, particularly when semantic features detected\nduring operation dynamically trigger new tasks. Recent large language model\n(LLMs)-based approaches for scene reasoning and planning primarily focus on\none-shot, end-to-end solutions in known environments, lacking both dynamic\nadaptation capabilities for online operation and explainability in the\nprocesses of planning. To address these issues, a novel framework (DEXTER-LLM)\nfor dynamic task planning in unknown environments, integrates four modules: (i)\na mission comprehension module that resolves partial ordering of tasks\nspecified by natural languages or linear temporal logic formulas (LTL); (ii) an\nonline subtask generator based on LLMs that improves the accuracy and\nexplainability of task decomposition via multi-stage reasoning; (iii) an\noptimal subtask assigner and scheduler that allocates subtasks to robots via\nsearch-based optimization; and (iv) a dynamic adaptation and human-in-the-loop\nverification module that implements multi-rate, event-based updates for both\nsubtasks and their assignments, to cope with new features and tasks detected\nonline. The framework effectively combines LLMs' open-world reasoning\ncapabilities with the optimality of model-based assignment methods,\nsimultaneously addressing the critical issue of online adaptability and\nexplainability. Experimental evaluations demonstrate exceptional performances,\nwith 100% success rates across all scenarios, 160 tasks and 480 subtasks\ncompleted on average (3 times the baselines), 62% less queries to LLMs during\nadaptation, and superior plan quality (2 times higher) for compound tasks.\nProject page at https://tcxm.github.io/DEXTER-LLM/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online coordination of multi-robot systems in open and unknown environments\nfaces significant challenges, particularly when semantic features detected\nduring operation dynamically trigger new tasks. Recent large language model\n(LLMs)-based approaches for scene reasoning and planning primarily focus on\none-shot, end-to-end solutions in known environments, lacking both dynamic\nadaptation capabilities for online operation and explainability in the\nprocesses of planning. To address these issues, a novel framework (DEXTER-LLM)\nfor dynamic task planning in unknown environments, integrates four modules: (i)\na mission comprehension module that resolves partial ordering of tasks\nspecified by natural languages or linear temporal logic formulas (LTL); (ii) an\nonline subtask generator based on LLMs that improves the accuracy and\nexplainability of task decomposition via multi-stage reasoning; (iii) an\noptimal subtask assigner and scheduler that allocates subtasks to robots via\nsearch-based optimization; and (iv) a dynamic adaptation and human-in-the-loop\nverification module that implements multi-rate, event-based updates for both\nsubtasks and their assignments, to cope with new features and tasks detected\nonline. The framework effectively combines LLMs' open-world reasoning\ncapabilities with the optimality of model-based assignment methods,\nsimultaneously addressing the critical issue of online adaptability and\nexplainability. Experimental evaluations demonstrate exceptional performances,\nwith 100% success rates across all scenarios, 160 tasks and 480 subtasks\ncompleted on average (3 times the baselines), 62% less queries to LLMs during\nadaptation, and superior plan quality (2 times higher) for compound tasks.\nProject page at https://tcxm.github.io/DEXTER-LLM/"
                },
                "authors": [
                    {
                        "name": "Yuxiao Zhu"
                    },
                    {
                        "name": "Junfeng Chen"
                    },
                    {
                        "name": "Xintong Zhang"
                    },
                    {
                        "name": "Meng Guo"
                    },
                    {
                        "name": "Zhongkui Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhongkui Li"
                },
                "author": "Zhongkui Li",
                "arxiv_comment": "submitted to IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05989v2",
                "updated": "2025-08-20T03:11:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    11,
                    51,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-08T03:51:24Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    3,
                    51,
                    24,
                    4,
                    220,
                    0
                ],
                "title": "ETA: Energy-based Test-time Adaptation for Depth Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETA: Energy-based Test-time Adaptation for Depth Completion"
                },
                "summary": "We propose a method for test-time adaptation of pretrained depth completion\nmodels. Depth completion models, trained on some ``source'' data, often predict\nerroneous outputs when transferred to ``target'' data captured in novel\nenvironmental conditions due to a covariate shift. The crux of our method lies\nin quantifying the likelihood of depth predictions belonging to the source data\ndistribution. The challenge is in the lack of access to out-of-distribution\n(target) data prior to deployment. Hence, rather than making assumptions\nregarding the target distribution, we utilize adversarial perturbations as a\nmechanism to explore the data space. This enables us to train an energy model\nthat scores local regions of depth predictions as in- or out-of-distribution.\nWe update the parameters of pretrained depth completion models at test time to\nminimize energy, effectively aligning test-time predictions to those of the\nsource distribution. We call our method ``Energy-based Test-time Adaptation'',\nor ETA for short. We evaluate our method across three indoor and three outdoor\ndatasets, where ETA improve over the previous state-of-the-art method by an\naverage of 6.94% for outdoors and 10.23% for indoors. Project Page:\nhttps://fuzzythecat.github.io/eta.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method for test-time adaptation of pretrained depth completion\nmodels. Depth completion models, trained on some ``source'' data, often predict\nerroneous outputs when transferred to ``target'' data captured in novel\nenvironmental conditions due to a covariate shift. The crux of our method lies\nin quantifying the likelihood of depth predictions belonging to the source data\ndistribution. The challenge is in the lack of access to out-of-distribution\n(target) data prior to deployment. Hence, rather than making assumptions\nregarding the target distribution, we utilize adversarial perturbations as a\nmechanism to explore the data space. This enables us to train an energy model\nthat scores local regions of depth predictions as in- or out-of-distribution.\nWe update the parameters of pretrained depth completion models at test time to\nminimize energy, effectively aligning test-time predictions to those of the\nsource distribution. We call our method ``Energy-based Test-time Adaptation'',\nor ETA for short. We evaluate our method across three indoor and three outdoor\ndatasets, where ETA improve over the previous state-of-the-art method by an\naverage of 6.94% for outdoors and 10.23% for indoors. Project Page:\nhttps://fuzzythecat.github.io/eta."
                },
                "authors": [
                    {
                        "name": "Younjoon Chung"
                    },
                    {
                        "name": "Hyoungseob Park"
                    },
                    {
                        "name": "Patrick Rim"
                    },
                    {
                        "name": "Xiaoran Zhang"
                    },
                    {
                        "name": "Jihe He"
                    },
                    {
                        "name": "Ziyao Zeng"
                    },
                    {
                        "name": "Safa Cicek"
                    },
                    {
                        "name": "Byung-Woo Hong"
                    },
                    {
                        "name": "James S. Duncan"
                    },
                    {
                        "name": "Alex Wong"
                    }
                ],
                "author_detail": {
                    "name": "Alex Wong"
                },
                "author": "Alex Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14377v1",
                "updated": "2025-08-20T03:08:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    8,
                    47,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T03:08:47Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    8,
                    47,
                    2,
                    232,
                    0
                ],
                "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students'\n  Cognitive Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students'\n  Cognitive Abilities"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in educational\napplications, yet their capacity to accurately assess the cognitive alignment\nof reading materials with students' developmental stages remains insufficiently\nexplored. This gap is particularly critical given the foundational educational\nprinciple of the Zone of Proximal Development (ZPD), which emphasizes the need\nto match learning resources with Students' Cognitive Abilities (SCA). Despite\nthe importance of this alignment, there is a notable absence of comprehensive\nstudies investigating LLMs' ability to evaluate reading comprehension\ndifficulty across different student age groups, especially in the context of\nChinese language education. To fill this gap, we introduce ZPD-SCA, a novel\nbenchmark specifically designed to assess stage-level Chinese reading\ncomprehension difficulty. The benchmark is annotated by 60 Special Grade\nteachers, a group that represents the top 0.15% of all in-service teachers\nnationwide. Experimental results reveal that LLMs perform poorly in zero-shot\nlearning scenarios, with Qwen-max and GLM even falling below the probability of\nrandom guessing. When provided with in-context examples, LLMs performance\nimproves substantially, with some models achieving nearly double the accuracy\nof their zero-shot baselines. These results reveal that LLMs possess emerging\nabilities to assess reading difficulty, while also exposing limitations in\ntheir current training for educationally aligned judgment. Notably, even the\nbest-performing models display systematic directional biases, suggesting\ndifficulties in accurately aligning material difficulty with SCA. Furthermore,\nsignificant variations in model performance across different genres underscore\nthe complexity of task. We envision that ZPD-SCA can provide a foundation for\nevaluating and improving LLMs in cognitively aligned educational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in educational\napplications, yet their capacity to accurately assess the cognitive alignment\nof reading materials with students' developmental stages remains insufficiently\nexplored. This gap is particularly critical given the foundational educational\nprinciple of the Zone of Proximal Development (ZPD), which emphasizes the need\nto match learning resources with Students' Cognitive Abilities (SCA). Despite\nthe importance of this alignment, there is a notable absence of comprehensive\nstudies investigating LLMs' ability to evaluate reading comprehension\ndifficulty across different student age groups, especially in the context of\nChinese language education. To fill this gap, we introduce ZPD-SCA, a novel\nbenchmark specifically designed to assess stage-level Chinese reading\ncomprehension difficulty. The benchmark is annotated by 60 Special Grade\nteachers, a group that represents the top 0.15% of all in-service teachers\nnationwide. Experimental results reveal that LLMs perform poorly in zero-shot\nlearning scenarios, with Qwen-max and GLM even falling below the probability of\nrandom guessing. When provided with in-context examples, LLMs performance\nimproves substantially, with some models achieving nearly double the accuracy\nof their zero-shot baselines. These results reveal that LLMs possess emerging\nabilities to assess reading difficulty, while also exposing limitations in\ntheir current training for educationally aligned judgment. Notably, even the\nbest-performing models display systematic directional biases, suggesting\ndifficulties in accurately aligning material difficulty with SCA. Furthermore,\nsignificant variations in model performance across different genres underscore\nthe complexity of task. We envision that ZPD-SCA can provide a foundation for\nevaluating and improving LLMs in cognitively aligned educational applications."
                },
                "authors": [
                    {
                        "name": "Wenhan Dong"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Yuemeng Zhao"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Ruiming Wang"
                    },
                    {
                        "name": "Xinyi Huang"
                    },
                    {
                        "name": "Lei Mo"
                    }
                ],
                "author_detail": {
                    "name": "Lei Mo"
                },
                "author": "Lei Mo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01519v2",
                "updated": "2025-08-20T02:50:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    2,
                    50,
                    14,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-02T09:06:23Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    6,
                    23,
                    2,
                    92,
                    0
                ],
                "title": "Chain of Correction for Full-text Speech Recognition with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Correction for Full-text Speech Recognition with Large Language\n  Models"
                },
                "summary": "Full-text error correction with Large Language Models (LLMs) for Automatic\nSpeech Recognition (ASR) is attracting increased attention for its ability to\naddress a wide range of error types, such as punctuation restoration and\ninverse text normalization, across long context. However, challenges remain\nregarding stability, controllability, completeness, and fluency. To mitigate\nthese issues, this paper proposes the Chain of Correction (CoC), which uses a\nmulti-turn chat format to correct errors segment by segment, guided by\npre-recognized text and full-text context for better semantic understanding.\nUtilizing the open-sourced ChFT dataset, we fine-tune a pre-trained LLM to\nevaluate CoC's performance. Experiments show that CoC significantly outperforms\nbaseline and benchmark systems in correcting full-text ASR outputs. We also\nanalyze correction thresholds to balance under-correction and over-rephrasing,\nextrapolate CoC on extra-long ASR outputs, and explore using other types of\ninformation to guide error correction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-text error correction with Large Language Models (LLMs) for Automatic\nSpeech Recognition (ASR) is attracting increased attention for its ability to\naddress a wide range of error types, such as punctuation restoration and\ninverse text normalization, across long context. However, challenges remain\nregarding stability, controllability, completeness, and fluency. To mitigate\nthese issues, this paper proposes the Chain of Correction (CoC), which uses a\nmulti-turn chat format to correct errors segment by segment, guided by\npre-recognized text and full-text context for better semantic understanding.\nUtilizing the open-sourced ChFT dataset, we fine-tune a pre-trained LLM to\nevaluate CoC's performance. Experiments show that CoC significantly outperforms\nbaseline and benchmark systems in correcting full-text ASR outputs. We also\nanalyze correction thresholds to balance under-correction and over-rephrasing,\nextrapolate CoC on extra-long ASR outputs, and explore using other types of\ninformation to guide error correction."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Tang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Zhikai Zhou"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Shen Huang"
                    },
                    {
                        "name": "Shidong Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shidong Shang"
                },
                "author": "Shidong Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19959v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19959v3",
                "updated": "2025-08-20T02:50:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    2,
                    50,
                    13,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-28T16:33:08Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    33,
                    8,
                    0,
                    118,
                    0
                ],
                "title": "From Concept to Practice: an Automated LLM-aided UVM Machine for RTL\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Concept to Practice: an Automated LLM-aided UVM Machine for RTL\n  Verification"
                },
                "summary": "Verification presents a major bottleneck in Integrated Circuit (IC)\ndevelopment, consuming nearly 70% of the total development effort. While the\nUniversal Verification Methodology (UVM) is widely used in industry to improve\nverification efficiency through structured and reusable testbenches,\nconstructing these testbenches and generating sufficient stimuli remain\nchallenging. These challenges arise from the considerable manual coding effort\nrequired, repetitive manual execution of multiple EDA tools, and the need for\nin-depth domain expertise to navigate complex designs.Here, we present UVM^2,\nan automated verification framework that leverages Large Language Models (LLMs)\nto generate UVM testbenches and iteratively refine them using coverage\nfeedback, significantly reducing manual effort while maintaining rigorous\nverification standards.To evaluate UVM^2, we introduce a benchmark suite\ncomprising Register Transfer Level (RTL) designs of up to 1.6K lines of\ncode.The results show that UVM^2 reduces testbench setup time by up to UVM^2\ncompared to experienced engineers, and achieve average code and function\ncoverage of 87.44% and 89.58%, outperforming state-of-the-art solutions by\n20.96% and 23.51%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification presents a major bottleneck in Integrated Circuit (IC)\ndevelopment, consuming nearly 70% of the total development effort. While the\nUniversal Verification Methodology (UVM) is widely used in industry to improve\nverification efficiency through structured and reusable testbenches,\nconstructing these testbenches and generating sufficient stimuli remain\nchallenging. These challenges arise from the considerable manual coding effort\nrequired, repetitive manual execution of multiple EDA tools, and the need for\nin-depth domain expertise to navigate complex designs.Here, we present UVM^2,\nan automated verification framework that leverages Large Language Models (LLMs)\nto generate UVM testbenches and iteratively refine them using coverage\nfeedback, significantly reducing manual effort while maintaining rigorous\nverification standards.To evaluate UVM^2, we introduce a benchmark suite\ncomprising Register Transfer Level (RTL) designs of up to 1.6K lines of\ncode.The results show that UVM^2 reduces testbench setup time by up to UVM^2\ncompared to experienced engineers, and achieve average code and function\ncoverage of 87.44% and 89.58%, outperforming state-of-the-art solutions by\n20.96% and 23.51%, respectively."
                },
                "authors": [
                    {
                        "name": "Junhao Ye"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Dingrong Pan"
                    },
                    {
                        "name": "Qichun Chen"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Xinwei Fang"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19959v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19959v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]