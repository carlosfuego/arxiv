[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.19678v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19678v1",
                "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion"
                },
                "updated": "2025-12-22T18:53:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    53,
                    50,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19678v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:53:50Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    53,
                    50,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Project page: https://hyokong.github.io/worldwarp-page/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hanyang Kong"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Xiaoxu Zheng"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang"
            },
            {
                "id": "http://arxiv.org/abs/2510.06415v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.06415v2",
                "title": "Neutron-Assisted Breakdown Enhancement in $β$-Ga$_2$O$_3$ Schottky Diodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron-Assisted Breakdown Enhancement in $β$-Ga$_2$O$_3$ Schottky Diodes"
                },
                "updated": "2025-12-22T16:32:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    32,
                    7,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.06415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.06415v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study demonstrates a substantial enhancement of breakdown voltage in $β$-Ga$_2$O$_3$ Schottky diodes through an approach that combines fast neutron irradiation with controlled post-irradiation electro-thermal annealing. Devices irradiated with 1 MeV neutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation, including a drastic reduction in on-current and an increase in on-resistance. Electrothermal testing, conducted through simultaneous current-voltage (J-V) measurements and thermal annealing, resulted in significant recovery. After four cycles of electro-thermal testing, the devices demonstrated significant improvements in performance, with a substantial recovery of on-current and a reduction in on-resistance compared to the post-radiation condition, approaching pre-radiation levels. Most recovery occurred during the first two cycles, with diminishing improvements in later cycles, indicating that most thermally recoverable traps were mitigated early. Capacitance-voltage (C-V) measurements revealed a substantial reduction in carrier concentration, decreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first electro-thermal testing cycle, indicating an over 82% reduction. Following the third cycle, the carrier concentration partially recovered to 9.9E15 cm^-3, reflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited a remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a ~325% improvement) after the first electro-thermal testing, attributed to the reduction in carrier concentration by compensating radiation-induced traps. Subsequent testing reduced breakdown voltage slightly to 940 V due to partial recovery of carrier concentration, but it remained significantly higher than pre-radiation levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates a substantial enhancement of breakdown voltage in $β$-Ga$_2$O$_3$ Schottky diodes through an approach that combines fast neutron irradiation with controlled post-irradiation electro-thermal annealing. Devices irradiated with 1 MeV neutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation, including a drastic reduction in on-current and an increase in on-resistance. Electrothermal testing, conducted through simultaneous current-voltage (J-V) measurements and thermal annealing, resulted in significant recovery. After four cycles of electro-thermal testing, the devices demonstrated significant improvements in performance, with a substantial recovery of on-current and a reduction in on-resistance compared to the post-radiation condition, approaching pre-radiation levels. Most recovery occurred during the first two cycles, with diminishing improvements in later cycles, indicating that most thermally recoverable traps were mitigated early. Capacitance-voltage (C-V) measurements revealed a substantial reduction in carrier concentration, decreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first electro-thermal testing cycle, indicating an over 82% reduction. Following the third cycle, the carrier concentration partially recovered to 9.9E15 cm^-3, reflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited a remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a ~325% improvement) after the first electro-thermal testing, attributed to the reduction in carrier concentration by compensating radiation-induced traps. Subsequent testing reduced breakdown voltage slightly to 940 V due to partial recovery of carrier concentration, but it remained significantly higher than pre-radiation levels."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T19:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Saleh Ahmed Khan"
                    },
                    {
                        "name": "Sudipto Saha"
                    },
                    {
                        "name": "Ahmed Ibreljic"
                    },
                    {
                        "name": "Stephen Margiotta"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Walid Amir"
                    },
                    {
                        "name": "Surajit Chakraborty"
                    },
                    {
                        "name": "Uttam Singisetti"
                    },
                    {
                        "name": "A F M Anhar Uddin Bhuiyan"
                    }
                ],
                "author_detail": {
                    "name": "A F M Anhar Uddin Bhuiyan"
                },
                "author": "A F M Anhar Uddin Bhuiyan"
            },
            {
                "id": "http://arxiv.org/abs/2512.19437v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19437v1",
                "title": "Ultra-high precision high voltage system for PTOLEMY",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-high precision high voltage system for PTOLEMY"
                },
                "updated": "2025-12-22T14:34:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    34,
                    51,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19437v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The PTOLEMY project is prototyping a novel electromagnetic filter for high-precision $β$ spectroscopy, with the ultimate and ambitious long-term goal of detecting the cosmic neutrino background through electron capture on tritium bound to graphene. Intermediate small-scale prototypes can achieve competitive sensitivity to the effective neutrino mass, even with reduced energy resolution. To reach an energy resolution better than \\SI{500}{meV} at the tritium $β$-spectrum endpoint of \\SI{18.6}{keV}, and accounting for all uncertainties in the filtering chain, the electrode voltage must be controlled at the level of a few parts per million and monitored in real time. In this work, we present the first results obtained in this effort, using a chain of commercial ultra-high-precision voltage references, read out by precision multimeters and a \\emph{field mill} device. The currently available precision on high voltage is, in the conservative case, as low as \\SI{0.2}{ppm} per \\SI{1}{kV} single board and $\\lesssim$ \\SI{50}{mV} over the \\SI{10}{kV} series, presently limited by field mill read-out noise. However, assuming uncorrelated Gaussian noise extrapolation, the real precision could in principle be as low as \\SI{0.05}{ppm} over \\SI{20}{kV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PTOLEMY project is prototyping a novel electromagnetic filter for high-precision $β$ spectroscopy, with the ultimate and ambitious long-term goal of detecting the cosmic neutrino background through electron capture on tritium bound to graphene. Intermediate small-scale prototypes can achieve competitive sensitivity to the effective neutrino mass, even with reduced energy resolution. To reach an energy resolution better than \\SI{500}{meV} at the tritium $β$-spectrum endpoint of \\SI{18.6}{keV}, and accounting for all uncertainties in the filtering chain, the electrode voltage must be controlled at the level of a few parts per million and monitored in real time. In this work, we present the first results obtained in this effort, using a chain of commercial ultra-high-precision voltage references, read out by precision multimeters and a \\emph{field mill} device. The currently available precision on high voltage is, in the conservative case, as low as \\SI{0.2}{ppm} per \\SI{1}{kV} single board and $\\lesssim$ \\SI{50}{mV} over the \\SI{10}{kV} series, presently limited by field mill read-out noise. However, assuming uncorrelated Gaussian noise extrapolation, the real precision could in principle be as low as \\SI{0.05}{ppm} over \\SI{20}{kV}."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:34:51Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    34,
                    51,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "17 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "R. Ammendola"
                    },
                    {
                        "name": "A. Apponi"
                    },
                    {
                        "name": "G. Benato"
                    },
                    {
                        "name": "M. G. Betti"
                    },
                    {
                        "name": "R. Biondim"
                    },
                    {
                        "name": "P. Bos"
                    },
                    {
                        "name": "G. Cavoto"
                    },
                    {
                        "name": "M. Cadeddu"
                    },
                    {
                        "name": "A. Casale"
                    },
                    {
                        "name": "O. Castellano"
                    },
                    {
                        "name": "E. Celasco"
                    },
                    {
                        "name": "L. Cecchini"
                    },
                    {
                        "name": "M. Chirico"
                    },
                    {
                        "name": "W. Chung"
                    },
                    {
                        "name": "A. G. Cocco"
                    },
                    {
                        "name": "A. P. Colijn"
                    },
                    {
                        "name": "B. Corcione"
                    },
                    {
                        "name": "N. D'Ambrosio"
                    },
                    {
                        "name": "M. D'Incecco"
                    },
                    {
                        "name": "G. De Bellis"
                    },
                    {
                        "name": "M. De Deo"
                    },
                    {
                        "name": "N. de Groot"
                    },
                    {
                        "name": "A. Esposito"
                    },
                    {
                        "name": "M. Farino"
                    },
                    {
                        "name": "S. Farinon"
                    },
                    {
                        "name": "A. D. Ferella"
                    },
                    {
                        "name": "L. Ferro"
                    },
                    {
                        "name": "L. Ficcadenti"
                    },
                    {
                        "name": "G. Galbato Muscio"
                    },
                    {
                        "name": "S. Gariazzo"
                    },
                    {
                        "name": "H. Garrone"
                    },
                    {
                        "name": "F. Gatti"
                    },
                    {
                        "name": "G. Korga"
                    },
                    {
                        "name": "F. Malnati"
                    },
                    {
                        "name": "G. Mangano"
                    },
                    {
                        "name": "L. E. Marcucci"
                    },
                    {
                        "name": "C. Mariani"
                    },
                    {
                        "name": "J. Mead"
                    },
                    {
                        "name": "G. Menichetti"
                    },
                    {
                        "name": "M. Messina"
                    },
                    {
                        "name": "E. Monticone"
                    },
                    {
                        "name": "M. Naafs"
                    },
                    {
                        "name": "V. Narcisi"
                    },
                    {
                        "name": "S. Nagorny"
                    },
                    {
                        "name": "G. Neri"
                    },
                    {
                        "name": "F. Pandolfi"
                    },
                    {
                        "name": "R. Pavarani"
                    },
                    {
                        "name": "C. Pèrez de los Heros"
                    },
                    {
                        "name": "O. Pisanti"
                    },
                    {
                        "name": "C. Pepe"
                    },
                    {
                        "name": "F. M. Pofi"
                    },
                    {
                        "name": "A. D. Polosa"
                    },
                    {
                        "name": "I. Rago"
                    },
                    {
                        "name": "M. Rajteri N. Rossi"
                    },
                    {
                        "name": "S. Ritarossi"
                    },
                    {
                        "name": "A. Ruocco"
                    },
                    {
                        "name": "G. Salina"
                    },
                    {
                        "name": "A. Santucci"
                    },
                    {
                        "name": "M. Sestu"
                    },
                    {
                        "name": "A. Tan"
                    },
                    {
                        "name": "V. Tozzini"
                    },
                    {
                        "name": "C. G. Tully"
                    },
                    {
                        "name": "I. van Rens"
                    },
                    {
                        "name": "F. Virzi"
                    },
                    {
                        "name": "G. Visser"
                    },
                    {
                        "name": "M. Vivian"
                    }
                ],
                "author_detail": {
                    "name": "M. Vivian"
                },
                "author": "M. Vivian"
            },
            {
                "id": "http://arxiv.org/abs/2512.19434v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19434v1",
                "title": "Hybrid Analytical-Machine Learning Framework for Ripple Factor Estimation in Cockcroft-Walton Voltage Multipliers with Residual Correction for Non-Ideal Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Analytical-Machine Learning Framework for Ripple Factor Estimation in Cockcroft-Walton Voltage Multipliers with Residual Correction for Non-Ideal Effects"
                },
                "updated": "2025-12-22T14:32:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    32,
                    19,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19434v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cockcroft-Walton (CW) voltage multipliers suffer from output ripple that classical analytical models underestimate due to neglected non-idealities like diode drops and capacitor ESR, particularly in high-stage, low-frequency and heavy-load regimes. This paper proposes a hybrid framework that generates a comprehensive 324-case MATLAB/Simulink dataset varying stages (2-8), input voltage (5-25 kV), capacitance (1-10 μF), frequency (50-500 Hz) and load (6-60 MΩ), then trains a Random Forest model to predict residuals between simulated and theoretical peak-to-peak ripple. The approach achieves 70.6% RMSE reduction (131 V vs. 448 V) globally and 66.7% in critical regimes, with near-zero bias, enabling physically interpretable design optimization while outperforming pure ML in extrapolation reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cockcroft-Walton (CW) voltage multipliers suffer from output ripple that classical analytical models underestimate due to neglected non-idealities like diode drops and capacitor ESR, particularly in high-stage, low-frequency and heavy-load regimes. This paper proposes a hybrid framework that generates a comprehensive 324-case MATLAB/Simulink dataset varying stages (2-8), input voltage (5-25 kV), capacitance (1-10 μF), frequency (50-500 Hz) and load (6-60 MΩ), then trains a Random Forest model to predict residuals between simulated and theoretical peak-to-peak ripple. The approach achieves 70.6% RMSE reduction (131 V vs. 448 V) globally and 66.7% in critical regimes, with near-zero bias, enabling physically interpretable design optimization while outperforming pure ML in extrapolation reliability."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:32:19Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    32,
                    19,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "6 Pages, 2 figures, IEEE Conference Template used",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Md. Tanvirul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Md. Tanvirul Islam"
                },
                "author": "Md. Tanvirul Islam"
            },
            {
                "id": "http://arxiv.org/abs/2512.17452v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17452v2",
                "title": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference"
                },
                "updated": "2025-12-22T10:23:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    23,
                    36,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17452v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV ."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T11:08:58Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    11,
                    8,
                    58,
                    4,
                    353,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yen-Chieh Huang"
                    },
                    {
                        "name": "Pi-Cheng Hsiu"
                    },
                    {
                        "name": "Rui Fang"
                    },
                    {
                        "name": "Ming-Syan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Syan Chen"
                },
                "author": "Ming-Syan Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.19206v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19206v1",
                "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning"
                },
                "updated": "2025-12-22T09:44:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    44,
                    26,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19206v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T09:44:26Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    44,
                    26,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen"
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.11496v3",
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model"
                },
                "updated": "2025-12-22T04:40:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    4,
                    40,
                    4,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.11496v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.11496v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu"
            },
            {
                "id": "http://arxiv.org/abs/2511.01385v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01385v2",
                "title": "Memory-Efficient Training with In-Place FFT Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training with In-Place FFT Implementation"
                },
                "updated": "2025-12-22T02:25:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    2,
                    25,
                    29,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01385v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T09:36:11Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025. Version 2 adds links to the ongoing PyTorch upstreaming discussion",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xinyu Ding"
                    },
                    {
                        "name": "Bangtian Liu"
                    },
                    {
                        "name": "Siyu Liao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.18741v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18741v1",
                "title": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation"
                },
                "updated": "2025-12-21T14:02:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    21,
                    14,
                    2,
                    53,
                    6,
                    355,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18741v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose \\textbf{Memorize-and-Generate (MAG)}, a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce \\textbf{MAG-Bench} to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose \\textbf{Memorize-and-Generate (MAG)}, a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce \\textbf{MAG-Bench} to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-21T14:02:53Z",
                "published_parsed": [
                    2025,
                    12,
                    21,
                    14,
                    2,
                    53,
                    6,
                    355,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Zhirui Sun"
                    },
                    {
                        "name": "Jingqi Tian"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang"
            },
            {
                "id": "http://arxiv.org/abs/2512.18674v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18674v1",
                "title": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing"
                },
                "updated": "2025-12-21T10:27:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    21,
                    10,
                    27,
                    50,
                    6,
                    355,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18674v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-21T10:27:50Z",
                "published_parsed": [
                    2025,
                    12,
                    21,
                    10,
                    27,
                    50,
                    6,
                    355,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Yuhao Hu"
                    },
                    {
                        "name": "Ruiting Zhou"
                    },
                    {
                        "name": "Baochun Li"
                    },
                    {
                        "name": "Ne Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ne Wang"
                },
                "author": "Ne Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.01802v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01802v4",
                "title": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford"
                },
                "updated": "2025-12-20T14:13:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    14,
                    13,
                    44,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01802v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01802v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from -31 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=10,000 nodes and 55,000,000 edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from -31 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=10,000 nodes and 55,000,000 edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T15:35:53Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    15,
                    35,
                    53,
                    0,
                    335,
                    0
                ],
                "arxiv_comment": "with editor,25 pages",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.18345v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18345v1",
                "title": "Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration"
                },
                "updated": "2025-12-20T12:18:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    18,
                    29,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18345v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fully homomorphic encryption (FHE) enables secure computation on encrypted data, mitigating privacy concerns in cloud and edge environments. However, due to its high compute and memory demands, extensive acceleration research has been pursued across diverse hardware platforms, especially GPUs. In this paper, we perform a microarchitectural analysis of CKKS, a popular FHE scheme, on modern GPUs. We focus on on-chip cache behavior, and show that the dominant kernels remain bound by memory bandwidth despite a high-bandwidth L2 cache, exposing a persistent memory wall. We further discover that the overall CKKS pipeline throughput is constrained by low per-kernel hardware utilization, caused by insufficient intra-kernel parallelism. Motivated by these findings, we introduce Theodosian, a set of complementary, memory-aware optimizations that improve cache efficiency and reduce runtime overheads. Our approach delivers consistent speedups across various CKKS workloads. On an RTX 5090, we reduce the bootstrapping latency for 32,768 complex numbers to 15.2ms with Theodosian, and further to 12.8ms with additional algorithmic optimizations, establishing new state-of-the-art GPU performance to the best of our knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully homomorphic encryption (FHE) enables secure computation on encrypted data, mitigating privacy concerns in cloud and edge environments. However, due to its high compute and memory demands, extensive acceleration research has been pursued across diverse hardware platforms, especially GPUs. In this paper, we perform a microarchitectural analysis of CKKS, a popular FHE scheme, on modern GPUs. We focus on on-chip cache behavior, and show that the dominant kernels remain bound by memory bandwidth despite a high-bandwidth L2 cache, exposing a persistent memory wall. We further discover that the overall CKKS pipeline throughput is constrained by low per-kernel hardware utilization, caused by insufficient intra-kernel parallelism. Motivated by these findings, we introduce Theodosian, a set of complementary, memory-aware optimizations that improve cache efficiency and reduce runtime overheads. Our approach delivers consistent speedups across various CKKS workloads. On an RTX 5090, we reduce the bootstrapping latency for 32,768 complex numbers to 15.2ms with Theodosian, and further to 12.8ms with additional algorithmic optimizations, establishing new state-of-the-art GPU performance to the best of our knowledge."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T12:18:29Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    18,
                    29,
                    5,
                    354,
                    0
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Wonseok Choi"
                    },
                    {
                        "name": "Hyunah Yu"
                    },
                    {
                        "name": "Jongmin Kim"
                    },
                    {
                        "name": "Hyesung Ji"
                    },
                    {
                        "name": "Jaiyoung Park"
                    },
                    {
                        "name": "Jung Ho Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Jung Ho Ahn"
                },
                "author": "Jung Ho Ahn"
            },
            {
                "id": "http://arxiv.org/abs/2512.18337v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18337v1",
                "title": "Towards Efficient Agents: A Co-Design of Inference Architecture and System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Agents: A Co-Design of Inference Architecture and System"
                },
                "updated": "2025-12-20T12:06:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    6,
                    13,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18337v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T12:06:13Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    6,
                    13,
                    5,
                    354,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xian Wang"
                    },
                    {
                        "name": "Renxi Liu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Wangze Zhang"
                    },
                    {
                        "name": "Chuansai Zhou"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Xiaosong Li"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.18300v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18300v1",
                "title": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism"
                },
                "updated": "2025-12-20T10:11:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    10,
                    11,
                    47,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18300v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper studies the impact of DRAM writes on DDR5-based system. To efficiently perform DRAM writes, modern systems buffer write requests and try to complete multiple write operations whenever the DRAM mode is switched from read to write. When the DRAM system is performing writes, it is not available to service read requests, thus increasing read latency and reducing performance. We observe that, given the presence of on-die ECC in DDR5 devices, the time to perform a write operation varies significantly: from 1x (for writes to banks of different bankgroups) to 6x (for writes to banks within the same bankgroup) to 24x (for conflicting requests to the same bank). If we can orchestrate the write stream to favor write requests that incur lower latency, then we can reduce the stall time from DRAM writes and improve performance. However, for current systems, the write stream is dictated by the cache replacement policy, which makes eviction decisions without being aware of the variable latency of DRAM writes. The key insight of our work is to improve performance by modifying the cache replacement policy to increase bank-parallelism of DRAM writes.\n  Our paper proposes {\\em BARD (Bank-Aware Replacement Decisions)}, which modifies the cache replacement policy to favor dirty lines that belong to banks without pending writes. We analyze two variants of BARD: BARD-E (Eviction-based), which changes the eviction policy to evict low-cost dirty lines, and BARD-C (Cleansing-Based), which proactively cleans low-cost dirty lines without modifying the eviction decisions. We develop a hybrid policy (BARD-H), which uses a selective combination of both eviction and writeback. Our evaluations across workloads from SPEC2017, LIGRA, STREAM, and Google server traces show that BARD-H improves performance by 4.3\\% on average and up-to 8.5\\%. BARD requires only 8 bytes of SRAM per LLC slice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the impact of DRAM writes on DDR5-based system. To efficiently perform DRAM writes, modern systems buffer write requests and try to complete multiple write operations whenever the DRAM mode is switched from read to write. When the DRAM system is performing writes, it is not available to service read requests, thus increasing read latency and reducing performance. We observe that, given the presence of on-die ECC in DDR5 devices, the time to perform a write operation varies significantly: from 1x (for writes to banks of different bankgroups) to 6x (for writes to banks within the same bankgroup) to 24x (for conflicting requests to the same bank). If we can orchestrate the write stream to favor write requests that incur lower latency, then we can reduce the stall time from DRAM writes and improve performance. However, for current systems, the write stream is dictated by the cache replacement policy, which makes eviction decisions without being aware of the variable latency of DRAM writes. The key insight of our work is to improve performance by modifying the cache replacement policy to increase bank-parallelism of DRAM writes.\n  Our paper proposes {\\em BARD (Bank-Aware Replacement Decisions)}, which modifies the cache replacement policy to favor dirty lines that belong to banks without pending writes. We analyze two variants of BARD: BARD-E (Eviction-based), which changes the eviction policy to evict low-cost dirty lines, and BARD-C (Cleansing-Based), which proactively cleans low-cost dirty lines without modifying the eviction decisions. We develop a hybrid policy (BARD-H), which uses a selective combination of both eviction and writeback. Our evaluations across workloads from SPEC2017, LIGRA, STREAM, and Google server traces show that BARD-H improves performance by 4.3\\% on average and up-to 8.5\\%. BARD requires only 8 bytes of SRAM per LLC slice."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T10:11:47Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    10,
                    11,
                    47,
                    5,
                    354,
                    0
                ],
                "arxiv_comment": "Accepted to HPCA 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Suhas Vittal"
                    },
                    {
                        "name": "Moinuddin Qureshi"
                    }
                ],
                "author_detail": {
                    "name": "Moinuddin Qureshi"
                },
                "author": "Moinuddin Qureshi"
            },
            {
                "id": "http://arxiv.org/abs/2512.18194v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18194v1",
                "title": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale"
                },
                "updated": "2025-12-20T03:42:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    3,
                    42,
                    19,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18194v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and existing systems rely on RDMA-based network paths for this exchange. As model sizes and context lengths increase, KV transfer dominates both time-to-first-token (TTFT) and peak throughput, and remains highly sensitive to network contention even when prefix reuse is high. This paper presents TraCT, a rack-scale LLM serving system that uses CXL shared memory as both a KV-transfer substrate and a rack-wide prefix-aware KV cache. TraCT enables GPUs to write and read KV blocks directly through CXL load/store and DMA operations, eliminating the NIC hop that constrains existing disaggregated pipelines. However, to realize this design, multiple new challenges such as synchronization, consistency, and data management on non-coherent CXL memory need to be addressed. TraCT proposes various software solutions such as the two-tier inter-node synchronization mechanism to address these challenges. We implement TraCT on the Dynamo LLM inference framework and show that, across static and synthetic workloads, TraCT reduces average TTFT by up to 9.8x, lowers P99 latency by up to 6.2x, and improves peak throughput by up to 1.6x compared to RDMA and DRAM-based caching baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and existing systems rely on RDMA-based network paths for this exchange. As model sizes and context lengths increase, KV transfer dominates both time-to-first-token (TTFT) and peak throughput, and remains highly sensitive to network contention even when prefix reuse is high. This paper presents TraCT, a rack-scale LLM serving system that uses CXL shared memory as both a KV-transfer substrate and a rack-wide prefix-aware KV cache. TraCT enables GPUs to write and read KV blocks directly through CXL load/store and DMA operations, eliminating the NIC hop that constrains existing disaggregated pipelines. However, to realize this design, multiple new challenges such as synchronization, consistency, and data management on non-coherent CXL memory need to be addressed. TraCT proposes various software solutions such as the two-tier inter-node synchronization mechanism to address these challenges. We implement TraCT on the Dynamo LLM inference framework and show that, across static and synthetic workloads, TraCT reduces average TTFT by up to 9.8x, lowers P99 latency by up to 6.2x, and improves peak throughput by up to 1.6x compared to RDMA and DRAM-based caching baselines."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T03:42:19Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    3,
                    42,
                    19,
                    5,
                    354,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Dongha Yoon"
                    },
                    {
                        "name": "Younghoon Min"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Sam H. Noh"
                    },
                    {
                        "name": "Jongryool Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jongryool Kim"
                },
                "author": "Jongryool Kim"
            },
            {
                "id": "http://arxiv.org/abs/2511.02230v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02230v2",
                "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live"
                },
                "updated": "2025-12-20T01:17:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    1,
                    17,
                    3,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02230v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T03:43:05Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Qiuyang Mang"
                    },
                    {
                        "name": "Runyuan He"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Huanzhi Mao"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Hangrui Zhou"
                    },
                    {
                        "name": "Alvin Cheung"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2512.18117v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18117v1",
                "title": "Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning"
                },
                "updated": "2025-12-19T22:50:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    22,
                    50,
                    49,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18117v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T22:50:49Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    22,
                    50,
                    49,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "Accepted by WSDM'26",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Xiwen Chen"
                    },
                    {
                        "name": "Yen-Chieh Lien"
                    },
                    {
                        "name": "Susan Liu"
                    },
                    {
                        "name": "María Castaños"
                    },
                    {
                        "name": "Abolfazl Razi"
                    },
                    {
                        "name": "Xiaoting Zhao"
                    },
                    {
                        "name": "Congzhe Su"
                    }
                ],
                "author_detail": {
                    "name": "Congzhe Su"
                },
                "author": "Congzhe Su"
            },
            {
                "id": "http://arxiv.org/abs/2512.17661v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17661v1",
                "title": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control"
                },
                "updated": "2025-12-19T15:04:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    15,
                    4,
                    24,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17661v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T15:04:24Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    15,
                    4,
                    24,
                    4,
                    353,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yao Feng"
                    },
                    {
                        "name": "Chendong Xiang"
                    },
                    {
                        "name": "Xinyi Mao"
                    },
                    {
                        "name": "Hengkai Tan"
                    },
                    {
                        "name": "Zuyue Zhang"
                    },
                    {
                        "name": "Shuhe Huang"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Haitian Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2512.11529v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11529v2",
                "title": "xGR: Efficient Generative Recommendation Serving at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xGR: Efficient Generative Recommendation Serving at Scale"
                },
                "updated": "2025-12-19T11:20:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    11,
                    20,
                    16,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11529v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T12:59:38Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    12,
                    59,
                    38,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Shen Zhang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Peijun Yang"
                    },
                    {
                        "name": "Haotian Liang"
                    },
                    {
                        "name": "Menxin Li"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Ziyi Ren"
                    },
                    {
                        "name": "Minchao Zhang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Depei Qian"
                    },
                    {
                        "name": "Hailong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Yang"
                },
                "author": "Hailong Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.12284v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12284v2",
                "title": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval"
                },
                "updated": "2025-12-19T08:02:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    8,
                    2,
                    44,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12284v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-13T11:02:04Z",
                "published_parsed": [
                    2025,
                    12,
                    13,
                    11,
                    2,
                    4,
                    5,
                    347,
                    0
                ],
                "arxiv_comment": "14 pages, 20 figures, conference",
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Donghyuk Kim"
                    },
                    {
                        "name": "Sejeong Yang"
                    },
                    {
                        "name": "Wonjin Shin"
                    },
                    {
                        "name": "Joo-Young Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Young Kim"
                },
                "author": "Joo-Young Kim"
            },
            {
                "id": "http://arxiv.org/abs/2512.17298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17298v1",
                "title": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration"
                },
                "updated": "2025-12-19T07:27:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    7,
                    27,
                    19,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T07:27:19Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    7,
                    27,
                    19,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "Accepted for poster presentation at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Fanpu Cao"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.17970v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17970v1",
                "title": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs"
                },
                "updated": "2025-12-19T06:16:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    6,
                    16,
                    32,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17970v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency-memory-accuracy trade-offs under a unified implementation. On Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency-memory-accuracy trade-offs under a unified implementation. On Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T06:16:32Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    6,
                    16,
                    32,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Gunho Park"
                    },
                    {
                        "name": "Jeongin Bae"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Baeseong park"
                    },
                    {
                        "name": "Jiwon Ryu"
                    },
                    {
                        "name": "Hoseung Kim"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee"
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.20116v2",
                "title": "PeerSync: Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeerSync: Accelerating Containerized Service Delivery at the Network Edge"
                },
                "updated": "2025-12-19T02:09:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    2,
                    9,
                    53,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.20116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.20116v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient container image distribution is crucial for enabling machine learning inference at the network edge, where resource limitations and dynamic network conditions create significant challenges. In this paper, we present PeerSync, a decentralized P2P-based system designed to optimize image distribution in edge environments. PeerSync employs a popularity- and network-aware download engine that dynamically adapts to content popularity and real-time network conditions. PeerSync further integrates automated tracker election for rapid peer discovery and dynamic cache management for efficient storage utilization. We implement PeerSync with 8000+ lines of Rust code and test its performance extensively on both large-scale Docker-based emulations and physical edge devices. Experimental results show that PeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$, and 1.28$\\times$ compared to the Baseline solution, Dragonfly, and Kraken, respectively, while significantly reducing cross-network traffic by 90.72% under congested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine learning inference at the network edge, where resource limitations and dynamic network conditions create significant challenges. In this paper, we present PeerSync, a decentralized P2P-based system designed to optimize image distribution in edge environments. PeerSync employs a popularity- and network-aware download engine that dynamically adapts to content popularity and real-time network conditions. PeerSync further integrates automated tracker election for rapid peer discovery and dynamic cache management for efficient storage utilization. We implement PeerSync with 8000+ lines of Rust code and test its performance extensively on both large-scale Docker-based emulations and physical edge devices. Experimental results show that PeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$, and 1.28$\\times$ compared to the Baseline solution, Dragonfly, and Kraken, respectively, while significantly reducing cross-network traffic by 90.72% under congested and varying network conditions."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng"
            },
            {
                "id": "http://arxiv.org/abs/2512.16843v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16843v1",
                "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference"
                },
                "updated": "2025-12-18T18:18:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    18,
                    18,
                    57,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16843v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T18:18:57Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    18,
                    18,
                    57,
                    3,
                    352,
                    0
                ],
                "arxiv_comment": "Accepted and presented at 13th IEEE International Conference on Intelligent Systems and Embedded Design (ISED-2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Harsh Vardhan Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Vardhan Bansal"
                },
                "author": "Harsh Vardhan Bansal"
            },
            {
                "id": "http://arxiv.org/abs/2512.16822v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16822v1",
                "title": "MEPIC: Memory Efficient Position Independent Caching for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEPIC: Memory Efficient Position Independent Caching for LLM Serving"
                },
                "updated": "2025-12-18T18:04:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    18,
                    4,
                    1,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16822v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.\n  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.\n  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T18:04:01Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    18,
                    4,
                    1,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Rongzhi Gu"
                    },
                    {
                        "name": "Bai Xiaolong"
                    },
                    {
                        "name": "Shan Yizhou"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Wang Lan"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan"
            },
            {
                "id": "http://arxiv.org/abs/2512.16650v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16650v1",
                "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models"
                },
                "updated": "2025-12-18T15:22:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    15,
                    22,
                    14,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16650v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T15:22:14Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    15,
                    22,
                    14,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jirui Yang"
                    },
                    {
                        "name": "Hengqi Guo"
                    },
                    {
                        "name": "Zhihui Lu"
                    },
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yuansen Zhang"
                    },
                    {
                        "name": "Shijing Hu"
                    },
                    {
                        "name": "Qiang Duan"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Tao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Tao Wei"
                },
                "author": "Tao Wei"
            },
            {
                "id": "http://arxiv.org/abs/2512.16615v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16615v1",
                "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers"
                },
                "updated": "2025-12-18T14:53:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    14,
                    53,
                    12,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16615v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T14:53:12Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    14,
                    53,
                    12,
                    3,
                    352,
                    0
                ],
                "arxiv_comment": "Code is available at: https://github.com/SingleZombie/LLSA",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Zeqi Xiao"
                    },
                    {
                        "name": "Tianyi Wei"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xingang Pan"
                    }
                ],
                "author_detail": {
                    "name": "Xingang Pan"
                },
                "author": "Xingang Pan"
            },
            {
                "id": "http://arxiv.org/abs/2512.04677v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04677v3",
                "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length"
                },
                "updated": "2025-12-18T13:02:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    13,
                    2,
                    34,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04677v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04677v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T11:11:24Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    11,
                    11,
                    24,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yubo Huang"
                    },
                    {
                        "name": "Hailong Guo"
                    },
                    {
                        "name": "Fangtai Wu"
                    },
                    {
                        "name": "Shifeng Zhang"
                    },
                    {
                        "name": "Shijie Huang"
                    },
                    {
                        "name": "Qijun Gan"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Steven Hoi"
                    }
                ],
                "author_detail": {
                    "name": "Steven Hoi"
                },
                "author": "Steven Hoi"
            },
            {
                "id": "http://arxiv.org/abs/2512.16473v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16473v1",
                "title": "Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems"
                },
                "updated": "2025-12-18T12:45:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    12,
                    45,
                    52,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16473v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T12:45:52Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    12,
                    45,
                    52,
                    3,
                    352,
                    0
                ],
                "arxiv_comment": "7 pages, 6 figures, to be published in ASP-DAC 2026",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "En-Ming Huang"
                    },
                    {
                        "name": "Li-Shang Lin"
                    },
                    {
                        "name": "Chun-Yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Yi Lee"
                },
                "author": "Chun-Yi Lee"
            },
            {
                "id": "http://arxiv.org/abs/2512.16148v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16148v1",
                "title": "FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store"
                },
                "updated": "2025-12-18T04:03:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    4,
                    3,
                    1,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16148v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T04:03:01Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    4,
                    3,
                    1,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Zhisheng Hu"
                    },
                    {
                        "name": "Jiacheng Shen"
                    },
                    {
                        "name": "Ming-Chang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Chang Yang"
                },
                "author": "Ming-Chang Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.12977v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12977v2",
                "title": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference"
                },
                "updated": "2025-12-18T02:59:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    2,
                    59,
                    27,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12977v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. We develop an experimental implementation of the proposed VLCache pipeline based on SGLang, enabling significantly faster inference in practical deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. We develop an experimental implementation of the proposed VLCache pipeline based on SGLang, enabling significantly faster inference in practical deployments."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T04:45:47Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    4,
                    45,
                    47,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shengling Qin"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Chenxin Wu"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Yizhong Cao"
                    },
                    {
                        "name": "Zhengyang Zhuge"
                    },
                    {
                        "name": "Yuxin Zhou"
                    },
                    {
                        "name": "Wentao Yao"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zhengheng Wang"
                    },
                    {
                        "name": "Shuai Bai"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.16056v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16056v1",
                "title": "MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services"
                },
                "updated": "2025-12-18T00:45:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    0,
                    45,
                    0,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16056v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T00:45:00Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    0,
                    45,
                    0,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Lingfeng Tang"
                    },
                    {
                        "name": "Daoping Zhang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Peihao Huang"
                    },
                    {
                        "name": "Feng Jin"
                    },
                    {
                        "name": "Chengguang Xu"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Feiqiang Sun"
                    },
                    {
                        "name": "Guo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guo Chen"
                },
                "author": "Guo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.16112v2",
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing"
                },
                "updated": "2025-12-17T21:40:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    21,
                    40,
                    17,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.16112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.16112v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "arxiv_comment": "6 pages",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hoshik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hoshik Kim"
                },
                "author": "Hoshik Kim"
            },
            {
                "id": "http://arxiv.org/abs/2512.15713v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15713v1",
                "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models"
                },
                "updated": "2025-12-17T18:59:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    59,
                    55,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15713v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T18:59:55Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    59,
                    55,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "11 pages, 5 figures, conference or other essential info",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lunbin Zeng"
                    },
                    {
                        "name": "Jingfeng Yao"
                    },
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.15705v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15705v1",
                "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX"
                },
                "updated": "2025-12-17T18:55:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    55,
                    45,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15705v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T18:55:45Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    55,
                    45,
                    2,
                    351,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xuting Liu"
                    },
                    {
                        "name": "Daniel Alexander"
                    },
                    {
                        "name": "Siva Kesava Reddy Kakarla"
                    },
                    {
                        "name": "Behnaz Arzani"
                    },
                    {
                        "name": "Vincent Liu"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Liu"
                },
                "author": "Vincent Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.15834v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15834v1",
                "title": "Optimizing Agentic Language Model Inference via Speculative Tool Calls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Agentic Language Model Inference via Speculative Tool Calls"
                },
                "updated": "2025-12-17T18:22:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    22,
                    44,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15834v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new \"tool cache\" API endpoint to enable LM providers to easily adopt these optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new \"tool cache\" API endpoint to enable LM providers to easily adopt these optimizations."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T18:22:44Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    22,
                    44,
                    2,
                    351,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Daniel Nichols"
                    },
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Charles Jekel"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    },
                    {
                        "name": "Harshitha Menon"
                    }
                ],
                "author_detail": {
                    "name": "Harshitha Menon"
                },
                "author": "Harshitha Menon"
            },
            {
                "id": "http://arxiv.org/abs/2512.15595v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15595v1",
                "title": "Optimizing Bloom Filters for Modern GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Bloom Filters for Modern GPU Architectures"
                },
                "updated": "2025-12-17T17:01:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    17,
                    1,
                    55,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15595v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.\n  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\\times$ ($15.4\\times$) for bulk filter lookup (construction), respectively, achieving above $92\\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.\n  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\\times$ ($15.4\\times$) for bulk filter lookup (construction), respectively, achieving above $92\\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T17:01:55Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    17,
                    1,
                    55,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "13 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Daniel Jünger"
                    },
                    {
                        "name": "Kevin Kristensen"
                    },
                    {
                        "name": "Yunsong Wang"
                    },
                    {
                        "name": "Xiangyao Yu"
                    },
                    {
                        "name": "Bertil Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Bertil Schmidt"
                },
                "author": "Bertil Schmidt"
            },
            {
                "id": "http://arxiv.org/abs/2512.15576v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15576v1",
                "title": "Radial electric field and density fluctuations measured by Doppler reflectometry during the post-pellet enhanced confinement phase in W7-X",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radial electric field and density fluctuations measured by Doppler reflectometry during the post-pellet enhanced confinement phase in W7-X"
                },
                "updated": "2025-12-17T16:34:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    16,
                    34,
                    7,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15576v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1088/1741-4326/abddee",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Radial profiles of density fluctuations and radial electric field, $E_r$, have been measured using Doppler reflectometry during the post-pellet enhanced confinement phase achieved, under different heating power levels and magnetic configurations, along the 2018 W7-X experimental campaign. A pronounced $E_r$-well is measured with local values as high as -40 kV/m in the radial range $ρ\\sim 0.7-0.8$ during the post-pellet enhanced confinement phase. The maximum $E_r$ intensity scales with both plasma density and Electron Cyclotron Heating (ECH) power level following a similar trend as the plasma energy content. A good agreement is found when the experimental $E_r$ profiles are compared to simulations carried out using the neoclassical codes DKES and KNOSOS. The density fluctuation level decreases from the plasma edge toward the plasma core and the drop is more pronounced in the post-pellet enhanced confinement phase than in reference gas fuelled plasmas. Besides, in the post-pellet phase, the density fluctuation level is lower in the high iota magnetic configuration than in the standard one. In order to discriminate whether this difference is related to the differences in the plasma profiles or in the stability properties of the two configurations, gyrokinetic simulations have been carried out using the codes \\texttt{stella} and EUTERPE. The simulation results point to the plasma profile evolution after the pellet injection and the stabilization effect of the radial electric field profile as the dominant players in the stabilization of the plasma turbulence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radial profiles of density fluctuations and radial electric field, $E_r$, have been measured using Doppler reflectometry during the post-pellet enhanced confinement phase achieved, under different heating power levels and magnetic configurations, along the 2018 W7-X experimental campaign. A pronounced $E_r$-well is measured with local values as high as -40 kV/m in the radial range $ρ\\sim 0.7-0.8$ during the post-pellet enhanced confinement phase. The maximum $E_r$ intensity scales with both plasma density and Electron Cyclotron Heating (ECH) power level following a similar trend as the plasma energy content. A good agreement is found when the experimental $E_r$ profiles are compared to simulations carried out using the neoclassical codes DKES and KNOSOS. The density fluctuation level decreases from the plasma edge toward the plasma core and the drop is more pronounced in the post-pellet enhanced confinement phase than in reference gas fuelled plasmas. Besides, in the post-pellet phase, the density fluctuation level is lower in the high iota magnetic configuration than in the standard one. In order to discriminate whether this difference is related to the differences in the plasma profiles or in the stability properties of the two configurations, gyrokinetic simulations have been carried out using the codes \\texttt{stella} and EUTERPE. The simulation results point to the plasma profile evolution after the pellet injection and the stabilization effect of the radial electric field profile as the dominant players in the stabilization of the plasma turbulence."
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T16:34:07Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    16,
                    34,
                    7,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "10 figure",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph"
                },
                "arxiv_journal_ref": "Nucl. Fusion 61 (2021) 046008",
                "authors": [
                    {
                        "name": "T. Estrada"
                    },
                    {
                        "name": "D. Carralero"
                    },
                    {
                        "name": "T. Windisch"
                    },
                    {
                        "name": "E. Sánchez"
                    },
                    {
                        "name": "J. M. García-Regaña"
                    },
                    {
                        "name": "J. Martínez-Fernández"
                    },
                    {
                        "name": "A. de la Peña"
                    },
                    {
                        "name": "J. L. Velasco"
                    },
                    {
                        "name": "J. A. Alonso"
                    },
                    {
                        "name": "M. Beurskens"
                    },
                    {
                        "name": "S. Bozhenkov"
                    },
                    {
                        "name": "H. Damm"
                    },
                    {
                        "name": "G. Fuchert"
                    },
                    {
                        "name": "R. Kleiber"
                    },
                    {
                        "name": "N. Pablant"
                    },
                    {
                        "name": "E. Pasch"
                    }
                ],
                "author_detail": {
                    "name": "E. Pasch"
                },
                "author": "E. Pasch",
                "arxiv_doi": "10.1088/1741-4326/abddee"
            },
            {
                "id": "http://arxiv.org/abs/2512.15550v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15550v1",
                "title": "CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing"
                },
                "updated": "2025-12-17T15:56:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    15,
                    56,
                    32,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15550v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly applied in long-context scenarios such as multi-turn conversations. However, long contexts pose significant challenges for inference efficiency, including high memory overhead from Key-Value (KV) cache and increased latency due to excessive memory accesses. Recent methods for dynamic KV selection struggle with trade-offs: block-level indexing degrades accuracy by retrieving irrelevant KV entries, while token-level indexing incurs high latency from inefficient retrieval mechanisms. In this paper, we propose CTKVR, a novel centroid-then-token KV retrieval scheme that addresses these limitations. CTKVR leverages a key observation: query vectors adjacent in position exhibit high similarity after Rotary Position Embedding (RoPE) and share most of their top-k KV cache entries. Based on this insight, CTKVR employs a two-stage retrieval strategy: lightweight centroids are precomputed during prefilling for centroid-grained indexing, followed by token-level refinement for precise KV retrieval. This approach balances retrieval efficiency and accuracy. To further enhance performance, we implement an optimized system for indexing construction and search using CPU-GPU co-execution. Experimentally, CTKVR achieves superior performance across multiple benchmarks with less than 1% accuracy degradation. Meanwhile, CTKVR delivers 3 times and 4 times throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied in long-context scenarios such as multi-turn conversations. However, long contexts pose significant challenges for inference efficiency, including high memory overhead from Key-Value (KV) cache and increased latency due to excessive memory accesses. Recent methods for dynamic KV selection struggle with trade-offs: block-level indexing degrades accuracy by retrieving irrelevant KV entries, while token-level indexing incurs high latency from inefficient retrieval mechanisms. In this paper, we propose CTKVR, a novel centroid-then-token KV retrieval scheme that addresses these limitations. CTKVR leverages a key observation: query vectors adjacent in position exhibit high similarity after Rotary Position Embedding (RoPE) and share most of their top-k KV cache entries. Based on this insight, CTKVR employs a two-stage retrieval strategy: lightweight centroids are precomputed during prefilling for centroid-grained indexing, followed by token-level refinement for precise KV retrieval. This approach balances retrieval efficiency and accuracy. To further enhance performance, we implement an optimized system for indexing construction and search using CPU-GPU co-execution. Experimentally, CTKVR achieves superior performance across multiple benchmarks with less than 1% accuracy degradation. Meanwhile, CTKVR delivers 3 times and 4 times throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T15:56:32Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    15,
                    56,
                    32,
                    2,
                    351,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kuan Lu"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Sai Wu"
                    },
                    {
                        "name": "Yichen Yao"
                    },
                    {
                        "name": "Junhan Yang"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Wei Chu"
                    },
                    {
                        "name": "Xu Yinghui"
                    },
                    {
                        "name": "Yuan Qi"
                    },
                    {
                        "name": "Gang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Gang Chen"
                },
                "author": "Gang Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.15206v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15206v1",
                "title": "Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT"
                },
                "updated": "2025-12-17T08:56:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    8,
                    56,
                    21,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15206v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T08:56:21Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    8,
                    56,
                    21,
                    2,
                    351,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Liyu Zhang"
                    },
                    {
                        "name": "Yejia Liu"
                    },
                    {
                        "name": "Kwun Ho Liu"
                    },
                    {
                        "name": "Runxi Huang"
                    },
                    {
                        "name": "Xiaomin Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaomin Ouyang"
                },
                "author": "Xiaomin Ouyang"
            },
            {
                "id": "http://arxiv.org/abs/2510.07318v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.07318v2",
                "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Hippocampus Networks for Efficient Long-Context Modeling"
                },
                "updated": "2025-12-17T07:08:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    7,
                    8,
                    49,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.07318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.07318v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and GatedDeltaNet to augment open-weight LLMs. We also propose an efficient self-distillation training method where the base model's all parameters are frozen and only the parameters from AHNs are optimized. For inference, our method sets a default large sliding window size of 32k for attention, and AHNs activate only when the sequence length exceeds the 32k window, addressing the quadratic-complexity issue of attention that emerges at that scale. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and GatedDeltaNet to augment open-weight LLMs. We also propose an efficient self-distillation training method where the base model's all parameters are frozen and only the parameters from AHNs are optimized. For inference, our method sets a default large sliding window size of 32k for attention, and AHNs activate only when the sequence length exceeds the 32k window, addressing the quadratic-complexity issue of attention that emerges at that scale. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-08T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "arxiv_comment": "Code: https://github.com/ByteDance-Seed/AHN",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Xuehan Xiong"
                    },
                    {
                        "name": "Lai Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lai Wei"
                },
                "author": "Lai Wei"
            },
            {
                "id": "http://arxiv.org/abs/2509.25155v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.25155v2",
                "title": "Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units"
                },
                "updated": "2025-12-17T05:01:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    5,
                    1,
                    59,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.25155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.25155v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The proliferation of large language models has driven demand for long-context inference on resource-constrained edge platforms. However, deploying these models on Neural Processing Units (NPUs) presents significant challenges due to architectural mismatch: the quadratic complexity of standard attention conflicts with NPU memory and compute patterns. This paper presents a comprehensive performance analysis of causal inference operators on a modern NPU, benchmarking quadratic attention against sub-quadratic alternatives including structured state-space models and causal convolutions. Our analysis reveals a spectrum of critical bottlenecks: quadratic attention becomes severely memory-bound with catastrophic cache inefficiency, while sub-quadratic variants span from compute-bound on programmable vector cores to memory-bound by data movement. These findings provide essential insights for co-designing hardware-aware models and optimization strategies to enable efficient long-context inference on edge platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models has driven demand for long-context inference on resource-constrained edge platforms. However, deploying these models on Neural Processing Units (NPUs) presents significant challenges due to architectural mismatch: the quadratic complexity of standard attention conflicts with NPU memory and compute patterns. This paper presents a comprehensive performance analysis of causal inference operators on a modern NPU, benchmarking quadratic attention against sub-quadratic alternatives including structured state-space models and causal convolutions. Our analysis reveals a spectrum of critical bottlenecks: quadratic attention becomes severely memory-bound with catastrophic cache inefficiency, while sub-quadratic variants span from compute-bound on programmable vector cores to memory-bound by data movement. These findings provide essential insights for co-designing hardware-aware models and optimization strategies to enable efficient long-context inference on edge platforms."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T17:55:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "arxiv_comment": "IEEE HiPC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna"
            },
            {
                "id": "http://arxiv.org/abs/2512.15016v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15016v1",
                "title": "Uncovering hidden protein conformations with high bandwidth nanopore measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering hidden protein conformations with high bandwidth nanopore measurements"
                },
                "updated": "2025-12-17T02:14:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    2,
                    14,
                    4,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15016v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Advanced nanopore measurements allow structural probing of molecules with high spatial and temporal resolution. We report high signal-to-noise, 1-10 MHz bandwidth, translocation measurements of the multi-state folding of heme protein cytochrome c in KCl solution through optimally designed silicon nitride pores of 2.3-3.3 nm diameter and 3.6-3.8 nm effective thickness, and an optimal concentration of a denaturant (Gdm-Cl). The pore diameter is slightly smaller than the protein size, forcing the protein to squeeze through the pore. The sufficiently large pore thickness allows enough time for protein probing at an applied field of approximately 250 kV/cm. Through Bayesian Information Criterion score analysis, current blockades reveal six distinct levels, attributed to specific protein states. We calculate the transition probabilities between the states and the conditional probabilities of the protein leaving the pore from each state. We validate the model by simulating events and comparing them to experimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced nanopore measurements allow structural probing of molecules with high spatial and temporal resolution. We report high signal-to-noise, 1-10 MHz bandwidth, translocation measurements of the multi-state folding of heme protein cytochrome c in KCl solution through optimally designed silicon nitride pores of 2.3-3.3 nm diameter and 3.6-3.8 nm effective thickness, and an optimal concentration of a denaturant (Gdm-Cl). The pore diameter is slightly smaller than the protein size, forcing the protein to squeeze through the pore. The sufficiently large pore thickness allows enough time for protein probing at an applied field of approximately 250 kV/cm. Through Bayesian Information Criterion score analysis, current blockades reveal six distinct levels, attributed to specific protein states. We calculate the transition probabilities between the states and the conditional probabilities of the protein leaving the pore from each state. We validate the model by simulating events and comparing them to experimental data."
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T02:14:04Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    2,
                    14,
                    4,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "19 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "physics.bio-ph"
                },
                "authors": [
                    {
                        "name": "Kyril Kavetsky"
                    },
                    {
                        "name": "Sabine Hong"
                    },
                    {
                        "name": "Chih-Yuan Lin"
                    },
                    {
                        "name": "Roger Yang"
                    },
                    {
                        "name": "Marija Drndic"
                    }
                ],
                "author_detail": {
                    "name": "Marija Drndic"
                },
                "author": "Marija Drndic"
            },
            {
                "id": "http://arxiv.org/abs/2512.14975v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14975v1",
                "title": "High voltage and electrode system for a cryogenic experiment to search for the neutron electric dipole moment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High voltage and electrode system for a cryogenic experiment to search for the neutron electric dipole moment"
                },
                "updated": "2025-12-17T00:04:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    0,
                    4,
                    30,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14975v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The cryogenic approach to the search for the neutron electric dipole moment--performing the experiment in superfluid liquid helium--holds promise for a substantial increase in sensitivity, potentially enabling a sensitivity level of $10^{-28}$ e-cm. A crucial component in realizing such an experiment is the high voltage and electrode system capable of providing an electric field of 75 kV/cm. This, in turn, requires an electric potential of 635 kV to be applied to the high voltage electrode, while simultaneously satisfying other experimental constraints, such as those on heat load and magnetic noise requirements. This paper describes the outcome of a comprehensive development program addressing these challenges. It outlines the system requirements, discusses new insights into relevant physical phenomena, and details selected technical solutions with their corresponding experimental demonstrations and expected performance. The results collectively demonstrate the successful development of the necessary technology for the high-voltage and electrode system for this approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cryogenic approach to the search for the neutron electric dipole moment--performing the experiment in superfluid liquid helium--holds promise for a substantial increase in sensitivity, potentially enabling a sensitivity level of $10^{-28}$ e-cm. A crucial component in realizing such an experiment is the high voltage and electrode system capable of providing an electric field of 75 kV/cm. This, in turn, requires an electric potential of 635 kV to be applied to the high voltage electrode, while simultaneously satisfying other experimental constraints, such as those on heat load and magnetic noise requirements. This paper describes the outcome of a comprehensive development program addressing these challenges. It outlines the system requirements, discusses new insights into relevant physical phenomena, and details selected technical solutions with their corresponding experimental demonstrations and expected performance. The results collectively demonstrate the successful development of the necessary technology for the high-voltage and electrode system for this approach."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T00:04:30Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    0,
                    4,
                    30,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "20 pages, 17 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "M. A. Blatnik"
                    },
                    {
                        "name": "S. M. Clayton"
                    },
                    {
                        "name": "S. A. Currie"
                    },
                    {
                        "name": "B. W. Filippone"
                    },
                    {
                        "name": "M. Makela"
                    },
                    {
                        "name": "C. M. O'Shaughnessy"
                    },
                    {
                        "name": "N. S. Phan"
                    },
                    {
                        "name": "J. C. Ramsey"
                    },
                    {
                        "name": "G. V. Riley"
                    },
                    {
                        "name": "A. Roberts"
                    },
                    {
                        "name": "T. Sandborn"
                    },
                    {
                        "name": "T. J Schaub"
                    },
                    {
                        "name": "G. M. Seidel"
                    },
                    {
                        "name": "E. Smith"
                    },
                    {
                        "name": "I. L. Smythe"
                    },
                    {
                        "name": "J. Surbrook"
                    },
                    {
                        "name": "W. Wei"
                    },
                    {
                        "name": "W. Yao"
                    },
                    {
                        "name": "T. M. Ito"
                    }
                ],
                "author_detail": {
                    "name": "T. M. Ito"
                },
                "author": "T. M. Ito"
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.24832v2",
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching"
                },
                "updated": "2025-12-16T22:36:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    22,
                    36,
                    44,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.24832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.24832v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \\textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \\textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "arxiv_comment": "11 figures, 14pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis"
            },
            {
                "id": "http://arxiv.org/abs/2512.14946v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14946v1",
                "title": "EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving"
                },
                "updated": "2025-12-16T22:21:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    22,
                    21,
                    55,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14946v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.\n  We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.\n  We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T22:21:55Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    22,
                    21,
                    55,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2512.14866v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14866v1",
                "title": "Development of a Custom kV-amplitude, pressure-tolerant Radio-Frequency transmitter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Custom kV-amplitude, pressure-tolerant Radio-Frequency transmitter"
                },
                "updated": "2025-12-16T19:28:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    19,
                    28,
                    33,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14866v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current experiments seeking first-ever observation of Ultra-High Energy Neutrinos (UHEN) typically utilize radio frequency (RF) receiver antennas deployed in cold, radio-transparent polar ice, to measure the coherent RF signals resulting from neutrino interactions with ice molecules. Accurate calibration of the receiver response, sampling the full range of possible neutrino geometries, is necessary to estimate the energy and incoming direction of the incident neutrino. Herein, we detail the design and performance of a custom radio-frequency calibration transmitter, consisting of a battery-powered, kiloVolt-scale signal generator (`IDL' pulser) driving an antenna (South Pole UNiversity of Kansas antenna, or `SPUNK') capable of operating at pressures of 200 atmospheres. Performance was validated by lowering the transmitter into a borehole at the South Pole to a depth of 1740 m, yielding high Signal-to-Noise ratio signals at a distance of 5 km from the source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current experiments seeking first-ever observation of Ultra-High Energy Neutrinos (UHEN) typically utilize radio frequency (RF) receiver antennas deployed in cold, radio-transparent polar ice, to measure the coherent RF signals resulting from neutrino interactions with ice molecules. Accurate calibration of the receiver response, sampling the full range of possible neutrino geometries, is necessary to estimate the energy and incoming direction of the incident neutrino. Herein, we detail the design and performance of a custom radio-frequency calibration transmitter, consisting of a battery-powered, kiloVolt-scale signal generator (`IDL' pulser) driving an antenna (South Pole UNiversity of Kansas antenna, or `SPUNK') capable of operating at pressures of 200 atmospheres. Performance was validated by lowering the transmitter into a borehole at the South Pole to a depth of 1740 m, yielding high Signal-to-Noise ratio signals at a distance of 5 km from the source."
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T19:28:33Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    19,
                    28,
                    33,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "To be submitted to Nucl. Instr. and Methods",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM"
                },
                "authors": [
                    {
                        "name": "Christian Hornhuber"
                    },
                    {
                        "name": "Mohammad Ful Hossain Seikh"
                    },
                    {
                        "name": "Mark Stockham"
                    },
                    {
                        "name": "Scott Voigt"
                    },
                    {
                        "name": "Rob Young"
                    },
                    {
                        "name": "Alisa Nozdrina"
                    },
                    {
                        "name": "Sanyukta Agarwal"
                    },
                    {
                        "name": "Shoukat Ali"
                    },
                    {
                        "name": "Kenny Couberly"
                    },
                    {
                        "name": "Dave Besson"
                    }
                ],
                "author_detail": {
                    "name": "Dave Besson"
                },
                "author": "Dave Besson"
            },
            {
                "id": "http://arxiv.org/abs/2512.14699v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14699v1",
                "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives"
                },
                "updated": "2025-12-16T18:59:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    18,
                    59,
                    59,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14699v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T18:59:59Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    18,
                    59,
                    59,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "Project Page: https://sihuiji.github.io/MemFlow.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Sihui Ji"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.14681v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14681v1",
                "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing"
                },
                "updated": "2025-12-16T18:45:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    18,
                    45,
                    18,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14681v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T18:45:18Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    18,
                    45,
                    18,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lanxiang Hu"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Yichao Fu"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Yuxiong He"
                    },
                    {
                        "name": "Zhijie Deng"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.06425v6",
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "updated": "2025-12-16T16:24:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    16,
                    24,
                    22,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.06425v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.06425v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page: https://github.com/tensorgi/TPA",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao"
            },
            {
                "id": "http://arxiv.org/abs/2512.14488v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14488v1",
                "title": "Hybrid Cognitive IoT with Cooperative Caching and SWIPT-EH: A Hierarchical Reinforcement Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Cognitive IoT with Cooperative Caching and SWIPT-EH: A Hierarchical Reinforcement Learning Framework"
                },
                "updated": "2025-12-16T15:18:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    15,
                    18,
                    50,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14488v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/JIOT.2025.3632391.",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This paper proposes a hierarchical deep reinforcement learning (DRL) framework based on the soft actor-critic (SAC) algorithm for hybrid underlay-overlay cognitive Internet of Things (CIoT) networks with simultaneous wireless information and power transfer (SWIPT)-energy harvesting (EH) and cooperative caching. Unlike prior hierarchical DRL approaches that focus primarily on spectrum access or power control, our work jointly optimizes EH, hybrid access coordination, power allocation, and caching in a unified framework. The joint optimization problem is formulated as a weighted-sum multi-objective task, designed to maximize throughput and cache hit ratio while simultaneously minimizing transmission delay. In the proposed model, CIoT agents jointly optimize EH and data transmission using a learnable time switching (TS) factor. They also coordinate spectrum access under hybrid overlay-underlay paradigms and make power control and cache placement decisions while considering energy, interference, and storage constraints. Specifically, in this work, cooperative caching is used to enable overlay access, while power control is used for underlay access. A novel three-level hierarchical SAC (H-SAC) agent decomposes the mixed discrete-continuous action space into modular subproblems, improving scalability and convergence over flat DRL methods. The high-level policy adjusts the TS factor, the mid-level policy manages spectrum access coordination and cache sharing, and the low-level policy decides transmit power and caching actions for both the CIoT agent and PU content. Simulation results show that the proposed hierarchical SAC approach significantly outperforms benchmark and greedy strategies. It achieves better performance in terms of average sum rate, delay, cache hit ratio, and energy efficiency, even under channel fading and uncertain conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a hierarchical deep reinforcement learning (DRL) framework based on the soft actor-critic (SAC) algorithm for hybrid underlay-overlay cognitive Internet of Things (CIoT) networks with simultaneous wireless information and power transfer (SWIPT)-energy harvesting (EH) and cooperative caching. Unlike prior hierarchical DRL approaches that focus primarily on spectrum access or power control, our work jointly optimizes EH, hybrid access coordination, power allocation, and caching in a unified framework. The joint optimization problem is formulated as a weighted-sum multi-objective task, designed to maximize throughput and cache hit ratio while simultaneously minimizing transmission delay. In the proposed model, CIoT agents jointly optimize EH and data transmission using a learnable time switching (TS) factor. They also coordinate spectrum access under hybrid overlay-underlay paradigms and make power control and cache placement decisions while considering energy, interference, and storage constraints. Specifically, in this work, cooperative caching is used to enable overlay access, while power control is used for underlay access. A novel three-level hierarchical SAC (H-SAC) agent decomposes the mixed discrete-continuous action space into modular subproblems, improving scalability and convergence over flat DRL methods. The high-level policy adjusts the TS factor, the mid-level policy manages spectrum access coordination and cache sharing, and the low-level policy decides transmit power and caching actions for both the CIoT agent and PU content. Simulation results show that the proposed hierarchical SAC approach significantly outperforms benchmark and greedy strategies. It achieves better performance in terms of average sum rate, delay, cache hit ratio, and energy efficiency, even under channel fading and uncertain conditions."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T15:18:50Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    15,
                    18,
                    50,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "Published in IEEE Internet of Things Journal (Early Access), 2025. This arXiv version is the authors' accepted manuscript",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "arxiv_journal_ref": "IEEE Internet of Things Journal, Early Access, 2025",
                "authors": [
                    {
                        "name": "Nadia Abdolkhani"
                    },
                    {
                        "name": "Walaa Hamouda"
                    }
                ],
                "author_detail": {
                    "name": "Walaa Hamouda"
                },
                "author": "Walaa Hamouda",
                "arxiv_doi": "10.1109/JIOT.2025.3632391."
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.21230v3",
                "title": "Kimina Lean Server: A High-Performance Lean Server for Large-Scale Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: A High-Performance Lean Server for Large-Scale Verification"
                },
                "updated": "2025-12-16T14:04:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    14,
                    4,
                    14,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.21230v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.21230v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce the Kimina Lean Server, an open-source project designed as a high-performance verifier for reinforcement learning pipelines. Built on top of the Lean REPL (Read-Eval-Print Loop) maintained by the Lean FRO, our server combines server-side parallelism by managing multiple Lean processes in parallel with a Least Recently Used (LRU) caching mechanism that reuses Lean imports across requests. On the client side, a lightweight Python package enables submitting proof batches and receiving Lean feedback, including extracted tactics and tactic states.\n  Together, these features enable a scalable workflow for large-scale verification and data extraction. In our experiments, the Kimina Lean Server outperforms previous Lean interaction tools, achieving a 1.5 to 2 times speedup in verification time. Moreover, its improved efficiency has enabled its use in the large-scale training of state-of-the-art models such as Kimina-Prover.\n  We hope that our open-source project will support the neural theorem proving community and accelerate future progress by enabling efficient large-scale verification and proof data extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project designed as a high-performance verifier for reinforcement learning pipelines. Built on top of the Lean REPL (Read-Eval-Print Loop) maintained by the Lean FRO, our server combines server-side parallelism by managing multiple Lean processes in parallel with a Least Recently Used (LRU) caching mechanism that reuses Lean imports across requests. On the client side, a lightweight Python package enables submitting proof batches and receiving Lean feedback, including extracted tactics and tactic states.\n  Together, these features enable a scalable workflow for large-scale verification and data extraction. In our experiments, the Kimina Lean Server outperforms previous Lean interaction tools, achieving a 1.5 to 2 times speedup in verification time. Moreover, its improved efficiency has enabled its use in the large-scale training of state-of-the-art models such as Kimina-Prover.\n  We hope that our open-source project will support the neural theorem proving community and accelerate future progress by enabling efficient large-scale verification and proof data extraction."
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "arxiv_comment": "Accepted to the 5th MATH-AI Workshop at NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LO"
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.14151v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14151v1",
                "title": "Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement"
                },
                "updated": "2025-12-16T07:16:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    7,
                    16,
                    10,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14151v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T07:16:10Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    7,
                    16,
                    10,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Songze Liu"
                    },
                    {
                        "name": "Hongkun Du"
                    },
                    {
                        "name": "Shaowen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shaowen Wang"
                },
                "author": "Shaowen Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.14142v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14142v1",
                "title": "Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents"
                },
                "updated": "2025-12-16T06:55:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    6,
                    55,
                    10,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14142v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T06:55:10Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    6,
                    55,
                    10,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hongqiu Ni"
                    },
                    {
                        "name": "Jiabao Zhang"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Zilong Wang"
                    },
                    {
                        "name": "Ruiqi Wu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Haisheng Tan"
                    }
                ],
                "author_detail": {
                    "name": "Haisheng Tan"
                },
                "author": "Haisheng Tan"
            },
            {
                "id": "http://arxiv.org/abs/2511.22333v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22333v2",
                "title": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel"
                },
                "updated": "2025-12-16T05:44:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    5,
                    44,
                    34,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22333v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: one-query-per-CTA execution repeatedly loads shared prefix KV cache, while one-size-fits-all tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.\n  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 53.5% on average and TPOT by 17.0-93.1% under the same configurations against state-of-the-art attention kernels. PAT's source code is publicly available at https://github.com/flashserve/PAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: one-query-per-CTA execution repeatedly loads shared prefix KV cache, while one-size-fits-all tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.\n  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 53.5% on average and TPOT by 17.0-93.1% under the same configurations against state-of-the-art attention kernels. PAT's source code is publicly available at https://github.com/flashserve/PAT."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T11:10:30Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    11,
                    10,
                    30,
                    3,
                    331,
                    0
                ],
                "arxiv_comment": "Accepted by ASPLOS'26, code available at https://github.com/flashserve/PAT",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jinjun Yi"
                    },
                    {
                        "name": "Zhixin Zhao"
                    },
                    {
                        "name": "Yitao Hu"
                    },
                    {
                        "name": "Ke Yan"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Laiping Zhao"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Wenxin Li"
                    },
                    {
                        "name": "Keqiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqiu Li"
                },
                "author": "Keqiu Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.14096v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14096v1",
                "title": "OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration"
                },
                "updated": "2025-12-16T05:11:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    5,
                    11,
                    54,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14096v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T05:11:54Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    5,
                    11,
                    54,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "29 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ruitong Sun"
                    },
                    {
                        "name": "Tianze Yang"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Jin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jin Sun"
                },
                "author": "Jin Sun"
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.13109v3",
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "updated": "2025-12-16T04:58:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    4,
                    58,
                    49,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.13109v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.13109v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\\times$ speedup compared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\\times$ speedup compared to SOTA KV retrieval methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Danning Ke"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.14080v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14080v1",
                "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations"
                },
                "updated": "2025-12-16T04:39:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    4,
                    39,
                    10,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14080v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T04:39:10Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    4,
                    39,
                    10,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Wentao Guo"
                    },
                    {
                        "name": "Mayank Mishra"
                    },
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao"
            },
            {
                "id": "http://arxiv.org/abs/2512.14067v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14067v1",
                "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed"
                },
                "updated": "2025-12-16T04:12:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    4,
                    12,
                    17,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14067v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T04:12:17Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    4,
                    12,
                    17,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lexington Whalen"
                    },
                    {
                        "name": "Zhifan Ye"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Maksim Khadkevich"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Yingyan Celine Lin"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov"
            },
            {
                "id": "http://arxiv.org/abs/2512.14029v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14029v1",
                "title": "Cooperative Caching Towards Efficient Spectrum Utilization in Cognitive-IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Caching Towards Efficient Spectrum Utilization in Cognitive-IoT Networks"
                },
                "updated": "2025-12-16T02:49:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    2,
                    49,
                    43,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14029v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ICC52391.2025.11160803",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "In cognitive Internet of Things (CIoT) networks, efficient spectrum sharing is essential to address increasing wireless demands. This paper presents a novel deep reinforcement learning (DRL)-based approach for joint cooperative caching and spectrum access coordination in CIoT networks, enabling the CIoT agents to collaborate with primary users (PUs) by caching PU content and serving their requests, fostering mutual benefits. The proposed DRL framework jointly optimizes caching policy and spectrum access under challenging conditions. Unlike traditional cognitive radio (CR) methods, where CIoT agents vacate the spectrum for PUs, or relaying techniques, which merely support spectrum sharing, caching brings data closer to the edge, reducing latency by minimizing retrieval distance. Simulations demonstrate that our approach outperforms others in lowering latency, increasing CIoT and PU cache hit rates, and enhancing network throughput. This approach redefines spectrum sharing, offering a fresh perspective on CIoT network design and illustrating the potential of DRL-guided caching to highlight the benefits of collaboration over dynamic spectrum access scenarios, elevating CIoT performance under constrained resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cognitive Internet of Things (CIoT) networks, efficient spectrum sharing is essential to address increasing wireless demands. This paper presents a novel deep reinforcement learning (DRL)-based approach for joint cooperative caching and spectrum access coordination in CIoT networks, enabling the CIoT agents to collaborate with primary users (PUs) by caching PU content and serving their requests, fostering mutual benefits. The proposed DRL framework jointly optimizes caching policy and spectrum access under challenging conditions. Unlike traditional cognitive radio (CR) methods, where CIoT agents vacate the spectrum for PUs, or relaying techniques, which merely support spectrum sharing, caching brings data closer to the edge, reducing latency by minimizing retrieval distance. Simulations demonstrate that our approach outperforms others in lowering latency, increasing CIoT and PU cache hit rates, and enhancing network throughput. This approach redefines spectrum sharing, offering a fresh perspective on CIoT network design and illustrating the potential of DRL-guided caching to highlight the benefits of collaboration over dynamic spectrum access scenarios, elevating CIoT performance under constrained resources."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T02:49:43Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    2,
                    49,
                    43,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "Published in Proc. IEEE ICC 2025. This is the authors' accepted manuscript version",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "arxiv_journal_ref": "Proc. IEEE ICC, 2025, pp. 1310-1315",
                "authors": [
                    {
                        "name": "Nadia Abdolkhani"
                    },
                    {
                        "name": "Walaa Hamouda"
                    }
                ],
                "author_detail": {
                    "name": "Walaa Hamouda"
                },
                "author": "Walaa Hamouda",
                "arxiv_doi": "10.1109/ICC52391.2025.11160803"
            },
            {
                "id": "http://arxiv.org/abs/2512.13615v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.13615v1",
                "title": "Hyper-Minrank: A Unified Hypergraph Characterization of Multi-Sender Index Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper-Minrank: A Unified Hypergraph Characterization of Multi-Sender Index Coding"
                },
                "updated": "2025-12-15T18:08:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    18,
                    8,
                    9,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.13615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.13615v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work introduces a hypergraph formulation that generalizes the classical paradigm of Bar-Yossef et al. to the multi-sender index coding (MSIC) setting. Central to the model is a 4-regular side-information hypergraph G, a new adjacency representation A_G = [A_1 ... A_N], and a simple fitting criterion for sub-hypergraph validity, in the presence of specially designed hyperedges that capture both side information and cross-sender signal cancellation. This formulation establishes a tight achievability-converse equivalence for the general N-sender, K-receiver problem: every valid fitting induces a valid linear multi-sender index code, every linear code induces a valid fitting, and the optimal scalar linear broadcast length equals the hyper-minrank l**lin(G) = hyperminrank(G) = min*{A fits G} sum_{n=1}^N rank(A_n). Beyond this exact characterization, the approach yields hypergraph analogues of Haemers-type bounds on the broadcast length, including a clique-cover upper bound and a lower bound via the clique number of a carefully defined complement hypergraph. Algorithmically, we provide an exact procedure to compute hyperminrank(G), and show that in certain regimes its complexity is asymptotically better than approximate LT-CMAR solutions. The framework captures well-known settings such as embedded index coding, and applies directly to multi-sender cache-aided communications, coded computation, distributed storage, and edge/satellite systems, where hyperminrank can serve as a unified design target.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces a hypergraph formulation that generalizes the classical paradigm of Bar-Yossef et al. to the multi-sender index coding (MSIC) setting. Central to the model is a 4-regular side-information hypergraph G, a new adjacency representation A_G = [A_1 ... A_N], and a simple fitting criterion for sub-hypergraph validity, in the presence of specially designed hyperedges that capture both side information and cross-sender signal cancellation. This formulation establishes a tight achievability-converse equivalence for the general N-sender, K-receiver problem: every valid fitting induces a valid linear multi-sender index code, every linear code induces a valid fitting, and the optimal scalar linear broadcast length equals the hyper-minrank l**lin(G) = hyperminrank(G) = min*{A fits G} sum_{n=1}^N rank(A_n). Beyond this exact characterization, the approach yields hypergraph analogues of Haemers-type bounds on the broadcast length, including a clique-cover upper bound and a lower bound via the clique number of a carefully defined complement hypergraph. Algorithmically, we provide an exact procedure to compute hyperminrank(G), and show that in certain regimes its complexity is asymptotically better than approximate LT-CMAR solutions. The framework captures well-known settings such as embedded index coding, and applies directly to multi-sender cache-aided communications, coded computation, distributed storage, and edge/satellite systems, where hyperminrank can serve as a unified design target."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T18:08:09Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    18,
                    8,
                    9,
                    0,
                    349,
                    0
                ],
                "arxiv_comment": "46 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Ali Khalesi"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia"
            },
            {
                "id": "http://arxiv.org/abs/2512.13586v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.13586v1",
                "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding"
                },
                "updated": "2025-12-15T17:41:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    17,
                    41,
                    19,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.13586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.13586v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T17:41:19Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    17,
                    41,
                    19,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.13109v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.13109v1",
                "title": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing"
                },
                "updated": "2025-12-15T09:04:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    9,
                    4,
                    6,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.13109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.13109v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T09:04:06Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    9,
                    4,
                    6,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zewen Qiang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.07155v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07155v3",
                "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics"
                },
                "updated": "2025-12-15T08:33:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    8,
                    33,
                    55,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07155v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07155v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T04:39:12Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    4,
                    39,
                    12,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "Please visit our project page at https://cmlab-korea.github.io/CHIMERA/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dahyeon Kye"
                    },
                    {
                        "name": "Jeahun Sung"
                    },
                    {
                        "name": "Mingyu Jeon"
                    },
                    {
                        "name": "Jihyong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Jihyong Oh"
                },
                "author": "Jihyong Oh"
            },
            {
                "id": "http://arxiv.org/abs/2512.13019v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.13019v1",
                "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SneakPeek: Future-Guided Instructional Streaming Video Generation"
                },
                "updated": "2025-12-15T06:32:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    6,
                    32,
                    57,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.13019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.13019v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T06:32:57Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    6,
                    32,
                    57,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Cheeun Hong"
                    },
                    {
                        "name": "German Barquero"
                    },
                    {
                        "name": "Fadime Sener"
                    },
                    {
                        "name": "Markos Georgopoulos"
                    },
                    {
                        "name": "Edgar Schönfeld"
                    },
                    {
                        "name": "Stefan Popov"
                    },
                    {
                        "name": "Yuming Du"
                    },
                    {
                        "name": "Oscar Mañas"
                    },
                    {
                        "name": "Albert Pumarola"
                    }
                ],
                "author_detail": {
                    "name": "Albert Pumarola"
                },
                "author": "Albert Pumarola"
            },
            {
                "id": "http://arxiv.org/abs/2512.12990v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12990v1",
                "title": "SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference"
                },
                "updated": "2025-12-15T05:33:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    5,
                    33,
                    7,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12990v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T05:33:07Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    5,
                    33,
                    7,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Yuseon Choi"
                    },
                    {
                        "name": "Sangjin Kim"
                    },
                    {
                        "name": "Jungjun Oh"
                    },
                    {
                        "name": "Gwangtae Park"
                    },
                    {
                        "name": "Byeongcheol Kim"
                    },
                    {
                        "name": "Hoi-Jun Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Hoi-Jun Yoo"
                },
                "author": "Hoi-Jun Yoo"
            },
            {
                "id": "http://arxiv.org/abs/2512.11203v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11203v2",
                "title": "AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path"
                },
                "updated": "2025-12-15T05:13:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    5,
                    13,
                    40,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11203v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T01:28:22Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    1,
                    28,
                    22,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhengyang Yu"
                    },
                    {
                        "name": "Akio Hayakawa"
                    },
                    {
                        "name": "Masato Ishii"
                    },
                    {
                        "name": "Qingtao Yu"
                    },
                    {
                        "name": "Takashi Shibuya"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji"
            },
            {
                "id": "http://arxiv.org/abs/2512.12963v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12963v1",
                "title": "SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer"
                },
                "updated": "2025-12-15T04:02:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    4,
                    2,
                    14,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12963v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T04:02:14Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    4,
                    2,
                    14,
                    0,
                    349,
                    0
                ],
                "arxiv_comment": "Accepted to WACV 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Luan Thanh Trinh"
                    },
                    {
                        "name": "Kenji Doi"
                    },
                    {
                        "name": "Atsuki Osanai"
                    }
                ],
                "author_detail": {
                    "name": "Atsuki Osanai"
                },
                "author": "Atsuki Osanai"
            },
            {
                "id": "http://arxiv.org/abs/2512.11306v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11306v2",
                "title": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training"
                },
                "updated": "2025-12-15T02:21:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    2,
                    21,
                    28,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11306v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable \"warm-star\" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable \"warm-star\" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T06:03:56Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    6,
                    3,
                    56,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "17 pages, 15 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Tianyuan Wu"
                    },
                    {
                        "name": "Lunxi Cao"
                    },
                    {
                        "name": "Yining Wei"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Yuheng Zhao"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang"
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.17756v2",
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling"
                },
                "updated": "2025-12-14T17:45:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    17,
                    45,
                    53,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.17756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.17756v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SUPERGEN, an efficient tile-based framework for ultra-high-resolution video generation. SUPERGEN features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SUPERGEN incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SUPERGEN also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations show that SUPERGEN maximizes performance gains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SUPERGEN, an efficient tile-based framework for ultra-high-resolution video generation. SUPERGEN features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SUPERGEN incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SUPERGEN also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations show that SUPERGEN maximizes performance gains while achieving high output quality across various benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.12876v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12876v2",
                "title": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making"
                },
                "updated": "2025-12-14T15:38:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    15,
                    38,
                    21,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12876v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T02:09:18Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    2,
                    9,
                    18,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Extended version of a submission to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Heyang Ma"
                    },
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Qipeng Yang"
                    },
                    {
                        "name": "Zijun Fan"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Haifeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Zhang"
                },
                "author": "Haifeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.06029v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06029v3",
                "title": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving"
                },
                "updated": "2025-12-14T14:18:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    14,
                    18,
                    27,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06029v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06029v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T14:52:43Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    14,
                    52,
                    43,
                    5,
                    312,
                    0
                ],
                "arxiv_comment": "aaai26 camera-ready version, 10 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hui Zeng"
                    },
                    {
                        "name": "Daming Zhao"
                    },
                    {
                        "name": "Pengfei Yang"
                    },
                    {
                        "name": "WenXuan Hou"
                    },
                    {
                        "name": "Tianyang Zheng"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Weiye Ji"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai"
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.16055v2",
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "updated": "2025-12-14T11:12:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    11,
                    12,
                    56,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.16055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.16055v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We examine the use of different randomisation policies for stochastic gradient algorithms used in sampling, based on first-order (or overdamped) Langevin dynamics, the most popular of which is known as Stochastic Gradient Langevin Dynamics. Conventionally, this algorithm is combined with a specific stochastic gradient strategy, called Robbins-Monro. In this work, we study an alternative strategy, Random Reshuffling, and show convincingly that it leads to improved performance via: a) a proof of reduced bias in the Wasserstein metric for strongly convex, gradient Lipschitz potentials; b) an analytical demonstration of reduced bias for a Gaussian model problem; and c) an empirical demonstration of reduced bias in numerical experiments for some logistic regression problems. This is especially important since Random Reshuffling is typically more efficient due to memory access and cache reasons. Such acceleration for the Random Reshuffling policy is familiar from the optimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic gradient algorithms used in sampling, based on first-order (or overdamped) Langevin dynamics, the most popular of which is known as Stochastic Gradient Langevin Dynamics. Conventionally, this algorithm is combined with a specific stochastic gradient strategy, called Robbins-Monro. In this work, we study an alternative strategy, Random Reshuffling, and show convincingly that it leads to improved performance via: a) a proof of reduced bias in the Wasserstein metric for strongly convex, gradient Lipschitz potentials; b) an analytical demonstration of reduced bias for a Gaussian model problem; and c) an empirical demonstration of reduced bias in numerical experiments for some logistic regression problems. This is especially important since Random Reshuffling is typically more efficient due to memory access and cache reasons. Such acceleration for the Random Reshuffling policy is familiar from the optimisation literature on stochastic gradient descent."
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "arxiv_comment": "23 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "math.NA"
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley"
            },
            {
                "id": "http://arxiv.org/abs/2512.12604v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12604v1",
                "title": "No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching"
                },
                "updated": "2025-12-14T09:02:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    9,
                    2,
                    18,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12604v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-14T09:02:18Z",
                "published_parsed": [
                    2025,
                    12,
                    14,
                    9,
                    2,
                    18,
                    6,
                    348,
                    0
                ],
                "arxiv_comment": "Project page: https://thu-accdiff.github.io/xslim-page/ Code: https://github.com/THU-AccDiff/xslim",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tingyan Wen"
                    },
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Yihuang Chen"
                    },
                    {
                        "name": "Xing Zhou"
                    },
                    {
                        "name": "Lifei Zhu"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.12595v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12595v1",
                "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation"
                },
                "updated": "2025-12-14T08:28:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    8,
                    28,
                    50,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12595v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-14T08:28:50Z",
                "published_parsed": [
                    2025,
                    12,
                    14,
                    8,
                    28,
                    50,
                    6,
                    348,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Karthikeya KV"
                    }
                ],
                "author_detail": {
                    "name": "Karthikeya KV"
                },
                "author": "Karthikeya KV"
            },
            {
                "id": "http://arxiv.org/abs/2505.18231v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.18231v2",
                "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache"
                },
                "updated": "2025-12-14T08:17:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    8,
                    17,
                    35,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.18231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.18231v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-23T12:40:07Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo"
            },
            {
                "id": "http://arxiv.org/abs/2512.12498v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12498v1",
                "title": "Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention"
                },
                "updated": "2025-12-13T23:53:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    13,
                    23,
                    53,
                    0,
                    5,
                    347,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12498v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the \"platinum minutes\" and the broader \"golden hour\" window in time-critical UAV-driven search-and-rescue and combat casualty care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the \"platinum minutes\" and the broader \"golden hour\" window in time-critical UAV-driven search-and-rescue and combat casualty care."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-13T23:53:00Z",
                "published_parsed": [
                    2025,
                    12,
                    13,
                    23,
                    53,
                    0,
                    5,
                    347,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tasweer Ahmad"
                    },
                    {
                        "name": "Arindam Sikdar"
                    },
                    {
                        "name": "Sandip Pradhan"
                    },
                    {
                        "name": "Ardhendu Behera"
                    }
                ],
                "author_detail": {
                    "name": "Ardhendu Behera"
                },
                "author": "Ardhendu Behera"
            },
            {
                "id": "http://arxiv.org/abs/2512.12243v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12243v1",
                "title": "CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement"
                },
                "updated": "2025-12-13T08:42:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    13,
                    8,
                    42,
                    18,
                    5,
                    347,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12243v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \\textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \\textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \\textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \\emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\\% and 50\\%) demonstrates a geometric mean speedup of 2.46$\\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\\% to 84.8\\% (+6.9 percentage points), reduce total runtime by 70.1\\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \\textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \\textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \\textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \\emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\\% and 50\\%) demonstrates a geometric mean speedup of 2.46$\\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\\% to 84.8\\% (+6.9 percentage points), reduce total runtime by 70.1\\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-13T08:42:18Z",
                "published_parsed": [
                    2025,
                    12,
                    13,
                    8,
                    42,
                    18,
                    5,
                    347,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "HT To"
                    },
                    {
                        "name": "S Nguyen"
                    },
                    {
                        "name": "NH Pham"
                    }
                ],
                "author_detail": {
                    "name": "NH Pham"
                },
                "author": "NH Pham"
            },
            {
                "id": "http://arxiv.org/abs/2510.05476v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.05476v2",
                "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications"
                },
                "updated": "2025-12-13T06:18:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    13,
                    6,
                    18,
                    18,
                    5,
                    347,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.05476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.05476v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3712285.3759816",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Message Passing Interface (MPI) is a foundational programming model for high-performance computing. MPI libraries traditionally employ network interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP and RoCE) with complex software stacks for cross-node communication. We present cMPI, the first work to optimize MPI point-to-point communication (both one-sided and two-sided) using CXL memory sharing on a real CXL platform, transforming cross-node communication into memory transactions and data copies within CXL memory, bypassing traditional network protocols. We analyze performance across various interconnects and find that CXL memory sharing achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in small- and medium-scale clusters. We address challenges of CXL memory sharing for MPI communication, including data object management over the dax representation [50], cache coherence, and atomic operations. Overall, cMPI outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x and 72x in latency and bandwidth, respectively, for small messages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message Passing Interface (MPI) is a foundational programming model for high-performance computing. MPI libraries traditionally employ network interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP and RoCE) with complex software stacks for cross-node communication. We present cMPI, the first work to optimize MPI point-to-point communication (both one-sided and two-sided) using CXL memory sharing on a real CXL platform, transforming cross-node communication into memory transactions and data copies within CXL memory, bypassing traditional network protocols. We analyze performance across various interconnects and find that CXL memory sharing achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in small- and medium-scale clusters. We address challenges of CXL memory sharing for MPI communication, including data object management over the dax representation [50], cache coherence, and atomic operations. Overall, cMPI outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x and 72x in latency and bandwidth, respectively, for small messages."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T00:32:45Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Jongryool Kim"
                    },
                    {
                        "name": "Byungil Koh"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_doi": "10.1145/3712285.3759816"
            },
            {
                "id": "http://arxiv.org/abs/2512.12091v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12091v1",
                "title": "GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes"
                },
                "updated": "2025-12-12T23:46:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    23,
                    46,
                    5,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12091v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache\n  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks\n  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core\n  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict\n  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.\n  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To\n  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),\n  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the\n  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,\n  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache\n  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks\n  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core\n  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict\n  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.\n  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To\n  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),\n  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the\n  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,\n  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T23:46:05Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    23,
                    46,
                    5,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "36 pages, 1 figure, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Mohammad Pivezhandi"
                    },
                    {
                        "name": "Mahdi Banisharif"
                    },
                    {
                        "name": "Saeed Bakhshan"
                    },
                    {
                        "name": "Abusayeed Saifullah"
                    },
                    {
                        "name": "Ali Jannesari"
                    }
                ],
                "author_detail": {
                    "name": "Ali Jannesari"
                },
                "author": "Ali Jannesari"
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.13059v2",
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "updated": "2025-12-12T19:51:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    19,
                    51,
                    26,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.13059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.13059v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "arxiv_comment": "15 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "NeurIPS 2025",
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami"
            },
            {
                "id": "http://arxiv.org/abs/2512.12008v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12008v1",
                "title": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning"
                },
                "updated": "2025-12-12T19:50:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    19,
                    50,
                    34,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12008v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T19:50:34Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    19,
                    50,
                    34,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Aadi Palnitkar"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Hyunwoo Jae"
                    },
                    {
                        "name": "Kyle Rui Sang"
                    },
                    {
                        "name": "Dixi Yao"
                    },
                    {
                        "name": "Shayan Shabihi"
                    },
                    {
                        "name": "Fuheng Zhao"
                    },
                    {
                        "name": "Tian Li"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Kunpeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kunpeng Zhang"
                },
                "author": "Kunpeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.11769v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11769v1",
                "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models"
                },
                "updated": "2025-12-12T18:30:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    18,
                    30,
                    45,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11769v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T18:30:45Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    18,
                    30,
                    45,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "10 pages, 3 figures. Code and integration scripts will be released at this http URL: https://github.com/JijiKing-Sam/BLURR-A-Boosted-Low-Resource-Inference-for-Vision-Language-Action-Model",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Zhengqing Yuan"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Kaiwen Shi"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye"
            },
            {
                "id": "http://arxiv.org/abs/2512.11550v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11550v1",
                "title": "PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration"
                },
                "updated": "2025-12-12T13:35:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    13,
                    35,
                    9,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11550v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T13:35:09Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    13,
                    35,
                    9,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zhiheng Chen"
                    },
                    {
                        "name": "Ye Qiao"
                    },
                    {
                        "name": "Sitao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Sitao Huang"
                },
                "author": "Sitao Huang"
            },
            {
                "id": "http://arxiv.org/abs/2511.00473v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00473v2",
                "title": "Drinfeld associators and Kashiwara-Vergne associators in higher genera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drinfeld associators and Kashiwara-Vergne associators in higher genera"
                },
                "updated": "2025-12-12T11:32:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    11,
                    32,
                    39,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00473v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by Alekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in relation to the formality problem of the Goldman-Turaev Lie bialgebra on an oriented surface with a framing, is directly constructed from a genus $g$ analogue of a Drinfeld associator formulated by Gonzalez, which we call a Gonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus 0. The framing is automatically determined from the choice of a Gonzalez-Drinfeld associator, and in the case of genus 1, we show that only one particular framing is realised by our construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by Alekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in relation to the formality problem of the Goldman-Turaev Lie bialgebra on an oriented surface with a framing, is directly constructed from a genus $g$ analogue of a Drinfeld associator formulated by Gonzalez, which we call a Gonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus 0. The framing is automatically determined from the choice of a Gonzalez-Drinfeld associator, and in the case of genus 1, we show that only one particular framing is realised by our construction."
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-01T09:53:47Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "arxiv_comment": "40 pages. Minor corrections",
                "arxiv_primary_category": {
                    "term": "math.QA"
                },
                "authors": [
                    {
                        "name": "Toyo Taniguchi"
                    }
                ],
                "author_detail": {
                    "name": "Toyo Taniguchi"
                },
                "author": "Toyo Taniguchi"
            },
            {
                "id": "http://arxiv.org/abs/2512.11458v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11458v1",
                "title": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation"
                },
                "updated": "2025-12-12T10:53:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    53,
                    51,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11458v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T10:53:51Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    53,
                    51,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jingmin Zhu"
                    },
                    {
                        "name": "Anqi Zhu"
                    },
                    {
                        "name": "Hossein Rahmani"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    }
                ],
                "author_detail": {
                    "name": "Qiuhong Ke"
                },
                "author": "Qiuhong Ke"
            },
            {
                "id": "http://arxiv.org/abs/2512.11431v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11431v1",
                "title": "Proving DNSSEC Correctness: A Formal Approach to Secure Domain Name Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proving DNSSEC Correctness: A Formal Approach to Secure Domain Name Resolution"
                },
                "updated": "2025-12-12T10:12:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    12,
                    6,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11431v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Domain Name System Security Extensions (DNSSEC) are critical for preventing DNS spoofing, yet its specifications contain ambiguities and vulnerabilities that elude traditional \"break-and-fix\" approaches. A holistic, foundational security analysis of the protocol has thus remained an open problem. This paper introduces DNSSECVerif, the first framework for comprehensive, automated formal security analysis of the DNSSEC protocol suite. Built on the SAPIC+ symbolic verifier, our high-fidelity model captures protocol-level interactions, including cryptographic operations and stateful caching with fine-grained concurrency control. Using DNSSECVerif, we formally prove four of DNSSEC's core security guarantees and uncover critical ambiguities in the standards--notably, the insecure coexistence of NSEC and NSEC3. Our model also automatically rediscovers three classes of known attacks, demonstrating fundamental weaknesses in the protocol design. To bridge the model-to-reality gap, we validate our findings through targeted testing of mainstream DNS software and a large-scale measurement study of over 2.2 million open resolvers, confirming the real-world impact of these flaws. Our work provides crucial, evidence-based recommendations for hardening DNSSEC specifications and implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Domain Name System Security Extensions (DNSSEC) are critical for preventing DNS spoofing, yet its specifications contain ambiguities and vulnerabilities that elude traditional \"break-and-fix\" approaches. A holistic, foundational security analysis of the protocol has thus remained an open problem. This paper introduces DNSSECVerif, the first framework for comprehensive, automated formal security analysis of the DNSSEC protocol suite. Built on the SAPIC+ symbolic verifier, our high-fidelity model captures protocol-level interactions, including cryptographic operations and stateful caching with fine-grained concurrency control. Using DNSSECVerif, we formally prove four of DNSSEC's core security guarantees and uncover critical ambiguities in the standards--notably, the insecure coexistence of NSEC and NSEC3. Our model also automatically rediscovers three classes of known attacks, demonstrating fundamental weaknesses in the protocol design. To bridge the model-to-reality gap, we validate our findings through targeted testing of mainstream DNS software and a large-scale measurement study of over 2.2 million open resolvers, confirming the real-world impact of these flaws. Our work provides crucial, evidence-based recommendations for hardening DNSSEC specifications and implementations."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T10:12:06Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    12,
                    6,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Qifan Zhang"
                    },
                    {
                        "name": "Zilin Shen"
                    },
                    {
                        "name": "Imtiaz Karim"
                    },
                    {
                        "name": "Elisa Bertino"
                    },
                    {
                        "name": "Zhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Li"
                },
                "author": "Zhou Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.11423v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11423v1",
                "title": "JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion"
                },
                "updated": "2025-12-12T10:06:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    6,
                    1,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11423v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T10:06:01Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    6,
                    1,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chaochao Li"
                    },
                    {
                        "name": "Ruikui Wang"
                    },
                    {
                        "name": "Liangbo Zhou"
                    },
                    {
                        "name": "Jinheng Feng"
                    },
                    {
                        "name": "Huaishao Luo"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Youzheng Wu"
                    },
                    {
                        "name": "Xiaodong He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong He"
                },
                "author": "Xiaodong He"
            },
            {
                "id": "http://arxiv.org/abs/2512.11274v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11274v1",
                "title": "FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion"
                },
                "updated": "2025-12-12T04:34:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    4,
                    34,
                    53,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11274v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \\textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \\textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T04:34:53Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    4,
                    34,
                    53,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "AAAI-2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiangyang Luo"
                    },
                    {
                        "name": "Qingyu Li"
                    },
                    {
                        "name": "Xiaokun Liu"
                    },
                    {
                        "name": "Wenyu Qin"
                    },
                    {
                        "name": "Miao Yang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Shao-Lun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shao-Lun Huang"
                },
                "author": "Shao-Lun Huang"
            },
            {
                "id": "http://arxiv.org/abs/2512.11264v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11264v1",
                "title": "Electrical Stability of Cr2O3/\\b{eta}-Ga2O3 and NiOx/\\b{eta}-Ga2O3 Heterojunction Diodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrical Stability of Cr2O3/\\b{eta}-Ga2O3 and NiOx/\\b{eta}-Ga2O3 Heterojunction Diodes"
                },
                "updated": "2025-12-12T03:57:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    3,
                    57,
                    52,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11264v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work reports the electrical characteristics comparison study between Cr2O3 and NiOx based heterojunction diodes (HJD) on halide vapor phase epitaxy (HVPE) grown \\b{eta}-Ga2O3 epitaxial layers. Both as-fabricated Cr2O3 and NiOx HJDs exhibited forward current density in a range of 130-150 A/cm^2 at 5 V with rectifying ratios >10^10 and a reverse leakage current density at 10^-8 A/cm^2 at -5 V. The differential specific on-resistance of Cr2O3 and NiOx HJDs was 12.01 mΩ*cm^2 and 12.05 mΩ*cm^2, respectively. Breakdown voltages of Cr2O3 HJDs ranged from 1.4-1.9 kV and 1.5-2.3 kV for NiOx HJDs. Theoretical band alignment between Cr2O3 and \\b{eta}-Ga2O3 was calculated from first principles. The ambient exposed NiOx/HVPE \\b{eta}-Ga2O3 HJDs forward current density degraded after 10 days while that of Cr2O3/HVPE \\b{eta}-Ga2O3 HJDs remained nearly unchanged after the same amount of time. It was later confirmed that the ambient exposed sputtered NiOx sheet resistance (Rsh) degradation gave rise to the reduction of the forward current density of the NiOx based HJDs, and water (H2O) was qualitatively determined to be the agent attributed to the forward conduction degradation by measuring the Rsh of NiOx-on-sapphire reference wafer after exposing it to different environments. The Cr2O3/HVPE \\b{eta}-Ga2O3 HJD also exhibited enhanced thermal stability compared to the NiOx/\\b{eta}-Ga2O3 heterostructures at elevated temperatures. Interfacial nickel gallate (Ga2NiO4) phase formation expected from phase diagrams can explain the reduced thermal stability of NiOx/\\b{eta}-Ga2O3 HJDs. This study indicates that Cr2O3 is a stable p-type oxide for the realization of robust multi-kV \\b{eta}-Ga2O3 HJDs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the electrical characteristics comparison study between Cr2O3 and NiOx based heterojunction diodes (HJD) on halide vapor phase epitaxy (HVPE) grown \\b{eta}-Ga2O3 epitaxial layers. Both as-fabricated Cr2O3 and NiOx HJDs exhibited forward current density in a range of 130-150 A/cm^2 at 5 V with rectifying ratios >10^10 and a reverse leakage current density at 10^-8 A/cm^2 at -5 V. The differential specific on-resistance of Cr2O3 and NiOx HJDs was 12.01 mΩ*cm^2 and 12.05 mΩ*cm^2, respectively. Breakdown voltages of Cr2O3 HJDs ranged from 1.4-1.9 kV and 1.5-2.3 kV for NiOx HJDs. Theoretical band alignment between Cr2O3 and \\b{eta}-Ga2O3 was calculated from first principles. The ambient exposed NiOx/HVPE \\b{eta}-Ga2O3 HJDs forward current density degraded after 10 days while that of Cr2O3/HVPE \\b{eta}-Ga2O3 HJDs remained nearly unchanged after the same amount of time. It was later confirmed that the ambient exposed sputtered NiOx sheet resistance (Rsh) degradation gave rise to the reduction of the forward current density of the NiOx based HJDs, and water (H2O) was qualitatively determined to be the agent attributed to the forward conduction degradation by measuring the Rsh of NiOx-on-sapphire reference wafer after exposing it to different environments. The Cr2O3/HVPE \\b{eta}-Ga2O3 HJD also exhibited enhanced thermal stability compared to the NiOx/\\b{eta}-Ga2O3 heterostructures at elevated temperatures. Interfacial nickel gallate (Ga2NiO4) phase formation expected from phase diagrams can explain the reduced thermal stability of NiOx/\\b{eta}-Ga2O3 HJDs. This study indicates that Cr2O3 is a stable p-type oxide for the realization of robust multi-kV \\b{eta}-Ga2O3 HJDs."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T03:57:52Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    3,
                    57,
                    52,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Chris G. Van de Walle"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy"
            },
            {
                "id": "http://arxiv.org/abs/2512.11229v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11229v1",
                "title": "REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation"
                },
                "updated": "2025-12-12T02:28:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    2,
                    28,
                    52,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11229v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T02:28:52Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    2,
                    28,
                    52,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "10pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yuzhe Weng"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Xiaoyan Wu"
                    },
                    {
                        "name": "Shan He"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Cong Liu"
                    },
                    {
                        "name": "Qingfeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qingfeng Liu"
                },
                "author": "Qingfeng Liu"
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.21228v2",
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks"
                },
                "updated": "2025-12-12T02:25:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    2,
                    25,
                    34,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.21228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.21228v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are susceptible to indirect prompt injection attacks, in which the model inadvertently responds to task messages injected within the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune, a defense method that identifies and prunes task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to interpret the input prompt context purely as data rather than as cues for instruction following. To identify these neurons, we introduce a neural attribution mechanism guided by a preferential attribution loss, which enables effective attribution with only a few samples while preserving response quality after pruning. We further enhance the efficacy of neural attribution by leveraging an observed triggering effect inherent in the model's response generation behavior. Notably, our approach does not impose additional formatting on the prompt or introduce extra test-time LLM calls. Experiments show that CachePrune can significantly reduce attack success rates while maintaining clean response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to indirect prompt injection attacks, in which the model inadvertently responds to task messages injected within the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune, a defense method that identifies and prunes task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to interpret the input prompt context purely as data rather than as cues for instruction following. To identify these neurons, we introduce a neural attribution mechanism guided by a preferential attribution loss, which enables effective attribution with only a few samples while preserving response quality after pruning. We further enhance the efficacy of neural attribution by leveraging an observed triggering effect inherent in the model's response generation behavior. Notably, our approach does not impose additional formatting on the prompt or introduce extra test-time LLM calls. Experiments show that CachePrune can significantly reduce attack success rates while maintaining clean response quality."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Subrata Mitra"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley"
            },
            {
                "id": "http://arxiv.org/abs/2512.11221v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11221v1",
                "title": "Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference"
                },
                "updated": "2025-12-12T02:02:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    2,
                    2,
                    2,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11221v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T02:02:02Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    2,
                    2,
                    2,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "6 pages, 3 tables , 1 figure",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Adilet Metinov"
                    },
                    {
                        "name": "Gulida M. Kudakeeva"
                    },
                    {
                        "name": "Bolotbek uulu Nursultan"
                    },
                    {
                        "name": "Gulnara D. Kabaeva"
                    }
                ],
                "author_detail": {
                    "name": "Gulnara D. Kabaeva"
                },
                "author": "Gulnara D. Kabaeva"
            },
            {
                "id": "http://arxiv.org/abs/2511.11907v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11907v2",
                "title": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference"
                },
                "updated": "2025-12-11T23:35:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    23,
                    35,
                    19,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11907v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T22:37:57Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    22,
                    37,
                    57,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Huawei Zhang"
                    },
                    {
                        "name": "Chunwei Xia"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.11920v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11920v1",
                "title": "CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving"
                },
                "updated": "2025-12-11T15:40:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    40,
                    36,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11920v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \\textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \\textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:40:36Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    40,
                    36,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Accepted to FPGA'26 Oral",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yanxuan Yu"
                },
                "author": "Yanxuan Yu"
            },
            {
                "id": "http://arxiv.org/abs/2512.10576v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10576v1",
                "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp"
                },
                "updated": "2025-12-11T12:06:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    6,
                    0,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10576v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T12:06:00Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    6,
                    0,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xinhang Chen"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jiahuan He"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jianming Zhang"
                    },
                    {
                        "name": "Wenlong Zhou"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Pai Zeng"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Yuanpan Qian"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zhaogeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhaogeng Li"
                },
                "author": "Zhaogeng Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.10547v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10547v1",
                "title": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders"
                },
                "updated": "2025-12-11T11:23:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    23,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10547v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:23:50Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    23,
                    50,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qingsen Ma"
                    },
                    {
                        "name": "Dianyun Wang"
                    },
                    {
                        "name": "Jiaming Lyu"
                    },
                    {
                        "name": "Yaoye Wang"
                    },
                    {
                        "name": "Lechen Ning"
                    },
                    {
                        "name": "Sujie Zhu"
                    },
                    {
                        "name": "Zhenbo Xu"
                    },
                    {
                        "name": "Liuyu Xiang"
                    },
                    {
                        "name": "Huining Li"
                    },
                    {
                        "name": "Huijia Wu"
                    },
                    {
                        "name": "Zhaofeng He"
                    }
                ],
                "author_detail": {
                    "name": "Zhaofeng He"
                },
                "author": "Zhaofeng He"
            },
            {
                "id": "http://arxiv.org/abs/2512.10405v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10405v1",
                "title": "Electric-Field-Controlled Altermagnetic Transition for Neuromorphic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric-Field-Controlled Altermagnetic Transition for Neuromorphic Computing"
                },
                "updated": "2025-12-11T08:14:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    14,
                    0,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10405v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1021/jacs.5c15276",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Altermagnets represent a novel magnetic phase with transformative potential for ultrafast spintronics, yet efficient control of their magnetic states remains challenging. We demonstrate an ultra-low-power electric-field control of altermagnetism in MnTe through strain-mediated coupling in MnTe/PMN-PT heterostructures with negligible Joule heating. Application of +6 kV/cm electric fields induces piezoelectric strain in PMN-PT, modulating the Néel temperature from 310 to 328 K. As a result, around the magnetic phase transition, the altermagnetic spin splitting of MnTe is reversibly switched \"on\" and \"off\" by the electric fields. Meanwhile, the piezoelectric strain generates lattice distortions and magnetic structure changes in MnTe, enabling up to 9.7% resistance modulation around the magnetic phase transition temperature. Leveraging this effect, we implement programmable resistance states in a Hopfield neuromorphic network, achieving 100% pattern recognition accuracy at <=40% noise levels. This approach establishes the electric-field control as a low-power strategy for altermagnetic manipulation while demonstrating the viability of altermagnetic materials for energy-efficient neuromorphic computing beyond conventional charge-based architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets represent a novel magnetic phase with transformative potential for ultrafast spintronics, yet efficient control of their magnetic states remains challenging. We demonstrate an ultra-low-power electric-field control of altermagnetism in MnTe through strain-mediated coupling in MnTe/PMN-PT heterostructures with negligible Joule heating. Application of +6 kV/cm electric fields induces piezoelectric strain in PMN-PT, modulating the Néel temperature from 310 to 328 K. As a result, around the magnetic phase transition, the altermagnetic spin splitting of MnTe is reversibly switched \"on\" and \"off\" by the electric fields. Meanwhile, the piezoelectric strain generates lattice distortions and magnetic structure changes in MnTe, enabling up to 9.7% resistance modulation around the magnetic phase transition temperature. Leveraging this effect, we implement programmable resistance states in a Hopfield neuromorphic network, achieving 100% pattern recognition accuracy at <=40% noise levels. This approach establishes the electric-field control as a low-power strategy for altermagnetic manipulation while demonstrating the viability of altermagnetic materials for energy-efficient neuromorphic computing beyond conventional charge-based architectures."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T08:14:00Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    14,
                    0,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "42 pages, 13 figures, published online at Journal of the American Chemical Society",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Duan"
                    },
                    {
                        "name": "Peixin Qin"
                    },
                    {
                        "name": "Chengyan Zhong"
                    },
                    {
                        "name": "Shaoxuan Zhang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Guojian Zhao"
                    },
                    {
                        "name": "Xiaoning Wang"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Ziang Meng"
                    },
                    {
                        "name": "Jingyu Li"
                    },
                    {
                        "name": "Sixu Jiang"
                    },
                    {
                        "name": "Xiaoyang Tan"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Zhiqi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqi Liu"
                },
                "author": "Zhiqi Liu",
                "arxiv_doi": "10.1021/jacs.5c15276"
            },
            {
                "id": "http://arxiv.org/abs/2511.00090v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00090v3",
                "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation"
                },
                "updated": "2025-12-11T08:10:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    10,
                    13,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00090v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00090v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T04:57:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Huanlin Gao"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Fuyuan Shi"
                    },
                    {
                        "name": "Chao Tan"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Fang Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian"
            },
            {
                "id": "http://arxiv.org/abs/2512.10310v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10310v1",
                "title": "Efficient-VLN: A Training-Efficient Vision-Language Navigation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient-VLN: A Training-Efficient Vision-Language Navigation Model"
                },
                "updated": "2025-12-11T05:57:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    5,
                    57,
                    48,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10310v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T05:57:48Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    5,
                    57,
                    48,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.19695v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19695v1",
                "title": "Ionizing Photon Production Efficiencies and Chemical Abundances at Cosmic Dawn Revealed by Ultra-Deep Rest-Frame Optical Spectroscopy of JADES-GS-z14-0",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ionizing Photon Production Efficiencies and Chemical Abundances at Cosmic Dawn Revealed by Ultra-Deep Rest-Frame Optical Spectroscopy of JADES-GS-z14-0"
                },
                "updated": "2025-12-22T18:59:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    59,
                    58,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19695v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "JWST has discovered an early period of galaxy formation that was more vigorous than expected, which has challenged our understanding of the early Universe. In this work, we present the longest spectroscopic integration ever acquired by JWST/MIRI. This spectrum covers the brightest rest-frame optical nebular emission lines for the luminous galaxy JADES-GS-z14-0 at $z > 14$. Most notably, we detect $[\\mathrm{OIII}] λλ4959,5007$ at $\\approx 11 σ$ and $\\mathrm{H}α$ at $\\approx 4 σ$ with these ultra-deep observations. These lines reveal that JADES-GS-z14-0 has low dust attenuation with a recent star-formation rate of $\\mathrm{SFR} \\approx 10 \\pm 2\\ M_{\\odot} / \\mathrm{yr}$, star-formation rate surface density of $Σ_{\\mathrm{SFR}} \\approx 23 \\pm 5\\ M_{\\odot}/\\mathrm{yr}/\\mathrm{kpc}^{2}$, and ionizing photon production efficiency of $ξ_{\\mathrm{ion}} \\approx 10^{25.3 \\pm 0.1}\\ \\mathrm{Hz/erg}$. Using standard strong-line diagnostics, we infer a gas-phase oxygen abundance of $[\\mathrm{O/H}] \\approx -1.1 \\pm 0.4$ ($\\approx 10\\%\\ Z_{\\odot}$), carbon-to-oxygen ratio of $[\\mathrm{C/O}] \\approx -0.4 \\pm 0.4$, ionization parameter of $\\mathrm{log}_{10}(U) \\gtrsim -2.4$, and density of $n_{\\mathrm{H}} \\approx 720 \\pm 210\\ \\mathrm{cm}^{-3}$. Using detailed photoionization modeling, we instead derive $[\\mathrm{O/H}] \\approx -0.3_{-0.4}^{+0.4}$ ($\\approx 50\\%\\ Z_{\\odot}$) and $\\mathrm{log}_{10}(U) \\approx -1.5_{-0.4}^{+0.3}$. The inferred properties of JADES-GS-z14-0 are similar to those measured for similarly luminous galaxies at $z > 10$ with previous MIRI/Spectroscopy, such as GHZ2/GLASSz12, GN-z11, and MACS0647-JD1. Existing simulations are unable to reproduce the empirical and inferred properties of JADES-GS-z14-0. This work demonstrates an important step toward understanding the formation of the first stars and heavy elements in the Universe. [Abridged]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST has discovered an early period of galaxy formation that was more vigorous than expected, which has challenged our understanding of the early Universe. In this work, we present the longest spectroscopic integration ever acquired by JWST/MIRI. This spectrum covers the brightest rest-frame optical nebular emission lines for the luminous galaxy JADES-GS-z14-0 at $z > 14$. Most notably, we detect $[\\mathrm{OIII}] λλ4959,5007$ at $\\approx 11 σ$ and $\\mathrm{H}α$ at $\\approx 4 σ$ with these ultra-deep observations. These lines reveal that JADES-GS-z14-0 has low dust attenuation with a recent star-formation rate of $\\mathrm{SFR} \\approx 10 \\pm 2\\ M_{\\odot} / \\mathrm{yr}$, star-formation rate surface density of $Σ_{\\mathrm{SFR}} \\approx 23 \\pm 5\\ M_{\\odot}/\\mathrm{yr}/\\mathrm{kpc}^{2}$, and ionizing photon production efficiency of $ξ_{\\mathrm{ion}} \\approx 10^{25.3 \\pm 0.1}\\ \\mathrm{Hz/erg}$. Using standard strong-line diagnostics, we infer a gas-phase oxygen abundance of $[\\mathrm{O/H}] \\approx -1.1 \\pm 0.4$ ($\\approx 10\\%\\ Z_{\\odot}$), carbon-to-oxygen ratio of $[\\mathrm{C/O}] \\approx -0.4 \\pm 0.4$, ionization parameter of $\\mathrm{log}_{10}(U) \\gtrsim -2.4$, and density of $n_{\\mathrm{H}} \\approx 720 \\pm 210\\ \\mathrm{cm}^{-3}$. Using detailed photoionization modeling, we instead derive $[\\mathrm{O/H}] \\approx -0.3_{-0.4}^{+0.4}$ ($\\approx 50\\%\\ Z_{\\odot}$) and $\\mathrm{log}_{10}(U) \\approx -1.5_{-0.4}^{+0.3}$. The inferred properties of JADES-GS-z14-0 are similar to those measured for similarly luminous galaxies at $z > 10$ with previous MIRI/Spectroscopy, such as GHZ2/GLASSz12, GN-z11, and MACS0647-JD1. Existing simulations are unable to reproduce the empirical and inferred properties of JADES-GS-z14-0. This work demonstrates an important step toward understanding the formation of the first stars and heavy elements in the Universe. [Abridged]"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:59:58Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    59,
                    58,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Submitted to ApJL; main text has 23 pages, 8 figures, and 2 tables; comments are welcome!",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Jakob M. Helton"
                    },
                    {
                        "name": "Jane E. Morrison"
                    },
                    {
                        "name": "Kevin N. Hainline"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "George H. Rieke"
                    },
                    {
                        "name": "Stacey Alberts"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Joel Leja"
                    },
                    {
                        "name": "Yijia Li"
                    },
                    {
                        "name": "Pierluigi Rinaldi"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Meredith Stone"
                    },
                    {
                        "name": "Christopher N. A. Willmer"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "William M. Baker"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stephane Charlot"
                    },
                    {
                        "name": "Jacopo Chevallard"
                    },
                    {
                        "name": "Nikko J. Cleri"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Emma Curtis-Lake"
                    },
                    {
                        "name": "Eiichi Egami"
                    },
                    {
                        "name": "Daniel J. Eisenstein"
                    },
                    {
                        "name": "Peter Jakobsen"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Xiaojing Lin"
                    },
                    {
                        "name": "Jianwei Lyu"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Michael Maseda"
                    },
                    {
                        "name": "Pablo G. Pérez-González"
                    },
                    {
                        "name": "Marcia J. Rieke"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Aayush Saxena"
                    },
                    {
                        "name": "Fengwu Sun"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Hannah Übler"
                    },
                    {
                        "name": "Giacomo Venturi"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Chris Willott"
                    },
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Yongda Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yongda Zhu"
                },
                "author": "Yongda Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2512.19691v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19691v1",
                "title": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight"
                },
                "updated": "2025-12-22T18:59:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    59,
                    34,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19691v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:59:34Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    59,
                    34,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Junze Ye"
                    },
                    {
                        "name": "Daniel Tawfik"
                    },
                    {
                        "name": "Alex J. Goodell"
                    },
                    {
                        "name": "Nikhil V. Kotha"
                    },
                    {
                        "name": "Mark K. Buyyounouski"
                    },
                    {
                        "name": "Mohsen Bayati"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Bayati"
                },
                "author": "Mohsen Bayati"
            },
            {
                "id": "http://arxiv.org/abs/2512.19682v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19682v1",
                "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators"
                },
                "updated": "2025-12-22T18:57:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    57,
                    13,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19682v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:57:13Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    57,
                    13,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Our codes are available at https://github.com/Gen-Verse/GenEnv",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jiacheng Guo"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Peter Chen"
                    },
                    {
                        "name": "Qixin Xiao"
                    },
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang"
            },
            {
                "id": "http://arxiv.org/abs/2510.09595v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.09595v2",
                "title": "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?"
                },
                "updated": "2025-12-22T18:56:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    56,
                    1,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.09595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.09595v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-10T17:54:24Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    54,
                    24,
                    4,
                    283,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Kaijian Zou"
                    },
                    {
                        "name": "Aaron Xiong"
                    },
                    {
                        "name": "Yunxiang Zhang"
                    },
                    {
                        "name": "Frederick Zhang"
                    },
                    {
                        "name": "Yueqi Ren"
                    },
                    {
                        "name": "Jirong Yang"
                    },
                    {
                        "name": "Ayoung Lee"
                    },
                    {
                        "name": "Shitanshu Bhushan"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.19675v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19675v1",
                "title": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)"
                },
                "updated": "2025-12-22T18:53:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    53,
                    3,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19675v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:53:03Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    53,
                    3,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Niclas Griesshaber"
                    },
                    {
                        "name": "Jochen Streb"
                    }
                ],
                "author_detail": {
                    "name": "Jochen Streb"
                },
                "author": "Jochen Streb"
            },
            {
                "id": "http://arxiv.org/abs/2512.19673v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19673v1",
                "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies"
                },
                "updated": "2025-12-22T18:51:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    51,
                    48,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19673v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:51:48Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    51,
                    48,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Preprint. Our code is available at https://github.com/Trae1ounG/BuPO",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuqiao Tan"
                    },
                    {
                        "name": "Minzheng Wang"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Chengfeng Zhao"
                    },
                    {
                        "name": "Qiunan Lu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2506.22552v7",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.22552v7",
                "title": "Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models"
                },
                "updated": "2025-12-22T18:48:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    48,
                    57,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.22552v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.22552v7",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often struggle to reproduce forced responses, limiting their use in causal studies such as Green's function experiments. To explore the origin of these limitations, we first examine a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and to probe causal mechanisms. We argue that the ability of emulators of multiscale systems to reproduce perturbed statistics depends critically on (i) the choice of an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. These insights highlight reduced-order models, tailored to specific goals, processes, and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application by developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system's probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. Overall, these results expose key challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory providing a principled framework to guide model design and enhance causal understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often struggle to reproduce forced responses, limiting their use in causal studies such as Green's function experiments. To explore the origin of these limitations, we first examine a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and to probe causal mechanisms. We argue that the ability of emulators of multiscale systems to reproduce perturbed statistics depends critically on (i) the choice of an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. These insights highlight reduced-order models, tailored to specific goals, processes, and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application by developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system's probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. Overall, these results expose key challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory providing a principled framework to guide model design and enhance causal understanding."
                },
                "tags": [
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-27T18:04:36Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    18,
                    4,
                    36,
                    4,
                    178,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "nlin.CD"
                },
                "authors": [
                    {
                        "name": "Fabrizio Falasca"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Falasca"
                },
                "author": "Fabrizio Falasca"
            },
            {
                "id": "http://arxiv.org/abs/2509.23004v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23004v2",
                "title": "Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference with AI-Noether",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference with AI-Noether"
                },
                "updated": "2025-12-22T18:45:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    45,
                    53,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23004v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Advances in AI have shown great potential in contributing to the acceleration of scientific discovery. Symbolic regression can fit interpretable models to data, but these models are not necessarily derivable from established theory. Recent systems (e.g., AI-Descartes, AI-Hilbert) enforce derivability from prior knowledge. However, when existing theories are incomplete or incorrect, these machine-generated hypotheses may fall outside the theoretical scope. Automatically finding corrections to axiom systems to close this gap remains a central challenge in scientific discovery. We propose a solution: an open-source algebraic geometry-based system that, given an incomplete axiom system expressible as polynomials and a hypothesis that the axioms cannot derive, generates a minimal set of candidate axioms that, when added to the theory, provably derive the (possibly noisy) hypothesis. We illustrate the efficacy of our approach by showing that it can reconstruct key axioms required to derive the carrier-resolved photo-Hall effect, Einstein's relativistic laws, and several other laws.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in AI have shown great potential in contributing to the acceleration of scientific discovery. Symbolic regression can fit interpretable models to data, but these models are not necessarily derivable from established theory. Recent systems (e.g., AI-Descartes, AI-Hilbert) enforce derivability from prior knowledge. However, when existing theories are incomplete or incorrect, these machine-generated hypotheses may fall outside the theoretical scope. Automatically finding corrections to axiom systems to close this gap remains a central challenge in scientific discovery. We propose a solution: an open-source algebraic geometry-based system that, given an incomplete axiom system expressible as polynomials and a hypothesis that the axioms cannot derive, generates a minimal set of candidate axioms that, when added to the theory, provably derive the (possibly noisy) hypothesis. We illustrate the efficacy of our approach by showing that it can reconstruct key axioms required to derive the carrier-resolved photo-Hall effect, Einstein's relativistic laws, and several other laws."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-26T23:50:25Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    23,
                    50,
                    25,
                    4,
                    269,
                    0
                ],
                "arxiv_comment": "47 Pages (20+appendix), 14 Figures, Preprint: Updated for recent submission",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Karan Srivastava"
                    },
                    {
                        "name": "Sanjeeb Dash"
                    },
                    {
                        "name": "Ryan Cory-Wright"
                    },
                    {
                        "name": "Barry Trager"
                    },
                    {
                        "name": "Cristina Cornelio"
                    },
                    {
                        "name": "Lior Horesh"
                    }
                ],
                "author_detail": {
                    "name": "Lior Horesh"
                },
                "author": "Lior Horesh"
            },
            {
                "id": "http://arxiv.org/abs/2306.00029v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2306.00029v2",
                "title": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLMs"
                },
                "updated": "2025-12-22T18:29:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    29,
                    22,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2306.00029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2306.00029v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-05-31T05:24:48Z",
                "published_parsed": [
                    2023,
                    5,
                    31,
                    5,
                    24,
                    48,
                    2,
                    151,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Nghi D. Q. Bui"
                    },
                    {
                        "name": "Hung Le"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Akhilesh Deepak Gotmare"
                    },
                    {
                        "name": "Steven C. H. Hoi"
                    }
                ],
                "author_detail": {
                    "name": "Steven C. H. Hoi"
                },
                "author": "Steven C. H. Hoi"
            },
            {
                "id": "http://arxiv.org/abs/2512.19652v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19652v1",
                "title": "High-Precision Differential Radial Velocities of C3PO Wide Binaries: A Test of Modified Newtonian Dynamics (MOND)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Precision Differential Radial Velocities of C3PO Wide Binaries: A Test of Modified Newtonian Dynamics (MOND)"
                },
                "updated": "2025-12-22T18:26:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    26,
                    53,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19652v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Wide-binary stars, separated by thousands of AU, reside in low-acceleration regimes where Modified Newtonian Dynamics (MOND) predicts deviation from Newtonian gravity. However, Gaia radial velocities (RVs) lack the precision to resolve the small velocity differences expected in these systems, limiting previous MOND analyses to two-dimensional kinematics. In this paper, we introduce a technique to measure differential RVs of wide binary stars using high resolution, high signal-to-noise spectra. We apply this method to measure differential RVs of 100 wide-binaries from the C3PO survey and achieved precisions of $8-15$ m/s per binary pair, a $\\sim 10-100 \\times$ improvement (median $\\sim 24 \\times$) over Gaia DR3. Combining these measurements with Gaia astrometry, we construct a hierarchical Bayesian model to infer the orbital elements of all wide-binary pairs and the global MOND acceleration scale ($a_0$). We test two commonly used interpolating functions in MOND formulation: the simple form ($b=1, μ= x/(1+x)$) and the standard form ($b=2, μ= x/\\sqrt{1+x^2}$). Our results indicate tension with MOND at the presently accepted $a_0$ value: for $b=1$, the canonical value is excluded at $3.1σ$, while for $b=2$, the exclusion is at $1.9σ$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wide-binary stars, separated by thousands of AU, reside in low-acceleration regimes where Modified Newtonian Dynamics (MOND) predicts deviation from Newtonian gravity. However, Gaia radial velocities (RVs) lack the precision to resolve the small velocity differences expected in these systems, limiting previous MOND analyses to two-dimensional kinematics. In this paper, we introduce a technique to measure differential RVs of wide binary stars using high resolution, high signal-to-noise spectra. We apply this method to measure differential RVs of 100 wide-binaries from the C3PO survey and achieved precisions of $8-15$ m/s per binary pair, a $\\sim 10-100 \\times$ improvement (median $\\sim 24 \\times$) over Gaia DR3. Combining these measurements with Gaia astrometry, we construct a hierarchical Bayesian model to infer the orbital elements of all wide-binary pairs and the global MOND acceleration scale ($a_0$). We test two commonly used interpolating functions in MOND formulation: the simple form ($b=1, μ= x/(1+x)$) and the standard form ($b=2, μ= x/\\sqrt{1+x^2}$). Our results indicate tension with MOND at the presently accepted $a_0$ value: for $b=1$, the canonical value is excluded at $3.1σ$, while for $b=2$, the exclusion is at $1.9σ$."
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:26:53Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    26,
                    53,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "22 Pages, 12 figures, 5 tables. To be submitted. Comments are welcome",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR"
                },
                "authors": [
                    {
                        "name": "Serat Mahmud Saad"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Sen Ting"
                },
                "author": "Yuan-Sen Ting"
            },
            {
                "id": "http://arxiv.org/abs/2512.19651v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19651v1",
                "title": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting"
                },
                "updated": "2025-12-22T18:23:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    23,
                    37,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19651v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:23:37Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    23,
                    37,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "9 pages, 3 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Filippos Ventirozos"
                    },
                    {
                        "name": "Peter Appleby"
                    },
                    {
                        "name": "Matthew Shardlow"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Shardlow"
                },
                "author": "Matthew Shardlow"
            },
            {
                "id": "http://arxiv.org/abs/2511.00898v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00898v2",
                "title": "Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning"
                },
                "updated": "2025-12-22T18:21:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    21,
                    46,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00898v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models have emerged as a promising approach for graph learning due to their powerful reasoning capabilities. However, existing methods exhibit systematic performance degradation on structurally important nodes such as bridges and hubs. We identify the root cause of these limitations. Current approaches encode graph topology into static features but lack reasoning scaffolds to transform topological patterns into role-based interpretations. This limitation becomes critical in zero-shot scenarios where no training data establishes structure-semantics mappings. To address this gap, we propose DuoGLM, a training-free dual-perspective framework for structure-aware graph reasoning. The local perspective constructs relation-aware templates capturing semantic interactions between nodes and neighbors. The global perspective performs topology-to-role inference to generate functional descriptions of structural positions. These complementary perspectives provide explicit reasoning mechanisms enabling LLMs to distinguish topologically similar but semantically different nodes. Extensive experiments across eight benchmark datasets demonstrate substantial improvements. DuoGLM achieves 14.3\\% accuracy gain in zero-shot node classification and 7.6\\% AUC improvement in cross-domain transfer compared to existing methods. The results validate the effectiveness of explicit role reasoning for graph understanding with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have emerged as a promising approach for graph learning due to their powerful reasoning capabilities. However, existing methods exhibit systematic performance degradation on structurally important nodes such as bridges and hubs. We identify the root cause of these limitations. Current approaches encode graph topology into static features but lack reasoning scaffolds to transform topological patterns into role-based interpretations. This limitation becomes critical in zero-shot scenarios where no training data establishes structure-semantics mappings. To address this gap, we propose DuoGLM, a training-free dual-perspective framework for structure-aware graph reasoning. The local perspective constructs relation-aware templates capturing semantic interactions between nodes and neighbors. The global perspective performs topology-to-role inference to generate functional descriptions of structural positions. These complementary perspectives provide explicit reasoning mechanisms enabling LLMs to distinguish topologically similar but semantically different nodes. Extensive experiments across eight benchmark datasets demonstrate substantial improvements. DuoGLM achieves 14.3\\% accuracy gain in zero-shot node classification and 7.6\\% AUC improvement in cross-domain transfer compared to existing methods. The results validate the effectiveness of explicit role reasoning for graph understanding with LLMs."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-02T11:33:14Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    11,
                    33,
                    14,
                    6,
                    306,
                    0
                ],
                "arxiv_comment": "This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results",
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Heng Zhang"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Haochen You"
                    },
                    {
                        "name": "Lubin Gan"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Shuai Chen"
                    },
                    {
                        "name": "Wenjun Huang"
                    },
                    {
                        "name": "Jin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jin Huang"
                },
                "author": "Jin Huang"
            },
            {
                "id": "http://arxiv.org/abs/2510.10585v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.10585v2",
                "title": "D3MAS: Decompose, Deduce, and Distribute for Enhanced Knowledge Sharing in Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D3MAS: Decompose, Deduce, and Distribute for Enhanced Knowledge Sharing in Multi-Agent Systems"
                },
                "updated": "2025-12-22T18:20:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    20,
                    32,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.10585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.10585v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent systems powered by large language models exhibit strong capabilities in collaborative problem-solving. However, these systems suffer from substantial knowledge redundancy. Agents duplicate efforts in retrieval and reasoning processes. This inefficiency stems from a deeper issue: current architectures lack mechanisms to ensure agents share minimal sufficient information at each operational stage. Empirical analysis reveals an average knowledge duplication rate of 47.3\\% across agent communications. We propose D3MAS (Decompose, Deduce, and Distribute), a hierarchical coordination framework addressing redundancy through structural design rather than explicit optimization. The framework organizes collaboration across three coordinated layers. Task decomposition filters irrelevant sub-problems early. Collaborative reasoning captures complementary inference paths across agents. Distributed memory provides access to non-redundant knowledge. These layers coordinate through structured message passing in a unified heterogeneous graph. This cross-layer alignment ensures information remains aligned with actual task needs. Experiments on four challenging datasets show that D3MAS consistently improves reasoning accuracy by 8.7\\% to 15.6\\% and reduces knowledge redundancy by 46\\% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems powered by large language models exhibit strong capabilities in collaborative problem-solving. However, these systems suffer from substantial knowledge redundancy. Agents duplicate efforts in retrieval and reasoning processes. This inefficiency stems from a deeper issue: current architectures lack mechanisms to ensure agents share minimal sufficient information at each operational stage. Empirical analysis reveals an average knowledge duplication rate of 47.3\\% across agent communications. We propose D3MAS (Decompose, Deduce, and Distribute), a hierarchical coordination framework addressing redundancy through structural design rather than explicit optimization. The framework organizes collaboration across three coordinated layers. Task decomposition filters irrelevant sub-problems early. Collaborative reasoning captures complementary inference paths across agents. Distributed memory provides access to non-redundant knowledge. These layers coordinate through structured message passing in a unified heterogeneous graph. This cross-layer alignment ensures information remains aligned with actual task needs. Experiments on four challenging datasets show that D3MAS consistently improves reasoning accuracy by 8.7\\% to 15.6\\% and reduces knowledge redundancy by 46\\% on average."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-12T13:01:41Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    1,
                    41,
                    6,
                    285,
                    0
                ],
                "arxiv_comment": "This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results",
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Heng Zhang"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Haochen You"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Lubin Gan"
                    },
                    {
                        "name": "Yilei Yuan"
                    },
                    {
                        "name": "Jin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jin Huang"
                },
                "author": "Jin Huang"
            },
            {
                "id": "http://arxiv.org/abs/2510.10581v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.10581v2",
                "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search"
                },
                "updated": "2025-12-22T18:19:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    19,
                    37,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.10581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.10581v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collaboration, yet they face high failure rates in multi-turn deep search scenarios. Existing temporal attribution methods struggle to accurately diagnose root causes, particularly when errors propagate across multiple agents. Attempts to automate failure attribution by analyzing action sequences remain ineffective due to their inability to account for information dependencies that span agents. This paper identifies two core challenges: \\textit{(i) distinguishing symptoms from root causes in multi-agent error propagation}, and \\textit{(ii) tracing information dependencies beyond temporal order}. To address these issues, we introduce \\textbf{GraphTracer}, a framework that redefines failure attribution through information flow analysis. GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly capture how agents reference and build on prior outputs. It localizes root causes by tracing through these dependency structures instead of relying on temporal sequences. GraphTracer also uses graph-aware synthetic data generation to target critical nodes, creating realistic failure scenarios. Evaluations on the Who\\&When benchmark and integration into production systems demonstrate that GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared to state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements in deployed multi-agent frameworks, establishing a robust solution for multi-agent system debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collaboration, yet they face high failure rates in multi-turn deep search scenarios. Existing temporal attribution methods struggle to accurately diagnose root causes, particularly when errors propagate across multiple agents. Attempts to automate failure attribution by analyzing action sequences remain ineffective due to their inability to account for information dependencies that span agents. This paper identifies two core challenges: \\textit{(i) distinguishing symptoms from root causes in multi-agent error propagation}, and \\textit{(ii) tracing information dependencies beyond temporal order}. To address these issues, we introduce \\textbf{GraphTracer}, a framework that redefines failure attribution through information flow analysis. GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly capture how agents reference and build on prior outputs. It localizes root causes by tracing through these dependency structures instead of relying on temporal sequences. GraphTracer also uses graph-aware synthetic data generation to target critical nodes, creating realistic failure scenarios. Evaluations on the Who\\&When benchmark and integration into production systems demonstrate that GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared to state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements in deployed multi-agent frameworks, establishing a robust solution for multi-agent system debugging."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-12T12:55:42Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    12,
                    55,
                    42,
                    6,
                    285,
                    0
                ],
                "arxiv_comment": "This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results",
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Heng Zhang"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Haochen You"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Lubin Gan"
                    },
                    {
                        "name": "Yilei Yuan"
                    },
                    {
                        "name": "Jin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jin Huang"
                },
                "author": "Jin Huang"
            },
            {
                "id": "http://arxiv.org/abs/2512.19643v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19643v1",
                "title": "The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference"
                },
                "updated": "2025-12-22T18:17:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    17,
                    28,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19643v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:17:28Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    17,
                    28,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "18 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Rajyasri Roy"
                    },
                    {
                        "name": "Dibyajyoti Nayak"
                    },
                    {
                        "name": "Somdatta Goswami"
                    }
                ],
                "author_detail": {
                    "name": "Somdatta Goswami"
                },
                "author": "Somdatta Goswami"
            },
            {
                "id": "http://arxiv.org/abs/2509.00767v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.00767v2",
                "title": "InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos"
                },
                "updated": "2025-12-22T18:14:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    14,
                    12,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.00767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.00767v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-31T09:38:59Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    9,
                    38,
                    59,
                    6,
                    243,
                    0
                ],
                "arxiv_comment": "Accepted to 3DV 2026. Project page: https://mael-zys.github.io/InterPose/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yangsong Zhang"
                    },
                    {
                        "name": "Abdul Ahad Butt"
                    },
                    {
                        "name": "Gül Varol"
                    },
                    {
                        "name": "Ivan Laptev"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Laptev"
                },
                "author": "Ivan Laptev"
            },
            {
                "id": "http://arxiv.org/abs/2512.19630v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19630v1",
                "title": "Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands Māori",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands Māori"
                },
                "updated": "2025-12-22T18:04:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    4,
                    24,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19630v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present experiments on diacritic restoration, a form of text normalization essential for natural language processing (NLP) tasks. Our study focuses on two extremely under-resourced languages: Bribri, a Chibchan language spoken in Costa Rica, and Cook Islands Māori, a Polynesian language spoken in the Cook Islands. Specifically, this paper: (i) compares algorithms for diacritics restoration in under-resourced languages, including tonal diacritics, (ii) examines the amount of data required to achieve target performance levels, (iii) contrasts results across varying resource conditions, and (iv) explores the related task of diacritic correction. We find that fine-tuned, character-level LLMs perform best, likely due to their ability to decompose complex characters into their UTF-8 byte representations. In contrast, massively multilingual models perform less effectively given our data constraints. Across all models, reliable performance begins to emerge with data budgets of around 10,000 words. Zero-shot approaches perform poorly in all cases. This study responds both to requests from the language communities and to broader NLP research questions concerning model performance and generalization in under-resourced contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present experiments on diacritic restoration, a form of text normalization essential for natural language processing (NLP) tasks. Our study focuses on two extremely under-resourced languages: Bribri, a Chibchan language spoken in Costa Rica, and Cook Islands Māori, a Polynesian language spoken in the Cook Islands. Specifically, this paper: (i) compares algorithms for diacritics restoration in under-resourced languages, including tonal diacritics, (ii) examines the amount of data required to achieve target performance levels, (iii) contrasts results across varying resource conditions, and (iv) explores the related task of diacritic correction. We find that fine-tuned, character-level LLMs perform best, likely due to their ability to decompose complex characters into their UTF-8 byte representations. In contrast, massively multilingual models perform less effectively given our data constraints. Across all models, reliable performance begins to emerge with data budgets of around 10,000 words. Zero-shot approaches perform poorly in all cases. This study responds both to requests from the language communities and to broader NLP research questions concerning model performance and generalization in under-resourced contexts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:04:24Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    4,
                    24,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rolando Coto-Solano"
                    },
                    {
                        "name": "Daisy Li"
                    },
                    {
                        "name": "Manoela Teleginski Ferraz"
                    },
                    {
                        "name": "Olivia Sasse"
                    },
                    {
                        "name": "Cha Krupka"
                    },
                    {
                        "name": "Sharid Loáiciga"
                    },
                    {
                        "name": "Sally Akevai Tenamu Nicholas"
                    }
                ],
                "author_detail": {
                    "name": "Sally Akevai Tenamu Nicholas"
                },
                "author": "Sally Akevai Tenamu Nicholas"
            },
            {
                "id": "http://arxiv.org/abs/2512.19620v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19620v1",
                "title": "Exploring the features used for summary evaluation by Human and GPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the features used for summary evaluation by Human and GPT"
                },
                "updated": "2025-12-22T17:54:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    54,
                    49,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19620v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T17:54:49Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    54,
                    49,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zahra Sadeghi"
                    },
                    {
                        "name": "Evangelos Milios"
                    },
                    {
                        "name": "Frank Rudzicz"
                    }
                ],
                "author_detail": {
                    "name": "Frank Rudzicz"
                },
                "author": "Frank Rudzicz"
            },
            {
                "id": "http://arxiv.org/abs/2406.11630v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.11630v5",
                "title": "A framework for the use of generative modelling in non-equilibrium statistical mechanics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A framework for the use of generative modelling in non-equilibrium statistical mechanics"
                },
                "updated": "2025-12-22T17:42:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    42,
                    55,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.11630v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.11630v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We discuss an approach to mathematically modelling systems made of objects that are coupled together, using generative models of the dependence relationships between states (or trajectories) of the things comprising such systems. This broad class includes open or non-equilibrium systems and is especially relevant to self-organising systems. The ensuing variational free energy principle (FEP) has certain advantages over using random dynamical systems explicitly, notably, by being more tractable and offering a parsimonious explanation of why the joint system evolves in the way that it does, based on the properties of the coupling between system components. The FEP is a method whose use allows us to build a model of the dynamics of an object as if it were a process of variational inference, because variational free energy (or surprisal) is a Lyapunov function for its dynamics. In short, we argue that using generative models to represent and track relations amongst subsystems leads us to a particular statistical theory of interacting systems. Conversely, this theory enables us to construct nested models that respect the known relations amongst subsystems. We point out that the fact that a physical object conforms to the FEP does not necessarily imply that this object performs inference in the literal sense; rather, it is a useful explanatory fiction which replaces the `explicit' dynamics of the object with an `implicit' flow on free energy gradients -- a fiction that may or may not be entertained by the object itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss an approach to mathematically modelling systems made of objects that are coupled together, using generative models of the dependence relationships between states (or trajectories) of the things comprising such systems. This broad class includes open or non-equilibrium systems and is especially relevant to self-organising systems. The ensuing variational free energy principle (FEP) has certain advantages over using random dynamical systems explicitly, notably, by being more tractable and offering a parsimonious explanation of why the joint system evolves in the way that it does, based on the properties of the coupling between system components. The FEP is a method whose use allows us to build a model of the dynamics of an object as if it were a process of variational inference, because variational free energy (or surprisal) is a Lyapunov function for its dynamics. In short, we argue that using generative models to represent and track relations amongst subsystems leads us to a particular statistical theory of interacting systems. Conversely, this theory enables us to construct nested models that respect the known relations amongst subsystems. We point out that the fact that a physical object conforms to the FEP does not necessarily imply that this object performs inference in the literal sense; rather, it is a useful explanatory fiction which replaces the `explicit' dynamics of the object with an `implicit' flow on free energy gradients -- a fiction that may or may not be entertained by the object itself."
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-17T15:13:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    13,
                    13,
                    0,
                    169,
                    0
                ],
                "arxiv_comment": "27+3 pages, ten svg figures. Replaces arXiv:2208.06924. This version to appear in Proc Roy Soc A",
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech"
                },
                "authors": [
                    {
                        "name": "Karl J Friston"
                    },
                    {
                        "name": "Maxwell J D Ramstead"
                    },
                    {
                        "name": "Dalton A R Sakthivadivel"
                    }
                ],
                "author_detail": {
                    "name": "Dalton A R Sakthivadivel"
                },
                "author": "Dalton A R Sakthivadivel"
            },
            {
                "id": "http://arxiv.org/abs/2512.19606v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19606v1",
                "title": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference"
                },
                "updated": "2025-12-22T17:42:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    42,
                    51,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19606v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "RAPID-LLM is a unified performance modeling framework for large language model (LLM) training and inference on GPU clusters. It couples a DeepFlow-based frontend that generates hardware-aware, operator-level Chakra execution traces from an abstract LLM specification (model shape, batch/sequence settings, training vs. inference, and hybrid parallelism choices) with an extended Astra-Sim backend that executes those traces on explicit multi-dimensional network topologies with congestion-aware routing and support for degraded and faulty links. The frontend assigns per-operator latency using a tile-based model that accounts for SM under-utilization and multi-level memory traffic (SRAM/ L2/ HBM), and prunes memory-infeasible configurations using an activation-liveness traversal under recomputation, parallelism and ZeRO/FDSP sharding policies.\n  Across A100-based validation cases, RAPID-LLM predicts Llama inference step latency and GPT-scale training time per batch within 10.4\\% relative to published measurements, and matches ns-3 packet-level results within 8\\% on representative communication workloads. Case studies demonstrate how RAPID-LLM enables fast, exhaustive sweeps over hybrid-parallel configurations, quantifies sensitivity to soft link faults under realistic routing and congestion, and evaluates hypothetical GPU design variants including HBM bandwidth throttling effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID-LLM is a unified performance modeling framework for large language model (LLM) training and inference on GPU clusters. It couples a DeepFlow-based frontend that generates hardware-aware, operator-level Chakra execution traces from an abstract LLM specification (model shape, batch/sequence settings, training vs. inference, and hybrid parallelism choices) with an extended Astra-Sim backend that executes those traces on explicit multi-dimensional network topologies with congestion-aware routing and support for degraded and faulty links. The frontend assigns per-operator latency using a tile-based model that accounts for SM under-utilization and multi-level memory traffic (SRAM/ L2/ HBM), and prunes memory-infeasible configurations using an activation-liveness traversal under recomputation, parallelism and ZeRO/FDSP sharding policies.\n  Across A100-based validation cases, RAPID-LLM predicts Llama inference step latency and GPT-scale training time per batch within 10.4\\% relative to published measurements, and matches ns-3 packet-level results within 8\\% on representative communication workloads. Case studies demonstrate how RAPID-LLM enables fast, exhaustive sweeps over hybrid-parallel configurations, quantifies sensitivity to soft link faults under realistic routing and congestion, and evaluates hypothetical GPU design variants including HBM bandwidth throttling effects."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T17:42:51Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    42,
                    51,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "11 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "George Karfakis"
                    },
                    {
                        "name": "Faraz Tahmasebi"
                    },
                    {
                        "name": "Binglu Chen"
                    },
                    {
                        "name": "Lime Yao"
                    },
                    {
                        "name": "Saptarshi Mitra"
                    },
                    {
                        "name": "Tianyue Pan"
                    },
                    {
                        "name": "Hyoukjun Kwon"
                    },
                    {
                        "name": "Puneet Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Puneet Gupta"
                },
                "author": "Puneet Gupta"
            },
            {
                "id": "http://arxiv.org/abs/2506.05594v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05594v2",
                "title": "SoK: Are Watermarks in LLMs Ready for Deployment?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Are Watermarks in LLMs Ready for Deployment?"
                },
                "updated": "2025-12-22T17:37:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    37,
                    53,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05594v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.\n  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.\n  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T21:12:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    21,
                    12,
                    51,
                    3,
                    156,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Kieu Dang"
                    },
                    {
                        "name": "Phung Lai"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Abdallah Khreishah"
                    },
                    {
                        "name": "My T. Thai"
                    }
                ],
                "author_detail": {
                    "name": "My T. Thai"
                },
                "author": "My T. Thai"
            },
            {
                "id": "http://arxiv.org/abs/2501.10509v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.10509v2",
                "title": "Stellar Stripping and Disruption in Disks around Supermassive Black Hole Binaries: Repeating nuclear transients prior to LISA events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar Stripping and Disruption in Disks around Supermassive Black Hole Binaries: Repeating nuclear transients prior to LISA events"
                },
                "updated": "2025-12-22T17:35:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    35,
                    38,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.10509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.10509v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.3847/1538-4357/ae0c9d",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "If supermassive black hole binaries (SMBHBs) are driven together by gas disks in galactic nuclei, then a surrounding nuclear star cluster or in-situ star-formation should deliver stars to the disk plane. Migration through the circumbinary disk will quickly bring stars to the edge of a low-density cavity cleared by the binary, where the stellar orbit becomes trapped and locked with the binary decay. Here we explore the scenario where the trapped stellar orbit decays with the binary until the binary tidally strips the star in a runaway process. For Sun-like stars, this occurs preferentially for $10^4-10^6 M_{\\odot}$ SMBHBs, as the SMBHB enters the LISA band. We estimate that the runaway stripping process will generate Eddington-level X-ray flares repeating on hours-to-days timescales and lasting for decades. The flaring timescales and energetics of these circumbinary-disk tidal-disruption events (CBD-TDEs) match well with the recently discovered Quasi-Periodic Eruptions. However, the inferred rates of the two phenomena are in tension, unless low-mass SMBHB mergers are more common than expected. For less-dense stars, stripping begins earlier in the SMBHB inspiral, has longer repetition times, lasts longer, is dimmer, and can occur for more massive SMBHBs. Whether CBD-TDEs are a known or a yet-undiscovered class of repeating nuclear transients, they could provide a new probe of the elusive SMBH mergers in low mass / dwarf galaxies, which lie in the sweet-spot of the LISA sensitivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If supermassive black hole binaries (SMBHBs) are driven together by gas disks in galactic nuclei, then a surrounding nuclear star cluster or in-situ star-formation should deliver stars to the disk plane. Migration through the circumbinary disk will quickly bring stars to the edge of a low-density cavity cleared by the binary, where the stellar orbit becomes trapped and locked with the binary decay. Here we explore the scenario where the trapped stellar orbit decays with the binary until the binary tidally strips the star in a runaway process. For Sun-like stars, this occurs preferentially for $10^4-10^6 M_{\\odot}$ SMBHBs, as the SMBHB enters the LISA band. We estimate that the runaway stripping process will generate Eddington-level X-ray flares repeating on hours-to-days timescales and lasting for decades. The flaring timescales and energetics of these circumbinary-disk tidal-disruption events (CBD-TDEs) match well with the recently discovered Quasi-Periodic Eruptions. However, the inferred rates of the two phenomena are in tension, unless low-mass SMBHB mergers are more common than expected. For less-dense stars, stripping begins earlier in the SMBHB inspiral, has longer repetition times, lasts longer, is dimmer, and can occur for more massive SMBHBs. Whether CBD-TDEs are a known or a yet-undiscovered class of repeating nuclear transients, they could provide a new probe of the elusive SMBH mergers in low mass / dwarf galaxies, which lie in the sweet-spot of the LISA sensitivity."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-17T19:00:05Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    19,
                    0,
                    5,
                    4,
                    17,
                    0
                ],
                "arxiv_comment": "Published in ApJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "arxiv_journal_ref": "The Astrophysical Journal, Volume 994, Issue 1, id.112, 21 pp. (November 2025)",
                "authors": [
                    {
                        "name": "Daniel J. D'Orazio"
                    },
                    {
                        "name": "Christopher Tiede"
                    },
                    {
                        "name": "Lorenz Zwick"
                    },
                    {
                        "name": "Kimitake Hayasaki"
                    },
                    {
                        "name": "Lucio Mayer"
                    }
                ],
                "author_detail": {
                    "name": "Lucio Mayer"
                },
                "author": "Lucio Mayer",
                "arxiv_doi": "10.3847/1538-4357/ae0c9d"
            },
            {
                "id": "http://arxiv.org/abs/2512.19602v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19602v1",
                "title": "No Data? No Problem: Robust Vision-Tabular Learning with Missing Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Data? No Problem: Robust Vision-Tabular Learning with Missing Values"
                },
                "updated": "2025-12-22T17:35:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    35,
                    32,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19602v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T17:35:32Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    35,
                    32,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Marta Hasny"
                    },
                    {
                        "name": "Laura Daza"
                    },
                    {
                        "name": "Keno Bressem"
                    },
                    {
                        "name": "Maxime Di Folco"
                    },
                    {
                        "name": "Julia Schnabel"
                    }
                ],
                "author_detail": {
                    "name": "Julia Schnabel"
                },
                "author": "Julia Schnabel"
            },
            {
                "id": "http://arxiv.org/abs/2505.17196v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.17196v3",
                "title": "Shape it Up! Restoring LLM Safety during Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shape it Up! Restoring LLM Safety during Finetuning"
                },
                "updated": "2025-12-22T17:30:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    30,
                    15,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.17196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.17196v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks. Our code is publicly available at https://github.com/poloclub/star-dss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks. Our code is publicly available at https://github.com/poloclub/star-dss."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-22T18:05:16Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    18,
                    5,
                    16,
                    3,
                    142,
                    0
                ],
                "arxiv_comment": "NeurIPS'25",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "ShengYun Peng"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Jianfeng Chi"
                    },
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Duen Horng Chau"
                    }
                ],
                "author_detail": {
                    "name": "Duen Horng Chau"
                },
                "author": "Duen Horng Chau"
            },
            {
                "id": "http://arxiv.org/abs/2512.19588v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19588v1",
                "title": "Possibilistic Inferential Models for Post-Selection Inference in High-Dimensional Linear Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Possibilistic Inferential Models for Post-Selection Inference in High-Dimensional Linear Regression"
                },
                "updated": "2025-12-22T17:14:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    14,
                    27,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19588v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Valid uncertainty quantification after model selection remains challenging in high-dimensional linear regression, especially within the possibilistic inferential model (PIM) framework. We develop possibilistic inferential models for post-selection inference based on a regularized split possibilistic construction (RSPIM) that combines generic high-dimensional selectors with PIM validification through sample splitting. A first subsample is used to select a sparse model; ordinary least-squares refits on an independent inference subsample yield classical t/F pivots, which are then turned into consonant plausibility contours. In Gaussian linear models this leads to coor-dinatewise intervals with exact finite-sample strong validity conditional on the split and selected model, uniformly over all selectors that use only the selection data. We further analyze RSPIM in a sparse p >> n regime under high-level screening conditions, develop orthogonalized and bootstrap-based extensions for low-dimensional targets with high-dimensional nuisance, and study a maxitive multi-split aggregation that stabilizes inference across random splits while preserving strong validity. Simulations and a riboflavin gene-expression example show that calibrated RSPIM intervals are well behaved under both Gaussian and heteroskedastic errors and are competitive with state-of-the-art post-selection methods, while plausibility contours provide transparent diagnostics of post-selection uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Valid uncertainty quantification after model selection remains challenging in high-dimensional linear regression, especially within the possibilistic inferential model (PIM) framework. We develop possibilistic inferential models for post-selection inference based on a regularized split possibilistic construction (RSPIM) that combines generic high-dimensional selectors with PIM validification through sample splitting. A first subsample is used to select a sparse model; ordinary least-squares refits on an independent inference subsample yield classical t/F pivots, which are then turned into consonant plausibility contours. In Gaussian linear models this leads to coor-dinatewise intervals with exact finite-sample strong validity conditional on the split and selected model, uniformly over all selectors that use only the selection data. We further analyze RSPIM in a sparse p >> n regime under high-level screening conditions, develop orthogonalized and bootstrap-based extensions for low-dimensional targets with high-dimensional nuisance, and study a maxitive multi-split aggregation that stabilizes inference across random splits while preserving strong validity. Simulations and a riboflavin gene-expression example show that calibrated RSPIM intervals are well behaved under both Gaussian and heteroskedastic errors and are competitive with state-of-the-art post-selection methods, while plausibility contours provide transparent diagnostics of post-selection uncertainty."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T17:14:27Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    14,
                    27,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Yaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yaohui Lin"
                },
                "author": "Yaohui Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.19584v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19584v1",
                "title": "Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior"
                },
                "updated": "2025-12-22T17:11:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    11,
                    33,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19584v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Dynamic PET enables the quantitative estimation of physiology-related parameters and is widely utilized in research and increasingly adopted in clinical settings. Parametric imaging in dynamic PET requires kinetic modeling to estimate voxel-wise physiological parameters based on specific kinetic models. However, parametric images estimated through kinetic model fitting often suffer from low image quality due to the inherently ill-posed nature of the fitting process and the limited counts resulting from non-continuous data acquisition across multiple bed positions in whole-body PET. In this work, we proposed a diffusion model-based kinetic modeling framework for parametric image estimation, using the Patlak model as an example. The score function of the diffusion model was pre-trained on static total-body PET images and served as a prior for both Patlak slope and intercept images by leveraging their patch-wise similarity. During inference, the kinetic model was incorporated as a data-consistency constraint to guide the parametric image estimation. The proposed framework was evaluated on total-body dynamic PET datasets with different dose levels, demonstrating the feasibility and promising performance of the proposed framework in improving parametric image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic PET enables the quantitative estimation of physiology-related parameters and is widely utilized in research and increasingly adopted in clinical settings. Parametric imaging in dynamic PET requires kinetic modeling to estimate voxel-wise physiological parameters based on specific kinetic models. However, parametric images estimated through kinetic model fitting often suffer from low image quality due to the inherently ill-posed nature of the fitting process and the limited counts resulting from non-continuous data acquisition across multiple bed positions in whole-body PET. In this work, we proposed a diffusion model-based kinetic modeling framework for parametric image estimation, using the Patlak model as an example. The score function of the diffusion model was pre-trained on static total-body PET images and served as a prior for both Patlak slope and intercept images by leveraging their patch-wise similarity. During inference, the kinetic model was incorporated as a data-consistency constraint to guide the parametric image estimation. The proposed framework was evaluated on total-body dynamic PET datasets with different dose levels, demonstrating the feasibility and promising performance of the proposed framework in improving parametric image quality."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T17:11:33Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    11,
                    33,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "10 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Ziqian Huang"
                    },
                    {
                        "name": "Boxiao Yu"
                    },
                    {
                        "name": "Siqi Li"
                    },
                    {
                        "name": "Savas Ozdemir"
                    },
                    {
                        "name": "Sangjin Bae"
                    },
                    {
                        "name": "Jae Sung Lee"
                    },
                    {
                        "name": "Guobao Wang"
                    },
                    {
                        "name": "Kuang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Kuang Gong"
                },
                "author": "Kuang Gong"
            },
            {
                "id": "http://arxiv.org/abs/2507.07935v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.07935v6",
                "title": "Working with AI: Measuring the Applicability of Generative AI to Occupations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Working with AI: Measuring the Applicability of Generative AI to Occupations"
                },
                "updated": "2025-12-22T17:01:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    1,
                    55,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.07935v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.07935v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With generative AI emerging as a general-purpose technology, understanding its economic effects is among society's most pressing questions. Existing studies of AI impact have largely relied on predictions of AI capabilities or focused narrowly on individual firms. Drawing instead on real-world AI usage, we analyze a dataset of 200k anonymized conversations with Microsoft Bing Copilot to measure AI applicability to occupations. We use an LLM-based pipeline to classify the O*NET work activities assisted or performed by AI in each conversation. We find that the most common and successful AI-assisted work activities involve information work--the creation, processing, and communication of information. At the occupation level, we find widespread AI applicability cutting across sectors, as most occupations have information work components. Our methodology also allows us to predict which occupations are more likely to delegate tasks to AI and which are more likely to use AI to assist existing workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With generative AI emerging as a general-purpose technology, understanding its economic effects is among society's most pressing questions. Existing studies of AI impact have largely relied on predictions of AI capabilities or focused narrowly on individual firms. Drawing instead on real-world AI usage, we analyze a dataset of 200k anonymized conversations with Microsoft Bing Copilot to measure AI applicability to occupations. We use an LLM-based pipeline to classify the O*NET work activities assisted or performed by AI in each conversation. We find that the most common and successful AI-assisted work activities involve information work--the creation, processing, and communication of information. At the occupation level, we find widespread AI applicability cutting across sectors, as most occupations have information work components. Our methodology also allows us to predict which occupations are more likely to delegate tasks to AI and which are more likely to use AI to assist existing workflows."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-10T17:16:33Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    16,
                    33,
                    3,
                    191,
                    0
                ],
                "arxiv_comment": "40 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Kiran Tomlinson"
                    },
                    {
                        "name": "Sonia Jaffe"
                    },
                    {
                        "name": "Will Wang"
                    },
                    {
                        "name": "Scott Counts"
                    },
                    {
                        "name": "Siddharth Suri"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Suri"
                },
                "author": "Siddharth Suri"
            },
            {
                "id": "http://arxiv.org/abs/2512.19570v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19570v1",
                "title": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge"
                },
                "updated": "2025-12-22T16:52:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    52,
                    37,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19570v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/s00146-025-02426-3",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:52:37Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    52,
                    37,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "AI & Soc (2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Angjelin Hila"
                    }
                ],
                "author_detail": {
                    "name": "Angjelin Hila"
                },
                "author": "Angjelin Hila",
                "arxiv_doi": "10.1007/s00146-025-02426-3"
            },
            {
                "id": "http://arxiv.org/abs/2508.21299v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.21299v2",
                "title": "On Zero-sum Game Representation for Replicator Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Zero-sum Game Representation for Replicator Dynamics"
                },
                "updated": "2025-12-22T16:48:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    48,
                    13,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.21299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.21299v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Replicator dynamics have been widely used in evolutionary game theory to model how strategy frequencies evolve over time in large populations. The so-called payoff matrix encodes the pairwise fitness that each strategy obtains when interacting with every other strategy, and it solely determines the replicator dynamics. If the payoff matrix is unknown, we show in this paper that it cannot be inferred from observed strategy frequencies alone -- distinct payoff matrices can induce the same replicator dynamics. We thus look for a canonical representative of the payoff matrix in the equivalence class. The main result of the paper is to show that for every polynomial replicator dynamics (i.e., the vector field is a polynomial), there always exists a skew-symmetric, polynomial payoff matrix that can induce the given dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replicator dynamics have been widely used in evolutionary game theory to model how strategy frequencies evolve over time in large populations. The so-called payoff matrix encodes the pairwise fitness that each strategy obtains when interacting with every other strategy, and it solely determines the replicator dynamics. If the payoff matrix is unknown, we show in this paper that it cannot be inferred from observed strategy frequencies alone -- distinct payoff matrices can induce the same replicator dynamics. We thus look for a canonical representative of the payoff matrix in the equivalence class. The main result of the paper is to show that for every polynomial replicator dynamics (i.e., the vector field is a polynomial), there always exists a skew-symmetric, polynomial payoff matrix that can induce the given dynamics."
                },
                "tags": [
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-29T01:43:38Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    1,
                    43,
                    38,
                    4,
                    241,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.DS"
                },
                "authors": [
                    {
                        "name": "Haoyu Yin"
                    },
                    {
                        "name": "Xudong Chen"
                    },
                    {
                        "name": "Bruno Sinopoli"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Sinopoli"
                },
                "author": "Bruno Sinopoli"
            },
            {
                "id": "http://arxiv.org/abs/2512.19563v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19563v1",
                "title": "On Network-Aware Semantic Communication and Edge-Cloud Collaborative Intelligence Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Network-Aware Semantic Communication and Edge-Cloud Collaborative Intelligence Systems"
                },
                "updated": "2025-12-22T16:44:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    44,
                    32,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19563v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Semantic communication and edge-cloud collaborative intelligence are increasingly recognized as foundational enablers for next-generation intelligent services operating under stringent bandwidth, latency, and resource constraints. By shifting the communication objective from bit-perfect delivery toward the transmission of task-relevant semantic representations, semantic communication enables adaptive tradeoffs among communication overhead, inference accuracy, computational load, and end-to-end latency. This survey provides a comprehensive and system-level synthesis of recent advances in semantic communication at the edge-cloud interface, encompassing architectural models for collaborative intelligence, representation learning and semantic abstraction techniques, network-aware and resource-adaptive semantic encoding strategies, and learning-driven optimization and orchestration mechanisms. Beyond efficiency considerations, the survey situates semantic communication within practical operational contexts, including security, trust, resilience, and scalability, drawing connections to zero-trust networking, physical-layer security, and emerging edge-cloud control paradigms. Finally, open challenges and research directions are identified, highlighting the role of semantic communication as a key building block for AI-native networking and 6G-ready intelligent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic communication and edge-cloud collaborative intelligence are increasingly recognized as foundational enablers for next-generation intelligent services operating under stringent bandwidth, latency, and resource constraints. By shifting the communication objective from bit-perfect delivery toward the transmission of task-relevant semantic representations, semantic communication enables adaptive tradeoffs among communication overhead, inference accuracy, computational load, and end-to-end latency. This survey provides a comprehensive and system-level synthesis of recent advances in semantic communication at the edge-cloud interface, encompassing architectural models for collaborative intelligence, representation learning and semantic abstraction techniques, network-aware and resource-adaptive semantic encoding strategies, and learning-driven optimization and orchestration mechanisms. Beyond efficiency considerations, the survey situates semantic communication within practical operational contexts, including security, trust, resilience, and scalability, drawing connections to zero-trust networking, physical-layer security, and emerging edge-cloud control paradigms. Finally, open challenges and research directions are identified, highlighting the role of semantic communication as a key building block for AI-native networking and 6G-ready intelligent systems."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:44:32Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    44,
                    32,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Murdadha Nasif"
                    },
                    {
                        "name": "Ahmed Refaey Hussein"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Refaey Hussein"
                },
                "author": "Ahmed Refaey Hussein"
            },
            {
                "id": "http://arxiv.org/abs/2512.19551v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19551v1",
                "title": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios"
                },
                "updated": "2025-12-22T16:31:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    31,
                    30,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19551v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:31:30Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    31,
                    30,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jiawen Wang"
                    },
                    {
                        "name": "Jingjing Wang Tianyang Chen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2512.10218v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10218v2",
                "title": "Does SWE-Bench-Verified Test Agent Ability or Model Memory?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does SWE-Bench-Verified Test Agent Ability or Model Memory?"
                },
                "updated": "2025-12-22T16:30:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    30,
                    1,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10218v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T02:11:06Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    2,
                    11,
                    6,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Thanosan Prathifkumar"
                    },
                    {
                        "name": "Noble Saji Mathews"
                    },
                    {
                        "name": "Meiyappan Nagappan"
                    }
                ],
                "author_detail": {
                    "name": "Meiyappan Nagappan"
                },
                "author": "Meiyappan Nagappan"
            },
            {
                "id": "http://arxiv.org/abs/2512.19537v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19537v1",
                "title": "Event Extraction in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event Extraction in Large Language Model"
                },
                "updated": "2025-12-22T16:22:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    22,
                    14,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19537v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:22:14Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    22,
                    14,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "38 pages, 9 Figures, 5 Tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bobo Li"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yuzhe Ding"
                    },
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Zhaoqi Zhang"
                    },
                    {
                        "name": "Jinheng Li"
                    },
                    {
                        "name": "Xinya Du"
                    },
                    {
                        "name": "Fei Li"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Aixin Sun"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Hao Fei"
                    }
                ],
                "author_detail": {
                    "name": "Hao Fei"
                },
                "author": "Hao Fei"
            },
            {
                "id": "http://arxiv.org/abs/2512.19528v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19528v1",
                "title": "Multi-Modal Soccer Scene Analysis with Masked Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Soccer Scene Analysis with Masked Pre-Training"
                },
                "updated": "2025-12-22T16:18:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    18,
                    45,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19528v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work we propose a multi-modal architecture for analyzing soccer scenes from tactical camera footage, with a focus on three core tasks: ball trajectory inference, ball state classification, and ball possessor identification. To this end, our solution integrates three distinct input modalities (player trajectories, player types and image crops of individual players) into a unified framework that processes spatial and temporal dynamics using a cascade of sociotemporal transformer blocks. Unlike prior methods, which rely heavily on accurate ball tracking or handcrafted heuristics, our approach infers the ball trajectory without direct access to its past or future positions, and robustly identifies the ball state and ball possessor under noisy or occluded conditions from real top league matches. We also introduce CropDrop, a modality-specific masking pre-training strategy that prevents over-reliance on image features and encourages the model to rely on cross-modal patterns during pre-training. We show the effectiveness of our approach on a large-scale dataset providing substantial improvements over state-of-the-art baselines in all tasks. Our results highlight the benefits of combining structured and visual cues in a transformer-based architecture, and the importance of realistic masking strategies in multi-modal learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we propose a multi-modal architecture for analyzing soccer scenes from tactical camera footage, with a focus on three core tasks: ball trajectory inference, ball state classification, and ball possessor identification. To this end, our solution integrates three distinct input modalities (player trajectories, player types and image crops of individual players) into a unified framework that processes spatial and temporal dynamics using a cascade of sociotemporal transformer blocks. Unlike prior methods, which rely heavily on accurate ball tracking or handcrafted heuristics, our approach infers the ball trajectory without direct access to its past or future positions, and robustly identifies the ball state and ball possessor under noisy or occluded conditions from real top league matches. We also introduce CropDrop, a modality-specific masking pre-training strategy that prevents over-reliance on image features and encourages the model to rely on cross-modal patterns during pre-training. We show the effectiveness of our approach on a large-scale dataset providing substantial improvements over state-of-the-art baselines in all tasks. Our results highlight the benefits of combining structured and visual cues in a transformer-based architecture, and the importance of realistic masking strategies in multi-modal learning."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:18:45Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    18,
                    45,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "10 pages, 2 figures. WACV 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Marc Peral"
                    },
                    {
                        "name": "Guillem Capellera"
                    },
                    {
                        "name": "Luis Ferraz"
                    },
                    {
                        "name": "Antonio Rubio"
                    },
                    {
                        "name": "Antonio Agudo"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Agudo"
                },
                "author": "Antonio Agudo"
            },
            {
                "id": "http://arxiv.org/abs/2512.19526v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19526v1",
                "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models"
                },
                "updated": "2025-12-22T16:18:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    18,
                    0,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19526v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:18:00Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    18,
                    0,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Li Puyin"
                    },
                    {
                        "name": "Tiange Xiang"
                    },
                    {
                        "name": "Ella Mao"
                    },
                    {
                        "name": "Shirley Wei"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Adnan Masood"
                    },
                    {
                        "name": "Li Fei-fei"
                    },
                    {
                        "name": "Ehsan Adeli"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Adeli"
                },
                "author": "Ehsan Adeli"
            },
            {
                "id": "http://arxiv.org/abs/2508.13281v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.13281v2",
                "title": "Quantum algorithms to detect ODMR-active defects for quantum sensing applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum algorithms to detect ODMR-active defects for quantum sensing applications"
                },
                "updated": "2025-12-22T16:10:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    10,
                    8,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.13281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.13281v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spin defects in two-dimensional materials are a promising platform for quantum sensing. Simulating the defect's optical response and optically detected magnetic resonance (ODMR) contrast is key to identifying suitable candidates. However, existing simulation methods are typically unable to supply the required accuracy. Here, we propose two quantum algorithms to detect an imbalance in the triplet-to-singlet intersystem crossing (ISC) rates between excited states with the same and different spin projections -- a necessary condition for nonzero ODMR response. The lowest-cost approach evaluates whether the evolution of an $S=0$ state under the spin-orbit coupling induces ISC to $S=1$, and also whether there is an imbalance in its intensity depending on the final state spin projection. The second approach works by comparing the emission spectrum of a spin defect with and without the spin-orbit coupling operator, inferring ISC intensity for different spin transition channels from spectrum intensity changes. Additionally, we present an improved scheme to evaluate the defect's optical response, building upon previous work. We study these quantum algorithms in the context of the negatively charged boron vacancy in hexagonal boron nitride. We generate an embedded active space of 18 spatial orbitals using quantum defect embedding theory (QDET) and show that the ISC rate imbalance can be detected with as few as 105 logical qubits and $4.41 \\times 10^8$ Toffoli gates. By avoiding direct and costly rate calculations, our methods enable faster screening of candidate defects for ODMR activity, advancing the prospect of using quantum simulations to aid the development of high-performing sensing devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin defects in two-dimensional materials are a promising platform for quantum sensing. Simulating the defect's optical response and optically detected magnetic resonance (ODMR) contrast is key to identifying suitable candidates. However, existing simulation methods are typically unable to supply the required accuracy. Here, we propose two quantum algorithms to detect an imbalance in the triplet-to-singlet intersystem crossing (ISC) rates between excited states with the same and different spin projections -- a necessary condition for nonzero ODMR response. The lowest-cost approach evaluates whether the evolution of an $S=0$ state under the spin-orbit coupling induces ISC to $S=1$, and also whether there is an imbalance in its intensity depending on the final state spin projection. The second approach works by comparing the emission spectrum of a spin defect with and without the spin-orbit coupling operator, inferring ISC intensity for different spin transition channels from spectrum intensity changes. Additionally, we present an improved scheme to evaluate the defect's optical response, building upon previous work. We study these quantum algorithms in the context of the negatively charged boron vacancy in hexagonal boron nitride. We generate an embedded active space of 18 spatial orbitals using quantum defect embedding theory (QDET) and show that the ISC rate imbalance can be detected with as few as 105 logical qubits and $4.41 \\times 10^8$ Toffoli gates. By avoiding direct and costly rate calculations, our methods enable faster screening of candidate defects for ODMR activity, advancing the prospect of using quantum simulations to aid the development of high-performing sensing devices."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-18T18:00:37Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    18,
                    0,
                    37,
                    0,
                    230,
                    0
                ],
                "arxiv_comment": "27 pages, 16 figures. The previous version contained an error on the success probabilities of QPE in the filtering process which has been now corrected by the use of Quantum Signal Processing",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Pablo A. M. Casares"
                    },
                    {
                        "name": "Yanbing Zhou"
                    },
                    {
                        "name": "Utkarsh Azad"
                    },
                    {
                        "name": "Stepan Fomichev"
                    },
                    {
                        "name": "Jack S. Baker"
                    },
                    {
                        "name": "Chen Ling"
                    },
                    {
                        "name": "Debasish Banerjee"
                    },
                    {
                        "name": "Alain Delgado"
                    },
                    {
                        "name": "Juan Miguel Arrazola"
                    }
                ],
                "author_detail": {
                    "name": "Juan Miguel Arrazola"
                },
                "author": "Juan Miguel Arrazola"
            },
            {
                "id": "http://arxiv.org/abs/2512.19513v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19513v1",
                "title": "Parameter estimation for the GWTC-4.0 catalog with phenomenological waveform models that include orbital eccentricity and an updated description of spin precession",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation for the GWTC-4.0 catalog with phenomenological waveform models that include orbital eccentricity and an updated description of spin precession"
                },
                "updated": "2025-12-22T16:06:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    6,
                    49,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19513v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The GWTC-4.0 catalog of transient gravitational wave signals describes observations made in the first part of the fourth observing run of the LIGO-Virgo-KAGRA (LVK) gravitational wave detector network. Here we extend the LVK's GWTC-4.0 analysis to elliptic orbits, and an improved description of spin precession in the frequency domain. For this study we use state-of-the-art waveforms from the IMRPhenom family (specifically XPNR, TPHM, and TEHM), and we consider the 84 confidently detected events that are consistent with binary-black-hole mergers. We present an extended catalog of updated posterior samples, quantify how incorporation of these waveform effects alters inferred source properties relative to previous analyses, and discuss waveform systematics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The GWTC-4.0 catalog of transient gravitational wave signals describes observations made in the first part of the fourth observing run of the LIGO-Virgo-KAGRA (LVK) gravitational wave detector network. Here we extend the LVK's GWTC-4.0 analysis to elliptic orbits, and an improved description of spin precession in the frequency domain. For this study we use state-of-the-art waveforms from the IMRPhenom family (specifically XPNR, TPHM, and TEHM), and we consider the 84 confidently detected events that are consistent with binary-black-hole mergers. We present an extended catalog of updated posterior samples, quantify how incorporation of these waveform effects alters inferred source properties relative to previous analyses, and discuss waveform systematics."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:06:49Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    6,
                    49,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "22 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "authors": [
                    {
                        "name": "Yumeng Xu"
                    },
                    {
                        "name": "Jorge Valencia"
                    },
                    {
                        "name": "Héctor Estellés Estrella"
                    },
                    {
                        "name": "Antoni Ramos Buades"
                    },
                    {
                        "name": "Sascha Husa"
                    },
                    {
                        "name": "Maria Rosselló-Sastre"
                    },
                    {
                        "name": "Joan Llobera Querol"
                    },
                    {
                        "name": "Felip Ramis Vidal"
                    },
                    {
                        "name": "Maria de Lluc Planas Llompart"
                    },
                    {
                        "name": "Marta Colleoni"
                    },
                    {
                        "name": "Eleanor Hamilton"
                    },
                    {
                        "name": "Arnau Montava Agudo"
                    },
                    {
                        "name": "Jesús Yébana Carrilero"
                    },
                    {
                        "name": "Anna Heffernan"
                    }
                ],
                "author_detail": {
                    "name": "Anna Heffernan"
                },
                "author": "Anna Heffernan"
            },
            {
                "id": "http://arxiv.org/abs/2509.23863v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23863v2",
                "title": "SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models"
                },
                "updated": "2025-12-22T16:06:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    6,
                    3,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23863v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-28T13:08:10Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    13,
                    8,
                    10,
                    6,
                    271,
                    0
                ],
                "arxiv_comment": "Preprint under review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Weizhou Shen"
                    },
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Xiaojun Quan"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang"
            },
            {
                "id": "http://arxiv.org/abs/2505.11076v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.11076v3",
                "title": "Addition is almost all you need: Compressing neural networks with double binary factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addition is almost all you need: Compressing neural networks with double binary factorization"
                },
                "updated": "2025-12-22T16:05:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    5,
                    29,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.11076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.11076v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria.\n  Code available at: https://github.com/usamec/double_binary",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria.\n  Code available at: https://github.com/usamec/double_binary"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-16T10:07:36Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    10,
                    7,
                    36,
                    4,
                    136,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Vladimír Boža"
                    },
                    {
                        "name": "Vladimír Macko"
                    }
                ],
                "author_detail": {
                    "name": "Vladimír Macko"
                },
                "author": "Vladimír Macko"
            },
            {
                "id": "http://arxiv.org/abs/2512.19510v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19510v1",
                "title": "Toward Scalable and Valid Conditional Independence Testing with Spectral Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Scalable and Valid Conditional Independence Testing with Spectral Representations"
                },
                "updated": "2025-12-22T16:05:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    5,
                    18,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19510v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Conditional independence (CI) is central to causal inference, feature selection, and graphical modeling, yet it is untestable in many settings without additional assumptions. Existing CI tests often rely on restrictive structural conditions, limiting their validity on real-world data. Kernel methods using the partial covariance operator offer a more principled approach but suffer from limited adaptivity, slow convergence, and poor scalability. In this work, we explore whether representation learning can help address these limitations. Specifically, we focus on representations derived from the singular value decomposition of the partial covariance operator and use them to construct a simple test statistic, reminiscent of the Hilbert-Schmidt Independence Criterion (HSIC). We also introduce a practical bi-level contrastive algorithm to learn these representations. Our theory links representation learning error to test performance and establishes asymptotic validity and power guarantees. Preliminary experiments suggest that this approach offers a practical and statistically grounded path toward scalable CI testing, bridging kernel-based theory with modern representation learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional independence (CI) is central to causal inference, feature selection, and graphical modeling, yet it is untestable in many settings without additional assumptions. Existing CI tests often rely on restrictive structural conditions, limiting their validity on real-world data. Kernel methods using the partial covariance operator offer a more principled approach but suffer from limited adaptivity, slow convergence, and poor scalability. In this work, we explore whether representation learning can help address these limitations. Specifically, we focus on representations derived from the singular value decomposition of the partial covariance operator and use them to construct a simple test statistic, reminiscent of the Hilbert-Schmidt Independence Criterion (HSIC). We also introduce a practical bi-level contrastive algorithm to learn these representations. Our theory links representation learning error to test performance and establishes asymptotic validity and power guarantees. Preliminary experiments suggest that this approach offers a practical and statistically grounded path toward scalable CI testing, bridging kernel-based theory with modern representation learning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:05:18Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    5,
                    18,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Alek Frohlich"
                    },
                    {
                        "name": "Vladimir Kostic"
                    },
                    {
                        "name": "Karim Lounici"
                    },
                    {
                        "name": "Daniel Perazzo"
                    },
                    {
                        "name": "Massimiliano Pontil"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Pontil"
                },
                "author": "Massimiliano Pontil"
            },
            {
                "id": "http://arxiv.org/abs/2512.19509v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19509v1",
                "title": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models"
                },
                "updated": "2025-12-22T16:04:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    4,
                    56,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19509v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:04:56Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    4,
                    56,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Accepted by FSE 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Shangbo Yun"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Jianghong Huang"
                    },
                    {
                        "name": "Beijun Shen"
                    }
                ],
                "author_detail": {
                    "name": "Beijun Shen"
                },
                "author": "Beijun Shen"
            },
            {
                "id": "http://arxiv.org/abs/2407.01751v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.01751v2",
                "title": "Asymptotic theory for nonparametric testing of $k$-monotonicity in discrete distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic theory for nonparametric testing of $k$-monotonicity in discrete distributions"
                },
                "updated": "2025-12-22T15:46:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    46,
                    3,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.01751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.01751v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In shape-constrained nonparametric inference, it is often necessary to perform preliminary tests to verify whether a probability mass function (p.m.f.) satisfies qualitative constraints such as monotonicity, convexity, or in general $k$-monotonicity. In this paper, we are interested in nonparametric testing of $k$-monotonicity of a finitely supported discrete distribution. We consider a unified testing framework based on a natural statistic which is directly derived from the very definition of $k$-monotonicity. The introduced framework allows us to design a new consistent method to select the unknown knot points that are required to consistently approximate the limit distribution of several test statistics based either on the empirical measure or the shape-constrained estimators of the p.m.f. We show that the resulting tests are asymptotically valid and consistent for any fixed alternative. Additionally, for the test based solely on the empirical measure, we study the asymptotic power under contiguous alternatives and derive a quantitative separation result that provides sufficient conditions to achieve a given power. We employ this test to design an estimator for the largest parameter $k \\in \\mathbb N_0$ such that the p.m.f. is $j$-monotone for all $j = 0, \\ldots, k$, and show that the estimator is different from the true parameter with probability which is asymptotically smaller than the nominal level of the test. A detailed simulation study is performed to assess the finite sample performance of all the proposed tests, and applications to several real datasets are presented to illustrate the theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In shape-constrained nonparametric inference, it is often necessary to perform preliminary tests to verify whether a probability mass function (p.m.f.) satisfies qualitative constraints such as monotonicity, convexity, or in general $k$-monotonicity. In this paper, we are interested in nonparametric testing of $k$-monotonicity of a finitely supported discrete distribution. We consider a unified testing framework based on a natural statistic which is directly derived from the very definition of $k$-monotonicity. The introduced framework allows us to design a new consistent method to select the unknown knot points that are required to consistently approximate the limit distribution of several test statistics based either on the empirical measure or the shape-constrained estimators of the p.m.f. We show that the resulting tests are asymptotically valid and consistent for any fixed alternative. Additionally, for the test based solely on the empirical measure, we study the asymptotic power under contiguous alternatives and derive a quantitative separation result that provides sufficient conditions to achieve a given power. We employ this test to design an estimator for the largest parameter $k \\in \\mathbb N_0$ such that the p.m.f. is $j$-monotone for all $j = 0, \\ldots, k$, and show that the estimator is different from the true parameter with probability which is asymptotically smaller than the nominal level of the test. A detailed simulation study is performed to assess the finite sample performance of all the proposed tests, and applications to several real datasets are presented to illustrate the theory."
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-01T19:29:07Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    19,
                    29,
                    7,
                    0,
                    183,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.ST"
                },
                "authors": [
                    {
                        "name": "Fadoua Balabdaoui"
                    },
                    {
                        "name": "Antonio Di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Di Noia"
                },
                "author": "Antonio Di Noia"
            },
            {
                "id": "http://arxiv.org/abs/2512.19488v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19488v1",
                "title": "Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks"
                },
                "updated": "2025-12-22T15:43:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    43,
                    39,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19488v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/CommNet68224.2025.11288887",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The widespread deployment of Internet of Things (IoT) devices requires intrusion detection systems (IDS) with high accuracy while operating under strict resource constraints. Conventional deep learning IDS are often too large and computationally intensive for edge deployment. We propose a lightweight IDS that combines SHAP-guided feature pruning with knowledge-distilled Kronecker networks. A high-capacity teacher model identifies the most relevant features through SHAP explanations, and a compressed student leverages Kronecker-structured layers to minimize parameters while preserving discriminative inputs. Knowledge distillation transfers softened decision boundaries from teacher to student, improving generalization under compression. Experiments on the TON\\_IoT dataset show that the student is nearly three orders of magnitude smaller than the teacher yet sustains macro-F1 above 0.986 with millisecond-level inference latency. The results demonstrate that explainability-driven pruning and structured compression can jointly enable scalable, low-latency, and energy-efficient IDS for heterogeneous IoT environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of Internet of Things (IoT) devices requires intrusion detection systems (IDS) with high accuracy while operating under strict resource constraints. Conventional deep learning IDS are often too large and computationally intensive for edge deployment. We propose a lightweight IDS that combines SHAP-guided feature pruning with knowledge-distilled Kronecker networks. A high-capacity teacher model identifies the most relevant features through SHAP explanations, and a compressed student leverages Kronecker-structured layers to minimize parameters while preserving discriminative inputs. Knowledge distillation transfers softened decision boundaries from teacher to student, improving generalization under compression. Experiments on the TON\\_IoT dataset show that the student is nearly three orders of magnitude smaller than the teacher yet sustains macro-F1 above 0.986 with millisecond-level inference latency. The results demonstrate that explainability-driven pruning and structured compression can jointly enable scalable, low-latency, and energy-efficient IDS for heterogeneous IoT environments."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:43:39Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    43,
                    39,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "This work has been published in the proceedings of the 2025 8th International Conference on Advanced Communication Technologies and Networking (CommNet)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hafsa Benaddi"
                    },
                    {
                        "name": "Mohammed Jouhari"
                    },
                    {
                        "name": "Nouha Laamech"
                    },
                    {
                        "name": "Anas Motii"
                    },
                    {
                        "name": "Khalil Ibrahimi"
                    }
                ],
                "author_detail": {
                    "name": "Khalil Ibrahimi"
                },
                "author": "Khalil Ibrahimi",
                "arxiv_doi": "10.1109/CommNet68224.2025.11288887"
            },
            {
                "id": "http://arxiv.org/abs/2302.11322v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2302.11322v3",
                "title": "Causal inference with misspecified network interference structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference with misspecified network interference structure"
                },
                "updated": "2025-12-22T15:43:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    43,
                    28,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2302.11322v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2302.11322v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Under interference, the treatment of one unit may affect the outcomes of other units. Such interference patterns between units are typically represented by a network. Correctly specifying this network requires identifying which units can affect others -- an inherently challenging task. Nevertheless, most existing approaches assume that a known and accurate network specification is given. In this paper, we study the consequences of such misspecification.\n  We derive bounds on the bias arising from estimating causal effects using a misspecified network, showing that the estimation bias grows with the divergence between the assumed and true networks, quantified through their induced exposure probabilities. To address this challenge, we propose a novel estimator that leverages multiple networks simultaneously and remains unbiased if at least one of the networks is correct, even when we do not know which one. Therefore, the proposed estimator provides robustness to network specification. We illustrate key properties and demonstrate the utility of our proposed estimator through simulations and analysis of a social network field experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under interference, the treatment of one unit may affect the outcomes of other units. Such interference patterns between units are typically represented by a network. Correctly specifying this network requires identifying which units can affect others -- an inherently challenging task. Nevertheless, most existing approaches assume that a known and accurate network specification is given. In this paper, we study the consequences of such misspecification.\n  We derive bounds on the bias arising from estimating causal effects using a misspecified network, showing that the estimation bias grows with the divergence between the assumed and true networks, quantified through their induced exposure probabilities. To address this challenge, we propose a novel estimator that leverages multiple networks simultaneously and remains unbiased if at least one of the networks is correct, even when we do not know which one. Therefore, the proposed estimator provides robustness to network specification. We illustrate key properties and demonstrate the utility of our proposed estimator through simulations and analysis of a social network field experiment."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-02-22T12:08:21Z",
                "published_parsed": [
                    2023,
                    2,
                    22,
                    12,
                    8,
                    21,
                    2,
                    53,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Bar Weinstein"
                    },
                    {
                        "name": "Daniel Nevo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Nevo"
                },
                "author": "Daniel Nevo"
            },
            {
                "id": "http://arxiv.org/abs/2512.19484v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19484v1",
                "title": "Structured Event Representation and Stock Return Predictability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Event Representation and Stock Return Predictability"
                },
                "updated": "2025-12-22T15:40:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    40,
                    27,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19484v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We find that event features extracted by large language models (LLMs) are effective for text-based stock return prediction. Using a pre-trained LLM to extract event features from news articles, we propose a novel deep learning model based on structured event representation (SER) and attention mechanisms to predict stock returns in the cross-section. Our SER-based model provides superior performance compared with other existing text-driven models to forecast stock returns out of sample and offers highly interpretable feature structures to examine the mechanisms underlying the stock return predictability. We further provide various implications based on SER and highlight the crucial benefit of structured model inputs in stock return predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find that event features extracted by large language models (LLMs) are effective for text-based stock return prediction. Using a pre-trained LLM to extract event features from news articles, we propose a novel deep learning model based on structured event representation (SER) and attention mechanisms to predict stock returns in the cross-section. Our SER-based model provides superior performance compared with other existing text-driven models to forecast stock returns out of sample and offers highly interpretable feature structures to examine the mechanisms underlying the stock return predictability. We further provide various implications based on SER and highlight the crucial benefit of structured model inputs in stock return predictability."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:40:27Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    40,
                    27,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Dandan Qiao"
                    },
                    {
                        "name": "Mingxuan Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Zheng"
                },
                "author": "Mingxuan Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2505.15925v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.15925v3",
                "title": "VERDI: VLM-Embedded Reasoning for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERDI: VLM-Embedded Reasoning for Autonomous Driving"
                },
                "updated": "2025-12-22T15:37:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    37,
                    49,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.15925v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.15925v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We validate VERDI in both open-loop (NuScenes and Bench2Drive benchmarks) and closed-loop (HugSim Simulator) settings. We find that VERDI outperforms existing e2e methods that do not embed reasoning by up to 11% in $\\ell_{2}$ distance and 11% in driving performance, while maintaining real-time inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We validate VERDI in both open-loop (NuScenes and Bench2Drive benchmarks) and closed-loop (HugSim Simulator) settings. We find that VERDI outperforms existing e2e methods that do not embed reasoning by up to 11% in $\\ell_{2}$ distance and 11% in driving performance, while maintaining real-time inference speed."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-21T18:24:36Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    18,
                    24,
                    36,
                    2,
                    141,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Bowen Feng"
                    },
                    {
                        "name": "Zhiting Mei"
                    },
                    {
                        "name": "Baiang Li"
                    },
                    {
                        "name": "Julian Ost"
                    },
                    {
                        "name": "Filippo Ghilotti"
                    },
                    {
                        "name": "Roger Girgis"
                    },
                    {
                        "name": "Anirudha Majumdar"
                    },
                    {
                        "name": "Felix Heide"
                    }
                ],
                "author_detail": {
                    "name": "Felix Heide"
                },
                "author": "Felix Heide"
            },
            {
                "id": "http://arxiv.org/abs/2508.00179v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.00179v2",
                "title": "Eccentricity signatures in LIGO-Virgo-KAGRA's BNS and NSBH binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eccentricity signatures in LIGO-Virgo-KAGRA's BNS and NSBH binaries"
                },
                "updated": "2025-12-22T15:36:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    36,
                    8,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.00179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.00179v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1103/jnsc-783p",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Measurement of eccentricity in low-mass binary systems through gravitational waves is crucial to distinguish between various formation channels. Detecting eccentricity in these systems is challenging due to a lack of accurate eccentric waveform models and the high computational cost of Bayesian inferences. We access the eccentricities of six previously observed low-mass gravitational wave events using publicly available data from the first four observing runs of the LIGO and Virgo collaboration. We analyze the events using the new eccentric waveform model, SEOBNRv5EHM, and compare our results with the existing model, TEOBResumS-Dali. We also present the first eccentricity constraints for GW190814. To improve accuracy, we include higher-order modes in both models and optimize inference using efficient marginalization and parallelization techniques. We find that GW200105 exhibits non-negligible eccentricity, with a measured eccentricity of $e=0.135^{+0.019}_{-0.088}$ at 20 Hz (90% credible level) for TEOBResumS-Dali and $e=0.125^{+0.029}_{-0.082}$ for SEOBNRv5EHM, given a uniform eccentricity prior from 0 to 0.2. This provides moderate support for the eccentric hypothesis, with a Bayes factor of $\\sim6-7$ in favor of the eccentric model. With a uniform log prior on eccentricity, the Bayes factor is reduced to 2.35. The remaining five sources are consistent with low eccentricity, with 90% upper limits from $e \\leq 0.011$ to $e \\leq 0.066$. We find no support for non-negligible eccentricity in GW190814. Finally, we discuss the challenges of performing Bayesian inference in eccentric, multi-modal parameter spaces, including issues related to sampling efficiency and waveform systematics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of eccentricity in low-mass binary systems through gravitational waves is crucial to distinguish between various formation channels. Detecting eccentricity in these systems is challenging due to a lack of accurate eccentric waveform models and the high computational cost of Bayesian inferences. We access the eccentricities of six previously observed low-mass gravitational wave events using publicly available data from the first four observing runs of the LIGO and Virgo collaboration. We analyze the events using the new eccentric waveform model, SEOBNRv5EHM, and compare our results with the existing model, TEOBResumS-Dali. We also present the first eccentricity constraints for GW190814. To improve accuracy, we include higher-order modes in both models and optimize inference using efficient marginalization and parallelization techniques. We find that GW200105 exhibits non-negligible eccentricity, with a measured eccentricity of $e=0.135^{+0.019}_{-0.088}$ at 20 Hz (90% credible level) for TEOBResumS-Dali and $e=0.125^{+0.029}_{-0.082}$ for SEOBNRv5EHM, given a uniform eccentricity prior from 0 to 0.2. This provides moderate support for the eccentric hypothesis, with a Bayes factor of $\\sim6-7$ in favor of the eccentric model. With a uniform log prior on eccentricity, the Bayes factor is reduced to 2.35. The remaining five sources are consistent with low eccentricity, with 90% upper limits from $e \\leq 0.011$ to $e \\leq 0.066$. We find no support for non-negligible eccentricity in GW190814. Finally, we discuss the challenges of performing Bayesian inference in eccentric, multi-modal parameter spaces, including issues related to sampling efficiency and waveform systematics."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-31T21:46:55Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    46,
                    55,
                    3,
                    212,
                    0
                ],
                "arxiv_comment": "16 pages, 4 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "arxiv_journal_ref": "Published in Phys. Rev. D 112, 122007 (2025)",
                "authors": [
                    {
                        "name": "Keisi Kacanja"
                    },
                    {
                        "name": "Kanchan Soni"
                    },
                    {
                        "name": "Alexander Harvey Nitz"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Harvey Nitz"
                },
                "author": "Alexander Harvey Nitz",
                "arxiv_doi": "10.1103/jnsc-783p"
            },
            {
                "id": "http://arxiv.org/abs/2512.19481v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19481v1",
                "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis"
                },
                "updated": "2025-12-22T15:32:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    32,
                    45,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19481v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:32:45Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    32,
                    45,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "6 pages",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Katharina Stengg"
                    },
                    {
                        "name": "Christian Macho"
                    },
                    {
                        "name": "Martin Pinzger"
                    }
                ],
                "author_detail": {
                    "name": "Martin Pinzger"
                },
                "author": "Martin Pinzger"
            },
            {
                "id": "http://arxiv.org/abs/2512.19475v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19475v1",
                "title": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting"
                },
                "updated": "2025-12-22T15:28:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    28,
                    55,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19475v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:28:55Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    28,
                    55,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "18 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ivan Decostanzi"
                    },
                    {
                        "name": "Yelena Mejova"
                    },
                    {
                        "name": "Kyriaki Kalimeri"
                    }
                ],
                "author_detail": {
                    "name": "Kyriaki Kalimeri"
                },
                "author": "Kyriaki Kalimeri"
            },
            {
                "id": "http://arxiv.org/abs/2512.19466v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19466v1",
                "title": "Epistemological Fault Lines Between Human and Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epistemological Fault Lines Between Human and Artificial Intelligence"
                },
                "updated": "2025-12-22T15:20:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    20,
                    21,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19466v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are widely described as artificial intelligence, yet their epistemic profile diverges sharply from human cognition. Here we show that the apparent alignment between human and machine outputs conceals a deeper structural mismatch in how judgments are produced. Tracing the historical shift from symbolic AI and information filtering systems to large-scale generative transformers, we argue that LLMs are not epistemic agents but stochastic pattern-completion systems, formally describable as walks on high-dimensional graphs of linguistic transitions rather than as systems that form beliefs or models of the world. By systematically mapping human and artificial epistemic pipelines, we identify seven epistemic fault lines, divergences in grounding, parsing, experience, motivation, causal reasoning, metacognition, and value. We call the resulting condition Epistemia: a structural situation in which linguistic plausibility substitutes for epistemic evaluation, producing the feeling of knowing without the labor of judgment. We conclude by outlining consequences for evaluation, governance, and epistemic literacy in societies increasingly organized around generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely described as artificial intelligence, yet their epistemic profile diverges sharply from human cognition. Here we show that the apparent alignment between human and machine outputs conceals a deeper structural mismatch in how judgments are produced. Tracing the historical shift from symbolic AI and information filtering systems to large-scale generative transformers, we argue that LLMs are not epistemic agents but stochastic pattern-completion systems, formally describable as walks on high-dimensional graphs of linguistic transitions rather than as systems that form beliefs or models of the world. By systematically mapping human and artificial epistemic pipelines, we identify seven epistemic fault lines, divergences in grounding, parsing, experience, motivation, causal reasoning, metacognition, and value. We call the resulting condition Epistemia: a structural situation in which linguistic plausibility substitutes for epistemic evaluation, producing the feeling of knowing without the labor of judgment. We conclude by outlining consequences for evaluation, governance, and epistemic literacy in societies increasingly organized around generative AI."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:20:21Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    20,
                    21,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "16 pages, 1 figure",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Walter Quattrociocchi"
                    },
                    {
                        "name": "Valerio Capraro"
                    },
                    {
                        "name": "Matjaž Perc"
                    }
                ],
                "author_detail": {
                    "name": "Matjaž Perc"
                },
                "author": "Matjaž Perc"
            },
            {
                "id": "http://arxiv.org/abs/2502.09354v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.09354v2",
                "title": "Trajectory Inference for Single Cell Omics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Inference for Single Cell Omics"
                },
                "updated": "2025-12-22T15:16:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    16,
                    15,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.09354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.09354v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Trajectory inference is used to order single-cell omics data along a path that reflects a continuous transition between cells. This approach is useful for studying processes like cell differentiation, where a stem cell matures into a specialized cell type, or investigating state changes in pathological conditions. In the current article, we provide a general introduction to trajectory inference, explaining the concepts and assumptions underlying the different methods. We then briefly discuss the strengths and weaknesses of different trajectory inference methods. We also describe best practices for using trajectory inference, such as how to validate the results and how to interpret them in the context of biological knowledge. Finally, the article highlights some applications of trajectory inference in single-cell omics research. These applications include studying cell differentiation, development, and disease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory inference is used to order single-cell omics data along a path that reflects a continuous transition between cells. This approach is useful for studying processes like cell differentiation, where a stem cell matures into a specialized cell type, or investigating state changes in pathological conditions. In the current article, we provide a general introduction to trajectory inference, explaining the concepts and assumptions underlying the different methods. We then briefly discuss the strengths and weaknesses of different trajectory inference methods. We also describe best practices for using trajectory inference, such as how to validate the results and how to interpret them in the context of biological knowledge. Finally, the article highlights some applications of trajectory inference in single-cell omics research. These applications include studying cell differentiation, development, and disease."
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-13T14:19:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    19,
                    33,
                    3,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM"
                },
                "authors": [
                    {
                        "name": "Alexandre Hutton"
                    },
                    {
                        "name": "Jesse G. Meyer"
                    }
                ],
                "author_detail": {
                    "name": "Jesse G. Meyer"
                },
                "author": "Jesse G. Meyer"
            },
            {
                "id": "http://arxiv.org/abs/2512.19458v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19458v1",
                "title": "An Agentic Framework for Autonomous Materials Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic Framework for Autonomous Materials Computation"
                },
                "updated": "2025-12-22T15:03:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    3,
                    57,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19458v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:03:57Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    3,
                    57,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zeyu Xia"
                    },
                    {
                        "name": "Jinzhe Ma"
                    },
                    {
                        "name": "Congjie Zheng"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "P. Hu"
                    },
                    {
                        "name": "Changshui Zhang"
                    },
                    {
                        "name": "Xingao Gong"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Mao Su"
                    }
                ],
                "author_detail": {
                    "name": "Mao Su"
                },
                "author": "Mao Su"
            },
            {
                "id": "http://arxiv.org/abs/2512.19456v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19456v1",
                "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations"
                },
                "updated": "2025-12-22T15:01:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    1,
                    7,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19456v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:01:07Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    1,
                    7,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jinwei Chi"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Xuanye Lin"
                    },
                    {
                        "name": "Qiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Xu"
                },
                "author": "Qiang Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.19453v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19453v1",
                "title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation"
                },
                "updated": "2025-12-22T14:58:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    58,
                    52,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19453v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:58:52Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    58,
                    52,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "8 pages, 10 figures, This work was completed in December 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zhenglong Guo"
                    },
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Heng Jin"
                    },
                    {
                        "name": "Zongbao Feng"
                    },
                    {
                        "name": "Jianbin Zhou"
                    },
                    {
                        "name": "Siyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Xu"
                },
                "author": "Siyuan Xu"
            },
            {
                "id": "http://arxiv.org/abs/2408.10566v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.10566v5",
                "title": "Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning"
                },
                "updated": "2025-12-22T14:46:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    46,
                    47,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.10566v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.10566v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In continual learning (CL), model growth enhances adaptability to new data. However, when model growth is applied improperly, especially in task-agnostic CL, where the entire grown model is used for inference, it can lead to severe degradation of learned knowledge, a problem we term growth-induced forgetting. Most existing methods that adopt model growth to improve adaptability often overlook the forgetting issue, resulting in compromised knowledge retention, making them unsuitable for task-agnostic settings. To promote both adaptability and knowledge retention with model growth, we identify the key: gradient and parameter sparsity. Introducing SparseGrow, which increases gradient sparsity through layer expansion and gradient gating to enable focused updates on parameters while preserving critical parameters, thus inhibiting forgetting. Moreover, it promotes parameter sparsity with sparse initialization and training, aiming at better control of model plasticity, improving adaptability over new data. Extensive experiments across diverse datasets, task-agnostic settings, and a large number of tasks demonstrate the necessity of controlled layer expansion and validate the effectiveness of SparseGrow in achieving high adaptability while minimizing forgetting in continual learning. By enabling model growth with sparsified gradients and parameters, SparseGrow paves the way for building scalable lifelong learning systems capable of continual adaptation with better knowledge retention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In continual learning (CL), model growth enhances adaptability to new data. However, when model growth is applied improperly, especially in task-agnostic CL, where the entire grown model is used for inference, it can lead to severe degradation of learned knowledge, a problem we term growth-induced forgetting. Most existing methods that adopt model growth to improve adaptability often overlook the forgetting issue, resulting in compromised knowledge retention, making them unsuitable for task-agnostic settings. To promote both adaptability and knowledge retention with model growth, we identify the key: gradient and parameter sparsity. Introducing SparseGrow, which increases gradient sparsity through layer expansion and gradient gating to enable focused updates on parameters while preserving critical parameters, thus inhibiting forgetting. Moreover, it promotes parameter sparsity with sparse initialization and training, aiming at better control of model plasticity, improving adaptability over new data. Extensive experiments across diverse datasets, task-agnostic settings, and a large number of tasks demonstrate the necessity of controlled layer expansion and validate the effectiveness of SparseGrow in achieving high adaptability while minimizing forgetting in continual learning. By enabling model growth with sparsified gradients and parameters, SparseGrow paves the way for building scalable lifelong learning systems capable of continual adaptation with better knowledge retention."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-20T06:05:52Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    5,
                    52,
                    1,
                    233,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuqing Zhao"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Divya Saxena"
                    },
                    {
                        "name": "Xiaoyun Liu"
                    },
                    {
                        "name": "Changlin Song"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Julie McCann"
                    }
                ],
                "author_detail": {
                    "name": "Julie McCann"
                },
                "author": "Julie McCann"
            },
            {
                "id": "http://arxiv.org/abs/2512.19442v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19442v1",
                "title": "Real-Time Streamable Generative Speech Restoration with Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Streamable Generative Speech Restoration with Flow Matching"
                },
                "updated": "2025-12-22T14:41:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    41,
                    17,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19442v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.\n  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.\n  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.\n  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.\n  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:41:17Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    41,
                    17,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Simon Welker"
                    },
                    {
                        "name": "Bunlong Lay"
                    },
                    {
                        "name": "Maris Hillemann"
                    },
                    {
                        "name": "Tal Peer"
                    },
                    {
                        "name": "Timo Gerkmann"
                    }
                ],
                "author_detail": {
                    "name": "Timo Gerkmann"
                },
                "author": "Timo Gerkmann"
            },
            {
                "id": "http://arxiv.org/abs/2507.04453v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.04453v3",
                "title": "ESSA: Evolutionary Strategies for Scalable Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESSA: Evolutionary Strategies for Scalable Alignment"
                },
                "updated": "2025-12-22T14:35:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    35,
                    3,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.04453v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.04453v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Alignment of Large Language Models (LLMs) typically relies on Reinforcement Learning from Human Feedback (RLHF) with gradient-based optimizers such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO). While effective, these methods require complex distributed training, large memory budgets, and careful hyperparameter tuning, all of which become increasingly difficult at billion-parameter scale. We present ESSA, Evolutionary Strategies for Scalable Alignment, a gradient-free framework that aligns LLMs using only forward inference and black-box optimization. ESSA focuses optimization on Low-Rank Adapters (LoRA) and further compresses their parameter space by optimizing only the singular values from an singular value decomposition (SVD) of each adapter matrix. This dimensionality reduction makes evolutionary search practical even for very large models and allows efficient operation in quantized INT4 and INT8 inference mode. Across these benchmarks ESSA improves the test accuracy of Qwen2.5-Math-7B by 12.6% on GSM8K and 14.8% on PRM800K, and raises the accuracy of LLaMA3.1-8B on IFEval by 22.5%, all compared with GRPO. In large-scale settings ESSA shows stronger scaling than gradient-based methods: on Qwen2.5-32B for PRM800K it reaches near-optimal accuracy twice as fast on 16 GPUs and six times as fast on 128 GPUs compared with GRPO. These results position evolutionary strategies as a compelling, hardware-friendly alternative to gradient-based LLM alignment, combining competitive quality with substantially reduced wall-clock time and engineering overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of Large Language Models (LLMs) typically relies on Reinforcement Learning from Human Feedback (RLHF) with gradient-based optimizers such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO). While effective, these methods require complex distributed training, large memory budgets, and careful hyperparameter tuning, all of which become increasingly difficult at billion-parameter scale. We present ESSA, Evolutionary Strategies for Scalable Alignment, a gradient-free framework that aligns LLMs using only forward inference and black-box optimization. ESSA focuses optimization on Low-Rank Adapters (LoRA) and further compresses their parameter space by optimizing only the singular values from an singular value decomposition (SVD) of each adapter matrix. This dimensionality reduction makes evolutionary search practical even for very large models and allows efficient operation in quantized INT4 and INT8 inference mode. Across these benchmarks ESSA improves the test accuracy of Qwen2.5-Math-7B by 12.6% on GSM8K and 14.8% on PRM800K, and raises the accuracy of LLaMA3.1-8B on IFEval by 22.5%, all compared with GRPO. In large-scale settings ESSA shows stronger scaling than gradient-based methods: on Qwen2.5-32B for PRM800K it reaches near-optimal accuracy twice as fast on 16 GPUs and six times as fast on 128 GPUs compared with GRPO. These results position evolutionary strategies as a compelling, hardware-friendly alternative to gradient-based LLM alignment, combining competitive quality with substantially reduced wall-clock time and engineering overhead."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-06T16:23:07Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    16,
                    23,
                    7,
                    6,
                    187,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Daria Korotyshova"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Alexey Malakhov"
                    },
                    {
                        "name": "Alexey Khokhulin"
                    },
                    {
                        "name": "Nikita Surnachev"
                    },
                    {
                        "name": "Kirill Ovcharenko"
                    },
                    {
                        "name": "George Bredis"
                    },
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov"
            },
            {
                "id": "http://arxiv.org/abs/2512.19428v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19428v1",
                "title": "Attention Is Not What You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is Not What You Need"
                },
                "updated": "2025-12-22T14:29:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    29,
                    18,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19428v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.\n  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.\n  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.\n  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.\n  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:29:18Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    29,
                    18,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhang Chong"
                    }
                ],
                "author_detail": {
                    "name": "Zhang Chong"
                },
                "author": "Zhang Chong"
            },
            {
                "id": "http://arxiv.org/abs/2512.19424v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19424v1",
                "title": "CodeSimpleQA: Scaling Factuality in Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSimpleQA: Scaling Factuality in Code Large Language Models"
                },
                "updated": "2025-12-22T14:27:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    27,
                    17,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19424v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:27:17Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    27,
                    17,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Shawn Guo"
                    },
                    {
                        "name": "Haowen Wang"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Weifeng Lv"
                    }
                ],
                "author_detail": {
                    "name": "Weifeng Lv"
                },
                "author": "Weifeng Lv"
            },
            {
                "id": "http://arxiv.org/abs/2406.08880v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.08880v3",
                "title": "Jackknife inference with two-way clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jackknife inference with two-way clustering"
                },
                "updated": "2025-12-22T14:16:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    16,
                    8,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.08880v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.08880v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "For linear regression models with cross-section or panel data, it is natural to assume that the disturbances are clustered in two dimensions. However, the finite-sample properties of two-way cluster-robust tests and confidence intervals are often poor. We discuss several ways to improve inference with two-way clustering. Two of these are existing methods for avoiding, or at least ameliorating, the problem of undefined standard errors when a cluster-robust variance matrix estimator (CRVE) is not positive definite. One is a new method that always avoids the problem. More importantly, we propose a family of new two-way CRVEs based on the cluster jackknife and prove that they yield valid inferences asymptotically. Simulations for models with two-way fixed effects suggest that, in many cases, the cluster-jackknife CRVE combined with our new method yields surprisingly accurate inferences. We provide a simple software package, twowayjack for Stata, that implements our recommended variance estimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For linear regression models with cross-section or panel data, it is natural to assume that the disturbances are clustered in two dimensions. However, the finite-sample properties of two-way cluster-robust tests and confidence intervals are often poor. We discuss several ways to improve inference with two-way clustering. Two of these are existing methods for avoiding, or at least ameliorating, the problem of undefined standard errors when a cluster-robust variance matrix estimator (CRVE) is not positive definite. One is a new method that always avoids the problem. More importantly, we propose a family of new two-way CRVEs based on the cluster jackknife and prove that they yield valid inferences asymptotically. Simulations for models with two-way fixed effects suggest that, in many cases, the cluster-jackknife CRVE combined with our new method yields surprisingly accurate inferences. We provide a simple software package, twowayjack for Stata, that implements our recommended variance estimator."
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-13T07:31:46Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    7,
                    31,
                    46,
                    3,
                    165,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM"
                },
                "authors": [
                    {
                        "name": "James G. MacKinnon"
                    },
                    {
                        "name": "Morten Ørregaard Nielsen"
                    },
                    {
                        "name": "Matthew D. Webb"
                    }
                ],
                "author_detail": {
                    "name": "Matthew D. Webb"
                },
                "author": "Matthew D. Webb"
            },
            {
                "id": "http://arxiv.org/abs/2512.19414v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19414v1",
                "title": "From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions"
                },
                "updated": "2025-12-22T14:13:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    13,
                    1,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19414v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:13:01Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    13,
                    1,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Jiaren Peng"
                    },
                    {
                        "name": "Hongda Sun"
                    },
                    {
                        "name": "Xuan Tian"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Zeqing Li"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan"
            },
            {
                "id": "http://arxiv.org/abs/2407.12538v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.12538v3",
                "title": "High Frequency Matters: Uncertainty Guided Image Compression with Wavelet Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Frequency Matters: Uncertainty Guided Image Compression with Wavelet Diffusion"
                },
                "updated": "2025-12-22T14:04:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    4,
                    56,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.12538v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.12538v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion probabilistic models have recently achieved remarkable success in generating high-quality images. However, balancing high perceptual quality and low distortion remains challenging in application of diffusion models in image compression. To address this issue, we propose a novel Uncertainty-Guided image compression approach with wavelet Diffusion (UGDiff). Our approach focuses on high frequency compression via the wavelet transform, since high frequency components are crucial for reconstructing image details. We introduce a wavelet conditional diffusion model for high frequency prediction, followed by a residual codec that compresses and transmits prediction residuals to the decoder. This diffusion prediction-then-residual compression paradigm effectively addresses the low fidelity issue common in direct reconstructions by existing diffusion models. Considering the uncertainty from the random sampling of the diffusion model, we further design an uncertainty-weighted rate-distortion (R-D) loss tailored for residual compression, providing a more rational trade-off between rate and distortion. Comprehensive experiments on two benchmark datasets validate the effectiveness of UGDiff, surpassing state-of-the-art image compression methods in R-D performance, perceptual quality, subjective quality, and inference time. Our code is available at: https://github.com/hejiaxiang1/Wavelet-Diffusion/tree/main.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion probabilistic models have recently achieved remarkable success in generating high-quality images. However, balancing high perceptual quality and low distortion remains challenging in application of diffusion models in image compression. To address this issue, we propose a novel Uncertainty-Guided image compression approach with wavelet Diffusion (UGDiff). Our approach focuses on high frequency compression via the wavelet transform, since high frequency components are crucial for reconstructing image details. We introduce a wavelet conditional diffusion model for high frequency prediction, followed by a residual codec that compresses and transmits prediction residuals to the decoder. This diffusion prediction-then-residual compression paradigm effectively addresses the low fidelity issue common in direct reconstructions by existing diffusion models. Considering the uncertainty from the random sampling of the diffusion model, we further design an uncertainty-weighted rate-distortion (R-D) loss tailored for residual compression, providing a more rational trade-off between rate and distortion. Comprehensive experiments on two benchmark datasets validate the effectiveness of UGDiff, surpassing state-of-the-art image compression methods in R-D performance, perceptual quality, subjective quality, and inference time. Our code is available at: https://github.com/hejiaxiang1/Wavelet-Diffusion/tree/main."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-17T13:21:31Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    13,
                    21,
                    31,
                    2,
                    199,
                    0
                ],
                "arxiv_comment": "Revised version for IEEE TMM submission",
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Juan Song"
                    },
                    {
                        "name": "Jiaxiang He"
                    },
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Mingtao Feng"
                    },
                    {
                        "name": "Keyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keyan Wang"
                },
                "author": "Keyan Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08398v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08398v2",
                "title": "Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring"
                },
                "updated": "2025-12-22T13:57:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    57,
                    26,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08398v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T09:26:37Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    26,
                    37,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "The authors have identified significant technical errors in the paper that invalidate the current findings",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Jiin Park"
                    },
                    {
                        "name": "Hyuna Jeon"
                    },
                    {
                        "name": "Yoonseo Lee"
                    },
                    {
                        "name": "Jisu Hong"
                    },
                    {
                        "name": "Misuk Kim"
                    }
                ],
                "author_detail": {
                    "name": "Misuk Kim"
                },
                "author": "Misuk Kim"
            },
            {
                "id": "http://arxiv.org/abs/2512.19399v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19399v1",
                "title": "Brain-Grounded Axes for Reading and Steering LLM States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-Grounded Axes for Reading and Steering LLM States"
                },
                "updated": "2025-12-22T13:51:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    51,
                    3,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19399v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:51:03Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    51,
                    3,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "10 pages, 4 figures. Code: https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sandro Andric"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Andric"
                },
                "author": "Sandro Andric"
            },
            {
                "id": "http://arxiv.org/abs/2512.19396v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19396v1",
                "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration"
                },
                "updated": "2025-12-22T13:42:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    42,
                    18,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19396v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:42:18Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    42,
                    18,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Runze Li"
                    },
                    {
                        "name": "Yuwen Zhai"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "LiWu Xu"
                    },
                    {
                        "name": "Nian Shi"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Ran Lin"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.19392v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19392v1",
                "title": "Holographic correlators from multi-mode AdS$_5$ bubbling geometries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holographic correlators from multi-mode AdS$_5$ bubbling geometries"
                },
                "updated": "2025-12-22T13:40:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    40,
                    16,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19392v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Supergravity calculations of holographic four-point correlation functions are a powerful tool for deriving dynamical information about the dual field theory at strong coupling. Recently, a number of such computations have been performed by studying light probes of smooth horizonless supergravity solutions, including certain asymptotically $AdS_5\\times S^5$ solutions of Lin-Lunin-Maldacena (LLM) type. We construct a new closed-form perturbative LLM solution involving a pair of linearized supergravitons with different mode numbers and their quadratic backreaction. Using this solution, we compute two infinite sequences of four-point correlators. Depending on the scaling of certain parameters, the background supergravity solutions are dual to states of 4D $\\mathcal{N}=4$ super Yang-Mills theory that are either heavy or light, whereupon the correlators in the dual field theory are either (perturbatively) heavy-heavy-light-light or all-light. In the all-light regime, the correlators we compute are related by superconformal Ward identities to correlators of four single-particle chiral primary operators (CPOs). We confirm a set of expressions in the literature for such correlators of CPOs, that were obtained via Mellin space bootstrap and/or Witten diagram methods. We also obtain a novel compact expression for one of the two infinite sequences of correlators of chiral primaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supergravity calculations of holographic four-point correlation functions are a powerful tool for deriving dynamical information about the dual field theory at strong coupling. Recently, a number of such computations have been performed by studying light probes of smooth horizonless supergravity solutions, including certain asymptotically $AdS_5\\times S^5$ solutions of Lin-Lunin-Maldacena (LLM) type. We construct a new closed-form perturbative LLM solution involving a pair of linearized supergravitons with different mode numbers and their quadratic backreaction. Using this solution, we compute two infinite sequences of four-point correlators. Depending on the scaling of certain parameters, the background supergravity solutions are dual to states of 4D $\\mathcal{N}=4$ super Yang-Mills theory that are either heavy or light, whereupon the correlators in the dual field theory are either (perturbatively) heavy-heavy-light-light or all-light. In the all-light regime, the correlators we compute are related by superconformal Ward identities to correlators of four single-particle chiral primary operators (CPOs). We confirm a set of expressions in the literature for such correlators of CPOs, that were obtained via Mellin space bootstrap and/or Witten diagram methods. We also obtain a novel compact expression for one of the two infinite sequences of correlators of chiral primaries."
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:40:16Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    40,
                    16,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "29 pages",
                "arxiv_primary_category": {
                    "term": "hep-th"
                },
                "authors": [
                    {
                        "name": "David Turton"
                    },
                    {
                        "name": "Alexander Tyukov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Tyukov"
                },
                "author": "Alexander Tyukov"
            },
            {
                "id": "http://arxiv.org/abs/2512.16229v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16229v2",
                "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding"
                },
                "updated": "2025-12-22T13:29:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    29,
                    11,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16229v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T06:22:01Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    6,
                    22,
                    1,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chenkai Xu"
                    },
                    {
                        "name": "Yijie Jin"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yi Tu"
                    },
                    {
                        "name": "Guoping Long"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Mingcong Song"
                    },
                    {
                        "name": "Hongjie Si"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng"
            },
            {
                "id": "http://arxiv.org/abs/2512.19382v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19382v1",
                "title": "Superconductivity in Electron Liquids: Precision Many-Body Treatment of Coulomb Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superconductivity in Electron Liquids: Precision Many-Body Treatment of Coulomb Interaction"
                },
                "updated": "2025-12-22T13:26:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    26,
                    33,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19382v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "More than a century after discovery, the theory of conventional superconductivity remains incomplete. While the importance of electron-phonon coupling is understood, a controlled first-principles treatment of Coulomb interaction is lacking. Current ab initio calculations of superconductivity rely on a phenomenological downfolding approximation, replacing Coulomb interaction with a repulsive pseudopotential μ*, and leaving ambiguities in electron-phonon coupling with dynamical Coulomb interactions unresolved. We address this via an effective field theory approach, integrating out high-energy electronic degrees of freedom using variational Diagrammatic Monte Carlo. Applied to the uniform electron gas, this establishes a microscopic procedure to implement downfolding, define the pseudopotential, and express dynamical Coulomb effects on electron-phonon coupling via the electron vertex function. We find the bare pseudopotential significantly larger than conventional values. This yields improved pseudopotential estimates in simple metals and tests density functional perturbation theory accuracy for effective electron-phonon coupling. We present an ab initio workflow computing superconducting Tc from the anomalous vertex's precursory Cooper flow. This infers Tc from normal state calculations, enabling reliable estimates of very low Tc (including near quantum phase transitions) beyond conventional reach. Validating our approach on simple metals without empirical tuning, we resolve long-standing discrepancies and predict a pressure-induced transition in Al from superconducting to non-superconducting above ~60GPa. We propose ambient-pressure Mg and Na are proximal to a similar critical point. Our work establishes a controlled ab initio framework for electron-phonon superconductivity beyond the weak-correlation limit, paving the way for reliable Tc calculations and novel material design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than a century after discovery, the theory of conventional superconductivity remains incomplete. While the importance of electron-phonon coupling is understood, a controlled first-principles treatment of Coulomb interaction is lacking. Current ab initio calculations of superconductivity rely on a phenomenological downfolding approximation, replacing Coulomb interaction with a repulsive pseudopotential μ*, and leaving ambiguities in electron-phonon coupling with dynamical Coulomb interactions unresolved. We address this via an effective field theory approach, integrating out high-energy electronic degrees of freedom using variational Diagrammatic Monte Carlo. Applied to the uniform electron gas, this establishes a microscopic procedure to implement downfolding, define the pseudopotential, and express dynamical Coulomb effects on electron-phonon coupling via the electron vertex function. We find the bare pseudopotential significantly larger than conventional values. This yields improved pseudopotential estimates in simple metals and tests density functional perturbation theory accuracy for effective electron-phonon coupling. We present an ab initio workflow computing superconducting Tc from the anomalous vertex's precursory Cooper flow. This infers Tc from normal state calculations, enabling reliable estimates of very low Tc (including near quantum phase transitions) beyond conventional reach. Validating our approach on simple metals without empirical tuning, we resolve long-standing discrepancies and predict a pressure-induced transition in Al from superconducting to non-superconducting above ~60GPa. We propose ambient-pressure Mg and Na are proximal to a similar critical point. Our work establishes a controlled ab initio framework for electron-phonon superconductivity beyond the weak-correlation limit, paving the way for reliable Tc calculations and novel material design."
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:26:33Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    26,
                    33,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con"
                },
                "authors": [
                    {
                        "name": "Xiansheng Cai"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Tiantian Zhang"
                    },
                    {
                        "name": "Andrew Millis"
                    },
                    {
                        "name": "Boris V. Svistunov"
                    },
                    {
                        "name": "Nikolay V. Prokof'ev"
                    },
                    {
                        "name": "Kun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kun Chen"
                },
                "author": "Kun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.19379v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19379v1",
                "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation"
                },
                "updated": "2025-12-22T13:23:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    23,
                    55,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19379v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:23:55Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    23,
                    55,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xueming Yan"
                    },
                    {
                        "name": "Boyan Xu"
                    },
                    {
                        "name": "Yaochu Jin"
                    },
                    {
                        "name": "Lixian Xiao"
                    },
                    {
                        "name": "Wenlong Ye"
                    },
                    {
                        "name": "Runyang Cai"
                    },
                    {
                        "name": "Zeqi Zheng"
                    },
                    {
                        "name": "Jingfa Liu"
                    },
                    {
                        "name": "Aimin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Aimin Yang"
                },
                "author": "Aimin Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.19378v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19378v1",
                "title": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models"
                },
                "updated": "2025-12-22T13:23:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    23,
                    29,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19378v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:23:29Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    23,
                    29,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Camera-ready version of the paper accepted for oral presentation at the 11th International Conference on Computer and Communications (ICCC 2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "In Proceedings of the 11th International Conference on Computer and Communications, 2025",
                "authors": [
                    {
                        "name": "Zhiqing Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Jiazhong Lu"
                    },
                    {
                        "name": "Xiaolei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolei Liu"
                },
                "author": "Xiaolei Liu"
            },
            {
                "id": "http://arxiv.org/abs/2508.17061v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.17061v2",
                "title": "REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework"
                },
                "updated": "2025-12-22T13:11:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    11,
                    30,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.17061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.17061v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Photorealism is an important aspect of modern video games since it can shape player experience and impact immersion, narrative engagement, and visual fidelity. To achieve photorealism, beyond traditional rendering pipelines, generative models have been increasingly adopted as an effective approach for bridging the gap between the visual realism of synthetic and real worlds. However, under real-time constraints of video games, existing generative approaches continue to face a tradeoff between visual quality and runtime efficiency. In this work, we present a framework for enhancing the photorealism of rendered game frames using generative networks. We propose REGEN, which first employs a robust unpaired image-to-image translation model to generate semantically consistent photorealistic frames. These generated frames are then used to create a paired dataset, which transforms the problem to a simpler unpaired image-to-image translation. This enables training with a lightweight method, achieving real-time inference without compromising visual quality. We evaluate REGEN on Unreal Engine, showing, by employing the CMMD metric, that it achieves comparable or slightly improved visual quality compared to the robust method, while improving the frame rate by 12x. Additional experiments also validate that REGEN adheres to the semantic preservation of the initial robust image-to-image translation method and maintains temporal consistency. Code, pre-trained models, and demos for this work are available at: https://github.com/stefanos50/REGEN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photorealism is an important aspect of modern video games since it can shape player experience and impact immersion, narrative engagement, and visual fidelity. To achieve photorealism, beyond traditional rendering pipelines, generative models have been increasingly adopted as an effective approach for bridging the gap between the visual realism of synthetic and real worlds. However, under real-time constraints of video games, existing generative approaches continue to face a tradeoff between visual quality and runtime efficiency. In this work, we present a framework for enhancing the photorealism of rendered game frames using generative networks. We propose REGEN, which first employs a robust unpaired image-to-image translation model to generate semantically consistent photorealistic frames. These generated frames are then used to create a paired dataset, which transforms the problem to a simpler unpaired image-to-image translation. This enables training with a lightweight method, achieving real-time inference without compromising visual quality. We evaluate REGEN on Unreal Engine, showing, by employing the CMMD metric, that it achieves comparable or slightly improved visual quality compared to the robust method, while improving the frame rate by 12x. Additional experiments also validate that REGEN adheres to the semantic preservation of the initial robust image-to-image translation method and maintains temporal consistency. Code, pre-trained models, and demos for this work are available at: https://github.com/stefanos50/REGEN"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-23T15:28:05Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    15,
                    28,
                    5,
                    5,
                    235,
                    0
                ],
                "arxiv_comment": "8 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Stefanos Pasios"
                    },
                    {
                        "name": "Nikos Nikolaidis"
                    }
                ],
                "author_detail": {
                    "name": "Nikos Nikolaidis"
                },
                "author": "Nikos Nikolaidis"
            },
            {
                "id": "http://arxiv.org/abs/2512.19365v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19365v1",
                "title": "Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization"
                },
                "updated": "2025-12-22T13:07:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    7,
                    4,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19365v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:07:04Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    7,
                    4,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhongwei Chen"
                    },
                    {
                        "name": "Hai-Jun Rong"
                    },
                    {
                        "name": "Zhao-Xu Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoqi Li"
                },
                "author": "Guoqi Li"
            },
            {
                "id": "http://arxiv.org/abs/2506.17231v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.17231v2",
                "title": "Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs"
                },
                "updated": "2025-12-22T13:04:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    4,
                    30,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.17231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.17231v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the scale and complexity of jailbreaking attacks on large language models (LLMs) continue to escalate, their efficiency and practical applicability are constrained, posing a profound challenge to LLM security. Jailbreaking techniques have advanced from manual prompt engineering to automated methodologies. Recent advances have automated jailbreaking approaches that harness LLMs to generate jailbreak instructions and adversarial examples, delivering encouraging results. Nevertheless, these methods universally include an LLM generation phase, which, due to the complexities of deploying and reasoning with LLMs, impedes effective implementation and broader adoption. To mitigate this issue, we introduce \\textbf{Adversarial Prompt Distillation}, an innovative framework that integrates masked language modeling, reinforcement learning, and dynamic temperature control to distill LLM jailbreaking prowess into smaller language models (SLMs). This methodology enables efficient, robust jailbreak attacks while maintaining high success rates and accommodating a broader range of application contexts. Empirical evaluations affirm the approach's superiority in attack efficacy, resource optimization, and cross-model versatility. Our research underscores the practicality of transferring jailbreak capabilities to SLMs, reveals inherent vulnerabilities in LLMs, and provides novel insights to advance LLM security investigations. Our code is available at: https://github.com/lxgem/Efficient_and_Stealthy_Jailbreak_Attacks_via_Adversarial_Prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the scale and complexity of jailbreaking attacks on large language models (LLMs) continue to escalate, their efficiency and practical applicability are constrained, posing a profound challenge to LLM security. Jailbreaking techniques have advanced from manual prompt engineering to automated methodologies. Recent advances have automated jailbreaking approaches that harness LLMs to generate jailbreak instructions and adversarial examples, delivering encouraging results. Nevertheless, these methods universally include an LLM generation phase, which, due to the complexities of deploying and reasoning with LLMs, impedes effective implementation and broader adoption. To mitigate this issue, we introduce \\textbf{Adversarial Prompt Distillation}, an innovative framework that integrates masked language modeling, reinforcement learning, and dynamic temperature control to distill LLM jailbreaking prowess into smaller language models (SLMs). This methodology enables efficient, robust jailbreak attacks while maintaining high success rates and accommodating a broader range of application contexts. Empirical evaluations affirm the approach's superiority in attack efficacy, resource optimization, and cross-model versatility. Our research underscores the practicality of transferring jailbreak capabilities to SLMs, reveals inherent vulnerabilities in LLMs, and provides novel insights to advance LLM security investigations. Our code is available at: https://github.com/lxgem/Efficient_and_Stealthy_Jailbreak_Attacks_via_Adversarial_Prompt."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-26T08:27:51Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    27,
                    51,
                    0,
                    146,
                    0
                ],
                "arxiv_comment": "19 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Fangyu Wu"
                    },
                    {
                        "name": "Yushi Li"
                    },
                    {
                        "name": "Xiaobo Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobo Jin"
                },
                "author": "Xiaobo Jin"
            },
            {
                "id": "http://arxiv.org/abs/2506.18998v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.18998v3",
                "title": "Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge"
                },
                "updated": "2025-12-22T12:50:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    50,
                    45,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.18998v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.18998v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "When artificial intelligence mistakes memorization for intelligence, it creates a dangerous mirage of reasoning. Existing studies treat memorization and self-knowledge deficits in LLMs as separate issues and do not recognize an intertwining link that degrades the trustworthiness of LLM responses. In our study, we utilize a novel framework to ascertain if LLMs genuinely learn reasoning patterns from training data or merely memorize them to assume competence across problems of similar complexity focused on STEM domains. Our analysis shows a noteworthy problem in generalization: LLMs draw confidence from memorized solutions to infer a higher self-knowledge about their reasoning ability, which manifests as an over 45% inconsistency in feasibility assessments when faced with self-validated, logically coherent task perturbations. This effect is most pronounced in science and medicine domains, which tend to have maximal standardized jargon and problems, further confirming our approach. Significant wavering within the self-knowledge of LLMs also shows flaws in current architectures and training patterns, highlighting the need for techniques that ensure a balanced, consistent stance on models' perceptions of their own knowledge for maximum AI explainability and trustworthiness. Our code and results are available publicly at https://github.com/Sahil-R-Kale/mirage_of_mastery",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When artificial intelligence mistakes memorization for intelligence, it creates a dangerous mirage of reasoning. Existing studies treat memorization and self-knowledge deficits in LLMs as separate issues and do not recognize an intertwining link that degrades the trustworthiness of LLM responses. In our study, we utilize a novel framework to ascertain if LLMs genuinely learn reasoning patterns from training data or merely memorize them to assume competence across problems of similar complexity focused on STEM domains. Our analysis shows a noteworthy problem in generalization: LLMs draw confidence from memorized solutions to infer a higher self-knowledge about their reasoning ability, which manifests as an over 45% inconsistency in feasibility assessments when faced with self-validated, logically coherent task perturbations. This effect is most pronounced in science and medicine domains, which tend to have maximal standardized jargon and problems, further confirming our approach. Significant wavering within the self-knowledge of LLMs also shows flaws in current architectures and training patterns, highlighting the need for techniques that ensure a balanced, consistent stance on models' perceptions of their own knowledge for maximum AI explainability and trustworthiness. Our code and results are available publicly at https://github.com/Sahil-R-Kale/mirage_of_mastery"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-23T18:01:16Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    18,
                    1,
                    16,
                    0,
                    174,
                    0
                ],
                "arxiv_comment": "12 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sahil Kale"
                    }
                ],
                "author_detail": {
                    "name": "Sahil Kale"
                },
                "author": "Sahil Kale"
            },
            {
                "id": "http://arxiv.org/abs/2511.02864v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02864v3",
                "title": "Mathematical exploration and discovery at scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical exploration and discovery at scale"
                },
                "updated": "2025-12-22T12:49:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    49,
                    1,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02864v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AlphaEvolve (Novikov et al., 2025) is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.\n  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.\n  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaEvolve (Novikov et al., 2025) is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.\n  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.\n  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time."
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T16:04:07Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    16,
                    4,
                    7,
                    0,
                    307,
                    0
                ],
                "arxiv_comment": "81 pages, 35 figures",
                "arxiv_primary_category": {
                    "term": "cs.NE"
                },
                "authors": [
                    {
                        "name": "Bogdan Georgiev"
                    },
                    {
                        "name": "Javier Gómez-Serrano"
                    },
                    {
                        "name": "Terence Tao"
                    },
                    {
                        "name": "Adam Zsolt Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Adam Zsolt Wagner"
                },
                "author": "Adam Zsolt Wagner"
            },
            {
                "id": "http://arxiv.org/abs/2512.19349v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19349v1",
                "title": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop"
                },
                "updated": "2025-12-22T12:48:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    48,
                    29,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19349v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:48:29Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    48,
                    29,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "7 pages,1 figure,4 tables",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "JiaWei Zhu"
                    },
                    {
                        "name": "ZiHeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "ZiHeng Liu"
                },
                "author": "ZiHeng Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.10713v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10713v2",
                "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code"
                },
                "updated": "2025-12-22T12:47:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    47,
                    11,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10713v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:49:56Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    49,
                    56,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Itay Dreyfuss"
                    },
                    {
                        "name": "Antonio Abu Nassar"
                    },
                    {
                        "name": "Samuel Ackerman"
                    },
                    {
                        "name": "Axel Ben David"
                    },
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "Rami Katan"
                    },
                    {
                        "name": "Orna Raz"
                    },
                    {
                        "name": "Marcel Zalmanovici"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Zalmanovici"
                },
                "author": "Marcel Zalmanovici"
            },
            {
                "id": "http://arxiv.org/abs/2512.19347v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19347v1",
                "title": "OMP: One-step Meanflow Policy with Directional Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMP: One-step Meanflow Policy with Directional Alignment"
                },
                "updated": "2025-12-22T12:45:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    45,
                    35,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19347v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:45:35Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    45,
                    35,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Yize Huang"
                    },
                    {
                        "name": "Yuheng Zhao"
                    },
                    {
                        "name": "Paul Weng"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Yutong Ban"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Ban"
                },
                "author": "Yutong Ban"
            },
            {
                "id": "http://arxiv.org/abs/2512.17022v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17022v2",
                "title": "The contribution from small scales on two-point shear analysis: comparison between power spectrum and correlation function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The contribution from small scales on two-point shear analysis: comparison between power spectrum and correlation function"
                },
                "updated": "2025-12-22T12:39:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    39,
                    50,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17022v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A known problem in cosmic shear two-point statistics is the apparent inconsistency between analyses performed in harmonic space (power spectrum) and real space (angular correlation). This arises mainly from two factors: first, scale cuts in one space correspond to soft cuts in the other, as the relationship between the two spaces is mediated by Bessel functions. For the same reason, astrophysical effects that are compact in one space may not be in the other, which can lead to biased parameter estimates. In this paper, we argue that these two statistics are complementary: we expect a robust theory to provide consistent constraints regardless of the chosen scale cuts. We present the consequences of pushing our analysis to smaller scales in both spaces, accounting for different models of Intrinsic Alignment and Baryonic Feedback in HSC Y3 data: we find that the harmonic-space analysis is significantly less sensitive to the specific modeling of small-scale physics, with model-choice-driven biases in $S_8$ being 2-3 times smaller than in real space. We show that using a flexible, simulation-based emulator for baryonic feedback (BACCO) in combination with the TATT model for intrinsic alignments provides the most consistent cosmological constraints between the two spaces when pushing to the smallest scales. In contrast, the standard HMCode-2016 model results in a $\\sim 1.1σ$ tension between the two statistics. While harmonic space appears more robust for cosmological inference given current model uncertainties, real-space analyses offer a clearer separation of baryonic effects and will play a crucial role in distinguishing between baryonic feedback models in upcoming surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A known problem in cosmic shear two-point statistics is the apparent inconsistency between analyses performed in harmonic space (power spectrum) and real space (angular correlation). This arises mainly from two factors: first, scale cuts in one space correspond to soft cuts in the other, as the relationship between the two spaces is mediated by Bessel functions. For the same reason, astrophysical effects that are compact in one space may not be in the other, which can lead to biased parameter estimates. In this paper, we argue that these two statistics are complementary: we expect a robust theory to provide consistent constraints regardless of the chosen scale cuts. We present the consequences of pushing our analysis to smaller scales in both spaces, accounting for different models of Intrinsic Alignment and Baryonic Feedback in HSC Y3 data: we find that the harmonic-space analysis is significantly less sensitive to the specific modeling of small-scale physics, with model-choice-driven biases in $S_8$ being 2-3 times smaller than in real space. We show that using a flexible, simulation-based emulator for baryonic feedback (BACCO) in combination with the TATT model for intrinsic alignments provides the most consistent cosmological constraints between the two spaces when pushing to the smallest scales. In contrast, the standard HMCode-2016 model results in a $\\sim 1.1σ$ tension between the two statistics. While harmonic space appears more robust for cosmological inference given current model uncertainties, real-space analyses offer a clearer separation of baryonic effects and will play a crucial role in distinguishing between baryonic feedback models in upcoming surveys."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T19:37:03Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    19,
                    37,
                    3,
                    3,
                    352,
                    0
                ],
                "arxiv_comment": "30 pages, 16 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "João Ferri"
                    },
                    {
                        "name": "Elisa G. M. Ferreira"
                    },
                    {
                        "name": "Ryo Terasawa"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Terasawa"
                },
                "author": "Ryo Terasawa"
            },
            {
                "id": "http://arxiv.org/abs/2512.02789v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02789v2",
                "title": "TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking"
                },
                "updated": "2025-12-22T12:38:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    38,
                    9,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02789v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T14:04:30Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    14,
                    4,
                    30,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tang Haonan"
                    },
                    {
                        "name": "Chen Yanjun"
                    },
                    {
                        "name": "Jiang Lezhi"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Lezhi"
                },
                "author": "Jiang Lezhi"
            },
            {
                "id": "http://arxiv.org/abs/2512.19342v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19342v1",
                "title": "Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives"
                },
                "updated": "2025-12-22T12:36:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    36,
                    54,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19342v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:36:54Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    36,
                    54,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Kiril Dichev"
                    },
                    {
                        "name": "Filip Pawlowski"
                    },
                    {
                        "name": "Albert-Jan Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "Albert-Jan Yzelman"
                },
                "author": "Albert-Jan Yzelman"
            },
            {
                "id": "http://arxiv.org/abs/2512.19338v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19338v1",
                "title": "A hybrid-Hill estimator enabled by heavy-tailed block maxima",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A hybrid-Hill estimator enabled by heavy-tailed block maxima"
                },
                "updated": "2025-12-22T12:33:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    33,
                    36,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19338v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "When analysing extreme values, two alternative statistical approaches have historically been held in contention: the seminal block maxima method (or annual maxima method, spurred by hydrological applications) and the peaks-over-threshold. Clamoured amongst statisticians as wasteful of potentially informative data, the block maxima method gradually fell into disfavour whilst peaks-over-threshold-based methodologies were ushered to the centre stage of extreme value statistics. This paper proposes a hybrid method which reconciles these two hitherto disconnected approaches. Appealing in its simplicity, our main result introduces a new universal limiting characterisation of extremes that eschews the customary requirement of a sufficiently large block size for the plausible block maxima-fit to an extreme value distribution. We advocate that inference should be drawn solely on larger block maxima, from which practice the mainstream peaks-over-threshold methodology coalesces. The asymptotic properties of the promised hybrid-Hill estimator herald more than its efficiency, but rather that a fully-fledged unified semi-parametric stream of statistics for extreme values is viable. A finite sample simulation study demonstrates that a reduced-bias off-shoot of the hybrid-Hill estimator fares exceptionally well against the incumbent maximum likelihood estimation that relies on a numerical fit to the entire sample of block maxima.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When analysing extreme values, two alternative statistical approaches have historically been held in contention: the seminal block maxima method (or annual maxima method, spurred by hydrological applications) and the peaks-over-threshold. Clamoured amongst statisticians as wasteful of potentially informative data, the block maxima method gradually fell into disfavour whilst peaks-over-threshold-based methodologies were ushered to the centre stage of extreme value statistics. This paper proposes a hybrid method which reconciles these two hitherto disconnected approaches. Appealing in its simplicity, our main result introduces a new universal limiting characterisation of extremes that eschews the customary requirement of a sufficiently large block size for the plausible block maxima-fit to an extreme value distribution. We advocate that inference should be drawn solely on larger block maxima, from which practice the mainstream peaks-over-threshold methodology coalesces. The asymptotic properties of the promised hybrid-Hill estimator herald more than its efficiency, but rather that a fully-fledged unified semi-parametric stream of statistics for extreme values is viable. A finite sample simulation study demonstrates that a reduced-bias off-shoot of the hybrid-Hill estimator fares exceptionally well against the incumbent maximum likelihood estimation that relies on a numerical fit to the entire sample of block maxima."
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:33:36Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    33,
                    36,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "31 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "math.ST"
                },
                "authors": [
                    {
                        "name": "Claudia Neves"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.19336v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19336v1",
                "title": "GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis"
                },
                "updated": "2025-12-22T12:32:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    32,
                    16,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19336v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\\times10^{-4}$ and $1\\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\\times160\\times192$ for MRI-to-CT and $32\\times128\\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\\times10^{-4}$ and $1\\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\\times160\\times192$ for MRI-to-CT and $32\\times128\\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:32:16Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    32,
                    16,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Siyuan Mei"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Fuxin Fan"
                    }
                ],
                "author_detail": {
                    "name": "Fuxin Fan"
                },
                "author": "Fuxin Fan"
            },
            {
                "id": "http://arxiv.org/abs/2510.02025v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.02025v2",
                "title": "Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models"
                },
                "updated": "2025-12-22T12:27:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    27,
                    15,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.02025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.02025v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluations of large language models (LLMs)' creativity have focused primarily on the quality of their outputs rather than the processes that shape them. This study takes a process-oriented approach, drawing on narratology to examine LLMs as computational authors. We introduce constraint-based decision-making as a lens for authorial creativity. Using controlled prompting to assign authorial personas, we analyze the creative preferences of the models. Our findings show that LLMs consistently emphasize Style over other elements, including Character, Event, and Setting. By also probing the reasoning the models provide for their choices, we show that distinctive profiles emerge across models and argue that our approach provides a novel systematic tool for analyzing AI's authorial creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluations of large language models (LLMs)' creativity have focused primarily on the quality of their outputs rather than the processes that shape them. This study takes a process-oriented approach, drawing on narratology to examine LLMs as computational authors. We introduce constraint-based decision-making as a lens for authorial creativity. Using controlled prompting to assign authorial personas, we analyze the creative preferences of the models. Our findings show that LLMs consistently emphasize Style over other elements, including Character, Event, and Setting. By also probing the reasoning the models provide for their choices, we show that distinctive profiles emerge across models and argue that our approach provides a novel systematic tool for analyzing AI's authorial creativity."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-02T13:57:14Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    57,
                    14,
                    3,
                    275,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Donghoon Jung"
                    },
                    {
                        "name": "Jiwoo Choi"
                    },
                    {
                        "name": "Songeun Chae"
                    },
                    {
                        "name": "Seohyon Jung"
                    }
                ],
                "author_detail": {
                    "name": "Seohyon Jung"
                },
                "author": "Seohyon Jung"
            },
            {
                "id": "http://arxiv.org/abs/2512.19317v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19317v1",
                "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models"
                },
                "updated": "2025-12-22T12:07:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    7,
                    33,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19317v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:07:33Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    7,
                    33,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "A. A. Gde Yogi Pramana"
                    },
                    {
                        "name": "Jason Ray"
                    },
                    {
                        "name": "Anthony Jaya"
                    },
                    {
                        "name": "Michael Wijaya"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wijaya"
                },
                "author": "Michael Wijaya"
            },
            {
                "id": "http://arxiv.org/abs/2512.19316v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19316v1",
                "title": "Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations"
                },
                "updated": "2025-12-22T12:07:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    7,
                    5,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19316v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:07:05Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    7,
                    5,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "42 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Marica Muffoletto"
                    },
                    {
                        "name": "Uxio Hermida"
                    },
                    {
                        "name": "Charlène Mauger"
                    },
                    {
                        "name": "Avan Suinesiaputra"
                    },
                    {
                        "name": "Yiyang Xu"
                    },
                    {
                        "name": "Richard Burns"
                    },
                    {
                        "name": "Lisa Pankewitz"
                    },
                    {
                        "name": "Andrew D McCulloch"
                    },
                    {
                        "name": "Steffen E Petersen"
                    },
                    {
                        "name": "Daniel Rueckert"
                    },
                    {
                        "name": "Alistair A Young"
                    }
                ],
                "author_detail": {
                    "name": "Alistair A Young"
                },
                "author": "Alistair A Young"
            },
            {
                "id": "http://arxiv.org/abs/2512.19314v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19314v1",
                "title": "Protecting Quantum Circuits Through Compiler-Resistant Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting Quantum Circuits Through Compiler-Resistant Obfuscation"
                },
                "updated": "2025-12-22T12:05:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    5,
                    23,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19314v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Quantum circuit obfuscation is becoming increasingly important to prevent theft and reverse engineering of quantum algorithms. As quantum computing advances, the need to protect the intellectual property contained in quantum circuits continues to grow. Existing methods often provide limited defense against structural and statistical analysis or introduce considerable overhead. In this paper, we propose a novel quantum obfuscation method that uses randomized U3 transformations to conceal circuit structure while preserving functionality. We implement and assess our approach on QASM circuits using Qiskit AER, achieving over 93\\% semantic accuracy with minimal runtime overhead. The method demonstrates strong resistance to reverse engineering and structural inference, making it a practical and effective approach for quantum software protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum circuit obfuscation is becoming increasingly important to prevent theft and reverse engineering of quantum algorithms. As quantum computing advances, the need to protect the intellectual property contained in quantum circuits continues to grow. Existing methods often provide limited defense against structural and statistical analysis or introduce considerable overhead. In this paper, we propose a novel quantum obfuscation method that uses randomized U3 transformations to conceal circuit structure while preserving functionality. We implement and assess our approach on QASM circuits using Qiskit AER, achieving over 93\\% semantic accuracy with minimal runtime overhead. The method demonstrates strong resistance to reverse engineering and structural inference, making it a practical and effective approach for quantum software protection."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:05:23Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    5,
                    23,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "24 pages, 12 figures, Accepted at 24th International Conference on Applied Cryptography and Network Security (ACNS 2026, New York)",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Pradyun Parayil"
                    },
                    {
                        "name": "Amal Raj"
                    },
                    {
                        "name": "Vivek Balachandran"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Balachandran"
                },
                "author": "Vivek Balachandran"
            },
            {
                "id": "http://arxiv.org/abs/2512.19305v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19305v1",
                "title": "CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs"
                },
                "updated": "2025-12-22T11:53:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    53,
                    1,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19305v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:53:01Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    53,
                    1,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Javier Vela-Tambo"
                    },
                    {
                        "name": "Jorge Gracia"
                    },
                    {
                        "name": "Fernando Dominguez-Castro"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Dominguez-Castro"
                },
                "author": "Fernando Dominguez-Castro"
            },
            {
                "id": "http://arxiv.org/abs/2503.04521v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.04521v2",
                "title": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market"
                },
                "updated": "2025-12-22T11:48:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    48,
                    52,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.04521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.04521v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/TMC.2025.3647531",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-06T15:08:31Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    8,
                    31,
                    3,
                    65,
                    0
                ],
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Mobile Computing. Index Terms: Edge-AI, DNN Inference Offloading, Resource Management, Dynamic Pricing, Auction Mechanism",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Songyuan Li"
                    },
                    {
                        "name": "Jia Hu"
                    },
                    {
                        "name": "Geyong Min"
                    },
                    {
                        "name": "Haojun Huang"
                    },
                    {
                        "name": "Jiwei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiwei Huang"
                },
                "author": "Jiwei Huang",
                "arxiv_doi": "10.1109/TMC.2025.3647531"
            },
            {
                "id": "http://arxiv.org/abs/2512.19304v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19304v1",
                "title": "Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA"
                },
                "updated": "2025-12-22T11:48:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    48,
                    24,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19304v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Binary neural networks provide a promising solution for low-power, high-speed inference by replacing expensive floating-point operations with bitwise logic. This makes them well-suited for deployment on resource-constrained platforms such as FPGAs. In this study, we present a fully custom BNN inference accelerator for handwritten digit recognition, implemented entirely in Verilog without the use of high-level synthesis tools. The design targets the Xilinx Artix-7 FPGA and achieves real-time classification at 80\\,MHz with low power consumption and predictable timing. Simulation results demonstrate 84\\% accuracy on the MNIST test set and highlight the advantages of manual HDL design for transparent, efficient, and flexible BNN deployment in embedded systems. The complete project including training scripts and Verilog source code are available at GitHub repo for reproducibility and future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary neural networks provide a promising solution for low-power, high-speed inference by replacing expensive floating-point operations with bitwise logic. This makes them well-suited for deployment on resource-constrained platforms such as FPGAs. In this study, we present a fully custom BNN inference accelerator for handwritten digit recognition, implemented entirely in Verilog without the use of high-level synthesis tools. The design targets the Xilinx Artix-7 FPGA and achieves real-time classification at 80\\,MHz with low power consumption and predictable timing. Simulation results demonstrate 84\\% accuracy on the MNIST test set and highlight the advantages of manual HDL design for transparent, efficient, and flexible BNN deployment in embedded systems. The complete project including training scripts and Verilog source code are available at GitHub repo for reproducibility and future development."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:48:24Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    48,
                    24,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "13 pages, 1 figure",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Emir Devlet Ertörer"
                    },
                    {
                        "name": "Cem Ünsalan"
                    }
                ],
                "author_detail": {
                    "name": "Cem Ünsalan"
                },
                "author": "Cem Ünsalan"
            },
            {
                "id": "http://arxiv.org/abs/2512.19300v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19300v1",
                "title": "RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning"
                },
                "updated": "2025-12-22T11:44:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    44,
                    32,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19300v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Novel object synthesis by integrating distinct textual concepts from diverse categories remains a significant challenge in Text-to-Image (T2I) generation. Existing methods often suffer from insufficient concept mixing, lack of rigorous evaluation, and suboptimal outputs-manifesting as conceptual imbalance, superficial combinations, or mere juxtapositions. To address these limitations, we propose Reinforcement Mixing Learning (RMLer), a framework that formulates cross-category concept fusion as a reinforcement learning problem: mixed features serve as states, mixing strategies as actions, and visual outcomes as rewards. Specifically, we design an MLP-policy network to predict dynamic coefficients for blending cross-category text embeddings. We further introduce visual rewards based on (1) semantic similarity and (2) compositional balance between the fused object and its constituent concepts, optimizing the policy via proximal policy optimization. At inference, a selection strategy leverages these rewards to curate the highest-quality fused objects. Extensive experiments demonstrate RMLer's superiority in synthesizing coherent, high-fidelity objects from diverse categories, outperforming existing methods. Our work provides a robust framework for generating novel visual concepts, with promising applications in film, gaming, and design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel object synthesis by integrating distinct textual concepts from diverse categories remains a significant challenge in Text-to-Image (T2I) generation. Existing methods often suffer from insufficient concept mixing, lack of rigorous evaluation, and suboptimal outputs-manifesting as conceptual imbalance, superficial combinations, or mere juxtapositions. To address these limitations, we propose Reinforcement Mixing Learning (RMLer), a framework that formulates cross-category concept fusion as a reinforcement learning problem: mixed features serve as states, mixing strategies as actions, and visual outcomes as rewards. Specifically, we design an MLP-policy network to predict dynamic coefficients for blending cross-category text embeddings. We further introduce visual rewards based on (1) semantic similarity and (2) compositional balance between the fused object and its constituent concepts, optimizing the policy via proximal policy optimization. At inference, a selection strategy leverages these rewards to curate the highest-quality fused objects. Extensive experiments demonstrate RMLer's superiority in synthesizing coherent, high-fidelity objects from diverse categories, outperforming existing methods. Our work provides a robust framework for generating novel visual concepts, with promising applications in film, gaming, and design."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:44:32Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    44,
                    32,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "accepted by AAAI2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Zikun Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Jian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yang"
                },
                "author": "Jian Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.19299v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19299v1",
                "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application"
                },
                "updated": "2025-12-22T11:43:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    43,
                    35,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19299v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:43:35Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    43,
                    35,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Haoyu Jiang"
                    },
                    {
                        "name": "Fanjie Zeng"
                    },
                    {
                        "name": "Boan Qu"
                    },
                    {
                        "name": "Xiaojie Lin"
                    },
                    {
                        "name": "Wei Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhong"
                },
                "author": "Wei Zhong"
            },
            {
                "id": "http://arxiv.org/abs/2512.19297v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19297v1",
                "title": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models"
                },
                "updated": "2025-12-22T11:40:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    40,
                    47,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19297v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:40:47Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    40,
                    47,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "NDSS 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Linzhi Chen"
                    },
                    {
                        "name": "Yang Sun"
                    },
                    {
                        "name": "Hongru Wei"
                    },
                    {
                        "name": "Yuqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Chen"
                },
                "author": "Yuqi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2509.23941v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23941v2",
                "title": "Brain-language fusion enables interactive neural readout and in-silico experimentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-language fusion enables interactive neural readout and in-silico experimentation"
                },
                "updated": "2025-12-22T11:33:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    33,
                    19,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23941v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have revolutionized human-machine interaction, and have been extended by embedding diverse modalities such as images into a shared language space. Yet, neural decoding has remained constrained by static, non-interactive methods. We introduce CorText, a framework that integrates neural activity directly into the latent space of an LLM, enabling open-ended, natural language interaction with brain data. Trained on fMRI data recorded during viewing of natural scenes, CorText generates accurate image captions and can answer more detailed questions better than controls, while having access to neural data only. We showcase that CorText achieves zero-shot generalization beyond semantic categories seen during training. In-silico microstimulation experiments, which enable counterfactual prompts on brain activity, reveal a consistent, and graded mapping between brain-state and language output. These advances mark a shift from passive decoding toward generative, flexible interfaces between brain activity and language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized human-machine interaction, and have been extended by embedding diverse modalities such as images into a shared language space. Yet, neural decoding has remained constrained by static, non-interactive methods. We introduce CorText, a framework that integrates neural activity directly into the latent space of an LLM, enabling open-ended, natural language interaction with brain data. Trained on fMRI data recorded during viewing of natural scenes, CorText generates accurate image captions and can answer more detailed questions better than controls, while having access to neural data only. We showcase that CorText achieves zero-shot generalization beyond semantic categories seen during training. In-silico microstimulation experiments, which enable counterfactual prompts on brain activity, reveal a consistent, and graded mapping between brain-state and language output. These advances mark a shift from passive decoding toward generative, flexible interfaces between brain activity and language."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-28T15:35:25Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    35,
                    25,
                    6,
                    271,
                    0
                ],
                "arxiv_comment": "v2",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Victoria Bosch"
                    },
                    {
                        "name": "Daniel Anthes"
                    },
                    {
                        "name": "Adrien Doerig"
                    },
                    {
                        "name": "Sushrut Thorat"
                    },
                    {
                        "name": "Peter König"
                    },
                    {
                        "name": "Tim Christian Kietzmann"
                    }
                ],
                "author_detail": {
                    "name": "Tim Christian Kietzmann"
                },
                "author": "Tim Christian Kietzmann"
            },
            {
                "id": "http://arxiv.org/abs/2512.19283v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19283v1",
                "title": "Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context"
                },
                "updated": "2025-12-22T11:26:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    26,
                    41,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19283v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:26:41Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    26,
                    41,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Project Page: https://kyungwoncho.github.io/HaMoS/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kyungwon Cho"
                    },
                    {
                        "name": "Hanbyul Joo"
                    }
                ],
                "author_detail": {
                    "name": "Hanbyul Joo"
                },
                "author": "Hanbyul Joo"
            },
            {
                "id": "http://arxiv.org/abs/2508.17054v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.17054v3",
                "title": "DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method"
                },
                "updated": "2025-12-22T10:54:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    54,
                    5,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.17054v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.17054v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($Δ$Flow), a lightweight 3D framework that captures motion cues via a $Δ$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2, Waymo and nuScenes datasets show that $Δ$Flow achieves state-of-the-art performance with up to 22% lower error and $2\\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($Δ$Flow), a lightweight 3D framework that captures motion cues via a $Δ$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2, Waymo and nuScenes datasets show that $Δ$Flow achieves state-of-the-art performance with up to 22% lower error and $2\\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-23T15:06:59Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    15,
                    6,
                    59,
                    5,
                    235,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 Spotlight, 18 pages (10 main pages + 8 supp materail), 11 figures, code at https://github.com/Kin-Zhang/DeltaFlow",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Qingwen Zhang"
                    },
                    {
                        "name": "Xiaomeng Zhu"
                    },
                    {
                        "name": "Yushan Zhang"
                    },
                    {
                        "name": "Yixi Cai"
                    },
                    {
                        "name": "Olov Andersson"
                    },
                    {
                        "name": "Patric Jensfelt"
                    }
                ],
                "author_detail": {
                    "name": "Patric Jensfelt"
                },
                "author": "Patric Jensfelt"
            },
            {
                "id": "http://arxiv.org/abs/2506.10035v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.10035v2",
                "title": "FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training"
                },
                "updated": "2025-12-22T10:46:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    46,
                    52,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.10035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.10035v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\\% of the hierarchy pruned. Our code will be available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\\% of the hierarchy pruned. Our code will be available soon."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-10T20:48:30Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    20,
                    48,
                    30,
                    1,
                    161,
                    0
                ],
                "arxiv_comment": "14 pages",
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Fuhan Cai"
                    },
                    {
                        "name": "Yong Guo"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Xiangzhong Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzhong Fang"
                },
                "author": "Xiangzhong Fang"
            },
            {
                "id": "http://arxiv.org/abs/2507.17860v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.17860v4",
                "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis"
                },
                "updated": "2025-12-22T10:41:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    41,
                    8,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.17860v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.17860v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-23T18:33:27Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    18,
                    33,
                    27,
                    2,
                    204,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ko Watanabe"
                    },
                    {
                        "name": "Stanislav Frolov"
                    },
                    {
                        "name": "Aya Hassan"
                    },
                    {
                        "name": "David Dembinsky"
                    },
                    {
                        "name": "Adriano Lucieri"
                    },
                    {
                        "name": "Andreas Dengel"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Dengel"
                },
                "author": "Andreas Dengel"
            },
            {
                "id": "http://arxiv.org/abs/2512.16280v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16280v2",
                "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams"
                },
                "updated": "2025-12-22T10:37:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    37,
                    8,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16280v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.\n  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.\n  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T07:59:15Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    7,
                    59,
                    15,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "arxiv_journal_ref": "Usenix Security Symposium 2026",
                "authors": [
                    {
                        "name": "Gilad Gressel"
                    },
                    {
                        "name": "Rahul Pankajakshan"
                    },
                    {
                        "name": "Shir Rozenfeld"
                    },
                    {
                        "name": "Ling Li"
                    },
                    {
                        "name": "Ivan Franceschini"
                    },
                    {
                        "name": "Krishnashree Achuthan"
                    },
                    {
                        "name": "Yisroel Mirsky"
                    }
                ],
                "author_detail": {
                    "name": "Yisroel Mirsky"
                },
                "author": "Yisroel Mirsky"
            },
            {
                "id": "http://arxiv.org/abs/2510.05956v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.05956v2",
                "title": "Transverse Velocities in Real-Time Cosmology: Position Drift in Relativistic N-Body Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transverse Velocities in Real-Time Cosmology: Position Drift in Relativistic N-Body Simulations"
                },
                "updated": "2025-12-22T10:29:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    29,
                    54,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.05956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.05956v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The era of real-time cosmology has begun. It is now possible to directly measure the apparent drift of high-redshift astronomical sources across the sky $\\textit{in real time}$. This so-called $\\textit{position drift}$ provides a valuable probe of the peculiar velocity field and cosmic structure formation by giving direct access to the transverse velocity, which is notoriously difficult to measure and is typically inferred statistically from the density field in a model-dependent way. To fully exploit this new window into the Universe, it is essential to understand how cosmological structures affect position drift measurements. Here we present the first position drift study based on the general relativistic N-body simulation code $\\texttt{gevolution}$. We calculate the position drift directly from the past light cone for ten different observers and compare the results to predictions from linear perturbation theory. At linear order, the position drift is directly proportional to the transverse velocity on the sky. This linear approximation reproduces our non-linear simulation results to within about 5%. We calculate power spectra for the position drift, splitting the signal into an E- and B-mode and compare the former to linear expectations, finding good agreement. The B-mode is suppressed on linear scales, but has similar amplitude as the E-mode on non-linear scales. We further demonstrate that light-cone inhomogeneities induce biases in the dipole of the drift, introducing redshift dependence of both the amplitude and direction. Although our analysis is not yet sufficient for a firm conclusion, our results suggest that these effects alone cannot explain the possible redshift-dependent dipole in Gaia DR3 data reported in the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The era of real-time cosmology has begun. It is now possible to directly measure the apparent drift of high-redshift astronomical sources across the sky $\\textit{in real time}$. This so-called $\\textit{position drift}$ provides a valuable probe of the peculiar velocity field and cosmic structure formation by giving direct access to the transverse velocity, which is notoriously difficult to measure and is typically inferred statistically from the density field in a model-dependent way. To fully exploit this new window into the Universe, it is essential to understand how cosmological structures affect position drift measurements. Here we present the first position drift study based on the general relativistic N-body simulation code $\\texttt{gevolution}$. We calculate the position drift directly from the past light cone for ten different observers and compare the results to predictions from linear perturbation theory. At linear order, the position drift is directly proportional to the transverse velocity on the sky. This linear approximation reproduces our non-linear simulation results to within about 5%. We calculate power spectra for the position drift, splitting the signal into an E- and B-mode and compare the former to linear expectations, finding good agreement. The B-mode is suppressed on linear scales, but has similar amplitude as the E-mode on non-linear scales. We further demonstrate that light-cone inhomogeneities induce biases in the dipole of the drift, introducing redshift dependence of both the amplitude and direction. Although our analysis is not yet sufficient for a firm conclusion, our results suggest that these effects alone cannot explain the possible redshift-dependent dipole in Gaia DR3 data reported in the literature."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T14:12:40Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    12,
                    40,
                    1,
                    280,
                    0
                ],
                "arxiv_comment": "Published in the Open Journal of Astrophysics. v2: fixed some typos in Sect. 3.1 and added clarifying remarks throughout, results unchanged",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Alexander Oestreicher"
                    },
                    {
                        "name": "Chris Clarkson"
                    },
                    {
                        "name": "Julian Adamek"
                    },
                    {
                        "name": "Sofie Marie Koksbang"
                    }
                ],
                "author_detail": {
                    "name": "Sofie Marie Koksbang"
                },
                "author": "Sofie Marie Koksbang"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.19691v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19691v1",
                "title": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight"
                },
                "updated": "2025-12-22T18:59:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    59,
                    34,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19691v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:59:34Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    59,
                    34,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Junze Ye"
                    },
                    {
                        "name": "Daniel Tawfik"
                    },
                    {
                        "name": "Alex J. Goodell"
                    },
                    {
                        "name": "Nikhil V. Kotha"
                    },
                    {
                        "name": "Mark K. Buyyounouski"
                    },
                    {
                        "name": "Mohsen Bayati"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Bayati"
                },
                "author": "Mohsen Bayati"
            },
            {
                "id": "http://arxiv.org/abs/2512.19682v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19682v1",
                "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators"
                },
                "updated": "2025-12-22T18:57:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    57,
                    13,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19682v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:57:13Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    57,
                    13,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Our codes are available at https://github.com/Gen-Verse/GenEnv",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jiacheng Guo"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Peter Chen"
                    },
                    {
                        "name": "Qixin Xiao"
                    },
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang"
            },
            {
                "id": "http://arxiv.org/abs/2510.09595v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.09595v2",
                "title": "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?"
                },
                "updated": "2025-12-22T18:56:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    56,
                    1,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.09595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.09595v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-10T17:54:24Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    54,
                    24,
                    4,
                    283,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Kaijian Zou"
                    },
                    {
                        "name": "Aaron Xiong"
                    },
                    {
                        "name": "Yunxiang Zhang"
                    },
                    {
                        "name": "Frederick Zhang"
                    },
                    {
                        "name": "Yueqi Ren"
                    },
                    {
                        "name": "Jirong Yang"
                    },
                    {
                        "name": "Ayoung Lee"
                    },
                    {
                        "name": "Shitanshu Bhushan"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.19675v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19675v1",
                "title": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)"
                },
                "updated": "2025-12-22T18:53:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    53,
                    3,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19675v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:53:03Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    53,
                    3,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Niclas Griesshaber"
                    },
                    {
                        "name": "Jochen Streb"
                    }
                ],
                "author_detail": {
                    "name": "Jochen Streb"
                },
                "author": "Jochen Streb"
            },
            {
                "id": "http://arxiv.org/abs/2512.19673v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19673v1",
                "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies"
                },
                "updated": "2025-12-22T18:51:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    51,
                    48,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19673v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:51:48Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    51,
                    48,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Preprint. Our code is available at https://github.com/Trae1ounG/BuPO",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuqiao Tan"
                    },
                    {
                        "name": "Minzheng Wang"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Chengfeng Zhao"
                    },
                    {
                        "name": "Qiunan Lu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2306.00029v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2306.00029v2",
                "title": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLMs"
                },
                "updated": "2025-12-22T18:29:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    29,
                    22,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2306.00029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2306.00029v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-05-31T05:24:48Z",
                "published_parsed": [
                    2023,
                    5,
                    31,
                    5,
                    24,
                    48,
                    2,
                    151,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Nghi D. Q. Bui"
                    },
                    {
                        "name": "Hung Le"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Akhilesh Deepak Gotmare"
                    },
                    {
                        "name": "Steven C. H. Hoi"
                    }
                ],
                "author_detail": {
                    "name": "Steven C. H. Hoi"
                },
                "author": "Steven C. H. Hoi"
            },
            {
                "id": "http://arxiv.org/abs/2506.23740v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.23740v2",
                "title": "Campus5G: A Campus Scale Private 5G Open RAN Testbed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Campus5G: A Campus Scale Private 5G Open RAN Testbed"
                },
                "updated": "2025-12-22T18:25:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    25,
                    28,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.23740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.23740v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mobile networks are embracing disaggregation, reflected by the industry trend towards Open RAN. Private 5G networks are viewed as particularly suitable contenders as early adopters of Open RAN, owing to their setting, high degree of control, and opportunity for innovation they present. Motivated by this, we have recently deployed Campus5G, the first of its kind campus-wide, O-RAN-compliant private 5G testbed across the central campus of the University of Edinburgh. We present in detail our process developing the testbed, from planning, to architecting, to deployment, and measuring the testbed performance. We then discuss the lessons learned from building the testbed, and highlight some research opportunities that emerged from our deployment experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile networks are embracing disaggregation, reflected by the industry trend towards Open RAN. Private 5G networks are viewed as particularly suitable contenders as early adopters of Open RAN, owing to their setting, high degree of control, and opportunity for innovation they present. Motivated by this, we have recently deployed Campus5G, the first of its kind campus-wide, O-RAN-compliant private 5G testbed across the central campus of the University of Edinburgh. We present in detail our process developing the testbed, from planning, to architecting, to deployment, and measuring the testbed performance. We then discuss the lessons learned from building the testbed, and highlight some research opportunities that emerged from our deployment experience."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-30T11:27:57Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    27,
                    57,
                    0,
                    181,
                    0
                ],
                "arxiv_comment": "To appear in ACM SIGCOMM Computer Communication Review (CCR), July 2025",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Andrew E. Ferguson"
                    },
                    {
                        "name": "Ujjwal Pawar"
                    },
                    {
                        "name": "Tianxin Wang"
                    },
                    {
                        "name": "Mahesh K. Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh K. Marina"
                },
                "author": "Mahesh K. Marina"
            },
            {
                "id": "http://arxiv.org/abs/2512.19651v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19651v1",
                "title": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting"
                },
                "updated": "2025-12-22T18:23:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    23,
                    37,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19651v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:23:37Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    23,
                    37,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "9 pages, 3 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Filippos Ventirozos"
                    },
                    {
                        "name": "Peter Appleby"
                    },
                    {
                        "name": "Matthew Shardlow"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Shardlow"
                },
                "author": "Matthew Shardlow"
            },
            {
                "id": "http://arxiv.org/abs/2511.00898v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00898v2",
                "title": "Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning"
                },
                "updated": "2025-12-22T18:21:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    21,
                    46,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00898v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models have emerged as a promising approach for graph learning due to their powerful reasoning capabilities. However, existing methods exhibit systematic performance degradation on structurally important nodes such as bridges and hubs. We identify the root cause of these limitations. Current approaches encode graph topology into static features but lack reasoning scaffolds to transform topological patterns into role-based interpretations. This limitation becomes critical in zero-shot scenarios where no training data establishes structure-semantics mappings. To address this gap, we propose DuoGLM, a training-free dual-perspective framework for structure-aware graph reasoning. The local perspective constructs relation-aware templates capturing semantic interactions between nodes and neighbors. The global perspective performs topology-to-role inference to generate functional descriptions of structural positions. These complementary perspectives provide explicit reasoning mechanisms enabling LLMs to distinguish topologically similar but semantically different nodes. Extensive experiments across eight benchmark datasets demonstrate substantial improvements. DuoGLM achieves 14.3\\% accuracy gain in zero-shot node classification and 7.6\\% AUC improvement in cross-domain transfer compared to existing methods. The results validate the effectiveness of explicit role reasoning for graph understanding with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have emerged as a promising approach for graph learning due to their powerful reasoning capabilities. However, existing methods exhibit systematic performance degradation on structurally important nodes such as bridges and hubs. We identify the root cause of these limitations. Current approaches encode graph topology into static features but lack reasoning scaffolds to transform topological patterns into role-based interpretations. This limitation becomes critical in zero-shot scenarios where no training data establishes structure-semantics mappings. To address this gap, we propose DuoGLM, a training-free dual-perspective framework for structure-aware graph reasoning. The local perspective constructs relation-aware templates capturing semantic interactions between nodes and neighbors. The global perspective performs topology-to-role inference to generate functional descriptions of structural positions. These complementary perspectives provide explicit reasoning mechanisms enabling LLMs to distinguish topologically similar but semantically different nodes. Extensive experiments across eight benchmark datasets demonstrate substantial improvements. DuoGLM achieves 14.3\\% accuracy gain in zero-shot node classification and 7.6\\% AUC improvement in cross-domain transfer compared to existing methods. The results validate the effectiveness of explicit role reasoning for graph understanding with LLMs."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-02T11:33:14Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    11,
                    33,
                    14,
                    6,
                    306,
                    0
                ],
                "arxiv_comment": "This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results",
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Heng Zhang"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Haochen You"
                    },
                    {
                        "name": "Lubin Gan"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Shuai Chen"
                    },
                    {
                        "name": "Wenjun Huang"
                    },
                    {
                        "name": "Jin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jin Huang"
                },
                "author": "Jin Huang"
            },
            {
                "id": "http://arxiv.org/abs/2510.10611v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.10611v2",
                "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication"
                },
                "updated": "2025-12-22T18:20:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    20,
                    47,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.10611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.10611v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in large language model-powered multi-agent systems have demonstrated remarkable collective intelligence through effective communication. However, existing approaches face two primary challenges: (i) \\textit{Ineffective group collaboration modeling}, as they rely on pairwise edge representations in graph structures, limiting their ability to capture relationships among multiple agents; and (ii) \\textit{Limited task-adaptiveness in communication topology design}, leading to excessive communication cost for simple tasks and insufficient coordination for complex scenarios. These issues restrict the scalability and practical deployment of adaptive collaboration frameworks. To address these challenges, we propose \\textbf{HyperAgent}, a hypergraph-based framework that optimizes communication topologies and effectively captures group collaboration patterns using direct hyperedge representations. Unlike edge-based approaches, HyperAgent uses hyperedges to link multiple agents within the same subtask and employs hypergraph convolutional layers to achieve one-step information aggregation in collaboration groups. Additionally, it incorporates a variational autoencoder framework with sparsity regularization to dynamically adjust hypergraph topologies based on task complexity. Experiments highlight the superiority of HyperAgent in both performance and efficiency. For instance, on GSM8K, HyperAgent achieves 95.07\\% accuracy while reducing token consumption by 25.33\\%, demonstrating the potential of hypergraph-based optimization for multi-agent communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model-powered multi-agent systems have demonstrated remarkable collective intelligence through effective communication. However, existing approaches face two primary challenges: (i) \\textit{Ineffective group collaboration modeling}, as they rely on pairwise edge representations in graph structures, limiting their ability to capture relationships among multiple agents; and (ii) \\textit{Limited task-adaptiveness in communication topology design}, leading to excessive communication cost for simple tasks and insufficient coordination for complex scenarios. These issues restrict the scalability and practical deployment of adaptive collaboration frameworks. To address these challenges, we propose \\textbf{HyperAgent}, a hypergraph-based framework that optimizes communication topologies and effectively captures group collaboration patterns using direct hyperedge representations. Unlike edge-based approaches, HyperAgent uses hyperedges to link multiple agents within the same subtask and employs hypergraph convolutional layers to achieve one-step information aggregation in collaboration groups. Additionally, it incorporates a variational autoencoder framework with sparsity regularization to dynamically adjust hypergraph topologies based on task complexity. Experiments highlight the superiority of HyperAgent in both performance and efficiency. For instance, on GSM8K, HyperAgent achieves 95.07\\% accuracy while reducing token consumption by 25.33\\%, demonstrating the potential of hypergraph-based optimization for multi-agent communication."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-12T13:47:42Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    47,
                    42,
                    6,
                    285,
                    0
                ],
                "arxiv_comment": "This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results",
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Heng Zhang"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Haochen You"
                    },
                    {
                        "name": "Lubin Gan"
                    },
                    {
                        "name": "Yilei Yuan"
                    },
                    {
                        "name": "Jin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jin Huang"
                },
                "author": "Jin Huang"
            },
            {
                "id": "http://arxiv.org/abs/2510.10581v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.10581v2",
                "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search"
                },
                "updated": "2025-12-22T18:19:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    19,
                    37,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.10581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.10581v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collaboration, yet they face high failure rates in multi-turn deep search scenarios. Existing temporal attribution methods struggle to accurately diagnose root causes, particularly when errors propagate across multiple agents. Attempts to automate failure attribution by analyzing action sequences remain ineffective due to their inability to account for information dependencies that span agents. This paper identifies two core challenges: \\textit{(i) distinguishing symptoms from root causes in multi-agent error propagation}, and \\textit{(ii) tracing information dependencies beyond temporal order}. To address these issues, we introduce \\textbf{GraphTracer}, a framework that redefines failure attribution through information flow analysis. GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly capture how agents reference and build on prior outputs. It localizes root causes by tracing through these dependency structures instead of relying on temporal sequences. GraphTracer also uses graph-aware synthetic data generation to target critical nodes, creating realistic failure scenarios. Evaluations on the Who\\&When benchmark and integration into production systems demonstrate that GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared to state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements in deployed multi-agent frameworks, establishing a robust solution for multi-agent system debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collaboration, yet they face high failure rates in multi-turn deep search scenarios. Existing temporal attribution methods struggle to accurately diagnose root causes, particularly when errors propagate across multiple agents. Attempts to automate failure attribution by analyzing action sequences remain ineffective due to their inability to account for information dependencies that span agents. This paper identifies two core challenges: \\textit{(i) distinguishing symptoms from root causes in multi-agent error propagation}, and \\textit{(ii) tracing information dependencies beyond temporal order}. To address these issues, we introduce \\textbf{GraphTracer}, a framework that redefines failure attribution through information flow analysis. GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly capture how agents reference and build on prior outputs. It localizes root causes by tracing through these dependency structures instead of relying on temporal sequences. GraphTracer also uses graph-aware synthetic data generation to target critical nodes, creating realistic failure scenarios. Evaluations on the Who\\&When benchmark and integration into production systems demonstrate that GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared to state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements in deployed multi-agent frameworks, establishing a robust solution for multi-agent system debugging."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-12T12:55:42Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    12,
                    55,
                    42,
                    6,
                    285,
                    0
                ],
                "arxiv_comment": "This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results",
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Heng Zhang"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Haochen You"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Lubin Gan"
                    },
                    {
                        "name": "Yilei Yuan"
                    },
                    {
                        "name": "Jin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jin Huang"
                },
                "author": "Jin Huang"
            },
            {
                "id": "http://arxiv.org/abs/2509.00767v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.00767v2",
                "title": "InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos"
                },
                "updated": "2025-12-22T18:14:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    14,
                    12,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.00767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.00767v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-31T09:38:59Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    9,
                    38,
                    59,
                    6,
                    243,
                    0
                ],
                "arxiv_comment": "Accepted to 3DV 2026. Project page: https://mael-zys.github.io/InterPose/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yangsong Zhang"
                    },
                    {
                        "name": "Abdul Ahad Butt"
                    },
                    {
                        "name": "Gül Varol"
                    },
                    {
                        "name": "Ivan Laptev"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Laptev"
                },
                "author": "Ivan Laptev"
            },
            {
                "id": "http://arxiv.org/abs/2512.19630v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19630v1",
                "title": "Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands Māori",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands Māori"
                },
                "updated": "2025-12-22T18:04:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    4,
                    24,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19630v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present experiments on diacritic restoration, a form of text normalization essential for natural language processing (NLP) tasks. Our study focuses on two extremely under-resourced languages: Bribri, a Chibchan language spoken in Costa Rica, and Cook Islands Māori, a Polynesian language spoken in the Cook Islands. Specifically, this paper: (i) compares algorithms for diacritics restoration in under-resourced languages, including tonal diacritics, (ii) examines the amount of data required to achieve target performance levels, (iii) contrasts results across varying resource conditions, and (iv) explores the related task of diacritic correction. We find that fine-tuned, character-level LLMs perform best, likely due to their ability to decompose complex characters into their UTF-8 byte representations. In contrast, massively multilingual models perform less effectively given our data constraints. Across all models, reliable performance begins to emerge with data budgets of around 10,000 words. Zero-shot approaches perform poorly in all cases. This study responds both to requests from the language communities and to broader NLP research questions concerning model performance and generalization in under-resourced contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present experiments on diacritic restoration, a form of text normalization essential for natural language processing (NLP) tasks. Our study focuses on two extremely under-resourced languages: Bribri, a Chibchan language spoken in Costa Rica, and Cook Islands Māori, a Polynesian language spoken in the Cook Islands. Specifically, this paper: (i) compares algorithms for diacritics restoration in under-resourced languages, including tonal diacritics, (ii) examines the amount of data required to achieve target performance levels, (iii) contrasts results across varying resource conditions, and (iv) explores the related task of diacritic correction. We find that fine-tuned, character-level LLMs perform best, likely due to their ability to decompose complex characters into their UTF-8 byte representations. In contrast, massively multilingual models perform less effectively given our data constraints. Across all models, reliable performance begins to emerge with data budgets of around 10,000 words. Zero-shot approaches perform poorly in all cases. This study responds both to requests from the language communities and to broader NLP research questions concerning model performance and generalization in under-resourced contexts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:04:24Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    4,
                    24,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rolando Coto-Solano"
                    },
                    {
                        "name": "Daisy Li"
                    },
                    {
                        "name": "Manoela Teleginski Ferraz"
                    },
                    {
                        "name": "Olivia Sasse"
                    },
                    {
                        "name": "Cha Krupka"
                    },
                    {
                        "name": "Sharid Loáiciga"
                    },
                    {
                        "name": "Sally Akevai Tenamu Nicholas"
                    }
                ],
                "author_detail": {
                    "name": "Sally Akevai Tenamu Nicholas"
                },
                "author": "Sally Akevai Tenamu Nicholas"
            },
            {
                "id": "http://arxiv.org/abs/2512.19620v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19620v1",
                "title": "Exploring the features used for summary evaluation by Human and GPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the features used for summary evaluation by Human and GPT"
                },
                "updated": "2025-12-22T17:54:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    54,
                    49,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19620v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T17:54:49Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    54,
                    49,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zahra Sadeghi"
                    },
                    {
                        "name": "Evangelos Milios"
                    },
                    {
                        "name": "Frank Rudzicz"
                    }
                ],
                "author_detail": {
                    "name": "Frank Rudzicz"
                },
                "author": "Frank Rudzicz"
            },
            {
                "id": "http://arxiv.org/abs/2512.19606v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19606v1",
                "title": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference"
                },
                "updated": "2025-12-22T17:42:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    42,
                    51,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19606v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "RAPID-LLM is a unified performance modeling framework for large language model (LLM) training and inference on GPU clusters. It couples a DeepFlow-based frontend that generates hardware-aware, operator-level Chakra execution traces from an abstract LLM specification (model shape, batch/sequence settings, training vs. inference, and hybrid parallelism choices) with an extended Astra-Sim backend that executes those traces on explicit multi-dimensional network topologies with congestion-aware routing and support for degraded and faulty links. The frontend assigns per-operator latency using a tile-based model that accounts for SM under-utilization and multi-level memory traffic (SRAM/ L2/ HBM), and prunes memory-infeasible configurations using an activation-liveness traversal under recomputation, parallelism and ZeRO/FDSP sharding policies.\n  Across A100-based validation cases, RAPID-LLM predicts Llama inference step latency and GPT-scale training time per batch within 10.4\\% relative to published measurements, and matches ns-3 packet-level results within 8\\% on representative communication workloads. Case studies demonstrate how RAPID-LLM enables fast, exhaustive sweeps over hybrid-parallel configurations, quantifies sensitivity to soft link faults under realistic routing and congestion, and evaluates hypothetical GPU design variants including HBM bandwidth throttling effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID-LLM is a unified performance modeling framework for large language model (LLM) training and inference on GPU clusters. It couples a DeepFlow-based frontend that generates hardware-aware, operator-level Chakra execution traces from an abstract LLM specification (model shape, batch/sequence settings, training vs. inference, and hybrid parallelism choices) with an extended Astra-Sim backend that executes those traces on explicit multi-dimensional network topologies with congestion-aware routing and support for degraded and faulty links. The frontend assigns per-operator latency using a tile-based model that accounts for SM under-utilization and multi-level memory traffic (SRAM/ L2/ HBM), and prunes memory-infeasible configurations using an activation-liveness traversal under recomputation, parallelism and ZeRO/FDSP sharding policies.\n  Across A100-based validation cases, RAPID-LLM predicts Llama inference step latency and GPT-scale training time per batch within 10.4\\% relative to published measurements, and matches ns-3 packet-level results within 8\\% on representative communication workloads. Case studies demonstrate how RAPID-LLM enables fast, exhaustive sweeps over hybrid-parallel configurations, quantifies sensitivity to soft link faults under realistic routing and congestion, and evaluates hypothetical GPU design variants including HBM bandwidth throttling effects."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T17:42:51Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    42,
                    51,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "11 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "George Karfakis"
                    },
                    {
                        "name": "Faraz Tahmasebi"
                    },
                    {
                        "name": "Binglu Chen"
                    },
                    {
                        "name": "Lime Yao"
                    },
                    {
                        "name": "Saptarshi Mitra"
                    },
                    {
                        "name": "Tianyue Pan"
                    },
                    {
                        "name": "Hyoukjun Kwon"
                    },
                    {
                        "name": "Puneet Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Puneet Gupta"
                },
                "author": "Puneet Gupta"
            },
            {
                "id": "http://arxiv.org/abs/2506.05594v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05594v2",
                "title": "SoK: Are Watermarks in LLMs Ready for Deployment?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Are Watermarks in LLMs Ready for Deployment?"
                },
                "updated": "2025-12-22T17:37:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    37,
                    53,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05594v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.\n  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.\n  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T21:12:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    21,
                    12,
                    51,
                    3,
                    156,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Kieu Dang"
                    },
                    {
                        "name": "Phung Lai"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Abdallah Khreishah"
                    },
                    {
                        "name": "My T. Thai"
                    }
                ],
                "author_detail": {
                    "name": "My T. Thai"
                },
                "author": "My T. Thai"
            },
            {
                "id": "http://arxiv.org/abs/2505.17196v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.17196v3",
                "title": "Shape it Up! Restoring LLM Safety during Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shape it Up! Restoring LLM Safety during Finetuning"
                },
                "updated": "2025-12-22T17:30:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    30,
                    15,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.17196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.17196v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks. Our code is publicly available at https://github.com/poloclub/star-dss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks. Our code is publicly available at https://github.com/poloclub/star-dss."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-22T18:05:16Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    18,
                    5,
                    16,
                    3,
                    142,
                    0
                ],
                "arxiv_comment": "NeurIPS'25",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "ShengYun Peng"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Jianfeng Chi"
                    },
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Duen Horng Chau"
                    }
                ],
                "author_detail": {
                    "name": "Duen Horng Chau"
                },
                "author": "Duen Horng Chau"
            },
            {
                "id": "http://arxiv.org/abs/2507.07935v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.07935v6",
                "title": "Working with AI: Measuring the Applicability of Generative AI to Occupations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Working with AI: Measuring the Applicability of Generative AI to Occupations"
                },
                "updated": "2025-12-22T17:01:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    17,
                    1,
                    55,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.07935v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.07935v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With generative AI emerging as a general-purpose technology, understanding its economic effects is among society's most pressing questions. Existing studies of AI impact have largely relied on predictions of AI capabilities or focused narrowly on individual firms. Drawing instead on real-world AI usage, we analyze a dataset of 200k anonymized conversations with Microsoft Bing Copilot to measure AI applicability to occupations. We use an LLM-based pipeline to classify the O*NET work activities assisted or performed by AI in each conversation. We find that the most common and successful AI-assisted work activities involve information work--the creation, processing, and communication of information. At the occupation level, we find widespread AI applicability cutting across sectors, as most occupations have information work components. Our methodology also allows us to predict which occupations are more likely to delegate tasks to AI and which are more likely to use AI to assist existing workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With generative AI emerging as a general-purpose technology, understanding its economic effects is among society's most pressing questions. Existing studies of AI impact have largely relied on predictions of AI capabilities or focused narrowly on individual firms. Drawing instead on real-world AI usage, we analyze a dataset of 200k anonymized conversations with Microsoft Bing Copilot to measure AI applicability to occupations. We use an LLM-based pipeline to classify the O*NET work activities assisted or performed by AI in each conversation. We find that the most common and successful AI-assisted work activities involve information work--the creation, processing, and communication of information. At the occupation level, we find widespread AI applicability cutting across sectors, as most occupations have information work components. Our methodology also allows us to predict which occupations are more likely to delegate tasks to AI and which are more likely to use AI to assist existing workflows."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-10T17:16:33Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    16,
                    33,
                    3,
                    191,
                    0
                ],
                "arxiv_comment": "40 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Kiran Tomlinson"
                    },
                    {
                        "name": "Sonia Jaffe"
                    },
                    {
                        "name": "Will Wang"
                    },
                    {
                        "name": "Scott Counts"
                    },
                    {
                        "name": "Siddharth Suri"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Suri"
                },
                "author": "Siddharth Suri"
            },
            {
                "id": "http://arxiv.org/abs/2512.19570v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19570v1",
                "title": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge"
                },
                "updated": "2025-12-22T16:52:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    52,
                    37,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19570v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/s00146-025-02426-3",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:52:37Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    52,
                    37,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "AI & Soc (2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Angjelin Hila"
                    }
                ],
                "author_detail": {
                    "name": "Angjelin Hila"
                },
                "author": "Angjelin Hila",
                "arxiv_doi": "10.1007/s00146-025-02426-3"
            },
            {
                "id": "http://arxiv.org/abs/2512.19551v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19551v1",
                "title": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios"
                },
                "updated": "2025-12-22T16:31:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    31,
                    30,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19551v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:31:30Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    31,
                    30,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jiawen Wang"
                    },
                    {
                        "name": "Jingjing Wang Tianyang Chen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2512.10218v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10218v2",
                "title": "Does SWE-Bench-Verified Test Agent Ability or Model Memory?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does SWE-Bench-Verified Test Agent Ability or Model Memory?"
                },
                "updated": "2025-12-22T16:30:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    30,
                    1,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10218v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T02:11:06Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    2,
                    11,
                    6,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Thanosan Prathifkumar"
                    },
                    {
                        "name": "Noble Saji Mathews"
                    },
                    {
                        "name": "Meiyappan Nagappan"
                    }
                ],
                "author_detail": {
                    "name": "Meiyappan Nagappan"
                },
                "author": "Meiyappan Nagappan"
            },
            {
                "id": "http://arxiv.org/abs/2512.16401v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16401v2",
                "title": "Navigating the Reality Gap: Privacy-Preserving Adaptation of ASR for Challenging Low-Resource Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Reality Gap: Privacy-Preserving Adaptation of ASR for Challenging Low-Resource Domains"
                },
                "updated": "2025-12-22T16:22:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    22,
                    23,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16401v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatic Speech Recognition (ASR) holds immense potential to assist in clinical documentation and patient report generation, particularly in resource-constrained regions. However, deployment is currently hindered by a technical deadlock: a severe \"Reality Gap\" between laboratory performance and noisy, real-world clinical audio, coupled with strict privacy and resource constraints. We quantify this gap, showing that a robust multilingual model (IndicWav2Vec) degrades to a 40.94% WER on rural clinical data from India, rendering it unusable. To address this, we explore a zero-data-exfiltration framework enabling localized, continual adaptation via Low-Rank Adaptation (LoRA). We conduct a rigorous investigative study of continual learning strategies, characterizing the trade-offs between data-driven and parameter-driven stability. Our results demonstrate that multi-domain Experience Replay (ER) yields the primary performance gains, achieving a 17.1% relative improvement in target WER and reducing catastrophic forgetting by 55% compared to naive adaptation. Furthermore, we observed that standard Elastic Weight Consolidation (EWC) faced numerical stability challenges when applied to LoRA in noisy environments. Our experiments show that a stabilized, linearized formulation effectively controls gradient magnitudes and enables stable convergence. Finally, we verify via a domain-specific spot check that acoustic adaptation is a fundamental prerequisite for usability which cannot be bypassed by language models alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) holds immense potential to assist in clinical documentation and patient report generation, particularly in resource-constrained regions. However, deployment is currently hindered by a technical deadlock: a severe \"Reality Gap\" between laboratory performance and noisy, real-world clinical audio, coupled with strict privacy and resource constraints. We quantify this gap, showing that a robust multilingual model (IndicWav2Vec) degrades to a 40.94% WER on rural clinical data from India, rendering it unusable. To address this, we explore a zero-data-exfiltration framework enabling localized, continual adaptation via Low-Rank Adaptation (LoRA). We conduct a rigorous investigative study of continual learning strategies, characterizing the trade-offs between data-driven and parameter-driven stability. Our results demonstrate that multi-domain Experience Replay (ER) yields the primary performance gains, achieving a 17.1% relative improvement in target WER and reducing catastrophic forgetting by 55% compared to naive adaptation. Furthermore, we observed that standard Elastic Weight Consolidation (EWC) faced numerical stability challenges when applied to LoRA in noisy environments. Our experiments show that a stabilized, linearized formulation effectively controls gradient magnitudes and enables stable convergence. Finally, we verify via a domain-specific spot check that acoustic adaptation is a fundamental prerequisite for usability which cannot be bypassed by language models alone."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T10:56:27Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    10,
                    56,
                    27,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Darshil Chauhan"
                    },
                    {
                        "name": "Adityasinh Solanki"
                    },
                    {
                        "name": "Vansh Patel"
                    },
                    {
                        "name": "Kanav Kapoor"
                    },
                    {
                        "name": "Ritvik Jain"
                    },
                    {
                        "name": "Aditya Bansal"
                    },
                    {
                        "name": "Pratik Narang"
                    },
                    {
                        "name": "Dhruv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Dhruv Kumar"
                },
                "author": "Dhruv Kumar"
            },
            {
                "id": "http://arxiv.org/abs/2512.19537v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19537v1",
                "title": "Event Extraction in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event Extraction in Large Language Model"
                },
                "updated": "2025-12-22T16:22:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    22,
                    14,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19537v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:22:14Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    22,
                    14,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "38 pages, 9 Figures, 5 Tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bobo Li"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yuzhe Ding"
                    },
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Zhaoqi Zhang"
                    },
                    {
                        "name": "Jinheng Li"
                    },
                    {
                        "name": "Xinya Du"
                    },
                    {
                        "name": "Fei Li"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Aixin Sun"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Hao Fei"
                    }
                ],
                "author_detail": {
                    "name": "Hao Fei"
                },
                "author": "Hao Fei"
            },
            {
                "id": "http://arxiv.org/abs/2509.23863v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23863v2",
                "title": "SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models"
                },
                "updated": "2025-12-22T16:06:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    6,
                    3,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23863v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-28T13:08:10Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    13,
                    8,
                    10,
                    6,
                    271,
                    0
                ],
                "arxiv_comment": "Preprint under review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Weizhou Shen"
                    },
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Xiaojun Quan"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang"
            },
            {
                "id": "http://arxiv.org/abs/2505.11076v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.11076v3",
                "title": "Addition is almost all you need: Compressing neural networks with double binary factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addition is almost all you need: Compressing neural networks with double binary factorization"
                },
                "updated": "2025-12-22T16:05:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    5,
                    29,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.11076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.11076v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria.\n  Code available at: https://github.com/usamec/double_binary",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria.\n  Code available at: https://github.com/usamec/double_binary"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-16T10:07:36Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    10,
                    7,
                    36,
                    4,
                    136,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Vladimír Boža"
                    },
                    {
                        "name": "Vladimír Macko"
                    }
                ],
                "author_detail": {
                    "name": "Vladimír Macko"
                },
                "author": "Vladimír Macko"
            },
            {
                "id": "http://arxiv.org/abs/2512.19509v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19509v1",
                "title": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models"
                },
                "updated": "2025-12-22T16:04:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    4,
                    56,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19509v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T16:04:56Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    4,
                    56,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Accepted by FSE 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Shangbo Yun"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Jianghong Huang"
                    },
                    {
                        "name": "Beijun Shen"
                    }
                ],
                "author_detail": {
                    "name": "Beijun Shen"
                },
                "author": "Beijun Shen"
            },
            {
                "id": "http://arxiv.org/abs/2512.19488v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19488v1",
                "title": "Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks"
                },
                "updated": "2025-12-22T15:43:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    43,
                    39,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19488v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/CommNet68224.2025.11288887",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The widespread deployment of Internet of Things (IoT) devices requires intrusion detection systems (IDS) with high accuracy while operating under strict resource constraints. Conventional deep learning IDS are often too large and computationally intensive for edge deployment. We propose a lightweight IDS that combines SHAP-guided feature pruning with knowledge-distilled Kronecker networks. A high-capacity teacher model identifies the most relevant features through SHAP explanations, and a compressed student leverages Kronecker-structured layers to minimize parameters while preserving discriminative inputs. Knowledge distillation transfers softened decision boundaries from teacher to student, improving generalization under compression. Experiments on the TON\\_IoT dataset show that the student is nearly three orders of magnitude smaller than the teacher yet sustains macro-F1 above 0.986 with millisecond-level inference latency. The results demonstrate that explainability-driven pruning and structured compression can jointly enable scalable, low-latency, and energy-efficient IDS for heterogeneous IoT environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of Internet of Things (IoT) devices requires intrusion detection systems (IDS) with high accuracy while operating under strict resource constraints. Conventional deep learning IDS are often too large and computationally intensive for edge deployment. We propose a lightweight IDS that combines SHAP-guided feature pruning with knowledge-distilled Kronecker networks. A high-capacity teacher model identifies the most relevant features through SHAP explanations, and a compressed student leverages Kronecker-structured layers to minimize parameters while preserving discriminative inputs. Knowledge distillation transfers softened decision boundaries from teacher to student, improving generalization under compression. Experiments on the TON\\_IoT dataset show that the student is nearly three orders of magnitude smaller than the teacher yet sustains macro-F1 above 0.986 with millisecond-level inference latency. The results demonstrate that explainability-driven pruning and structured compression can jointly enable scalable, low-latency, and energy-efficient IDS for heterogeneous IoT environments."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:43:39Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    43,
                    39,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "This work has been published in the proceedings of the 2025 8th International Conference on Advanced Communication Technologies and Networking (CommNet)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hafsa Benaddi"
                    },
                    {
                        "name": "Mohammed Jouhari"
                    },
                    {
                        "name": "Nouha Laamech"
                    },
                    {
                        "name": "Anas Motii"
                    },
                    {
                        "name": "Khalil Ibrahimi"
                    }
                ],
                "author_detail": {
                    "name": "Khalil Ibrahimi"
                },
                "author": "Khalil Ibrahimi",
                "arxiv_doi": "10.1109/CommNet68224.2025.11288887"
            },
            {
                "id": "http://arxiv.org/abs/2512.19484v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19484v1",
                "title": "Structured Event Representation and Stock Return Predictability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Event Representation and Stock Return Predictability"
                },
                "updated": "2025-12-22T15:40:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    40,
                    27,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19484v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We find that event features extracted by large language models (LLMs) are effective for text-based stock return prediction. Using a pre-trained LLM to extract event features from news articles, we propose a novel deep learning model based on structured event representation (SER) and attention mechanisms to predict stock returns in the cross-section. Our SER-based model provides superior performance compared with other existing text-driven models to forecast stock returns out of sample and offers highly interpretable feature structures to examine the mechanisms underlying the stock return predictability. We further provide various implications based on SER and highlight the crucial benefit of structured model inputs in stock return predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find that event features extracted by large language models (LLMs) are effective for text-based stock return prediction. Using a pre-trained LLM to extract event features from news articles, we propose a novel deep learning model based on structured event representation (SER) and attention mechanisms to predict stock returns in the cross-section. Our SER-based model provides superior performance compared with other existing text-driven models to forecast stock returns out of sample and offers highly interpretable feature structures to examine the mechanisms underlying the stock return predictability. We further provide various implications based on SER and highlight the crucial benefit of structured model inputs in stock return predictability."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:40:27Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    40,
                    27,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Dandan Qiao"
                    },
                    {
                        "name": "Mingxuan Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Zheng"
                },
                "author": "Mingxuan Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.19481v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19481v1",
                "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis"
                },
                "updated": "2025-12-22T15:32:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    32,
                    45,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19481v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:32:45Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    32,
                    45,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "6 pages",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Katharina Stengg"
                    },
                    {
                        "name": "Christian Macho"
                    },
                    {
                        "name": "Martin Pinzger"
                    }
                ],
                "author_detail": {
                    "name": "Martin Pinzger"
                },
                "author": "Martin Pinzger"
            },
            {
                "id": "http://arxiv.org/abs/2512.19475v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19475v1",
                "title": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting"
                },
                "updated": "2025-12-22T15:28:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    28,
                    55,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19475v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:28:55Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    28,
                    55,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "18 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ivan Decostanzi"
                    },
                    {
                        "name": "Yelena Mejova"
                    },
                    {
                        "name": "Kyriaki Kalimeri"
                    }
                ],
                "author_detail": {
                    "name": "Kyriaki Kalimeri"
                },
                "author": "Kyriaki Kalimeri"
            },
            {
                "id": "http://arxiv.org/abs/2512.19466v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19466v1",
                "title": "Epistemological Fault Lines Between Human and Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epistemological Fault Lines Between Human and Artificial Intelligence"
                },
                "updated": "2025-12-22T15:20:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    20,
                    21,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19466v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are widely described as artificial intelligence, yet their epistemic profile diverges sharply from human cognition. Here we show that the apparent alignment between human and machine outputs conceals a deeper structural mismatch in how judgments are produced. Tracing the historical shift from symbolic AI and information filtering systems to large-scale generative transformers, we argue that LLMs are not epistemic agents but stochastic pattern-completion systems, formally describable as walks on high-dimensional graphs of linguistic transitions rather than as systems that form beliefs or models of the world. By systematically mapping human and artificial epistemic pipelines, we identify seven epistemic fault lines, divergences in grounding, parsing, experience, motivation, causal reasoning, metacognition, and value. We call the resulting condition Epistemia: a structural situation in which linguistic plausibility substitutes for epistemic evaluation, producing the feeling of knowing without the labor of judgment. We conclude by outlining consequences for evaluation, governance, and epistemic literacy in societies increasingly organized around generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely described as artificial intelligence, yet their epistemic profile diverges sharply from human cognition. Here we show that the apparent alignment between human and machine outputs conceals a deeper structural mismatch in how judgments are produced. Tracing the historical shift from symbolic AI and information filtering systems to large-scale generative transformers, we argue that LLMs are not epistemic agents but stochastic pattern-completion systems, formally describable as walks on high-dimensional graphs of linguistic transitions rather than as systems that form beliefs or models of the world. By systematically mapping human and artificial epistemic pipelines, we identify seven epistemic fault lines, divergences in grounding, parsing, experience, motivation, causal reasoning, metacognition, and value. We call the resulting condition Epistemia: a structural situation in which linguistic plausibility substitutes for epistemic evaluation, producing the feeling of knowing without the labor of judgment. We conclude by outlining consequences for evaluation, governance, and epistemic literacy in societies increasingly organized around generative AI."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:20:21Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    20,
                    21,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "16 pages, 1 figure",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Walter Quattrociocchi"
                    },
                    {
                        "name": "Valerio Capraro"
                    },
                    {
                        "name": "Matjaž Perc"
                    }
                ],
                "author_detail": {
                    "name": "Matjaž Perc"
                },
                "author": "Matjaž Perc"
            },
            {
                "id": "http://arxiv.org/abs/2512.19458v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19458v1",
                "title": "An Agentic Framework for Autonomous Materials Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic Framework for Autonomous Materials Computation"
                },
                "updated": "2025-12-22T15:03:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    3,
                    57,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19458v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:03:57Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    3,
                    57,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zeyu Xia"
                    },
                    {
                        "name": "Jinzhe Ma"
                    },
                    {
                        "name": "Congjie Zheng"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "P. Hu"
                    },
                    {
                        "name": "Changshui Zhang"
                    },
                    {
                        "name": "Xingao Gong"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Mao Su"
                    }
                ],
                "author_detail": {
                    "name": "Mao Su"
                },
                "author": "Mao Su"
            },
            {
                "id": "http://arxiv.org/abs/2512.19456v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19456v1",
                "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations"
                },
                "updated": "2025-12-22T15:01:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    1,
                    7,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19456v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T15:01:07Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    15,
                    1,
                    7,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jinwei Chi"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Xuanye Lin"
                    },
                    {
                        "name": "Qiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Xu"
                },
                "author": "Qiang Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.19453v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19453v1",
                "title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation"
                },
                "updated": "2025-12-22T14:58:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    58,
                    52,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19453v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:58:52Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    58,
                    52,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "8 pages, 10 figures, This work was completed in December 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zhenglong Guo"
                    },
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Heng Jin"
                    },
                    {
                        "name": "Zongbao Feng"
                    },
                    {
                        "name": "Jianbin Zhou"
                    },
                    {
                        "name": "Siyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Xu"
                },
                "author": "Siyuan Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.19451v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19451v1",
                "title": "Sign Language Recognition using Parallel Bidirectional Reservoir Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Language Recognition using Parallel Bidirectional Reservoir Computing"
                },
                "updated": "2025-12-22T14:55:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    55,
                    54,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19451v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sign language recognition (SLR) facilitates communication between deaf and hearing communities. Deep learning based SLR models are commonly used but require extensive computational resources, making them unsuitable for deployment on edge devices. To address these limitations, we propose a lightweight SLR system that combines parallel bidirectional reservoir computing (PBRC) with MediaPipe. MediaPipe enables real-time hand tracking and precise extraction of hand joint coordinates, which serve as input features for the PBRC architecture. The proposed PBRC architecture consists of two echo state network (ESN) based bidirectional reservoir computing (BRC) modules arranged in parallel to capture temporal dependencies, thereby creating a rich feature representation for classification. We trained our PBRC-based SLR system on the Word-Level American Sign Language (WLASL) video dataset, achieving top-1, top-5, and top-10 accuracies of 60.85%, 85.86%, and 91.74%, respectively. Training time was significantly reduced to 18.67 seconds due to the intrinsic properties of reservoir computing, compared to over 55 minutes for deep learning based methods such as Bi-GRU. This approach offers a lightweight, cost-effective solution for real-time SLR on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign language recognition (SLR) facilitates communication between deaf and hearing communities. Deep learning based SLR models are commonly used but require extensive computational resources, making them unsuitable for deployment on edge devices. To address these limitations, we propose a lightweight SLR system that combines parallel bidirectional reservoir computing (PBRC) with MediaPipe. MediaPipe enables real-time hand tracking and precise extraction of hand joint coordinates, which serve as input features for the PBRC architecture. The proposed PBRC architecture consists of two echo state network (ESN) based bidirectional reservoir computing (BRC) modules arranged in parallel to capture temporal dependencies, thereby creating a rich feature representation for classification. We trained our PBRC-based SLR system on the Word-Level American Sign Language (WLASL) video dataset, achieving top-1, top-5, and top-10 accuracies of 60.85%, 85.86%, and 91.74%, respectively. Training time was significantly reduced to 18.67 seconds due to the intrinsic properties of reservoir computing, compared to over 55 minutes for deep learning based methods such as Bi-GRU. This approach offers a lightweight, cost-effective solution for real-time SLR on edge devices."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:55:54Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    55,
                    54,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Nitin Kumar Singh"
                    },
                    {
                        "name": "Arie Rachmad Syulistyo"
                    },
                    {
                        "name": "Yuichiro Tanaka"
                    },
                    {
                        "name": "Hakaru Tamukoh"
                    }
                ],
                "author_detail": {
                    "name": "Hakaru Tamukoh"
                },
                "author": "Hakaru Tamukoh"
            },
            {
                "id": "http://arxiv.org/abs/2507.04453v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.04453v3",
                "title": "ESSA: Evolutionary Strategies for Scalable Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESSA: Evolutionary Strategies for Scalable Alignment"
                },
                "updated": "2025-12-22T14:35:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    35,
                    3,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.04453v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.04453v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Alignment of Large Language Models (LLMs) typically relies on Reinforcement Learning from Human Feedback (RLHF) with gradient-based optimizers such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO). While effective, these methods require complex distributed training, large memory budgets, and careful hyperparameter tuning, all of which become increasingly difficult at billion-parameter scale. We present ESSA, Evolutionary Strategies for Scalable Alignment, a gradient-free framework that aligns LLMs using only forward inference and black-box optimization. ESSA focuses optimization on Low-Rank Adapters (LoRA) and further compresses their parameter space by optimizing only the singular values from an singular value decomposition (SVD) of each adapter matrix. This dimensionality reduction makes evolutionary search practical even for very large models and allows efficient operation in quantized INT4 and INT8 inference mode. Across these benchmarks ESSA improves the test accuracy of Qwen2.5-Math-7B by 12.6% on GSM8K and 14.8% on PRM800K, and raises the accuracy of LLaMA3.1-8B on IFEval by 22.5%, all compared with GRPO. In large-scale settings ESSA shows stronger scaling than gradient-based methods: on Qwen2.5-32B for PRM800K it reaches near-optimal accuracy twice as fast on 16 GPUs and six times as fast on 128 GPUs compared with GRPO. These results position evolutionary strategies as a compelling, hardware-friendly alternative to gradient-based LLM alignment, combining competitive quality with substantially reduced wall-clock time and engineering overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of Large Language Models (LLMs) typically relies on Reinforcement Learning from Human Feedback (RLHF) with gradient-based optimizers such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO). While effective, these methods require complex distributed training, large memory budgets, and careful hyperparameter tuning, all of which become increasingly difficult at billion-parameter scale. We present ESSA, Evolutionary Strategies for Scalable Alignment, a gradient-free framework that aligns LLMs using only forward inference and black-box optimization. ESSA focuses optimization on Low-Rank Adapters (LoRA) and further compresses their parameter space by optimizing only the singular values from an singular value decomposition (SVD) of each adapter matrix. This dimensionality reduction makes evolutionary search practical even for very large models and allows efficient operation in quantized INT4 and INT8 inference mode. Across these benchmarks ESSA improves the test accuracy of Qwen2.5-Math-7B by 12.6% on GSM8K and 14.8% on PRM800K, and raises the accuracy of LLaMA3.1-8B on IFEval by 22.5%, all compared with GRPO. In large-scale settings ESSA shows stronger scaling than gradient-based methods: on Qwen2.5-32B for PRM800K it reaches near-optimal accuracy twice as fast on 16 GPUs and six times as fast on 128 GPUs compared with GRPO. These results position evolutionary strategies as a compelling, hardware-friendly alternative to gradient-based LLM alignment, combining competitive quality with substantially reduced wall-clock time and engineering overhead."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-06T16:23:07Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    16,
                    23,
                    7,
                    6,
                    187,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Daria Korotyshova"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Alexey Malakhov"
                    },
                    {
                        "name": "Alexey Khokhulin"
                    },
                    {
                        "name": "Nikita Surnachev"
                    },
                    {
                        "name": "Kirill Ovcharenko"
                    },
                    {
                        "name": "George Bredis"
                    },
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov"
            },
            {
                "id": "http://arxiv.org/abs/2512.19424v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19424v1",
                "title": "CodeSimpleQA: Scaling Factuality in Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSimpleQA: Scaling Factuality in Code Large Language Models"
                },
                "updated": "2025-12-22T14:27:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    27,
                    17,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19424v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:27:17Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    27,
                    17,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Shawn Guo"
                    },
                    {
                        "name": "Haowen Wang"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Weifeng Lv"
                    }
                ],
                "author_detail": {
                    "name": "Weifeng Lv"
                },
                "author": "Weifeng Lv"
            },
            {
                "id": "http://arxiv.org/abs/2512.19414v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19414v1",
                "title": "From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions"
                },
                "updated": "2025-12-22T14:13:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    13,
                    1,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19414v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:13:01Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    13,
                    1,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Jiaren Peng"
                    },
                    {
                        "name": "Hongda Sun"
                    },
                    {
                        "name": "Xuan Tian"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Zeqing Li"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan"
            },
            {
                "id": "http://arxiv.org/abs/2512.08398v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08398v2",
                "title": "Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring"
                },
                "updated": "2025-12-22T13:57:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    57,
                    26,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08398v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T09:26:37Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    26,
                    37,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "The authors have identified significant technical errors in the paper that invalidate the current findings",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Jiin Park"
                    },
                    {
                        "name": "Hyuna Jeon"
                    },
                    {
                        "name": "Yoonseo Lee"
                    },
                    {
                        "name": "Jisu Hong"
                    },
                    {
                        "name": "Misuk Kim"
                    }
                ],
                "author_detail": {
                    "name": "Misuk Kim"
                },
                "author": "Misuk Kim"
            },
            {
                "id": "http://arxiv.org/abs/2512.19399v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19399v1",
                "title": "Brain-Grounded Axes for Reading and Steering LLM States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-Grounded Axes for Reading and Steering LLM States"
                },
                "updated": "2025-12-22T13:51:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    51,
                    3,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19399v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:51:03Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    51,
                    3,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "10 pages, 4 figures. Code: https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sandro Andric"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Andric"
                },
                "author": "Sandro Andric"
            },
            {
                "id": "http://arxiv.org/abs/2512.19392v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19392v1",
                "title": "Holographic correlators from multi-mode AdS$_5$ bubbling geometries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holographic correlators from multi-mode AdS$_5$ bubbling geometries"
                },
                "updated": "2025-12-22T13:40:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    40,
                    16,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19392v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Supergravity calculations of holographic four-point correlation functions are a powerful tool for deriving dynamical information about the dual field theory at strong coupling. Recently, a number of such computations have been performed by studying light probes of smooth horizonless supergravity solutions, including certain asymptotically $AdS_5\\times S^5$ solutions of Lin-Lunin-Maldacena (LLM) type. We construct a new closed-form perturbative LLM solution involving a pair of linearized supergravitons with different mode numbers and their quadratic backreaction. Using this solution, we compute two infinite sequences of four-point correlators. Depending on the scaling of certain parameters, the background supergravity solutions are dual to states of 4D $\\mathcal{N}=4$ super Yang-Mills theory that are either heavy or light, whereupon the correlators in the dual field theory are either (perturbatively) heavy-heavy-light-light or all-light. In the all-light regime, the correlators we compute are related by superconformal Ward identities to correlators of four single-particle chiral primary operators (CPOs). We confirm a set of expressions in the literature for such correlators of CPOs, that were obtained via Mellin space bootstrap and/or Witten diagram methods. We also obtain a novel compact expression for one of the two infinite sequences of correlators of chiral primaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supergravity calculations of holographic four-point correlation functions are a powerful tool for deriving dynamical information about the dual field theory at strong coupling. Recently, a number of such computations have been performed by studying light probes of smooth horizonless supergravity solutions, including certain asymptotically $AdS_5\\times S^5$ solutions of Lin-Lunin-Maldacena (LLM) type. We construct a new closed-form perturbative LLM solution involving a pair of linearized supergravitons with different mode numbers and their quadratic backreaction. Using this solution, we compute two infinite sequences of four-point correlators. Depending on the scaling of certain parameters, the background supergravity solutions are dual to states of 4D $\\mathcal{N}=4$ super Yang-Mills theory that are either heavy or light, whereupon the correlators in the dual field theory are either (perturbatively) heavy-heavy-light-light or all-light. In the all-light regime, the correlators we compute are related by superconformal Ward identities to correlators of four single-particle chiral primary operators (CPOs). We confirm a set of expressions in the literature for such correlators of CPOs, that were obtained via Mellin space bootstrap and/or Witten diagram methods. We also obtain a novel compact expression for one of the two infinite sequences of correlators of chiral primaries."
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:40:16Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    40,
                    16,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "29 pages",
                "arxiv_primary_category": {
                    "term": "hep-th"
                },
                "authors": [
                    {
                        "name": "David Turton"
                    },
                    {
                        "name": "Alexander Tyukov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Tyukov"
                },
                "author": "Alexander Tyukov"
            },
            {
                "id": "http://arxiv.org/abs/2512.16229v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16229v2",
                "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding"
                },
                "updated": "2025-12-22T13:29:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    29,
                    11,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16229v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T06:22:01Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    6,
                    22,
                    1,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chenkai Xu"
                    },
                    {
                        "name": "Yijie Jin"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yi Tu"
                    },
                    {
                        "name": "Guoping Long"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Mingcong Song"
                    },
                    {
                        "name": "Hongjie Si"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng"
            },
            {
                "id": "http://arxiv.org/abs/2512.17436v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17436v2",
                "title": "Xiaomi MiMo-VL-Miloco Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xiaomi MiMo-VL-Miloco Technical Report"
                },
                "updated": "2025-12-22T13:27:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    27,
                    24,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17436v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We open-source MiMo-VL-Miloco-7B and its quantized variant MiMo-VL-Miloco-7B-GGUF, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at https://github.com/XiaoMi/xiaomi-mimo-vl-miloco to support research and deployment in real-world smart-home applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We open-source MiMo-VL-Miloco-7B and its quantized variant MiMo-VL-Miloco-7B-GGUF, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at https://github.com/XiaoMi/xiaomi-mimo-vl-miloco to support research and deployment in real-world smart-home applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T10:43:37Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    10,
                    43,
                    37,
                    4,
                    353,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiaze Li"
                    },
                    {
                        "name": "Jingyang Chen"
                    },
                    {
                        "name": "Yuxun Qu"
                    },
                    {
                        "name": "Shijie Xu"
                    },
                    {
                        "name": "Zhenru Lin"
                    },
                    {
                        "name": "Junyou Zhu"
                    },
                    {
                        "name": "Boshen Xu"
                    },
                    {
                        "name": "Wenhui Tan"
                    },
                    {
                        "name": "Pei Fu"
                    },
                    {
                        "name": "Jianzhong Ju"
                    },
                    {
                        "name": "Zhenbo Luo"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan"
            },
            {
                "id": "http://arxiv.org/abs/2306.02766v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2306.02766v6",
                "title": "Networked Communication for Decentralised Agents in Mean-Field Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Networked Communication for Decentralised Agents in Mean-Field Games"
                },
                "updated": "2025-12-22T13:25:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    25,
                    46,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2306.02766v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2306.02766v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Methods like multi-agent reinforcement learning struggle to scale with growing population size. Mean-field games (MFGs) are a game-theoretic approach that can circumvent this by finding a solution for an abstract infinite population, which can then be used as an approximate solution for the $N$-agent problem. However, classical mean-field algorithms usually only work under restrictive conditions. We take steps to address this by introducing networked communication to MFGs, in particular to settings that use a single, non-episodic run of $N$ decentralised agents to simulate the infinite population, as is likely to be most reasonable in real-world deployments. We prove that our architecture's sample guarantees lie between those of earlier theoretical algorithms for the centralised- and independent-learning architectures, varying dependent on network structure and the number of communication rounds. However, the sample guarantees of the three theoretical algorithms do not actually result in practical convergence times. We thus contribute practical enhancements to all three algorithms allowing us to present their first empirical demonstrations. We then show that in practical settings where the theoretical hyperparameters are not observed, giving fewer loops but poorer estimation of the Q-function, our communication scheme still respects the earlier theoretical analysis: it considerably accelerates learning over the independent case, which hardly seems to learn at all, and often performs similarly to the centralised case, while removing the restrictive assumption of the latter. We provide ablations and additional studies showing that our networked approach also has advantages over both alternatives in terms of robustness to update failures and to changes in population size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods like multi-agent reinforcement learning struggle to scale with growing population size. Mean-field games (MFGs) are a game-theoretic approach that can circumvent this by finding a solution for an abstract infinite population, which can then be used as an approximate solution for the $N$-agent problem. However, classical mean-field algorithms usually only work under restrictive conditions. We take steps to address this by introducing networked communication to MFGs, in particular to settings that use a single, non-episodic run of $N$ decentralised agents to simulate the infinite population, as is likely to be most reasonable in real-world deployments. We prove that our architecture's sample guarantees lie between those of earlier theoretical algorithms for the centralised- and independent-learning architectures, varying dependent on network structure and the number of communication rounds. However, the sample guarantees of the three theoretical algorithms do not actually result in practical convergence times. We thus contribute practical enhancements to all three algorithms allowing us to present their first empirical demonstrations. We then show that in practical settings where the theoretical hyperparameters are not observed, giving fewer loops but poorer estimation of the Q-function, our communication scheme still respects the earlier theoretical analysis: it considerably accelerates learning over the independent case, which hardly seems to learn at all, and often performs similarly to the centralised case, while removing the restrictive assumption of the latter. We provide ablations and additional studies showing that our networked approach also has advantages over both alternatives in terms of robustness to update failures and to changes in population size."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-06-05T10:45:39Z",
                "published_parsed": [
                    2023,
                    6,
                    5,
                    10,
                    45,
                    39,
                    0,
                    156,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Patrick Benjamin"
                    },
                    {
                        "name": "Alessandro Abate"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Abate"
                },
                "author": "Alessandro Abate"
            },
            {
                "id": "http://arxiv.org/abs/2512.19379v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19379v1",
                "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation"
                },
                "updated": "2025-12-22T13:23:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    23,
                    55,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19379v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:23:55Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    23,
                    55,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xueming Yan"
                    },
                    {
                        "name": "Boyan Xu"
                    },
                    {
                        "name": "Yaochu Jin"
                    },
                    {
                        "name": "Lixian Xiao"
                    },
                    {
                        "name": "Wenlong Ye"
                    },
                    {
                        "name": "Runyang Cai"
                    },
                    {
                        "name": "Zeqi Zheng"
                    },
                    {
                        "name": "Jingfa Liu"
                    },
                    {
                        "name": "Aimin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Aimin Yang"
                },
                "author": "Aimin Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.19378v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19378v1",
                "title": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models"
                },
                "updated": "2025-12-22T13:23:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    23,
                    29,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19378v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:23:29Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    23,
                    29,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Camera-ready version of the paper accepted for oral presentation at the 11th International Conference on Computer and Communications (ICCC 2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "In Proceedings of the 11th International Conference on Computer and Communications, 2025",
                "authors": [
                    {
                        "name": "Zhiqing Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Jiazhong Lu"
                    },
                    {
                        "name": "Xiaolei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolei Liu"
                },
                "author": "Xiaolei Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.19364v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19364v1",
                "title": "ForeSpeed: A real-world video dataset of CCTV cameras with different settings for vehicle speed estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForeSpeed: A real-world video dataset of CCTV cameras with different settings for vehicle speed estimation"
                },
                "updated": "2025-12-22T13:05:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    5,
                    40,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19364v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The need to estimate the speed of road vehicles has become increasingly important in the field of video forensics, particularly with the widespread deployment of CCTV cameras worldwide. Despite the development of various approaches, the accuracy of forensic speed estimation from real-world footage remains highly dependent on several factors, including camera specifications, acquisition methods, spatial and temporal resolution, compression methods, and scene perspective, which can significantly influence performance.\n  In this paper, we introduce ForeSpeed, a comprehensive dataset designed to support the evaluation of speed estimation techniques in real-world scenarios using CCTV footage. The dataset includes recordings of a vehicle traveling at known speeds, captured by three digital and three analog cameras from two distinct perspectives. Real-world road metrics are provided to enable the restoration of the scene geometry. Videos were stored with multiple compression factors and settings, to simulate real world scenarios in which export procedures are not always performed according to forensic standards. Overall, ForeSpeed, includes a collection of 322 videos.\n  As a case study, we employed the ForeSpeed dataset to benchmark a speed estimation algorithm available in a commercial product (Amped FIVE). Results demonstrate that while the method reliably estimates average speed across various conditions, its uncertainty range significantly increases when the scene involves strong perspective distortion. The ForeSpeed dataset is publicly available to the forensic community, with the aim of facilitating the evaluation of current methodologies and inspiring the development of new, robust solutions tailored to collision investigation and forensic incident analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to estimate the speed of road vehicles has become increasingly important in the field of video forensics, particularly with the widespread deployment of CCTV cameras worldwide. Despite the development of various approaches, the accuracy of forensic speed estimation from real-world footage remains highly dependent on several factors, including camera specifications, acquisition methods, spatial and temporal resolution, compression methods, and scene perspective, which can significantly influence performance.\n  In this paper, we introduce ForeSpeed, a comprehensive dataset designed to support the evaluation of speed estimation techniques in real-world scenarios using CCTV footage. The dataset includes recordings of a vehicle traveling at known speeds, captured by three digital and three analog cameras from two distinct perspectives. Real-world road metrics are provided to enable the restoration of the scene geometry. Videos were stored with multiple compression factors and settings, to simulate real world scenarios in which export procedures are not always performed according to forensic standards. Overall, ForeSpeed, includes a collection of 322 videos.\n  As a case study, we employed the ForeSpeed dataset to benchmark a speed estimation algorithm available in a commercial product (Amped FIVE). Results demonstrate that while the method reliably estimates average speed across various conditions, its uncertainty range significantly increases when the scene involves strong perspective distortion. The ForeSpeed dataset is publicly available to the forensic community, with the aim of facilitating the evaluation of current methodologies and inspiring the development of new, robust solutions tailored to collision investigation and forensic incident analysis."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T13:05:40Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    5,
                    40,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Massimo Iuliani"
                    },
                    {
                        "name": "Blake Sawyer"
                    },
                    {
                        "name": "Marco Fontani"
                    },
                    {
                        "name": "David Spreadborough"
                    },
                    {
                        "name": "Martino Jerian"
                    }
                ],
                "author_detail": {
                    "name": "Martino Jerian"
                },
                "author": "Martino Jerian"
            },
            {
                "id": "http://arxiv.org/abs/2506.17231v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.17231v2",
                "title": "Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs"
                },
                "updated": "2025-12-22T13:04:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    13,
                    4,
                    30,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.17231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.17231v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the scale and complexity of jailbreaking attacks on large language models (LLMs) continue to escalate, their efficiency and practical applicability are constrained, posing a profound challenge to LLM security. Jailbreaking techniques have advanced from manual prompt engineering to automated methodologies. Recent advances have automated jailbreaking approaches that harness LLMs to generate jailbreak instructions and adversarial examples, delivering encouraging results. Nevertheless, these methods universally include an LLM generation phase, which, due to the complexities of deploying and reasoning with LLMs, impedes effective implementation and broader adoption. To mitigate this issue, we introduce \\textbf{Adversarial Prompt Distillation}, an innovative framework that integrates masked language modeling, reinforcement learning, and dynamic temperature control to distill LLM jailbreaking prowess into smaller language models (SLMs). This methodology enables efficient, robust jailbreak attacks while maintaining high success rates and accommodating a broader range of application contexts. Empirical evaluations affirm the approach's superiority in attack efficacy, resource optimization, and cross-model versatility. Our research underscores the practicality of transferring jailbreak capabilities to SLMs, reveals inherent vulnerabilities in LLMs, and provides novel insights to advance LLM security investigations. Our code is available at: https://github.com/lxgem/Efficient_and_Stealthy_Jailbreak_Attacks_via_Adversarial_Prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the scale and complexity of jailbreaking attacks on large language models (LLMs) continue to escalate, their efficiency and practical applicability are constrained, posing a profound challenge to LLM security. Jailbreaking techniques have advanced from manual prompt engineering to automated methodologies. Recent advances have automated jailbreaking approaches that harness LLMs to generate jailbreak instructions and adversarial examples, delivering encouraging results. Nevertheless, these methods universally include an LLM generation phase, which, due to the complexities of deploying and reasoning with LLMs, impedes effective implementation and broader adoption. To mitigate this issue, we introduce \\textbf{Adversarial Prompt Distillation}, an innovative framework that integrates masked language modeling, reinforcement learning, and dynamic temperature control to distill LLM jailbreaking prowess into smaller language models (SLMs). This methodology enables efficient, robust jailbreak attacks while maintaining high success rates and accommodating a broader range of application contexts. Empirical evaluations affirm the approach's superiority in attack efficacy, resource optimization, and cross-model versatility. Our research underscores the practicality of transferring jailbreak capabilities to SLMs, reveals inherent vulnerabilities in LLMs, and provides novel insights to advance LLM security investigations. Our code is available at: https://github.com/lxgem/Efficient_and_Stealthy_Jailbreak_Attacks_via_Adversarial_Prompt."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-26T08:27:51Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    27,
                    51,
                    0,
                    146,
                    0
                ],
                "arxiv_comment": "19 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Fangyu Wu"
                    },
                    {
                        "name": "Yushi Li"
                    },
                    {
                        "name": "Xiaobo Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobo Jin"
                },
                "author": "Xiaobo Jin"
            },
            {
                "id": "http://arxiv.org/abs/2506.18998v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.18998v3",
                "title": "Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge"
                },
                "updated": "2025-12-22T12:50:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    50,
                    45,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.18998v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.18998v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "When artificial intelligence mistakes memorization for intelligence, it creates a dangerous mirage of reasoning. Existing studies treat memorization and self-knowledge deficits in LLMs as separate issues and do not recognize an intertwining link that degrades the trustworthiness of LLM responses. In our study, we utilize a novel framework to ascertain if LLMs genuinely learn reasoning patterns from training data or merely memorize them to assume competence across problems of similar complexity focused on STEM domains. Our analysis shows a noteworthy problem in generalization: LLMs draw confidence from memorized solutions to infer a higher self-knowledge about their reasoning ability, which manifests as an over 45% inconsistency in feasibility assessments when faced with self-validated, logically coherent task perturbations. This effect is most pronounced in science and medicine domains, which tend to have maximal standardized jargon and problems, further confirming our approach. Significant wavering within the self-knowledge of LLMs also shows flaws in current architectures and training patterns, highlighting the need for techniques that ensure a balanced, consistent stance on models' perceptions of their own knowledge for maximum AI explainability and trustworthiness. Our code and results are available publicly at https://github.com/Sahil-R-Kale/mirage_of_mastery",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When artificial intelligence mistakes memorization for intelligence, it creates a dangerous mirage of reasoning. Existing studies treat memorization and self-knowledge deficits in LLMs as separate issues and do not recognize an intertwining link that degrades the trustworthiness of LLM responses. In our study, we utilize a novel framework to ascertain if LLMs genuinely learn reasoning patterns from training data or merely memorize them to assume competence across problems of similar complexity focused on STEM domains. Our analysis shows a noteworthy problem in generalization: LLMs draw confidence from memorized solutions to infer a higher self-knowledge about their reasoning ability, which manifests as an over 45% inconsistency in feasibility assessments when faced with self-validated, logically coherent task perturbations. This effect is most pronounced in science and medicine domains, which tend to have maximal standardized jargon and problems, further confirming our approach. Significant wavering within the self-knowledge of LLMs also shows flaws in current architectures and training patterns, highlighting the need for techniques that ensure a balanced, consistent stance on models' perceptions of their own knowledge for maximum AI explainability and trustworthiness. Our code and results are available publicly at https://github.com/Sahil-R-Kale/mirage_of_mastery"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-23T18:01:16Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    18,
                    1,
                    16,
                    0,
                    174,
                    0
                ],
                "arxiv_comment": "12 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sahil Kale"
                    }
                ],
                "author_detail": {
                    "name": "Sahil Kale"
                },
                "author": "Sahil Kale"
            },
            {
                "id": "http://arxiv.org/abs/2511.02864v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02864v3",
                "title": "Mathematical exploration and discovery at scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical exploration and discovery at scale"
                },
                "updated": "2025-12-22T12:49:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    49,
                    1,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02864v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AlphaEvolve (Novikov et al., 2025) is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.\n  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.\n  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaEvolve (Novikov et al., 2025) is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.\n  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.\n  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time."
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T16:04:07Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    16,
                    4,
                    7,
                    0,
                    307,
                    0
                ],
                "arxiv_comment": "81 pages, 35 figures",
                "arxiv_primary_category": {
                    "term": "cs.NE"
                },
                "authors": [
                    {
                        "name": "Bogdan Georgiev"
                    },
                    {
                        "name": "Javier Gómez-Serrano"
                    },
                    {
                        "name": "Terence Tao"
                    },
                    {
                        "name": "Adam Zsolt Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Adam Zsolt Wagner"
                },
                "author": "Adam Zsolt Wagner"
            },
            {
                "id": "http://arxiv.org/abs/2512.19349v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19349v1",
                "title": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop"
                },
                "updated": "2025-12-22T12:48:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    48,
                    29,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19349v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:48:29Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    48,
                    29,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "7 pages,1 figure,4 tables",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "JiaWei Zhu"
                    },
                    {
                        "name": "ZiHeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "ZiHeng Liu"
                },
                "author": "ZiHeng Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.10713v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10713v2",
                "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code"
                },
                "updated": "2025-12-22T12:47:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    47,
                    11,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10713v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:49:56Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    49,
                    56,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Itay Dreyfuss"
                    },
                    {
                        "name": "Antonio Abu Nassar"
                    },
                    {
                        "name": "Samuel Ackerman"
                    },
                    {
                        "name": "Axel Ben David"
                    },
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "Rami Katan"
                    },
                    {
                        "name": "Orna Raz"
                    },
                    {
                        "name": "Marcel Zalmanovici"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Zalmanovici"
                },
                "author": "Marcel Zalmanovici"
            },
            {
                "id": "http://arxiv.org/abs/2511.20944v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.20944v3",
                "title": "Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection"
                },
                "updated": "2025-12-22T12:31:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    31,
                    0,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.20944v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.20944v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual losses, presenting a massive economic asymmetry: the financial cost of a False Negative (fraud loss) exceeds the operational cost of a False Positive (manual review) by a ratio of approximately 5,480:1. This paper contrasts two detection paradigms: a Forensic Psycholinguistic Stream (CatBoost), which analyzes linguistic cues like urgency and authority with high interpretability, and a Semantic Stream (DistilBERT), which utilizes deep learning for contextual understanding. We evaluated both streams on a hybrid dataset (N=7,990) containing human-legitimate and AI-synthesized adversarial fraud. Benchmarked on Tesla T4 infrastructure, DistilBERT achieved near-perfect detection on synthetic threats (AUC >0.99, F1 =0.998) with acceptable real-time latency (7.4 ms). CatBoost achieved competitive detection (AUC =0.991, F1 =0.949) at 8.4x lower latency (0.8 ms) with negligible resource consumption. We conclude that while DistilBERT offers maximum accuracy for GPU-equipped organizations, CatBoost provides a viable, cost-effective alternative for edge deployments. Both approaches demonstrate a theoretical ROI exceeding 99.9% when optimized via cost-sensitive learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual losses, presenting a massive economic asymmetry: the financial cost of a False Negative (fraud loss) exceeds the operational cost of a False Positive (manual review) by a ratio of approximately 5,480:1. This paper contrasts two detection paradigms: a Forensic Psycholinguistic Stream (CatBoost), which analyzes linguistic cues like urgency and authority with high interpretability, and a Semantic Stream (DistilBERT), which utilizes deep learning for contextual understanding. We evaluated both streams on a hybrid dataset (N=7,990) containing human-legitimate and AI-synthesized adversarial fraud. Benchmarked on Tesla T4 infrastructure, DistilBERT achieved near-perfect detection on synthetic threats (AUC >0.99, F1 =0.998) with acceptable real-time latency (7.4 ms). CatBoost achieved competitive detection (AUC =0.991, F1 =0.949) at 8.4x lower latency (0.8 ms) with negligible resource consumption. We conclude that while DistilBERT offers maximum accuracy for GPU-equipped organizations, CatBoost provides a viable, cost-effective alternative for edge deployments. Both approaches demonstrate a theoretical ROI exceeding 99.9% when optimized via cost-sensitive learning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T00:34:46Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    0,
                    34,
                    46,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "8 pages, 12 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yaw Osei Adjei"
                    },
                    {
                        "name": "Frederick Ayivor"
                    },
                    {
                        "name": "Davis Opoku"
                    }
                ],
                "author_detail": {
                    "name": "Davis Opoku"
                },
                "arxiv_affiliation": "Kwame Nkrumah University of Science and Technology",
                "author": "Davis Opoku"
            },
            {
                "id": "http://arxiv.org/abs/2510.02025v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.02025v2",
                "title": "Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models"
                },
                "updated": "2025-12-22T12:27:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    27,
                    15,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.02025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.02025v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluations of large language models (LLMs)' creativity have focused primarily on the quality of their outputs rather than the processes that shape them. This study takes a process-oriented approach, drawing on narratology to examine LLMs as computational authors. We introduce constraint-based decision-making as a lens for authorial creativity. Using controlled prompting to assign authorial personas, we analyze the creative preferences of the models. Our findings show that LLMs consistently emphasize Style over other elements, including Character, Event, and Setting. By also probing the reasoning the models provide for their choices, we show that distinctive profiles emerge across models and argue that our approach provides a novel systematic tool for analyzing AI's authorial creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluations of large language models (LLMs)' creativity have focused primarily on the quality of their outputs rather than the processes that shape them. This study takes a process-oriented approach, drawing on narratology to examine LLMs as computational authors. We introduce constraint-based decision-making as a lens for authorial creativity. Using controlled prompting to assign authorial personas, we analyze the creative preferences of the models. Our findings show that LLMs consistently emphasize Style over other elements, including Character, Event, and Setting. By also probing the reasoning the models provide for their choices, we show that distinctive profiles emerge across models and argue that our approach provides a novel systematic tool for analyzing AI's authorial creativity."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-02T13:57:14Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    57,
                    14,
                    3,
                    275,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Donghoon Jung"
                    },
                    {
                        "name": "Jiwoo Choi"
                    },
                    {
                        "name": "Songeun Chae"
                    },
                    {
                        "name": "Seohyon Jung"
                    }
                ],
                "author_detail": {
                    "name": "Seohyon Jung"
                },
                "author": "Seohyon Jung"
            },
            {
                "id": "http://arxiv.org/abs/2512.19317v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19317v1",
                "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models"
                },
                "updated": "2025-12-22T12:07:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    7,
                    33,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19317v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:07:33Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    7,
                    33,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "A. A. Gde Yogi Pramana"
                    },
                    {
                        "name": "Jason Ray"
                    },
                    {
                        "name": "Anthony Jaya"
                    },
                    {
                        "name": "Michael Wijaya"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wijaya"
                },
                "author": "Michael Wijaya"
            },
            {
                "id": "http://arxiv.org/abs/2512.19309v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19309v1",
                "title": "Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring"
                },
                "updated": "2025-12-22T11:59:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    59,
                    47,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19309v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Structural Health Monitoring (SHM) plays a crucial role in maintaining the safety and resilience of infrastructure. As sensor networks grow in scale and complexity, identifying the most informative sensors becomes essential to reduce deployment costs without compromising monitoring quality. While Graph Signal Processing (GSP) has shown promise by leveraging spatial correlations among sensor nodes, conventional approaches often overlook the temporal dynamics of structural behavior. To overcome this limitation, we propose Time-Vertex Machine Learning (TVML), a novel framework that integrates GSP, time-domain analysis, and machine learning to enable interpretable and efficient sensor placement by identifying representative nodes that minimize redundancy while preserving critical information. We evaluate the proposed approach on two bridge datasets for damage detection and time-varying graph signal reconstruction tasks. The results demonstrate the effectiveness of our approach in enhancing SHM systems by providing a robust, adaptive, and efficient solution for sensor placement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural Health Monitoring (SHM) plays a crucial role in maintaining the safety and resilience of infrastructure. As sensor networks grow in scale and complexity, identifying the most informative sensors becomes essential to reduce deployment costs without compromising monitoring quality. While Graph Signal Processing (GSP) has shown promise by leveraging spatial correlations among sensor nodes, conventional approaches often overlook the temporal dynamics of structural behavior. To overcome this limitation, we propose Time-Vertex Machine Learning (TVML), a novel framework that integrates GSP, time-domain analysis, and machine learning to enable interpretable and efficient sensor placement by identifying representative nodes that minimize redundancy while preserving critical information. We evaluate the proposed approach on two bridge datasets for damage detection and time-varying graph signal reconstruction tasks. The results demonstrate the effectiveness of our approach in enhancing SHM systems by providing a robust, adaptive, and efficient solution for sensor placement."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:59:47Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    59,
                    47,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Keivan Faghih Niresi"
                    },
                    {
                        "name": "Jun Qing"
                    },
                    {
                        "name": "Mengjie Zhao"
                    },
                    {
                        "name": "Olga Fink"
                    }
                ],
                "author_detail": {
                    "name": "Olga Fink"
                },
                "author": "Olga Fink"
            },
            {
                "id": "http://arxiv.org/abs/2512.19305v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19305v1",
                "title": "CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs"
                },
                "updated": "2025-12-22T11:53:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    53,
                    1,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19305v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:53:01Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    53,
                    1,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Javier Vela-Tambo"
                    },
                    {
                        "name": "Jorge Gracia"
                    },
                    {
                        "name": "Fernando Dominguez-Castro"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Dominguez-Castro"
                },
                "author": "Fernando Dominguez-Castro"
            },
            {
                "id": "http://arxiv.org/abs/2503.04521v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.04521v2",
                "title": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market"
                },
                "updated": "2025-12-22T11:48:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    48,
                    52,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.04521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.04521v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/TMC.2025.3647531",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-06T15:08:31Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    8,
                    31,
                    3,
                    65,
                    0
                ],
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Mobile Computing. Index Terms: Edge-AI, DNN Inference Offloading, Resource Management, Dynamic Pricing, Auction Mechanism",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Songyuan Li"
                    },
                    {
                        "name": "Jia Hu"
                    },
                    {
                        "name": "Geyong Min"
                    },
                    {
                        "name": "Haojun Huang"
                    },
                    {
                        "name": "Jiwei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiwei Huang"
                },
                "author": "Jiwei Huang",
                "arxiv_doi": "10.1109/TMC.2025.3647531"
            },
            {
                "id": "http://arxiv.org/abs/2512.19304v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19304v1",
                "title": "Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA"
                },
                "updated": "2025-12-22T11:48:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    48,
                    24,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19304v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Binary neural networks provide a promising solution for low-power, high-speed inference by replacing expensive floating-point operations with bitwise logic. This makes them well-suited for deployment on resource-constrained platforms such as FPGAs. In this study, we present a fully custom BNN inference accelerator for handwritten digit recognition, implemented entirely in Verilog without the use of high-level synthesis tools. The design targets the Xilinx Artix-7 FPGA and achieves real-time classification at 80\\,MHz with low power consumption and predictable timing. Simulation results demonstrate 84\\% accuracy on the MNIST test set and highlight the advantages of manual HDL design for transparent, efficient, and flexible BNN deployment in embedded systems. The complete project including training scripts and Verilog source code are available at GitHub repo for reproducibility and future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary neural networks provide a promising solution for low-power, high-speed inference by replacing expensive floating-point operations with bitwise logic. This makes them well-suited for deployment on resource-constrained platforms such as FPGAs. In this study, we present a fully custom BNN inference accelerator for handwritten digit recognition, implemented entirely in Verilog without the use of high-level synthesis tools. The design targets the Xilinx Artix-7 FPGA and achieves real-time classification at 80\\,MHz with low power consumption and predictable timing. Simulation results demonstrate 84\\% accuracy on the MNIST test set and highlight the advantages of manual HDL design for transparent, efficient, and flexible BNN deployment in embedded systems. The complete project including training scripts and Verilog source code are available at GitHub repo for reproducibility and future development."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:48:24Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    48,
                    24,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "13 pages, 1 figure",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Emir Devlet Ertörer"
                    },
                    {
                        "name": "Cem Ünsalan"
                    }
                ],
                "author_detail": {
                    "name": "Cem Ünsalan"
                },
                "author": "Cem Ünsalan"
            },
            {
                "id": "http://arxiv.org/abs/2512.19299v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19299v1",
                "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application"
                },
                "updated": "2025-12-22T11:43:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    43,
                    35,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19299v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:43:35Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    43,
                    35,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Haoyu Jiang"
                    },
                    {
                        "name": "Fanjie Zeng"
                    },
                    {
                        "name": "Boan Qu"
                    },
                    {
                        "name": "Xiaojie Lin"
                    },
                    {
                        "name": "Wei Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhong"
                },
                "author": "Wei Zhong"
            },
            {
                "id": "http://arxiv.org/abs/2512.19297v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19297v1",
                "title": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models"
                },
                "updated": "2025-12-22T11:40:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    40,
                    47,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19297v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T11:40:47Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    40,
                    47,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "NDSS 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Linzhi Chen"
                    },
                    {
                        "name": "Yang Sun"
                    },
                    {
                        "name": "Hongru Wei"
                    },
                    {
                        "name": "Yuqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Chen"
                },
                "author": "Yuqi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2509.23941v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23941v2",
                "title": "Brain-language fusion enables interactive neural readout and in-silico experimentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-language fusion enables interactive neural readout and in-silico experimentation"
                },
                "updated": "2025-12-22T11:33:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    11,
                    33,
                    19,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23941v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have revolutionized human-machine interaction, and have been extended by embedding diverse modalities such as images into a shared language space. Yet, neural decoding has remained constrained by static, non-interactive methods. We introduce CorText, a framework that integrates neural activity directly into the latent space of an LLM, enabling open-ended, natural language interaction with brain data. Trained on fMRI data recorded during viewing of natural scenes, CorText generates accurate image captions and can answer more detailed questions better than controls, while having access to neural data only. We showcase that CorText achieves zero-shot generalization beyond semantic categories seen during training. In-silico microstimulation experiments, which enable counterfactual prompts on brain activity, reveal a consistent, and graded mapping between brain-state and language output. These advances mark a shift from passive decoding toward generative, flexible interfaces between brain activity and language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized human-machine interaction, and have been extended by embedding diverse modalities such as images into a shared language space. Yet, neural decoding has remained constrained by static, non-interactive methods. We introduce CorText, a framework that integrates neural activity directly into the latent space of an LLM, enabling open-ended, natural language interaction with brain data. Trained on fMRI data recorded during viewing of natural scenes, CorText generates accurate image captions and can answer more detailed questions better than controls, while having access to neural data only. We showcase that CorText achieves zero-shot generalization beyond semantic categories seen during training. In-silico microstimulation experiments, which enable counterfactual prompts on brain activity, reveal a consistent, and graded mapping between brain-state and language output. These advances mark a shift from passive decoding toward generative, flexible interfaces between brain activity and language."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-28T15:35:25Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    35,
                    25,
                    6,
                    271,
                    0
                ],
                "arxiv_comment": "v2",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Victoria Bosch"
                    },
                    {
                        "name": "Daniel Anthes"
                    },
                    {
                        "name": "Adrien Doerig"
                    },
                    {
                        "name": "Sushrut Thorat"
                    },
                    {
                        "name": "Peter König"
                    },
                    {
                        "name": "Tim Christian Kietzmann"
                    }
                ],
                "author_detail": {
                    "name": "Tim Christian Kietzmann"
                },
                "author": "Tim Christian Kietzmann"
            },
            {
                "id": "http://arxiv.org/abs/2506.10035v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.10035v2",
                "title": "FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training"
                },
                "updated": "2025-12-22T10:46:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    46,
                    52,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.10035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.10035v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\\% of the hierarchy pruned. Our code will be available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\\% of the hierarchy pruned. Our code will be available soon."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-10T20:48:30Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    20,
                    48,
                    30,
                    1,
                    161,
                    0
                ],
                "arxiv_comment": "14 pages",
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Fuhan Cai"
                    },
                    {
                        "name": "Yong Guo"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Xiangzhong Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzhong Fang"
                },
                "author": "Xiangzhong Fang"
            },
            {
                "id": "http://arxiv.org/abs/2512.16280v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16280v2",
                "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams"
                },
                "updated": "2025-12-22T10:37:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    37,
                    8,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16280v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.\n  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.\n  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T07:59:15Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    7,
                    59,
                    15,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "arxiv_journal_ref": "Usenix Security Symposium 2026",
                "authors": [
                    {
                        "name": "Gilad Gressel"
                    },
                    {
                        "name": "Rahul Pankajakshan"
                    },
                    {
                        "name": "Shir Rozenfeld"
                    },
                    {
                        "name": "Ling Li"
                    },
                    {
                        "name": "Ivan Franceschini"
                    },
                    {
                        "name": "Krishnashree Achuthan"
                    },
                    {
                        "name": "Yisroel Mirsky"
                    }
                ],
                "author_detail": {
                    "name": "Yisroel Mirsky"
                },
                "author": "Yisroel Mirsky"
            },
            {
                "id": "http://arxiv.org/abs/2512.19247v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19247v1",
                "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics"
                },
                "updated": "2025-12-22T10:29:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    29,
                    51,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19247v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T10:29:51Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    29,
                    51,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Do Minh Duc"
                    },
                    {
                        "name": "Quan Xuan Truong"
                    },
                    {
                        "name": "Nguyen Tat Dat"
                    },
                    {
                        "name": "Nguyen Van Vinh"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Van Vinh"
                },
                "author": "Nguyen Van Vinh"
            },
            {
                "id": "http://arxiv.org/abs/2512.17452v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17452v2",
                "title": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference"
                },
                "updated": "2025-12-22T10:23:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    23,
                    36,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17452v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV ."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T11:08:58Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    11,
                    8,
                    58,
                    4,
                    353,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yen-Chieh Huang"
                    },
                    {
                        "name": "Pi-Cheng Hsiu"
                    },
                    {
                        "name": "Rui Fang"
                    },
                    {
                        "name": "Ming-Syan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Syan Chen"
                },
                "author": "Ming-Syan Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.19240v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19240v1",
                "title": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models"
                },
                "updated": "2025-12-22T10:21:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    21,
                    40,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19240v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T10:21:40Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    21,
                    40,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mingxu Zhang"
                    },
                    {
                        "name": "Dazhong Shen"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Ying Sun"
                    }
                ],
                "author_detail": {
                    "name": "Ying Sun"
                },
                "author": "Ying Sun"
            },
            {
                "id": "http://arxiv.org/abs/2512.19238v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19238v1",
                "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation"
                },
                "updated": "2025-12-22T10:20:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    20,
                    20,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19238v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T10:20:20Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    20,
                    20,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Anna-Maria Gueorguieva"
                    },
                    {
                        "name": "Aylin Caliskan"
                    }
                ],
                "author_detail": {
                    "name": "Aylin Caliskan"
                },
                "author": "Aylin Caliskan"
            },
            {
                "id": "http://arxiv.org/abs/2509.06716v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.06716v2",
                "title": "Efficiently Ranking Software Variants with Minimal Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Ranking Software Variants with Minimal Benchmarks"
                },
                "updated": "2025-12-22T10:17:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    17,
                    59,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.06716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.06716v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Benchmarking is a common practice in software engineering to assess the qualities and performance of software variants, coming from multiple competing systems or from configurations of the same system. Benchmarks are used notably to compare and understand variant performance, fine-tune software, detect regressions, or design new software systems. The execution of benchmarks to get a complete picture of software variants is highly costly in terms of computational resources and time. In this paper, we propose a novel approach for reducing benchmarks while maintaining stable rankings, using test suite optimization techniques. That is, we remove instances from the benchmarks while trying to keep the same rankings of the variants on all tests. Our method, BISection Sampling, BISS, strategically retains the most critical tests and applies a novel divide-and-conquer approach to efficiently sample among relevant remaining tests. We experiment with datasets and use cases from LLM leaderboards, SAT competitions, and configurable systems for performance modeling. Our results show that our method outperforms baselines even when operating on a subset of variants. Using BISS, we reduce the computational cost of the benchmarks on average to 44% and on more than half the benchmarks by up to 99% without loss in ranking stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking is a common practice in software engineering to assess the qualities and performance of software variants, coming from multiple competing systems or from configurations of the same system. Benchmarks are used notably to compare and understand variant performance, fine-tune software, detect regressions, or design new software systems. The execution of benchmarks to get a complete picture of software variants is highly costly in terms of computational resources and time. In this paper, we propose a novel approach for reducing benchmarks while maintaining stable rankings, using test suite optimization techniques. That is, we remove instances from the benchmarks while trying to keep the same rankings of the variants on all tests. Our method, BISection Sampling, BISS, strategically retains the most critical tests and applies a novel divide-and-conquer approach to efficiently sample among relevant remaining tests. We experiment with datasets and use cases from LLM leaderboards, SAT competitions, and configurable systems for performance modeling. Our results show that our method outperforms baselines even when operating on a subset of variants. Using BISS, we reduce the computational cost of the benchmarks on average to 44% and on more than half the benchmarks by up to 99% without loss in ranking stability."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-08T14:11:35Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    11,
                    35,
                    0,
                    251,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Théo Matricon"
                    },
                    {
                        "name": "Mathieu Acher"
                    },
                    {
                        "name": "Helge Spieker"
                    },
                    {
                        "name": "Arnaud Gotlieb"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud Gotlieb"
                },
                "author": "Arnaud Gotlieb"
            },
            {
                "id": "http://arxiv.org/abs/2512.19234v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19234v1",
                "title": "DeliveryBench: Can Agents Earn Profit in Real World?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeliveryBench: Can Agents Earn Profit in Real World?"
                },
                "updated": "2025-12-22T10:17:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    17,
                    49,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19234v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T10:17:49Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    17,
                    49,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Lingjun Mao"
                    },
                    {
                        "name": "Jiawei Ren"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Jixuan Chen"
                    },
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Lianhui Qin"
                    }
                ],
                "author_detail": {
                    "name": "Lianhui Qin"
                },
                "author": "Lianhui Qin"
            },
            {
                "id": "http://arxiv.org/abs/2512.19228v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19228v1",
                "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models"
                },
                "updated": "2025-12-22T10:08:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    8,
                    25,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19228v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T10:08:25Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    8,
                    25,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Accepted at ICMLA 2025, the first two authors contributed equally",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Valentin Schmidberger"
                    },
                    {
                        "name": "Manuel Eberhardinger"
                    },
                    {
                        "name": "Setareh Maghsudi"
                    },
                    {
                        "name": "Johannes Maucher"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Maucher"
                },
                "author": "Johannes Maucher"
            },
            {
                "id": "http://arxiv.org/abs/2512.00086v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00086v2",
                "title": "Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs"
                },
                "updated": "2025-12-22T10:06:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    6,
                    8,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00086v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/JIOT.2025.3636826",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $μ$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $μ$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T09:46:09Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    9,
                    46,
                    9,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "14 pages, 9 figures, 3 tables. Associated open-source release available at: https://github.com/idsia-robotics/ultralow-power-monocular-depth-ondevice-learning",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Davide Nadalini"
                    },
                    {
                        "name": "Manuele Rusci"
                    },
                    {
                        "name": "Elia Cereda"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Francesco Conti"
                    },
                    {
                        "name": "Daniele Palossi"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Palossi"
                },
                "author": "Daniele Palossi",
                "arxiv_doi": "10.1109/JIOT.2025.3636826"
            },
            {
                "id": "http://arxiv.org/abs/2512.19210v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19210v1",
                "title": "Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation"
                },
                "updated": "2025-12-22T09:49:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    49,
                    13,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19210v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine \"understanding\" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine \"understanding\" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T09:49:13Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    49,
                    13,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS Workshop on Foundations of Reasoning in Language Models and Workshop on Bridging Language, Agent, and World Model",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jerry Wang"
                    },
                    {
                        "name": "Ting Yiu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Yiu Liu"
                },
                "author": "Ting Yiu Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.19206v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19206v1",
                "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning"
                },
                "updated": "2025-12-22T09:44:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    44,
                    26,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19206v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T09:44:26Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    44,
                    26,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen"
            },
            {
                "id": "http://arxiv.org/abs/2505.11792v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.11792v3",
                "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling"
                },
                "updated": "2025-12-22T09:39:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    39,
                    43,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.11792v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.11792v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimization modeling is fundamental to decision-making across diverse domains. Despite progress in automating optimization formulation from natural language descriptions, Large Language Models (LLMs) often struggle to generate formally correct and usable models against hallucinations, posing a challenge for reliable automation. Inspired by the success of Reinforcement Learning (RL) in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement Learning (SIRL), a novel framework that significantly improves the authenticity of LLMs for optimization modeling using Reinforcement Learning with Verifiable Reward by leveraging external optimization solvers as verifiers. These verifiers automatically assess the executable code and the instance-level mathematical model represented by the associated LP file, yielding precise and comprehensive feedback signals -- including syntax, feasibility, and solution quality, serving as direct rewards for the RL process. This automated verification process, particularly from classic optimization solvers, also underpins our instance-enhanced self-consistency method to synthesize high-quality training data. Extensive experiments on diverse public benchmarks demonstrate that SIRL achieves state-of-the-art performance, substantially outperforming existing methods in generating accurate and executable optimization models. Our code is publicly available at https://github.com/Cardinal-Operations/SIRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization modeling is fundamental to decision-making across diverse domains. Despite progress in automating optimization formulation from natural language descriptions, Large Language Models (LLMs) often struggle to generate formally correct and usable models against hallucinations, posing a challenge for reliable automation. Inspired by the success of Reinforcement Learning (RL) in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement Learning (SIRL), a novel framework that significantly improves the authenticity of LLMs for optimization modeling using Reinforcement Learning with Verifiable Reward by leveraging external optimization solvers as verifiers. These verifiers automatically assess the executable code and the instance-level mathematical model represented by the associated LP file, yielding precise and comprehensive feedback signals -- including syntax, feasibility, and solution quality, serving as direct rewards for the RL process. This automated verification process, particularly from classic optimization solvers, also underpins our instance-enhanced self-consistency method to synthesize high-quality training data. Extensive experiments on diverse public benchmarks demonstrate that SIRL achieves state-of-the-art performance, substantially outperforming existing methods in generating accurate and executable optimization models. Our code is publicly available at https://github.com/Cardinal-Operations/SIRL."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-17T02:32:03Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    2,
                    32,
                    3,
                    5,
                    137,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "arxiv_journal_ref": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "authors": [
                    {
                        "name": "Yitian Chen"
                    },
                    {
                        "name": "Jingfan Xia"
                    },
                    {
                        "name": "Siyu Shao"
                    },
                    {
                        "name": "Dongdong Ge"
                    },
                    {
                        "name": "Yinyu Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yinyu Ye"
                },
                "author": "Yinyu Ye"
            },
            {
                "id": "http://arxiv.org/abs/2512.17280v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17280v2",
                "title": "Sensor Management System (SMS): Open-source software for FAIR sensor metadata management in Earth system sciences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensor Management System (SMS): Open-source software for FAIR sensor metadata management in Earth system sciences"
                },
                "updated": "2025-12-22T09:29:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    29,
                    13,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17280v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deriving reliable conclusions and insights from environmental observational data urgently requires the enrichment with consistent and comprehensive metadata, including time-resolved context such as changing deployments, configurations, and maintenance actions. We have therefore developed the Sensor Management System (SMS), which provides a user-friendly and feature-rich platform for modeling even the most complex sensor systems and managing all sensor-related information across their life cycle. Each entity is described via well-defined terms like Devices, Platforms and Configurations, as well as Sites that are further enhanced with attributes for, e.g., instrument manufacturers, contact information or measured quantities and complemented by a continuous history of system-related actions. By further linking the SMS to sub-sequent systems and services like PID-registration or controlled vocabularies and establishing a community of end-users, the SMS provides the central element of a digital ecosystem, that fosters a more consistent, sustainable and FAIR provision of sensor-related metadata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving reliable conclusions and insights from environmental observational data urgently requires the enrichment with consistent and comprehensive metadata, including time-resolved context such as changing deployments, configurations, and maintenance actions. We have therefore developed the Sensor Management System (SMS), which provides a user-friendly and feature-rich platform for modeling even the most complex sensor systems and managing all sensor-related information across their life cycle. Each entity is described via well-defined terms like Devices, Platforms and Configurations, as well as Sites that are further enhanced with attributes for, e.g., instrument manufacturers, contact information or measured quantities and complemented by a continuous history of system-related actions. By further linking the SMS to sub-sequent systems and services like PID-registration or controlled vocabularies and establishing a community of end-users, the SMS provides the central element of a digital ecosystem, that fosters a more consistent, sustainable and FAIR provision of sensor-related metadata."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T06:55:44Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    6,
                    55,
                    44,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "Submitted to SoftwareX",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Christof Lorenz"
                    },
                    {
                        "name": "Nils Brinckmann"
                    },
                    {
                        "name": "Jan Bumberger"
                    },
                    {
                        "name": "Marc Hanisch"
                    },
                    {
                        "name": "Tobias Kuhnert"
                    },
                    {
                        "name": "Ulrich Loup"
                    },
                    {
                        "name": "Rubankumar Moorthy"
                    },
                    {
                        "name": "Florian Obsersteiner"
                    },
                    {
                        "name": "David Schäfer"
                    },
                    {
                        "name": "Thomas Schnicke"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Schnicke"
                },
                "author": "Thomas Schnicke"
            },
            {
                "id": "http://arxiv.org/abs/2512.19189v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19189v1",
                "title": "Configuration Work: Four Consequences of LLMs-in-use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configuration Work: Four Consequences of LLMs-in-use"
                },
                "updated": "2025-12-22T09:27:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    27,
                    5,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19189v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This article examines what it means to use Large Language Models in everyday work. Drawing on a seven-month longitudinal qualitative study, we argue that LLMs do not straightforwardly automate or augment tasks. We propose the concept of configuration work to describe the labor through which workers make a generic system usable for a specific professional task. Configuration work materializes in four intertwined consequences. First, workers must discretize their activity, breaking it into units that the system can process. Second, operating the system generates cluttering, as prompting, evaluating, and correcting responses add scattered layers of work that get in the way of existing routines. Third, users gradually attune their practices and expectations to the machine's generic rigidity, making sense of the system's limits and finding space for it within their practices. Fourth, as LLMs absorb repetitive tasks, they desaturate the texture of work, shifting activity toward logistical manipulation of outputs and away from forms of engagement that sustain a sense of accomplishment. Taken together, these consequences suggest that LLMs reshape work through the individualized labor required to configure a universal, task-agnostic system within situated professional ecologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article examines what it means to use Large Language Models in everyday work. Drawing on a seven-month longitudinal qualitative study, we argue that LLMs do not straightforwardly automate or augment tasks. We propose the concept of configuration work to describe the labor through which workers make a generic system usable for a specific professional task. Configuration work materializes in four intertwined consequences. First, workers must discretize their activity, breaking it into units that the system can process. Second, operating the system generates cluttering, as prompting, evaluating, and correcting responses add scattered layers of work that get in the way of existing routines. Third, users gradually attune their practices and expectations to the machine's generic rigidity, making sense of the system's limits and finding space for it within their practices. Fourth, as LLMs absorb repetitive tasks, they desaturate the texture of work, shifting activity toward logistical manipulation of outputs and away from forms of engagement that sustain a sense of accomplishment. Taken together, these consequences suggest that LLMs reshape work through the individualized labor required to configure a universal, task-agnostic system within situated professional ecologies."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T09:27:05Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    27,
                    5,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Gabriel Alcaras"
                    },
                    {
                        "name": "Donato Ricci"
                    }
                ],
                "author_detail": {
                    "name": "Donato Ricci"
                },
                "arxiv_affiliation": "médialab",
                "author": "Donato Ricci"
            },
            {
                "id": "http://arxiv.org/abs/2512.19183v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19183v1",
                "title": "Raman Spectroscopy of Salt Deposits from the Simulated Subsurface Ocean of Enceladus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Raman Spectroscopy of Salt Deposits from the Simulated Subsurface Ocean of Enceladus"
                },
                "updated": "2025-12-22T09:17:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    17,
                    45,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19183v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Saturn's ice-covered moon Enceladus may host a subsurface ocean with biologically relevant chemistry. Plumes released from this ocean preserve information on its chemical state, and previous analyses suggest weakly to strongly alkaline pH (approximately pH 8--12). Constraining the pH requires identification of pH-sensitive minerals in plume deposits. Several analytical techniques could provide such mineralogical information, but few are practical for deployment on planetary missions. Raman spectrometers, which have recently advanced for \\textit{in situ} exploration and have been incorporated into flight instruments, offer a feasible approach for mineral identification on icy moons. However, their applicability to pH estimation from plume-derived minerals has not been investigated. In this study, we evaluate whether Raman measurements of plume particles deposited on the surface of Enceladus can be used to distinguish between weakly and strongly alkaline subsurface ocean models. Fluids with pH values of 9 and 11 were frozen under vacuum conditions analogous to those on Enceladus. The resulting salt deposits were then analyzed using a flight-like Raman spectrometer. The Raman spectra show pH-dependent carbonate precipitation: NaHCO$_3$ and Na$_2$CO$_3$ peaks were detected at pH 9, whereas only Na$_2$CO$_3$ peaks were detected at pH 11. These findings demonstrate that Raman spectroscopy can distinguish pH-dependent carbonate phases. This capability allows us to constrain whether the pH of the subsurface ocean is weakly alkaline or strongly alkaline, which is a key parameter for assessing its chemical evolution and potential habitability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saturn's ice-covered moon Enceladus may host a subsurface ocean with biologically relevant chemistry. Plumes released from this ocean preserve information on its chemical state, and previous analyses suggest weakly to strongly alkaline pH (approximately pH 8--12). Constraining the pH requires identification of pH-sensitive minerals in plume deposits. Several analytical techniques could provide such mineralogical information, but few are practical for deployment on planetary missions. Raman spectrometers, which have recently advanced for \\textit{in situ} exploration and have been incorporated into flight instruments, offer a feasible approach for mineral identification on icy moons. However, their applicability to pH estimation from plume-derived minerals has not been investigated. In this study, we evaluate whether Raman measurements of plume particles deposited on the surface of Enceladus can be used to distinguish between weakly and strongly alkaline subsurface ocean models. Fluids with pH values of 9 and 11 were frozen under vacuum conditions analogous to those on Enceladus. The resulting salt deposits were then analyzed using a flight-like Raman spectrometer. The Raman spectra show pH-dependent carbonate precipitation: NaHCO$_3$ and Na$_2$CO$_3$ peaks were detected at pH 9, whereas only Na$_2$CO$_3$ peaks were detected at pH 11. These findings demonstrate that Raman spectroscopy can distinguish pH-dependent carbonate phases. This capability allows us to constrain whether the pH of the subsurface ocean is weakly alkaline or strongly alkaline, which is a key parameter for assessing its chemical evolution and potential habitability."
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T09:17:45Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    17,
                    45,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP"
                },
                "authors": [
                    {
                        "name": "Jun Takeshita"
                    },
                    {
                        "name": "Yuichiro Cho"
                    },
                    {
                        "name": "Haruhisa Tabata"
                    },
                    {
                        "name": "Yoshio Takahashi"
                    },
                    {
                        "name": "Daigo Shoji"
                    },
                    {
                        "name": "Seiji Sugita"
                    }
                ],
                "author_detail": {
                    "name": "Seiji Sugita"
                },
                "author": "Seiji Sugita"
            },
            {
                "id": "http://arxiv.org/abs/2512.19179v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19179v1",
                "title": "L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling"
                },
                "updated": "2025-12-22T09:13:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    13,
                    40,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19179v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present L4, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. L4 partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. L4 devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, L4 reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present L4, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. L4 partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. L4 devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, L4 reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T09:13:40Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    13,
                    40,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "15 pages, 16 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yitao Yuan"
                    },
                    {
                        "name": "Chenqi Zhao"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zane Cao"
                    },
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Wenfei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wenfei Wu"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Wenfei Wu"
            },
            {
                "id": "http://arxiv.org/abs/2512.19159v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19159v1",
                "title": "OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions"
                },
                "updated": "2025-12-22T08:55:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    55,
                    23,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19159v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T08:55:23Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    55,
                    23,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wendong Bu"
                    },
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Yuze Lin"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang"
            },
            {
                "id": "http://arxiv.org/abs/2504.09566v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.09566v3",
                "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution"
                },
                "updated": "2025-12-22T08:34:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    34,
                    18,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.09566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.09566v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\", \"Exactness\" and \"Minimality\", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\", \"Exactness\" and \"Minimality\", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-13T13:35:41Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    13,
                    35,
                    41,
                    6,
                    103,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chenghao Li"
                    },
                    {
                        "name": "Chaoning Zhang"
                    },
                    {
                        "name": "Yi Lu"
                    },
                    {
                        "name": "Jiaquan Zhang"
                    },
                    {
                        "name": "Qigan Sun"
                    },
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Jiwei Wei"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen"
            },
            {
                "id": "http://arxiv.org/abs/2512.19135v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19135v1",
                "title": "Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis"
                },
                "updated": "2025-12-22T08:28:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    28,
                    8,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19135v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T08:28:08Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    28,
                    8,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chenghao Li"
                    },
                    {
                        "name": "Chaoning Zhang"
                    },
                    {
                        "name": "Yi Lu"
                    },
                    {
                        "name": "Shuxu Chen"
                    },
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Jiaquan Zhang"
                    },
                    {
                        "name": "Zhicheng Wang"
                    },
                    {
                        "name": "Zhengxun Jin"
                    },
                    {
                        "name": "Kuien Liu"
                    },
                    {
                        "name": "Sung-Ho Bae"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Hen Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hen Tao Shen"
                },
                "author": "Hen Tao Shen"
            },
            {
                "id": "http://arxiv.org/abs/2512.19134v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19134v1",
                "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation"
                },
                "updated": "2025-12-22T08:28:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    28,
                    5,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19134v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T08:28:05Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    28,
                    5,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dehai Min"
                    },
                    {
                        "name": "Kailin Zhang"
                    },
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Lu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Cheng"
                },
                "author": "Lu Cheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.14806v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14806v4",
                "title": "Let the Barbarians In: How AI Can Accelerate Systems Performance Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Barbarians In: How AI Can Accelerate Systems Performance Research"
                },
                "updated": "2025-12-22T08:18:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    18,
                    12,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14806v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14806v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight.\n  Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight.\n  Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T18:51:23Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    18,
                    51,
                    23,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2510.06189",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Melissa Pan"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Mert Cemri"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Alexander Krentsel"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Jongseok Park"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jeff Chen"
                    },
                    {
                        "name": "Lakshya Agrawal"
                    },
                    {
                        "name": "Ashwin Naren"
                    },
                    {
                        "name": "Shulu Li"
                    },
                    {
                        "name": "Ruiying Ma"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2512.19126v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19126v1",
                "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards"
                },
                "updated": "2025-12-22T08:07:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    7,
                    0,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19126v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T08:07:00Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    7,
                    0,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zihan Lin"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Hexiong Yang"
                    },
                    {
                        "name": "Jiajun Chai"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He"
            },
            {
                "id": "http://arxiv.org/abs/2512.13142v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.13142v3",
                "title": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels"
                },
                "updated": "2025-12-22T07:53:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    53,
                    31,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.13142v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.13142v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (worries about judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (worries about judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T09:50:00Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    9,
                    50,
                    0,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Anika Sharma"
                    },
                    {
                        "name": "Malavika Mampally"
                    },
                    {
                        "name": "Chidaksh Ravuru"
                    },
                    {
                        "name": "Kandyce Brennan"
                    },
                    {
                        "name": "Neil Gaikwad"
                    }
                ],
                "author_detail": {
                    "name": "Neil Gaikwad"
                },
                "author": "Neil Gaikwad"
            },
            {
                "id": "http://arxiv.org/abs/2512.19122v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19122v1",
                "title": "BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation"
                },
                "updated": "2025-12-22T07:53:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    53,
                    16,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19122v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T07:53:16Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    53,
                    16,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Accepted at BLP Workshop @ IJCNLP-AACL 2025. Code is available at https://github.com/mahirlabibdihan/BanglaForge",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Sadif Ahmed"
                    },
                    {
                        "name": "Md Nafiu Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Md Nafiu Rahman"
                },
                "author": "Md Nafiu Rahman"
            },
            {
                "id": "http://arxiv.org/abs/2512.19117v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19117v1",
                "title": "Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?"
                },
                "updated": "2025-12-22T07:43:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    43,
                    43,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19117v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T07:43:43Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    43,
                    43,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "in French language",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Amar Lakel"
                    }
                ],
                "author_detail": {
                    "name": "Amar Lakel"
                },
                "arxiv_affiliation": "MICA",
                "author": "Amar Lakel"
            },
            {
                "id": "http://arxiv.org/abs/2512.19114v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19114v1",
                "title": "HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction"
                },
                "updated": "2025-12-22T07:35:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    35,
                    16,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19114v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T07:35:16Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    35,
                    16,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Haoyu Jiang"
                    },
                    {
                        "name": "Boan Qu"
                    },
                    {
                        "name": "Junjie Zhu"
                    },
                    {
                        "name": "Fanjie Zeng"
                    },
                    {
                        "name": "Xiaojie Lin"
                    },
                    {
                        "name": "Wei Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhong"
                },
                "author": "Wei Zhong"
            },
            {
                "id": "http://arxiv.org/abs/2508.08985v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.08985v3",
                "title": "Low-Regret and Low-Complexity Learning for Hierarchical Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Regret and Low-Complexity Learning for Hierarchical Inference"
                },
                "updated": "2025-12-22T07:34:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    34,
                    52,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.08985v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.08985v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work focuses on Hierarchical Inference (HI) in edge intelligence systems, where a compact Local-ML model on an end-device works in conjunction with a high-accuracy Remote-ML model on an edge-server. HI aims to reduce latency, improve accuracy, and lower bandwidth usage by first using the Local-ML model for inference and offloading to the Remote-ML only when the local inference is likely incorrect. A critical challenge in HI is estimating the likelihood of the local inference being incorrect, especially when data distributions and offloading costs change over time -- a problem we term Hierarchical Inference Learning (HIL). We introduce a novel approach to HIL by modeling the probability of correct inference by the Local-ML as an increasing function of the model's confidence measure, a structure motivated by empirical observations but previously unexploited. We propose two policies, HI-LCB and HI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We demonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a significant improvement over existing HIL policies with $O(T^{2/3})$ regret guarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational complexity, making it well-suited for deployment on devices with severe resource limitations. Simulations using real-world datasets confirm that our policies outperform existing state-of-the-art HIL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on Hierarchical Inference (HI) in edge intelligence systems, where a compact Local-ML model on an end-device works in conjunction with a high-accuracy Remote-ML model on an edge-server. HI aims to reduce latency, improve accuracy, and lower bandwidth usage by first using the Local-ML model for inference and offloading to the Remote-ML only when the local inference is likely incorrect. A critical challenge in HI is estimating the likelihood of the local inference being incorrect, especially when data distributions and offloading costs change over time -- a problem we term Hierarchical Inference Learning (HIL). We introduce a novel approach to HIL by modeling the probability of correct inference by the Local-ML as an increasing function of the model's confidence measure, a structure motivated by empirical observations but previously unexploited. We propose two policies, HI-LCB and HI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We demonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a significant improvement over existing HIL policies with $O(T^{2/3})$ regret guarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational complexity, making it well-suited for deployment on devices with severe resource limitations. Simulations using real-world datasets confirm that our policies outperform existing state-of-the-art HIL methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-12T14:53:54Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    53,
                    54,
                    1,
                    224,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sameep Chattopadhyay"
                    },
                    {
                        "name": "Vinay Sutar"
                    },
                    {
                        "name": "Jaya Prakash Champati"
                    },
                    {
                        "name": "Sharayu Moharir"
                    }
                ],
                "author_detail": {
                    "name": "Sharayu Moharir"
                },
                "author": "Sharayu Moharir"
            },
            {
                "id": "http://arxiv.org/abs/2512.19107v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19107v1",
                "title": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning"
                },
                "updated": "2025-12-22T07:21:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    21,
                    7,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19107v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and \"surprising\" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and \"surprising\" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T07:21:07Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    21,
                    7,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Xiaoshuang Sheng"
                    },
                    {
                        "name": "Zhengnan Zhang"
                    },
                    {
                        "name": "Jidong Wu"
                    },
                    {
                        "name": "Zexing Wang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shenghua Xu"
                    },
                    {
                        "name": "Guanjing Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Guanjing Xiong"
                },
                "author": "Guanjing Xiong"
            },
            {
                "id": "http://arxiv.org/abs/2509.25300v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.25300v3",
                "title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning"
                },
                "updated": "2025-12-22T07:20:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    20,
                    59,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.25300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.25300v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: 1. Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2. The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3. Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4. In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: 1. Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2. The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3. Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4. In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T17:10:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    10,
                    35,
                    0,
                    272,
                    0
                ],
                "arxiv_comment": "V3 version:27 pages, 14 figures, add code and dataset url",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zelin Tan"
                    },
                    {
                        "name": "Hejia Geng"
                    },
                    {
                        "name": "Xiaohang Yu"
                    },
                    {
                        "name": "Mulei Zhang"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Qiang He"
                    },
                    {
                        "name": "Xiangyuan Xue"
                    },
                    {
                        "name": "Heng Zhou"
                    },
                    {
                        "name": "Yutao Fan"
                    },
                    {
                        "name": "Zhongzhi Li"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai"
            },
            {
                "id": "http://arxiv.org/abs/2512.19092v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19092v1",
                "title": "A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs"
                },
                "updated": "2025-12-22T07:01:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    1,
                    5,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19092v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reasoning over knowledge graphs (KGs) with first-order logic (FOL) queries is challenging due to the inherent incompleteness of real-world KGs and the compositional complexity of logical query structures. Most existing methods rely on embedding entities and relations into continuous geometric spaces and answer queries via differentiable set operations. While effective for simple query patterns, these approaches often struggle to generalize to complex queries involving multiple operators, deeper reasoning chains, or heterogeneous KG schemas. We propose ROG (Reasoning Over knowledge Graphs with large language models), an ensemble-style framework that combines query-aware KG neighborhood retrieval with large language model (LLM)-based chain-of-thought reasoning. ROG decomposes complex FOL queries into sequences of simpler sub-queries, retrieves compact, query-relevant subgraphs as contextual evidence, and performs step-by-step logical inference using an LLM, avoiding the need for task-specific embedding optimization. Experiments on standard KG reasoning benchmarks demonstrate that ROG consistently outperforms strong embedding-based baselines in terms of mean reciprocal rank (MRR), with particularly notable gains on high-complexity query types. These results suggest that integrating structured KG retrieval with LLM-driven logical reasoning offers a robust and effective alternative for complex KG reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over knowledge graphs (KGs) with first-order logic (FOL) queries is challenging due to the inherent incompleteness of real-world KGs and the compositional complexity of logical query structures. Most existing methods rely on embedding entities and relations into continuous geometric spaces and answer queries via differentiable set operations. While effective for simple query patterns, these approaches often struggle to generalize to complex queries involving multiple operators, deeper reasoning chains, or heterogeneous KG schemas. We propose ROG (Reasoning Over knowledge Graphs with large language models), an ensemble-style framework that combines query-aware KG neighborhood retrieval with large language model (LLM)-based chain-of-thought reasoning. ROG decomposes complex FOL queries into sequences of simpler sub-queries, retrieves compact, query-relevant subgraphs as contextual evidence, and performs step-by-step logical inference using an LLM, avoiding the need for task-specific embedding optimization. Experiments on standard KG reasoning benchmarks demonstrate that ROG consistently outperforms strong embedding-based baselines in terms of mean reciprocal rank (MRR), with particularly notable gains on high-complexity query types. These results suggest that integrating structured KG retrieval with LLM-driven logical reasoning offers a robust and effective alternative for complex KG reasoning tasks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T07:01:05Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    7,
                    1,
                    5,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyan Zhang"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Chiyi Li"
                    },
                    {
                        "name": "Kai Song"
                    }
                ],
                "author_detail": {
                    "name": "Kai Song"
                },
                "author": "Kai Song"
            },
            {
                "id": "http://arxiv.org/abs/2512.19088v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19088v1",
                "title": "Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation"
                },
                "updated": "2025-12-22T06:57:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    6,
                    57,
                    42,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19088v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T06:57:42Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    6,
                    57,
                    42,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026 Workshop on New Frontiers in Information Retrieval",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Khanh Nguyen"
                    },
                    {
                        "name": "Dasith de Silva Edirimuni"
                    },
                    {
                        "name": "Ghulam Mubashar Hassan"
                    },
                    {
                        "name": "Ajmal Mian"
                    }
                ],
                "author_detail": {
                    "name": "Ajmal Mian"
                },
                "author": "Ajmal Mian"
            },
            {
                "id": "http://arxiv.org/abs/2512.19084v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19084v1",
                "title": "$γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics"
                },
                "updated": "2025-12-22T06:48:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    6,
                    48,
                    53,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19084v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $γ(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $γ(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T06:48:53Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    6,
                    48,
                    53,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Mark Burgess"
                    }
                ],
                "author_detail": {
                    "name": "Mark Burgess"
                },
                "author": "Mark Burgess"
            },
            {
                "id": "http://arxiv.org/abs/2512.19081v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19081v1",
                "title": "Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning"
                },
                "updated": "2025-12-22T06:42:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    6,
                    42,
                    46,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19081v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T06:42:46Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    6,
                    42,
                    46,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Yitong Duan"
                    },
                    {
                        "name": "Zhaoxi Zhang"
                    },
                    {
                        "name": "Jiyan He"
                    },
                    {
                        "name": "Shuxin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Shuxin Zheng"
                },
                "author": "Shuxin Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.19075v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19075v1",
                "title": "Optimal 3D Directional WPT Charging via UAV for 3D Wireless Rechargeable Sensor Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal 3D Directional WPT Charging via UAV for 3D Wireless Rechargeable Sensor Networks"
                },
                "updated": "2025-12-22T06:36:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    6,
                    36,
                    37,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19075v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The high mobility and flexible deployment capability of UAVs make them an impressive option for charging nodes in Wireless Rechargeable Sensor Networks (WRSNs) using Directional Wireless Power Transfer (WPT) technology. However, existing studies largely focus on 2D-WRSNs, lacking designs catering to real 3D-WRSNs. The spatial distribution characteristics of nodes in a 3D-WRSN further increase the complexity of the charging scheduling task, thus requiring a systematic framework to solve this problem. In this paper, we investigated the Directional UAV Charging Scheduling problem for 3D-WRSNs (DCS-3D) and established its NP-hard property, and then proposed a three-step framework named as directional charging scheduling algorithm using Functional Equivalent (FuncEqv) direction set and Lin-Kernighan heuristic (LKH) for 3D-WRSNs (FELKH-3D) to solve it. In FELKH-3D, the challenge of infinite charging direction space is solved by designing an algorithm generating a minimum-size direction set guaranteed to be FuncEqv to the infinite set of whole sphere surface, and the optimaility of the method was proved.To determine the optimal charging tour for the UAV, the LKH algorithm is employed.Simulation experiments demonstrated the superiority of FELKH-3D over other classical algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high mobility and flexible deployment capability of UAVs make them an impressive option for charging nodes in Wireless Rechargeable Sensor Networks (WRSNs) using Directional Wireless Power Transfer (WPT) technology. However, existing studies largely focus on 2D-WRSNs, lacking designs catering to real 3D-WRSNs. The spatial distribution characteristics of nodes in a 3D-WRSN further increase the complexity of the charging scheduling task, thus requiring a systematic framework to solve this problem. In this paper, we investigated the Directional UAV Charging Scheduling problem for 3D-WRSNs (DCS-3D) and established its NP-hard property, and then proposed a three-step framework named as directional charging scheduling algorithm using Functional Equivalent (FuncEqv) direction set and Lin-Kernighan heuristic (LKH) for 3D-WRSNs (FELKH-3D) to solve it. In FELKH-3D, the challenge of infinite charging direction space is solved by designing an algorithm generating a minimum-size direction set guaranteed to be FuncEqv to the infinite set of whole sphere surface, and the optimaility of the method was proved.To determine the optimal charging tour for the UAV, the LKH algorithm is employed.Simulation experiments demonstrated the superiority of FELKH-3D over other classical algorithms."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T06:36:37Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    6,
                    36,
                    37,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Zhenguo Gao"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Yiqin Chen"
                    },
                    {
                        "name": "Qingyu Gao"
                    },
                    {
                        "name": "Zhufang Kuang"
                    },
                    {
                        "name": "Shih-Hau Fang"
                    },
                    {
                        "name": "Hsiao-Chun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hsiao-Chun Wu"
                },
                "author": "Hsiao-Chun Wu"
            },
            {
                "id": "http://arxiv.org/abs/2512.19069v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19069v1",
                "title": "Can abstract concepts from LLM improve SLM performance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can abstract concepts from LLM improve SLM performance?"
                },
                "updated": "2025-12-22T06:17:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    6,
                    17,
                    25,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19069v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\\% of accuracy improvement for Qwen3-0.6B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\\% of accuracy improvement for Qwen3-0.6B."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T06:17:25Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    6,
                    17,
                    25,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Siddharth Tandon"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Tandon"
                },
                "author": "Siddharth Tandon"
            },
            {
                "id": "http://arxiv.org/abs/2512.16963v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16963v2",
                "title": "Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models"
                },
                "updated": "2025-12-22T05:58:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    5,
                    58,
                    51,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16963v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.\n  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: Compression is Routing. We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a 64x sequence length compression (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of 99.47% on the in-domain (code) validation set; accuracy drops sharply to 47.76% on a semi-out-of-distribution domain (Wiki text); and further plummets to just 0.57% on a fully out-of-distribution domain (random sequences).\n  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an Intrinsic Distribution Fingerprint. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.\n  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: Compression is Routing. We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a 64x sequence length compression (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of 99.47% on the in-domain (code) validation set; accuracy drops sharply to 47.76% on a semi-out-of-distribution domain (Wiki text); and further plummets to just 0.57% on a fully out-of-distribution domain (random sequences).\n  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an Intrinsic Distribution Fingerprint. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T09:02:03Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    9,
                    2,
                    3,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang"
            },
            {
                "id": "http://arxiv.org/abs/2503.11185v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.11185v2",
                "title": "Bleeding Pathways: Vanishing Discriminability in LLM Hidden States Fuels Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bleeding Pathways: Vanishing Discriminability in LLM Hidden States Fuels Jailbreak Attacks"
                },
                "updated": "2025-12-22T05:32:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    5,
                    32,
                    2,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.11185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.11185v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLMs remain vulnerable to jailbreak attacks that exploit adversarial prompts to circumvent safety measures. Current safety fine-tuning approaches face two critical limitations. First, they often fail to strike a balance between security and utility, where stronger safety measures tend to over-reject harmless user requests. Second, they frequently miss malicious intent concealed within seemingly benign tasks, leaving models exposed to exploitation. Our work identifies a fundamental cause of these issues: during response generation, an LLM's capacity to differentiate harmful from safe outputs deteriorates. Experimental evidence confirms this, revealing that the separability between hidden states for safe and harmful responses diminishes as generation progresses. This weakening discrimination forces models to make compliance judgments earlier in the generation process, restricting their ability to recognize developing harmful intent and contributing to both aforementioned failures. To mitigate this vulnerability, we introduce DEEPALIGN - an inherent defense framework that enhances the safety of LLMs. By applying contrastive hidden-state steering at the midpoint of response generation, DEEPALIGN amplifies the separation between harmful and benign hidden states, enabling continuous intrinsic toxicity detection and intervention throughout the generation process. Across diverse LLMs spanning varying architectures and scales, it reduced attack success rates of nine distinct jailbreak attacks to near-zero or minimal. Crucially, it preserved model capability while reducing over-refusal. Models equipped with DEEPALIGN exhibited up to 3.5% lower error rates in rejecting challenging benign queries and maintained standard task performance with less than 1% decline. This marks a substantial advance in the safety-utility Pareto frontier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs remain vulnerable to jailbreak attacks that exploit adversarial prompts to circumvent safety measures. Current safety fine-tuning approaches face two critical limitations. First, they often fail to strike a balance between security and utility, where stronger safety measures tend to over-reject harmless user requests. Second, they frequently miss malicious intent concealed within seemingly benign tasks, leaving models exposed to exploitation. Our work identifies a fundamental cause of these issues: during response generation, an LLM's capacity to differentiate harmful from safe outputs deteriorates. Experimental evidence confirms this, revealing that the separability between hidden states for safe and harmful responses diminishes as generation progresses. This weakening discrimination forces models to make compliance judgments earlier in the generation process, restricting their ability to recognize developing harmful intent and contributing to both aforementioned failures. To mitigate this vulnerability, we introduce DEEPALIGN - an inherent defense framework that enhances the safety of LLMs. By applying contrastive hidden-state steering at the midpoint of response generation, DEEPALIGN amplifies the separation between harmful and benign hidden states, enabling continuous intrinsic toxicity detection and intervention throughout the generation process. Across diverse LLMs spanning varying architectures and scales, it reduced attack success rates of nine distinct jailbreak attacks to near-zero or minimal. Crucially, it preserved model capability while reducing over-refusal. Models equipped with DEEPALIGN exhibited up to 3.5% lower error rates in rejecting challenging benign queries and maintained standard task performance with less than 1% decline. This marks a substantial advance in the safety-utility Pareto frontier."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-14T08:32:12Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    32,
                    12,
                    4,
                    73,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Yingjie Zhang"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Guozhu Meng"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen"
            }
        ]
    }
]